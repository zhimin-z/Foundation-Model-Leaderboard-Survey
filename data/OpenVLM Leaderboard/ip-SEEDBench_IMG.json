[
    {
        "Model":"Qwen-VL",
        "Parameters":"9.6B",
        "Language Model":"Qwen-7B",
        "Vision Model":"ViT-G\/16",
        "Org":"Alibaba Group",
        "Time":"2023\/8\/24",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":52.5,
        "Instance Attributes":56.4,
        "Instance Identity":56.3,
        "Instance Interaction":60.8,
        "Instance Location":47.5,
        "Instances Counting":32.3,
        "Scene Understanding":62.7,
        "Spatial Relation":40.5,
        "Text Understanding":50.0,
        "Visual Reasoning":66.2,
        "Overall (prefetch)":52.7,
        "Overall (official)":"62.3 (PPL)"
    },
    {
        "Model":"Qwen-VL-Chat",
        "Parameters":"9.6B",
        "Language Model":"Qwen-7B",
        "Vision Model":"ViT-G\/16",
        "Org":"Alibaba Group",
        "Time":"2023\/8\/24",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":64.8,
        "Instance Attributes":68.7,
        "Instance Identity":67.8,
        "Instance Interaction":61.9,
        "Instance Location":55.5,
        "Instances Counting":52.9,
        "Scene Understanding":73.2,
        "Spatial Relation":44.9,
        "Text Understanding":34.5,
        "Visual Reasoning":77.3,
        "Overall (prefetch)":66.6,
        "Overall (official)":"65.4 (PPL)"
    },
    {
        "Model":"PandaGPT-13B",
        "Parameters":"14B",
        "Language Model":"Vicuna 13B",
        "Vision Model":"ImageBind ViT-H\/14",
        "Org":"Tencent AI Lab",
        "Time":"2023\/5\/25",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":47.6,
        "Instance Attributes":45.1,
        "Instance Identity":49.5,
        "Instance Interaction":55.7,
        "Instance Location":42.1,
        "Instances Counting":34.7,
        "Scene Understanding":62.2,
        "Spatial Relation":38.5,
        "Text Understanding":38.1,
        "Visual Reasoning":63.1,
        "Overall (prefetch)":48.4,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"OpenFlamingo v2",
        "Parameters":"9B",
        "Language Model":"MPT 7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Org":"The University of Washington",
        "Time":"2023\/6\/28",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":28.8,
        "Instance Attributes":29.1,
        "Instance Identity":29.1,
        "Instance Interaction":34.0,
        "Instance Location":30.4,
        "Instances Counting":25.5,
        "Scene Understanding":30.3,
        "Spatial Relation":28.8,
        "Text Understanding":20.2,
        "Visual Reasoning":30.8,
        "Overall (prefetch)":28.8,
        "Overall (official)":"42.7 (PPL)"
    },
    {
        "Model":"IDEFICS-9B-Instruct",
        "Parameters":"9B",
        "Language Model":"LLaMA 7B",
        "Vision Model":"CLIP ViT-H\/14",
        "Org":"Hugging Face",
        "Time":"2023\/7\/25",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":45.0,
        "Instance Attributes":42.8,
        "Instance Identity":46.0,
        "Instance Interaction":51.5,
        "Instance Location":40.8,
        "Instances Counting":31.5,
        "Scene Understanding":59.4,
        "Spatial Relation":37.1,
        "Text Understanding":48.8,
        "Visual Reasoning":57.7,
        "Overall (prefetch)":45.0,
        "Overall (official)":"44.5 (Not Given)"
    },
    {
        "Model":"IDEFICS-80B-Instruct",
        "Parameters":"80B",
        "Language Model":"LLaMA65B",
        "Vision Model":"CLIP ViT-H\/14",
        "Org":"Hugging Face",
        "Time":"2023\/7\/25",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":52.0,
        "Instance Attributes":48.4,
        "Instance Identity":54.5,
        "Instance Interaction":59.8,
        "Instance Location":47.0,
        "Instances Counting":40.1,
        "Scene Understanding":65.6,
        "Spatial Relation":44.7,
        "Text Understanding":53.6,
        "Visual Reasoning":71.9,
        "Overall (prefetch)":52.0,
        "Overall (official)":"53.2 (Not Given)"
    },
    {
        "Model":"LLaVA-v1.5-7B",
        "Parameters":"7.2B",
        "Language Model":"Vicuna-v1.5-7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Org":"University of Wisconsin\u2013Madison",
        "Time":"2023\/10\/05",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":65.8,
        "Instance Attributes":67.0,
        "Instance Identity":68.5,
        "Instance Interaction":70.1,
        "Instance Location":60.0,
        "Instances Counting":58.0,
        "Scene Understanding":73.1,
        "Spatial Relation":50.4,
        "Text Understanding":36.9,
        "Visual Reasoning":76.7,
        "Overall (prefetch)":65.8,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"ShareGPT4V-7B",
        "Parameters":"7.2B",
        "Language Model":"Vicuna-v1.5-7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Org":"Shanghai AI Laboratory",
        "Time":"2023\/12\/08",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":69.3,
        "Instance Attributes":71.8,
        "Instance Identity":71.1,
        "Instance Interaction":72.2,
        "Instance Location":63.3,
        "Instances Counting":61.5,
        "Scene Understanding":75.2,
        "Spatial Relation":53.4,
        "Text Understanding":56.0,
        "Visual Reasoning":78.5,
        "Overall (prefetch)":69.3,
        "Overall (official)":"69.7 (Gen)"
    },
    {
        "Model":"ShareGPT4V-13B",
        "Parameters":"13.4B",
        "Language Model":"Vicuna-v1.5-13B",
        "Vision Model":"CLIP ViT-L\/14",
        "Org":"Shanghai AI Laboratory",
        "Time":"2023\/12\/08",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":70.6,
        "Instance Attributes":73.1,
        "Instance Identity":74.2,
        "Instance Interaction":75.3,
        "Instance Location":66.5,
        "Instances Counting":62.3,
        "Scene Understanding":75.5,
        "Spatial Relation":54.8,
        "Text Understanding":48.8,
        "Visual Reasoning":76.4,
        "Overall (prefetch)":70.6,
        "Overall (official)":"70.8 (Gen)"
    },
    {
        "Model":"LLaVA-v1.5-13B",
        "Parameters":"13.4B",
        "Language Model":"Vicuna-v1.5-13B",
        "Vision Model":"CLIP ViT-L\/14",
        "Org":"University of Wisconsin\u2013Madison",
        "Time":"2023\/10\/05",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":68.2,
        "Instance Attributes":68.7,
        "Instance Identity":71.9,
        "Instance Interaction":74.2,
        "Instance Location":64.1,
        "Instances Counting":61.1,
        "Scene Understanding":75.2,
        "Spatial Relation":51.3,
        "Text Understanding":54.8,
        "Visual Reasoning":75.5,
        "Overall (prefetch)":68.2,
        "Overall (official)":"68.2 (Gen)"
    },
    {
        "Model":"LLaVA-v1-7B",
        "Parameters":"7.2B",
        "Language Model":"LLaMA 7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Org":"University of Wisconsin-Madison",
        "Time":"2023\/4\/17",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":50.4,
        "Instance Attributes":49.4,
        "Instance Identity":51.8,
        "Instance Interaction":50.5,
        "Instance Location":37.9,
        "Instances Counting":41.4,
        "Scene Understanding":63.5,
        "Spatial Relation":36.5,
        "Text Understanding":27.4,
        "Visual Reasoning":68.9,
        "Overall (prefetch)":42.5,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"TransCore-M",
        "Parameters":"13.4B",
        "Language Model":"PCITransGPT-13B",
        "Vision Model":"CLIP ViT\/L-14",
        "Org":"PCI Research",
        "Time":"2023\/11\/30",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":71.2,
        "Instance Attributes":73.2,
        "Instance Identity":73.2,
        "Instance Interaction":75.3,
        "Instance Location":66.8,
        "Instances Counting":64.0,
        "Scene Understanding":76.5,
        "Spatial Relation":57.1,
        "Text Understanding":59.5,
        "Visual Reasoning":77.6,
        "Overall (prefetch)":71.2,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"InstructBLIP-7B",
        "Parameters":"8B",
        "Language Model":"Vicuna 7B",
        "Vision Model":"EVA-G",
        "Org":"Salesforce Research",
        "Time":"2023\/5\/11",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":44.5,
        "Instance Attributes":43.5,
        "Instance Identity":52.3,
        "Instance Interaction":52.6,
        "Instance Location":42.2,
        "Instances Counting":29.0,
        "Scene Understanding":54.3,
        "Spatial Relation":37.3,
        "Text Understanding":29.8,
        "Visual Reasoning":58.9,
        "Overall (prefetch)":49.6,
        "Overall (official)":"58.8 (PPL)"
    },
    {
        "Model":"VisualGLM",
        "Parameters":"8B",
        "Language Model":"ChatGLM 6B",
        "Vision Model":"EVA-CLIP",
        "Org":"Tsinghua University",
        "Time":"2023\/5\/17",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":47.0,
        "Instance Attributes":46.7,
        "Instance Identity":49.9,
        "Instance Interaction":57.7,
        "Instance Location":36.3,
        "Instances Counting":32.2,
        "Scene Understanding":62.3,
        "Spatial Relation":33.3,
        "Text Understanding":31.0,
        "Visual Reasoning":59.8,
        "Overall (prefetch)":47.7,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"InternLM-XComposer-VL",
        "Parameters":"8B",
        "Language Model":"InternLM-7B",
        "Vision Model":"EVA-G",
        "Org":"Shanghai AI Laboratory",
        "Time":"2023\/9\/26",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":66.1,
        "Instance Attributes":67.2,
        "Instance Identity":70.7,
        "Instance Interaction":77.3,
        "Instance Location":60.2,
        "Instances Counting":53.7,
        "Scene Understanding":74.8,
        "Spatial Relation":53.3,
        "Text Understanding":42.9,
        "Visual Reasoning":76.7,
        "Overall (prefetch)":66.1,
        "Overall (official)":"66.9 (PPL)"
    },
    {
        "Model":"mPLUG-Owl2",
        "Parameters":"8.2B",
        "Language Model":"LLaMA2 7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Org":"DAMO Academy",
        "Time":"2023\/11\/10",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":64.5,
        "Instance Attributes":63.8,
        "Instance Identity":68.4,
        "Instance Interaction":69.1,
        "Instance Location":55.5,
        "Instances Counting":58.9,
        "Scene Understanding":72.8,
        "Spatial Relation":50.7,
        "Text Understanding":32.1,
        "Visual Reasoning":76.4,
        "Overall (prefetch)":64.5,
        "Overall (official)":"64.1 (Not Given)"
    },
    {
        "Model":"GeminiProVision",
        "Parameters":"",
        "Language Model":"",
        "Vision Model":"",
        "Org":"Google",
        "Time":"2023\/12\/23",
        "Verified":"Yes",
        "OpenSource":"No",
        "Overall":70.7,
        "Instance Attributes":70.7,
        "Instance Identity":75.8,
        "Instance Interaction":76.3,
        "Instance Location":64.6,
        "Instances Counting":64.4,
        "Scene Understanding":77.8,
        "Spatial Relation":54.2,
        "Text Understanding":35.7,
        "Visual Reasoning":80.4,
        "Overall (prefetch)":71.1,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"LLaVA-InternLM-7B (QLoRA)",
        "Parameters":"7.6B",
        "Language Model":"InternLM-7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Org":"Shanghai AI Laboratory",
        "Time":"2023\/12\/28",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":65.7,
        "Instance Attributes":67.5,
        "Instance Identity":69.6,
        "Instance Interaction":73.2,
        "Instance Location":58.0,
        "Instances Counting":57.4,
        "Scene Understanding":73.8,
        "Spatial Relation":45.7,
        "Text Understanding":27.4,
        "Visual Reasoning":74.9,
        "Overall (prefetch)":65.7,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"LLaVA-v1.5-7B (QLoRA)",
        "Parameters":"7.2B",
        "Language Model":"Vicuna-v1.5-7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Org":"Shanghai AI Laboratory",
        "Time":"2023\/12\/28",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":66.4,
        "Instance Attributes":66.5,
        "Instance Identity":70.0,
        "Instance Interaction":71.1,
        "Instance Location":60.9,
        "Instances Counting":59.5,
        "Scene Understanding":74.6,
        "Spatial Relation":48.1,
        "Text Understanding":44.0,
        "Visual Reasoning":74.3,
        "Overall (prefetch)":66.4,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"LLaVA-v1.5-13B (QLoRA)",
        "Parameters":"13.4B",
        "Language Model":"Vicuna-v1.5-13B",
        "Vision Model":"CLIP ViT-L\/14",
        "Org":"Shanghai AI Laboratory",
        "Time":"2023\/12\/28",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":67.9,
        "Instance Attributes":69.2,
        "Instance Identity":71.9,
        "Instance Interaction":72.2,
        "Instance Location":63.5,
        "Instances Counting":59.3,
        "Scene Understanding":75.7,
        "Spatial Relation":50.2,
        "Text Understanding":19.0,
        "Visual Reasoning":78.2,
        "Overall (prefetch)":67.9,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"CogVLM-17B-Chat",
        "Parameters":"17B",
        "Language Model":"Vicuna-v1.5-7B",
        "Vision Model":"EVA2-CLIP-E",
        "Org":"Zhipu AI",
        "Time":"2024\/01\/03",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":68.8,
        "Instance Attributes":73.2,
        "Instance Identity":73.5,
        "Instance Interaction":77.3,
        "Instance Location":57.7,
        "Instances Counting":55.5,
        "Scene Understanding":76.8,
        "Spatial Relation":47.6,
        "Text Understanding":58.3,
        "Visual Reasoning":77.0,
        "Overall (prefetch)":68.8,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"SharedCaptioner",
        "Parameters":"8B",
        "Language Model":"InternLM-7B",
        "Vision Model":"EVA-G",
        "Org":"Shanghai AI Laboratory",
        "Time":"2024\/01\/03",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":61.2,
        "Instance Attributes":69.1,
        "Instance Identity":63.9,
        "Instance Interaction":73.2,
        "Instance Location":55.9,
        "Instances Counting":50.3,
        "Scene Understanding":60.2,
        "Spatial Relation":51.8,
        "Text Understanding":47.6,
        "Visual Reasoning":60.4,
        "Overall (prefetch)":64.7,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"Qwen-VL-Plus",
        "Parameters":"",
        "Language Model":"QwenLM",
        "Vision Model":"",
        "Org":"Alibaba Group",
        "Time":"2024\/01\/10",
        "Verified":"Yes",
        "OpenSource":"No",
        "Overall":65.7,
        "Instance Attributes":64.3,
        "Instance Identity":72.0,
        "Instance Interaction":66.0,
        "Instance Location":57.1,
        "Instances Counting":63.8,
        "Scene Understanding":72.4,
        "Spatial Relation":47.9,
        "Text Understanding":23.8,
        "Visual Reasoning":74.0,
        "Overall (prefetch)":71.2,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"Monkey",
        "Parameters":"9.8B",
        "Language Model":"Qwen-7B",
        "Vision Model":"ViT-BigHuge",
        "Org":"Huazhong University of Science and Technology",
        "Time":"2024\/01\/11",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":64.3,
        "Instance Attributes":70.2,
        "Instance Identity":67.0,
        "Instance Interaction":64.9,
        "Instance Location":56.9,
        "Instances Counting":46.4,
        "Scene Understanding":72.8,
        "Spatial Relation":48.7,
        "Text Understanding":56.0,
        "Visual Reasoning":73.4,
        "Overall (prefetch)":64.3,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"Monkey-Chat",
        "Parameters":"9.8B",
        "Language Model":"Qwen-7B",
        "Vision Model":"ViT-BigHuge",
        "Org":"Huazhong University of Science and Technology",
        "Time":"2024\/01\/16",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":68.9,
        "Instance Attributes":72.7,
        "Instance Identity":70.9,
        "Instance Interaction":73.2,
        "Instance Location":63.7,
        "Instances Counting":57.0,
        "Scene Understanding":75.6,
        "Spatial Relation":51.8,
        "Text Understanding":66.7,
        "Visual Reasoning":76.7,
        "Overall (prefetch)":68.9,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"MiniGPT-4-v2",
        "Parameters":"8B",
        "Language Model":"LLaMA2 13B",
        "Vision Model":"EVA-G",
        "Org":"King Abdullah University of Science and Technology",
        "Time":"2023\/10\/27",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":29.4,
        "Instance Attributes":30.3,
        "Instance Identity":32.8,
        "Instance Interaction":25.8,
        "Instance Location":26.3,
        "Instances Counting":18.5,
        "Scene Understanding":35.3,
        "Spatial Relation":27.1,
        "Text Understanding":31.0,
        "Visual Reasoning":35.6,
        "Overall (prefetch)":31.8,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"MiniGPT-4-v1-7B",
        "Parameters":"8B",
        "Language Model":"Vicuna 7B",
        "Vision Model":"EVA-G",
        "Org":"King Abdullah University of Science and Technology",
        "Time":"2023\/4\/20",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":31.6,
        "Instance Attributes":31.9,
        "Instance Identity":34.0,
        "Instance Interaction":32.0,
        "Instance Location":27.4,
        "Instances Counting":25.6,
        "Scene Understanding":37.0,
        "Spatial Relation":23.9,
        "Text Understanding":25.0,
        "Visual Reasoning":34.1,
        "Overall (prefetch)":33.6,
        "Overall (official)":"47.4 (PPL)"
    },
    {
        "Model":"Emu2_chat",
        "Parameters":"37B",
        "Language Model":"LLaMA-33B",
        "Vision Model":"EVA-02-CLIP-E-plus",
        "Org":"Beijing Academy of Artificial Intelligence",
        "Time":"2024\/01\/17",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":68.9,
        "Instance Attributes":70.8,
        "Instance Identity":76.1,
        "Instance Interaction":73.2,
        "Instance Location":61.0,
        "Instances Counting":58.1,
        "Scene Understanding":76.7,
        "Spatial Relation":47.2,
        "Text Understanding":69.0,
        "Visual Reasoning":72.2,
        "Overall (prefetch)":69.1,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"LLaVA-InternLM2-7B (QLoRA)",
        "Parameters":"8.1B",
        "Language Model":"InternLM2-7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Org":"Shanghai AI Laboratory",
        "Time":"2024\/01\/17",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":71.2,
        "Instance Attributes":72.6,
        "Instance Identity":74.5,
        "Instance Interaction":74.2,
        "Instance Location":68.5,
        "Instances Counting":63.8,
        "Scene Understanding":77.0,
        "Spatial Relation":56.0,
        "Text Understanding":38.1,
        "Visual Reasoning":77.9,
        "Overall (prefetch)":71.2,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"LLaVA-InternLM2-20B (QLoRA)",
        "Parameters":"20.2B",
        "Language Model":"InternLM2-20B",
        "Vision Model":"CLIP ViT-L\/14",
        "Org":"Shanghai AI Laboratory",
        "Time":"2024\/01\/17",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":70.2,
        "Instance Attributes":71.4,
        "Instance Identity":74.1,
        "Instance Interaction":73.2,
        "Instance Location":67.7,
        "Instances Counting":62.1,
        "Scene Understanding":77.3,
        "Spatial Relation":52.4,
        "Text Understanding":32.1,
        "Visual Reasoning":77.6,
        "Overall (prefetch)":70.2,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"Yi-VL-6B",
        "Parameters":"6.6B",
        "Language Model":"Yi-6B",
        "Vision Model":"CLIP ViT-H\/14",
        "Org":"01-AI",
        "Time":"2024\/01\/25",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":67.5,
        "Instance Attributes":68.1,
        "Instance Identity":70.2,
        "Instance Interaction":73.2,
        "Instance Location":64.2,
        "Instances Counting":60.0,
        "Scene Understanding":74.4,
        "Spatial Relation":52.5,
        "Text Understanding":45.2,
        "Visual Reasoning":75.8,
        "Overall (prefetch)":67.5,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"Yi-VL-34B",
        "Parameters":"34.6B",
        "Language Model":"Yi-34B",
        "Vision Model":"CLIP ViT-H\/14",
        "Org":"01-AI",
        "Time":"2024\/01\/25",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":68.1,
        "Instance Attributes":70.1,
        "Instance Identity":69.3,
        "Instance Interaction":72.2,
        "Instance Location":61.8,
        "Instances Counting":58.4,
        "Scene Understanding":75.4,
        "Spatial Relation":57.7,
        "Text Understanding":61.9,
        "Visual Reasoning":76.4,
        "Overall (prefetch)":68.1,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"InternLM-XComposer2-VL",
        "Parameters":"7B",
        "Language Model":"InternLM2",
        "Vision Model":"CLIP ViT-L\/14",
        "Org":"Shanghai AI Lab",
        "Time":"2024\/01\/26",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":74.9,
        "Instance Attributes":76.9,
        "Instance Identity":76.8,
        "Instance Interaction":77.3,
        "Instance Location":71.6,
        "Instances Counting":69.2,
        "Scene Understanding":78.9,
        "Spatial Relation":62.6,
        "Text Understanding":59.5,
        "Visual Reasoning":78.5,
        "Overall (prefetch)":74.9,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"MMAlaya",
        "Parameters":"7.8B",
        "Language Model":"Alaya-7B-Chat",
        "Vision Model":"EVA-G",
        "Org":"DataCanvas",
        "Time":"2024\/01\/29",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":57.5,
        "Instance Attributes":57.3,
        "Instance Identity":59.6,
        "Instance Interaction":62.9,
        "Instance Location":50.7,
        "Instances Counting":48.6,
        "Scene Understanding":68.9,
        "Spatial Relation":39.4,
        "Text Understanding":26.2,
        "Visual Reasoning":66.2,
        "Overall (prefetch)":57.5,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"Qwen-VL-Max",
        "Parameters":"",
        "Language Model":"QwenLM",
        "Vision Model":"",
        "Org":"Alibaba Group",
        "Time":"2024\/02\/02",
        "Verified":"Yes",
        "OpenSource":"No",
        "Overall":72.7,
        "Instance Attributes":72.7,
        "Instance Identity":77.0,
        "Instance Interaction":77.3,
        "Instance Location":65.8,
        "Instances Counting":67.9,
        "Scene Understanding":78.5,
        "Spatial Relation":60.0,
        "Text Understanding":29.8,
        "Visual Reasoning":82.2,
        "Overall (prefetch)":72.4,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"OmniLMM-12B",
        "Parameters":"12B",
        "Language Model":"Zephyr-7B-\u03b2",
        "Vision Model":"EVA-02-5B",
        "Org":"OpenBMB",
        "Time":"2024\/02\/07",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":71.5,
        "Instance Attributes":74.3,
        "Instance Identity":73.2,
        "Instance Interaction":74.2,
        "Instance Location":64.6,
        "Instances Counting":66.3,
        "Scene Understanding":76.4,
        "Spatial Relation":51.9,
        "Text Understanding":47.6,
        "Visual Reasoning":77.9,
        "Overall (prefetch)":71.4,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"MiniCPM-V",
        "Parameters":"3B",
        "Language Model":"MiniCPM-2.4B",
        "Vision Model":"SigLip-400M",
        "Org":"OpenBMB",
        "Time":"2024\/02\/07",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":65.6,
        "Instance Attributes":66.3,
        "Instance Identity":67.9,
        "Instance Interaction":75.3,
        "Instance Location":56.2,
        "Instances Counting":59.0,
        "Scene Understanding":74.9,
        "Spatial Relation":44.0,
        "Text Understanding":47.6,
        "Visual Reasoning":74.3,
        "Overall (prefetch)":64.4,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"InternVL-Chat-V1.1",
        "Parameters":"19B",
        "Language Model":"LLaMA2-13B",
        "Vision Model":"InternViT-6B",
        "Org":"Shanghai AI Laboratory",
        "Time":"2024\/03\/10",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":71.7,
        "Instance Attributes":73.2,
        "Instance Identity":76.2,
        "Instance Interaction":74.2,
        "Instance Location":69.5,
        "Instances Counting":62.1,
        "Scene Understanding":77.6,
        "Spatial Relation":59.2,
        "Text Understanding":39.3,
        "Visual Reasoning":77.6,
        "Overall (prefetch)":71.4,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"InternVL-Chat-V1.2",
        "Parameters":"40B",
        "Language Model":"Nous-Hermes-2-Yi-34B",
        "Vision Model":"InternViT-6B",
        "Org":"Shanghai AI Laboratory",
        "Time":"2024\/02\/13",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":75.4,
        "Instance Attributes":76.6,
        "Instance Identity":79.8,
        "Instance Interaction":75.3,
        "Instance Location":68.6,
        "Instances Counting":71.1,
        "Scene Understanding":79.3,
        "Spatial Relation":62.9,
        "Text Understanding":45.2,
        "Visual Reasoning":81.0,
        "Overall (prefetch)":75.4,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"InternVL-Chat-V1.2-Plus",
        "Parameters":"40B",
        "Language Model":"Nous-Hermes-2-Yi-34B",
        "Vision Model":"InternViT-6B",
        "Org":"Shanghai AI Laboratory",
        "Time":"2024\/03\/10",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":75.7,
        "Instance Attributes":77.5,
        "Instance Identity":79.6,
        "Instance Interaction":73.2,
        "Instance Location":71.0,
        "Instances Counting":70.5,
        "Scene Understanding":79.2,
        "Spatial Relation":63.0,
        "Text Understanding":47.6,
        "Visual Reasoning":80.4,
        "Overall (prefetch)":75.7,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"Step-1V",
        "Parameters":"",
        "Language Model":"",
        "Vision Model":"",
        "Org":"Step AI",
        "Time":"2024\/03\/15",
        "Verified":"Yes",
        "OpenSource":"Yes",
        "Overall":70.3,
        "Instance Attributes":70.9,
        "Instance Identity":76.4,
        "Instance Interaction":74.2,
        "Instance Location":65.3,
        "Instances Counting":59.4,
        "Scene Understanding":78.1,
        "Spatial Relation":57.5,
        "Text Understanding":35.7,
        "Visual Reasoning":82.8,
        "Overall (prefetch)":70.3,
        "Overall (official)":"N\/A"
    },
    {
        "Model":"GPT-4v",
        "Parameters":"",
        "Language Model":"",
        "Vision Model":"",
        "Org":"OpenAI",
        "Time":"2023\/12\/23",
        "Verified":"Yes",
        "OpenSource":"No",
        "Overall":71.6,
        "Instance Attributes":72.0,
        "Instance Identity":74.1,
        "Instance Interaction":81.4,
        "Instance Location":65.3,
        "Instances Counting":65.0,
        "Scene Understanding":78.1,
        "Spatial Relation":60.9,
        "Text Understanding":64.3,
        "Visual Reasoning":77.0,
        "Overall (prefetch)":73.9,
        "Overall (official)":"69.1 (Gen)"
    }
]