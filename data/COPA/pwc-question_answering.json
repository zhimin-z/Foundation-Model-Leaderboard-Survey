[
    {
        "table_id":1464,
        "row_id":51237,
        "rank":1,
        "method":"PaLM 540B (finetuned) ",
        "mlmodel":{

        },
        "Model":"PaLM 540B ",
        "method_details":"finetuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-05",
        "metrics":{
            "Accuracy":"100"
        },
        "raw_metrics":{
            "Accuracy":100.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":989558,
            "title":"PaLM: Scaling Language Modeling with Pathways",
            "url":"\/paper\/palm-scaling-language-modeling-with-pathways-1",
            "published":"2022-04-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":24570,
        "rank":2,
        "method":"DeBERTa-Ensemble",
        "mlmodel":{

        },
        "Model":"DeBERTa-Ensemble",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-05",
        "metrics":{
            "Accuracy":"98.4"
        },
        "raw_metrics":{
            "Accuracy":98.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":201217,
            "title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
            "url":"\/paper\/deberta-decoding-enhanced-bert-with",
            "published":"2020-06-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deberta-decoding-enhanced-bert-with\/review\/?hl=24570"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":24569,
        "rank":3,
        "method":"DeBERTa-1.5B",
        "mlmodel":{

        },
        "Model":"DeBERTa-1.5B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-05",
        "metrics":{
            "Accuracy":"96.8"
        },
        "raw_metrics":{
            "Accuracy":96.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":201217,
            "title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
            "url":"\/paper\/deberta-decoding-enhanced-bert-with",
            "published":"2020-06-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deberta-decoding-enhanced-bert-with\/review\/?hl=24569"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":102565,
        "rank":4,
        "method":"PaLM 2-L (one-shot)",
        "mlmodel":{

        },
        "Model":"PaLM 2-L ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Accuracy":"96.0"
        },
        "raw_metrics":{
            "Accuracy":96.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102565"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":8377,
        "rank":5,
        "method":"T5-11B",
        "mlmodel":{

        },
        "Model":"T5-11B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "Accuracy":"94.8"
        },
        "raw_metrics":{
            "Accuracy":94.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=8377"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":16815,
        "rank":6,
        "method":"GPT-3 175B (Few-Shot)",
        "mlmodel":{

        },
        "Model":"GPT-3 175B ",
        "method_details":"Few-Shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "Accuracy":"92"
        },
        "raw_metrics":{
            "Accuracy":92.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=16815"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":39036,
        "rank":7,
        "method":"FLAN 137B zero-shot",
        "mlmodel":{

        },
        "Model":"FLAN 137B zero-shot",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-03",
        "metrics":{
            "Accuracy":"91.0"
        },
        "raw_metrics":{
            "Accuracy":91.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":861409,
            "title":"Finetuned Language Models Are Zero-Shot Learners",
            "url":"\/paper\/finetuned-language-models-are-zero-shot",
            "published":"2021-09-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/finetuned-language-models-are-zero-shot\/review\/?hl=39036"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":112310,
        "rank":8,
        "method":"T0-3B + CoT Fine-Tuning",
        "mlmodel":{

        },
        "Model":"T0-3B + CoT Fine-Tuning",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-23",
        "metrics":{
            "Accuracy":"90.88"
        },
        "raw_metrics":{
            "Accuracy":90.88
        },
        "uses_additional_data":false,
        "paper":{
            "id":1214294,
            "title":"The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
            "url":"\/paper\/the-cot-collection-improving-zero-shot-and",
            "published":"2023-05-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/the-cot-collection-improving-zero-shot-and\/review\/?hl=112310"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":102564,
        "rank":9,
        "method":"PaLM 2-M (one-shot)",
        "mlmodel":{

        },
        "Model":"PaLM 2-M ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Accuracy":"90.0"
        },
        "raw_metrics":{
            "Accuracy":90.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102564"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":102563,
        "rank":10,
        "method":"PaLM 2-S (one-shot)",
        "mlmodel":{

        },
        "Model":"PaLM 2-S ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Accuracy":"89.0"
        },
        "raw_metrics":{
            "Accuracy":89.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102563"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":100818,
        "rank":11,
        "method":"GPT-NeoX (one-shot)",
        "mlmodel":{

        },
        "Model":"GPT-NeoX ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"88"
        },
        "raw_metrics":{
            "Accuracy":88.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100818"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":100817,
        "rank":12,
        "method":"Bloomberg GPT (one-shot)",
        "mlmodel":{

        },
        "Model":"Bloomberg GPT ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"86"
        },
        "raw_metrics":{
            "Accuracy":86.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100817"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":100819,
        "rank":13,
        "method":"OPT 66B (one-shot)",
        "mlmodel":{

        },
        "Model":"OPT 66B ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"86"
        },
        "raw_metrics":{
            "Accuracy":86.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100819"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":71012,
        "rank":14,
        "method":"Neo-6B (QA + WS)",
        "mlmodel":{

        },
        "Model":"Neo-6B ",
        "method_details":"QA + WS",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-05",
        "metrics":{
            "Accuracy":"84.0"
        },
        "raw_metrics":{
            "Accuracy":84.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087115,
            "title":"Ask Me Anything: A simple strategy for prompting language models",
            "url":"\/paper\/ask-me-anything-a-simple-strategy-for",
            "published":"2022-10-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ask-me-anything-a-simple-strategy-for\/review\/?hl=71012"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":100820,
        "rank":15,
        "method":"BLOOM 176B (one-shot)",
        "mlmodel":{

        },
        "Model":"BLOOM 176B ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"84"
        },
        "raw_metrics":{
            "Accuracy":84.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100820"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":39234,
        "rank":16,
        "method":"KELM (finetuning BERT-large based single model)",
        "mlmodel":{

        },
        "Model":"KELM ",
        "method_details":"finetuning BERT-large based single model",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-09",
        "metrics":{
            "Accuracy":"78.0"
        },
        "raw_metrics":{
            "Accuracy":78.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":864209,
            "title":"KELM: Knowledge Enhanced Pre-Trained Language Representations with Message Passing on Hierarchical Relational Graphs",
            "url":"\/paper\/kelm-knowledge-enhanced-pre-trained-language",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/kelm-knowledge-enhanced-pre-trained-language\/review\/?hl=39234"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":96234,
        "rank":17,
        "method":"AlexaTM 20B",
        "mlmodel":{

        },
        "Model":"AlexaTM 20B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-02",
        "metrics":{
            "Accuracy":"78.0"
        },
        "raw_metrics":{
            "Accuracy":78.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1053502,
            "title":"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
            "url":"\/paper\/alexatm-20b-few-shot-learning-using-a-large",
            "published":"2022-08-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/alexatm-20b-few-shot-learning-using-a-large\/review\/?hl=96234"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":71014,
        "rank":18,
        "method":"Neo-6B (few-shot)",
        "mlmodel":{

        },
        "Model":"Neo-6B ",
        "method_details":"few-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-05",
        "metrics":{
            "Accuracy":"77.0"
        },
        "raw_metrics":{
            "Accuracy":77.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087115,
            "title":"Ask Me Anything: A simple strategy for prompting language models",
            "url":"\/paper\/ask-me-anything-a-simple-strategy-for",
            "published":"2022-10-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ask-me-anything-a-simple-strategy-for\/review\/?hl=71014"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":60037,
        "rank":19,
        "method":"N-Grammer",
        "mlmodel":{

        },
        "Model":"N-Grammer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-13",
        "metrics":{
            "Accuracy":"60.0"
        },
        "raw_metrics":{
            "Accuracy":60.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1043167,
            "title":"N-Grammer: Augmenting Transformers with latent n-grams",
            "url":"\/paper\/n-grammer-augmenting-transformers-with-latent-1",
            "published":"2022-07-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/n-grammer-augmenting-transformers-with-latent-1\/review\/?hl=60037"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1464,
        "row_id":71013,
        "rank":20,
        "method":"Neo-6B (QA)",
        "mlmodel":{

        },
        "Model":"Neo-6B ",
        "method_details":"QA",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-05",
        "metrics":{
            "Accuracy":"58.2"
        },
        "raw_metrics":{
            "Accuracy":58.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087115,
            "title":"Ask Me Anything: A simple strategy for prompting language models",
            "url":"\/paper\/ask-me-anything-a-simple-strategy-for",
            "published":"2022-10-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ask-me-anything-a-simple-strategy-for\/review\/?hl=71013"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]