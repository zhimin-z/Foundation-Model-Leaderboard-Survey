[
    {
        "table_id":15219,
        "row_id":113523,
        "rank":1,
        "method":"Gemini Ultra (CoT@8)",
        "mlmodel":{

        },
        "Model":"Gemini Ultra ",
        "method_details":"CoT@8",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-12-06",
        "metrics":{
            "Average (%)":"90.04",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":90.04,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":99170,
        "rank":2,
        "method":"GPT-4 (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"GPT-4 ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-15",
        "metrics":{
            "Average (%)":"86.5",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":86.5,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1174373,
            "title":"GPT-4 Technical Report",
            "url":"\/paper\/gpt-4-technical-report-1",
            "published":"2023-03-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/gpt-4-technical-report-1\/review\/?hl=99170"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":113522,
        "rank":3,
        "method":"Gemini Ultra (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Gemini Ultra ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-12-06",
        "metrics":{
            "Average (%)":"83.7",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":83.7,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":102464,
        "rank":4,
        "method":"Flan-PaLM 2 (L) (5-shot)",
        "mlmodel":{

        },
        "Model":"Flan-PaLM 2 ",
        "method_details":"L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Average (%)":"81.2",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":81.2,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102464"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":113521,
        "rank":5,
        "method":"Gemini Pro (CoT@8)",
        "mlmodel":{

        },
        "Model":"Gemini Pro ",
        "method_details":"CoT@8",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-12-06",
        "metrics":{
            "Average (%)":"79.13",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":79.13,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":102465,
        "rank":6,
        "method":"PaLM 2 (large) (5-shot)",
        "mlmodel":{

        },
        "Model":"PaLM 2 ",
        "method_details":"large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Average (%)":"78.3",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":78.3,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102465"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":72867,
        "rank":7,
        "method":"Flan-PaLM (5-shot, finetuned, CoT + SC)",
        "mlmodel":{

        },
        "Model":"Flan-PaLM ",
        "method_details":"5-shot, finetuned, CoT + SC",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"75.2",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"540",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":75.2,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":540.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=72867"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73150,
        "rank":8,
        "method":"Flan-U-PaLM 540B",
        "mlmodel":{

        },
        "Model":"Flan-U-PaLM 540B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"74.1",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"540",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":74.1,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":540.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73150"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":72866,
        "rank":9,
        "method":"Flan-PaLM (5-shot, finetuned)",
        "mlmodel":{

        },
        "Model":"Flan-PaLM ",
        "method_details":"5-shot, finetuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"72.2",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"540",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":72.2,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":540.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=72866"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":96120,
        "rank":10,
        "method":"Codex + REPLUG LSR (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Codex + REPLUG LSR ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Average (%)":"71.8",
            "Humanities":"76.5",
            "STEM":"58.9",
            "Social Sciences":"79.9",
            "Other":"73.2",
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":71.8,
            "Humanities":76.5,
            "STEM":58.9,
            "Social Sciences":79.9,
            "Other":73.2,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149330,
            "title":"REPLUG: Retrieval-Augmented Black-Box Language Models",
            "url":"\/paper\/replug-retrieval-augmented-black-box-language",
            "published":"2023-01-30T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":113520,
        "rank":11,
        "method":"Gemini Pro (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Gemini Pro ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-12-06",
        "metrics":{
            "Average (%)":"71.8",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":71.8,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73159,
        "rank":12,
        "method":"Flan-PaLM 540B (CoT)",
        "mlmodel":{

        },
        "Model":"Flan-PaLM 540B ",
        "method_details":"CoT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"70.9",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"540",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":70.9,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":540.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73159"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":72741,
        "rank":13,
        "method":"U-PaLM (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"U-PaLM ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"70.7",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"540",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":70.7,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":540.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097009,
            "title":"Transcending Scaling Laws with 0.1% Extra Compute",
            "url":"\/paper\/transcending-scaling-laws-with-0-1-extra",
            "published":"2022-10-20T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/transcending-scaling-laws-with-0-1-extra\/review\/?hl=72741"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73151,
        "rank":14,
        "method":"Flan-PaLM (5-shot, finetuned, CoT)",
        "mlmodel":{

        },
        "Model":"Flan-PaLM ",
        "method_details":"5-shot, finetuned, CoT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"70.2",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"540",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":70.2,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":540.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73151"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":99169,
        "rank":15,
        "method":"GPT-3.5 (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"GPT-3.5 ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-15",
        "metrics":{
            "Average (%)":"70",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":70.0,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1174373,
            "title":"GPT-4 Technical Report",
            "url":"\/paper\/gpt-4-technical-report-1",
            "published":"2023-03-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/gpt-4-technical-report-1\/review\/?hl=99169"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73161,
        "rank":16,
        "method":"Flan-U-PaLM (CoT)",
        "mlmodel":{

        },
        "Model":"Flan-U-PaLM ",
        "method_details":"CoT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"69.8",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"540",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":69.8,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":540.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73161"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":58550,
        "rank":17,
        "method":"PaLM 540B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"PaLM 540B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-05",
        "metrics":{
            "Average (%)":"69.3",
            "Humanities":"77.0",
            "STEM":"55.6 ",
            "Social Sciences":"81.0",
            "Other":"69.6",
            "Parameters (Billions)":"540",
            "Tokens (Billions)":"780"
        },
        "raw_metrics":{
            "Average (%)":69.3,
            "Humanities":77.0,
            "STEM":55.6,
            "Social Sciences":81.0,
            "Other":69.6,
            "Parameters (Billions)":540.0,
            "Tokens (Billions)":780.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":989558,
            "title":"PaLM: Scaling Language Modeling with Pathways",
            "url":"\/paper\/palm-scaling-language-modeling-with-pathways-1",
            "published":"2022-04-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":97685,
        "rank":18,
        "method":"LLaMA 65B (fine-tuned)",
        "mlmodel":{

        },
        "Model":"LLaMA 65B ",
        "method_details":"fine-tuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Average (%)":"68.9",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"65",
            "Tokens (Billions)":"1400"
        },
        "raw_metrics":{
            "Average (%)":68.9,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":65.0,
            "Tokens (Billions)":1400.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97685"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":106287,
        "rank":19,
        "method":"LLaMA 2 70B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"LLaMA 2 70B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Average (%)":"68.9",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"70",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":68.9,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":70.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106287"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":96121,
        "rank":20,
        "method":"Codex (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Codex ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Average (%)":"68.3",
            "Humanities":"74.2",
            "STEM":"57.8",
            "Social Sciences":"76.9",
            "Other":"70.1",
            "Parameters (Billions)":"175",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":68.3,
            "Humanities":74.2,
            "STEM":57.8,
            "Social Sciences":76.9,
            "Other":70.1,
            "Parameters (Billions)":175.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149330,
            "title":"REPLUG: Retrieval-Augmented Black-Box Language Models",
            "url":"\/paper\/replug-retrieval-augmented-black-box-language",
            "published":"2023-01-30T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":50746,
        "rank":21,
        "method":"Chinchilla (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Chinchilla ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-29",
        "metrics":{
            "Average (%)":"67.5",
            "Humanities":"73.1",
            "STEM":"55",
            "Social Sciences":"78.8",
            "Other":"70.3",
            "Parameters (Billions)":"70",
            "Tokens (Billions)":"1400"
        },
        "raw_metrics":{
            "Average (%)":67.5,
            "Humanities":73.1,
            "STEM":55.0,
            "Social Sciences":78.8,
            "Other":70.3,
            "Parameters (Billions)":70.0,
            "Tokens (Billions)":1400.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":985465,
            "title":"Training Compute-Optimal Large Language Models",
            "url":"\/paper\/training-compute-optimal-large-language",
            "published":"2022-03-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73149,
        "rank":22,
        "method":"Flan-cont-PaLM",
        "mlmodel":{

        },
        "Model":"Flan-cont-PaLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"66.1",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"62",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":66.1,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":62.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73149"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":97684,
        "rank":23,
        "method":"LLaMA 65B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"LLaMA 65B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Average (%)":"63.4",
            "Humanities":"61.8",
            "STEM":"51.7",
            "Social Sciences":"72.9",
            "Other":"67.4",
            "Parameters (Billions)":"65",
            "Tokens (Billions)":"1400"
        },
        "raw_metrics":{
            "Average (%)":63.4,
            "Humanities":61.8,
            "STEM":51.7,
            "Social Sciences":72.9,
            "Other":67.4,
            "Parameters (Billions)":65.0,
            "Tokens (Billions)":1400.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97684"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":106299,
        "rank":24,
        "method":"LLaMA 2 34B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"LLaMA 2 34B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Average (%)":"62.6",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":62.6,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106299"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73160,
        "rank":25,
        "method":"Flan-cont-PaLM (CoT)",
        "mlmodel":{

        },
        "Model":"Flan-cont-PaLM ",
        "method_details":"CoT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"62",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"540",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":62.0,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":540.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73160"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":112769,
        "rank":26,
        "method":"Mistral 7B",
        "mlmodel":{

        },
        "Model":"Mistral 7B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-10",
        "metrics":{
            "Average (%)":"60.1",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":60.1,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1297015,
            "title":"Mistral 7B",
            "url":"\/paper\/mistral-7b",
            "published":"2023-10-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mistral-7b\/review\/?hl=112769"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47494,
        "rank":27,
        "method":"Gopher (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Gopher ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-08",
        "metrics":{
            "Average (%)":"60.0",
            "Humanities":"65.8",
            "STEM":"48.0",
            "Social Sciences":"71.2",
            "Other":"64.0",
            "Parameters (Billions)":"280",
            "Tokens (Billions)":"300"
        },
        "raw_metrics":{
            "Average (%)":60.0,
            "Humanities":65.8,
            "STEM":48.0,
            "Social Sciences":71.2,
            "Other":64.0,
            "Parameters (Billions)":280.0,
            "Tokens (Billions)":300.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":942590,
            "title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
            "url":"\/paper\/scaling-language-models-methods-analysis-1",
            "published":"2021-12-08T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73147,
        "rank":28,
        "method":"Flan-PaLM 62B",
        "mlmodel":{

        },
        "Model":"Flan-PaLM 62B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"59.6",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"62",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":59.6,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":62.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73147"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":97683,
        "rank":29,
        "method":"LLaMA 33B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"LLaMA 33B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Average (%)":"57.8",
            "Humanities":"55.8",
            "STEM":"46.0",
            "Social Sciences":"66.7",
            "Other":"63.4",
            "Parameters (Billions)":"33",
            "Tokens (Billions)":"1400"
        },
        "raw_metrics":{
            "Average (%)":57.8,
            "Humanities":55.8,
            "STEM":46.0,
            "Social Sciences":66.7,
            "Other":63.4,
            "Parameters (Billions)":33.0,
            "Tokens (Billions)":1400.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97683"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73158,
        "rank":30,
        "method":"Flan-PaLM 62B (CoT)",
        "mlmodel":{

        },
        "Model":"Flan-PaLM 62B ",
        "method_details":"CoT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"56.9",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":56.9,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73158"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73141,
        "rank":31,
        "method":"Flan-T5-XXL",
        "mlmodel":{

        },
        "Model":"Flan-T5-XXL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"55.1",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"11",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":55.1,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":11.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73141"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":106298,
        "rank":32,
        "method":"LLaMA 2 13B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"LLaMA 2 13B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Average (%)":"54.8",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":54.8,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106298"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47498,
        "rank":33,
        "method":"GPT-3 (fine-tuned)",
        "mlmodel":{

        },
        "Model":"GPT-3 ",
        "method_details":"fine-tuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "Average (%)":"53.9",
            "Humanities":"52.5",
            "STEM":"41.4",
            "Social Sciences":"63.9",
            "Other":"57.9",
            "Parameters (Billions)":"175",
            "Tokens (Billions)":"300"
        },
        "raw_metrics":{
            "Average (%)":53.9,
            "Humanities":52.5,
            "STEM":41.4,
            "Social Sciences":63.9,
            "Other":57.9,
            "Parameters (Billions)":175.0,
            "Tokens (Billions)":300.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=47498"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":85951,
        "rank":34,
        "method":"GAL 120B (zero-shot)",
        "mlmodel":{

        },
        "Model":"GAL 120B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-16",
        "metrics":{
            "Average (%)":"52.6",
            "Humanities":null,
            "STEM":"49.6",
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"120",
            "Tokens (Billions)":"450"
        },
        "raw_metrics":{
            "Average (%)":52.6,
            "Humanities":null,
            "STEM":49.6,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":120.0,
            "Tokens (Billions)":450.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1112728,
            "title":"Galactica: A Large Language Model for Science",
            "url":"\/paper\/galactica-a-large-language-model-for-science-1",
            "published":"2022-11-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/galactica-a-large-language-model-for-science-1\/review\/?hl=85951"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            },
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73140,
        "rank":35,
        "method":"Flan-T5-XL",
        "mlmodel":{

        },
        "Model":"Flan-T5-XL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"52.4",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"3",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":52.4,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":3.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73140"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73146,
        "rank":36,
        "method":"Flan-PaLM 8B",
        "mlmodel":{

        },
        "Model":"Flan-PaLM 8B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"49.3",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"8",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":49.3,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":8.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73146"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47497,
        "rank":37,
        "method":"UnifiedQA",
        "mlmodel":{

        },
        "Model":"UnifiedQA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-02",
        "metrics":{
            "Average (%)":"48.9",
            "Humanities":"45.6",
            "STEM":"40.2",
            "Social Sciences":"56.6",
            "Other":"54.6",
            "Parameters (Billions)":"11",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":48.9,
            "Humanities":45.6,
            "STEM":40.2,
            "Social Sciences":56.6,
            "Other":54.6,
            "Parameters (Billions)":11.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":193599,
            "title":"UnifiedQA: Crossing Format Boundaries With a Single QA System",
            "url":"\/paper\/unifiedqa-crossing-format-boundaries-with-a",
            "published":"2020-05-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unifiedqa-crossing-format-boundaries-with-a\/review\/?hl=47497"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73156,
        "rank":38,
        "method":"Flan-T5-XXL (CoT)",
        "mlmodel":{

        },
        "Model":"Flan-T5-XXL ",
        "method_details":"CoT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"48.6",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":48.6,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73156"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":66197,
        "rank":39,
        "method":"Atlas (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Atlas ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-05",
        "metrics":{
            "Average (%)":"47.9",
            "Humanities":"46.1",
            "STEM":"38.8",
            "Social Sciences":"54.6",
            "Other":"52.8",
            "Parameters (Billions)":"11",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":47.9,
            "Humanities":46.1,
            "STEM":38.8,
            "Social Sciences":54.6,
            "Other":52.8,
            "Parameters (Billions)":11.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1055376,
            "title":"Atlas: Few-shot Learning with Retrieval Augmented Language Models",
            "url":"\/paper\/few-shot-learning-with-retrieval-augmented",
            "published":"2022-08-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/few-shot-learning-with-retrieval-augmented\/review\/?hl=66197"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":97682,
        "rank":40,
        "method":"LLaMA 13B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"LLaMA 13B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Average (%)":"46.9",
            "Humanities":"45.0",
            "STEM":"35.8",
            "Social Sciences":"53.8",
            "Other":"53.3",
            "Parameters (Billions)":"13",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":46.9,
            "Humanities":45.0,
            "STEM":35.8,
            "Social Sciences":53.8,
            "Other":53.3,
            "Parameters (Billions)":13.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97682"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73155,
        "rank":41,
        "method":"Flan-T5-XL (CoT)",
        "mlmodel":{

        },
        "Model":"Flan-T5-XL ",
        "method_details":"CoT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"45.5",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":45.5,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73155"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":106297,
        "rank":42,
        "method":"LLaMA 2 7B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"LLaMA 2 7B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Average (%)":"45.3",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":45.3,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106297"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73129,
        "rank":43,
        "method":"Flan-T5-Large 780M",
        "mlmodel":{

        },
        "Model":"Flan-T5-Large 780M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"45.1",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":45.1,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73129"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":88975,
        "rank":44,
        "method":"GLM-130B",
        "mlmodel":{

        },
        "Model":"GLM-130B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-05",
        "metrics":{
            "Average (%)":"44.8",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":44.8,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1086925,
            "title":"GLM-130B: An Open Bilingual Pre-trained Model",
            "url":"\/paper\/glm-130b-an-open-bilingual-pre-trained-model",
            "published":"2022-10-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/glm-130b-an-open-bilingual-pre-trained-model\/review\/?hl=88975"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47496,
        "rank":45,
        "method":"GPT-3  175B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"GPT-3  175B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "Average (%)":"43.9",
            "Humanities":null,
            "STEM":"36.7",
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":43.9,
            "Humanities":null,
            "STEM":36.7,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=47496"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47499,
        "rank":46,
        "method":"GPT-3 6.7B (fine-tuned)",
        "mlmodel":{

        },
        "Model":"GPT-3 6.7B ",
        "method_details":"fine-tuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "Average (%)":"43.2",
            "Humanities":"42.1",
            "STEM":"35.1",
            "Social Sciences":"49.2",
            "Other":"46.9",
            "Parameters (Billions)":"6.7",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":43.2,
            "Humanities":42.1,
            "STEM":35.1,
            "Social Sciences":49.2,
            "Other":46.9,
            "Parameters (Billions)":6.7,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=47499"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73157,
        "rank":47,
        "method":"Flan-PaLM 8B (CoT)",
        "mlmodel":{

        },
        "Model":"Flan-PaLM 8B ",
        "method_details":"CoT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"41.3",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":41.3,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73157"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73154,
        "rank":48,
        "method":"Flan-T5-Large (CoT)",
        "mlmodel":{

        },
        "Model":"Flan-T5-Large ",
        "method_details":"CoT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"40.5",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":40.5,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73154"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":100780,
        "rank":49,
        "method":"Bloomberg GPT (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Bloomberg GPT ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Average (%)":"39.18",
            "Humanities":"36.26",
            "STEM":"35.12",
            "Social Sciences":"40.04",
            "Other":"46.36",
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":39.18,
            "Humanities":36.26,
            "STEM":35.12,
            "Social Sciences":40.04,
            "Other":46.36,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100780"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":100783,
        "rank":50,
        "method":"BLOOM 176B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"BLOOM 176B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Average (%)":"39.13",
            "Humanities":"34.05",
            "STEM":"36.75",
            "Social Sciences":"41.50",
            "Other":"46.48",
            "Parameters (Billions)":"176",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":39.13,
            "Humanities":34.05,
            "STEM":36.75,
            "Social Sciences":41.5,
            "Other":46.48,
            "Parameters (Billions)":176.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100783"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":100782,
        "rank":51,
        "method":"OPT 66B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"OPT 66B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Average (%)":"35.99",
            "Humanities":"33.28",
            "STEM":"30.72",
            "Social Sciences":"38.32",
            "Other":"42.63",
            "Parameters (Billions)":"66",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":35.99,
            "Humanities":33.28,
            "STEM":30.72,
            "Social Sciences":38.32,
            "Other":42.63,
            "Parameters (Billions)":66.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100782"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":100781,
        "rank":52,
        "method":"GPT-NeoX (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"GPT-NeoX ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Average (%)":"35.95",
            "Humanities":"32.75",
            "STEM":"33.43",
            "Social Sciences":"36.63",
            "Other":"42.29",
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":35.95,
            "Humanities":32.75,
            "STEM":33.43,
            "Social Sciences":36.63,
            "Other":42.29,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100781"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73128,
        "rank":53,
        "method":"Flan-T5-Base 250M",
        "mlmodel":{

        },
        "Model":"Flan-T5-Base 250M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"35.9",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":35.9,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73128"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":97681,
        "rank":54,
        "method":"LLaMA 7B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"LLaMA 7B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Average (%)":"35.1",
            "Humanities":"34.0",
            "STEM":"30.5",
            "Social Sciences":"38.3",
            "Other":"38.1",
            "Parameters (Billions)":"7",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":35.1,
            "Humanities":34.0,
            "STEM":30.5,
            "Social Sciences":38.3,
            "Other":38.1,
            "Parameters (Billions)":7.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97681"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73153,
        "rank":55,
        "method":"Flan-T5-Base (CoT)",
        "mlmodel":{

        },
        "Model":"Flan-T5-Base ",
        "method_details":"CoT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"33.7",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":33.7,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73153"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47575,
        "rank":56,
        "method":"GPT-NeoX-20B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"GPT-NeoX-20B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Average (%)":"33.6",
            "Humanities":"29.8",
            "STEM":"34.9",
            "Social Sciences":"33.7",
            "Other":"37.7",
            "Parameters (Billions)":"20",
            "Tokens (Billions)":"300"
        },
        "raw_metrics":{
            "Average (%)":33.6,
            "Humanities":29.8,
            "STEM":34.9,
            "Social Sciences":33.7,
            "Other":37.7,
            "Parameters (Billions)":20.0,
            "Tokens (Billions)":300.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":994573,
            "title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model",
            "url":"\/paper\/gpt-neox-20b-an-open-source-autoregressive-1",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47495,
        "rank":57,
        "method":"GPT-2 1.5B (fine-tuned)",
        "mlmodel":{

        },
        "Model":"GPT-2 1.5B ",
        "method_details":"fine-tuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Average (%)":"32.4",
            "Humanities":"32.8",
            "STEM":"30.2",
            "Social Sciences":"33.3",
            "Other":"33.1",
            "Parameters (Billions)":"1.5",
            "Tokens (Billions)":"300"
        },
        "raw_metrics":{
            "Average (%)":32.4,
            "Humanities":32.8,
            "STEM":30.2,
            "Social Sciences":33.3,
            "Other":33.1,
            "Parameters (Billions)":1.5,
            "Tokens (Billions)":300.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":105884,
            "title":"Language Models are Unsupervised Multitask Learners",
            "url":"\/paper\/language-models-are-unsupervised-multitask",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47502,
        "rank":58,
        "method":"Gopher-7.1B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Gopher-7.1B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-08",
        "metrics":{
            "Average (%)":"29.5",
            "Humanities":"28.0",
            "STEM":"30.1",
            "Social Sciences":"31.0",
            "Other":"31.0",
            "Parameters (Billions)":"7.1",
            "Tokens (Billions)":"300"
        },
        "raw_metrics":{
            "Average (%)":29.5,
            "Humanities":28.0,
            "STEM":30.1,
            "Social Sciences":31.0,
            "Other":31.0,
            "Parameters (Billions)":7.1,
            "Tokens (Billions)":300.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":942590,
            "title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
            "url":"\/paper\/scaling-language-models-methods-analysis-1",
            "published":"2021-12-08T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73127,
        "rank":59,
        "method":"Flan-T5-Small 80M",
        "mlmodel":{

        },
        "Model":"Flan-T5-Small 80M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"28.7",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":28.7,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73127"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47556,
        "rank":60,
        "method":"GPT-NeoX-20B (zero-shot)",
        "mlmodel":{

        },
        "Model":"GPT-NeoX-20B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Average (%)":"28.6",
            "Humanities":"28.2",
            "STEM":"27.2",
            "Social Sciences":"30.8",
            "Other":"29.2",
            "Parameters (Billions)":"20",
            "Tokens (Billions)":"300"
        },
        "raw_metrics":{
            "Average (%)":28.6,
            "Humanities":28.2,
            "STEM":27.2,
            "Social Sciences":30.8,
            "Other":29.2,
            "Parameters (Billions)":20.0,
            "Tokens (Billions)":300.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":994573,
            "title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model",
            "url":"\/paper\/gpt-neox-20b-an-open-source-autoregressive-1",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47581,
        "rank":61,
        "method":"RoBERTa (fine-tuned)",
        "mlmodel":{

        },
        "Model":"RoBERTa ",
        "method_details":"fine-tuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-26",
        "metrics":{
            "Average (%)":"27.9",
            "Humanities":"27.9",
            "STEM":"27.0",
            "Social Sciences":"28.8",
            "Other":"27.7",
            "Parameters (Billions)":"0.354",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":27.9,
            "Humanities":27.9,
            "STEM":27.0,
            "Social Sciences":28.8,
            "Other":27.7,
            "Parameters (Billions)":0.354,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":148282,
            "title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "url":"\/paper\/roberta-a-robustly-optimized-bert-pretraining",
            "published":"2019-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/roberta-a-robustly-optimized-bert-pretraining\/review\/?hl=47581"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47555,
        "rank":62,
        "method":"GPT-J-6B (zero-shot)",
        "mlmodel":{

        },
        "Model":"GPT-J-6B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-01",
        "metrics":{
            "Average (%)":"27.3",
            "Humanities":"27.8",
            "STEM":"25.7",
            "Social Sciences":"28.7",
            "Other":"28.0",
            "Parameters (Billions)":"6",
            "Tokens (Billions)":"300"
        },
        "raw_metrics":{
            "Average (%)":27.3,
            "Humanities":27.8,
            "STEM":25.7,
            "Social Sciences":28.7,
            "Other":28.0,
            "Parameters (Billions)":6.0,
            "Tokens (Billions)":300.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":994573,
            "title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model",
            "url":"\/paper\/gpt-neox-20b-an-open-source-autoregressive-1",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47501,
        "rank":63,
        "method":"Gopher-1.4B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Gopher-1.4B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-08",
        "metrics":{
            "Average (%)":"27.3",
            "Humanities":"27.5",
            "STEM":"26.6",
            "Social Sciences":"30.0",
            "Other":"24.7",
            "Parameters (Billions)":"1.4",
            "Tokens (Billions)":"300"
        },
        "raw_metrics":{
            "Average (%)":27.3,
            "Humanities":27.5,
            "STEM":26.6,
            "Social Sciences":30.0,
            "Other":24.7,
            "Parameters (Billions)":1.4,
            "Tokens (Billions)":300.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":942590,
            "title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
            "url":"\/paper\/scaling-language-models-methods-analysis-1",
            "published":"2021-12-08T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47580,
        "rank":64,
        "method":"ALBERT (fine-tuned)",
        "mlmodel":{

        },
        "Model":"ALBERT ",
        "method_details":"fine-tuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-26",
        "metrics":{
            "Average (%)":"27.1",
            "Humanities":"27.2",
            "STEM":"27.7",
            "Social Sciences":"25.7",
            "Other":"27.9",
            "Parameters (Billions)":"0.031",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":27.1,
            "Humanities":27.2,
            "STEM":27.7,
            "Social Sciences":25.7,
            "Other":27.9,
            "Parameters (Billions)":0.031,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":156146,
            "title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
            "url":"\/paper\/albert-a-lite-bert-for-self-supervised",
            "published":"2019-09-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/albert-a-lite-bert-for-self-supervised\/review\/?hl=47580"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47577,
        "rank":65,
        "method":"GPT-3 13B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"GPT-3 13B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "Average (%)":"26",
            "Humanities":"27.1",
            "STEM":"24.3",
            "Social Sciences":"25.6",
            "Other":"26.5",
            "Parameters (Billions)":"13",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":26.0,
            "Humanities":27.1,
            "STEM":24.3,
            "Social Sciences":25.6,
            "Other":26.5,
            "Parameters (Billions)":13.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=47577"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47579,
        "rank":66,
        "method":"GPT-3 2.7B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"GPT-3 2.7B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "Average (%)":"25.9",
            "Humanities":"24.4",
            "STEM":"26.0",
            "Social Sciences":"30.9",
            "Other":"24.1",
            "Parameters (Billions)":"2.7",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":25.9,
            "Humanities":24.4,
            "STEM":26.0,
            "Social Sciences":30.9,
            "Other":24.1,
            "Parameters (Billions)":2.7,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=47579"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47500,
        "rank":67,
        "method":"Gopher-0.4B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Gopher-0.4B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-08",
        "metrics":{
            "Average (%)":"25.7",
            "Humanities":"26.6",
            "STEM":"26.0",
            "Social Sciences":"23.4",
            "Other":"24.1",
            "Parameters (Billions)":"0.4",
            "Tokens (Billions)":"300"
        },
        "raw_metrics":{
            "Average (%)":25.7,
            "Humanities":26.6,
            "STEM":26.0,
            "Social Sciences":23.4,
            "Other":24.1,
            "Parameters (Billions)":0.4,
            "Tokens (Billions)":300.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":942590,
            "title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
            "url":"\/paper\/scaling-language-models-methods-analysis-1",
            "published":"2021-12-08T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47493,
        "rank":68,
        "method":"Random Baseline",
        "mlmodel":{

        },
        "Model":"Random Baseline",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-09-07",
        "metrics":{
            "Average (%)":"25.0",
            "Humanities":"25.0",
            "STEM":"25.0",
            "Social Sciences":"25.0",
            "Other":"25.0",
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":25.0,
            "Humanities":25.0,
            "STEM":25.0,
            "Social Sciences":25.0,
            "Other":25.0,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":216419,
            "title":"Measuring Massive Multitask Language Understanding",
            "url":"\/paper\/measuring-massive-multitask-language",
            "published":"2020-09-07T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":47578,
        "rank":69,
        "method":"GPT-3 6.7B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"GPT-3 6.7B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "Average (%)":"24.9",
            "Humanities":"26.1",
            "STEM":"25.6",
            "Social Sciences":"21.6",
            "Other":"25.5",
            "Parameters (Billions)":"6.7",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":24.9,
            "Humanities":26.1,
            "STEM":25.6,
            "Social Sciences":21.6,
            "Other":25.5,
            "Parameters (Billions)":6.7,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=47578"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73152,
        "rank":70,
        "method":"Flan-T5-Small (CoT)",
        "mlmodel":{

        },
        "Model":"Flan-T5-Small ",
        "method_details":"CoT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":"12.1",
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":12.1,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":null,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73152"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":99006,
        "rank":71,
        "method":"GPT-3 175B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"GPT-3 175B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "Average (%)":null,
            "Humanities":"40.8",
            "STEM":null,
            "Social Sciences":"50.4",
            "Other":"48.8",
            "Parameters (Billions)":"175",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":null,
            "Humanities":40.8,
            "STEM":null,
            "Social Sciences":50.4,
            "Other":48.8,
            "Parameters (Billions)":175.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=99006"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":58548,
        "rank":72,
        "method":"Minerva 540B-maj1@16 (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Minerva 540B-maj1@16 ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-29",
        "metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":"75.0",
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"540",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":75.0,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":540.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1035722,
            "title":"Solving Quantitative Reasoning Problems with Language Models",
            "url":"\/paper\/solving-quantitative-reasoning-problems-with",
            "published":"2022-06-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/solving-quantitative-reasoning-problems-with\/review\/?hl=58548"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":58549,
        "rank":73,
        "method":"Minerva 540B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Minerva 540B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-29",
        "metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":"63.9",
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"540",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":63.9,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":540.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1035722,
            "title":"Solving Quantitative Reasoning Problems with Language Models",
            "url":"\/paper\/solving-quantitative-reasoning-problems-with",
            "published":"2022-06-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/solving-quantitative-reasoning-problems-with\/review\/?hl=58549"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":58551,
        "rank":74,
        "method":"Minerva 62B-maj1@16 (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Minerva 62B-maj1@16 ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-29",
        "metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":"63.5",
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"62",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":63.5,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":62.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1035722,
            "title":"Solving Quantitative Reasoning Problems with Language Models",
            "url":"\/paper\/solving-quantitative-reasoning-problems-with",
            "published":"2022-06-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/solving-quantitative-reasoning-problems-with\/review\/?hl=58551"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":58552,
        "rank":75,
        "method":"Minerva 62B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Minerva 62B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-29",
        "metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":"53.9",
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"62",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":53.9,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":62.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1035722,
            "title":"Solving Quantitative Reasoning Problems with Language Models",
            "url":"\/paper\/solving-quantitative-reasoning-problems-with",
            "published":"2022-06-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/solving-quantitative-reasoning-problems-with\/review\/?hl=58552"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":58554,
        "rank":76,
        "method":"Minerva 8B-maj1@16 (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Minerva 8B-maj1@16 ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-29",
        "metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":"43.4",
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"8",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":43.4,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":8.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1035722,
            "title":"Solving Quantitative Reasoning Problems with Language Models",
            "url":"\/paper\/solving-quantitative-reasoning-problems-with",
            "published":"2022-06-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/solving-quantitative-reasoning-problems-with\/review\/?hl=58554"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":58553,
        "rank":77,
        "method":"PaLM 62B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"PaLM 62B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-29",
        "metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":"39.1",
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"62",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":39.1,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":62.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1035722,
            "title":"Solving Quantitative Reasoning Problems with Language Models",
            "url":"\/paper\/solving-quantitative-reasoning-problems-with",
            "published":"2022-06-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/solving-quantitative-reasoning-problems-with\/review\/?hl=58553"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":58555,
        "rank":78,
        "method":"Minerva 8B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"Minerva 8B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-29",
        "metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":"35.6",
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"8",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":35.6,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":8.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1035722,
            "title":"Solving Quantitative Reasoning Problems with Language Models",
            "url":"\/paper\/solving-quantitative-reasoning-problems-with",
            "published":"2022-06-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/solving-quantitative-reasoning-problems-with\/review\/?hl=58555"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":58556,
        "rank":79,
        "method":"PaLM 8B (few-shot, k=5)",
        "mlmodel":{

        },
        "Model":"PaLM 8B ",
        "method_details":"few-shot, k=5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-29",
        "metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":"22.0",
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"8",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":22.0,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":8.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1035722,
            "title":"Solving Quantitative Reasoning Problems with Language Models",
            "url":"\/paper\/solving-quantitative-reasoning-problems-with",
            "published":"2022-06-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/solving-quantitative-reasoning-problems-with\/review\/?hl=58556"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73137,
        "rank":80,
        "method":"Flan-T5-Small",
        "mlmodel":{

        },
        "Model":"Flan-T5-Small",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"80",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":80.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73137"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73138,
        "rank":81,
        "method":"Flan-T5-Base",
        "mlmodel":{

        },
        "Model":"Flan-T5-Base",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"250",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":250.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73138"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":15219,
        "row_id":73139,
        "rank":82,
        "method":"Flan-T5-Large",
        "mlmodel":{

        },
        "Model":"Flan-T5-Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":"780",
            "Tokens (Billions)":null
        },
        "raw_metrics":{
            "Average (%)":null,
            "Humanities":null,
            "STEM":null,
            "Social Sciences":null,
            "Other":null,
            "Parameters (Billions)":780.0,
            "Tokens (Billions)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097139,
            "title":"Scaling Instruction-Finetuned Language Models",
            "url":"\/paper\/scaling-instruction-finetuned-language-models",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-instruction-finetuned-language-models\/review\/?hl=73139"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]