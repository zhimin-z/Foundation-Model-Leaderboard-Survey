[
    {
        "table_id":2560,
        "row_id":64304,
        "rank":1,
        "Model":"BEiT-3",
        "mlmodel":{

        },
        "method_short":"BEiT-3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-22",
        "metrics":{
            "Accuracy":"91.51"
        },
        "raw_metrics":{
            "Accuracy":91.51
        },
        "uses_additional_data":false,
        "paper":{
            "id":1062207,
            "title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
            "url":"\/paper\/image-as-a-foreign-language-beit-pretraining",
            "published":"2022-08-22T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2560,
        "row_id":80506,
        "rank":2,
        "Model":"X2-VLM (large)",
        "mlmodel":{

        },
        "method_short":"X2-VLM ",
        "method_details":"large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-22",
        "metrics":{
            "Accuracy":"88.7"
        },
        "raw_metrics":{
            "Accuracy":88.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1116038,
            "title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
            "url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for",
            "published":"2022-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for\/review\/?hl=80506"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2560,
        "row_id":103925,
        "rank":3,
        "Model":"XFM (base)",
        "mlmodel":{

        },
        "method_short":"XFM ",
        "method_details":"base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-12",
        "metrics":{
            "Accuracy":"87.6"
        },
        "raw_metrics":{
            "Accuracy":87.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1141541,
            "title":"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",
            "url":"\/paper\/toward-building-general-foundation-models-for",
            "published":"2023-01-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/toward-building-general-foundation-models-for\/review\/?hl=103925"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2560,
        "row_id":80505,
        "rank":4,
        "Model":"X2-VLM (base)",
        "mlmodel":{

        },
        "method_short":"X2-VLM ",
        "method_details":"base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-22",
        "metrics":{
            "Accuracy":"86.2"
        },
        "raw_metrics":{
            "Accuracy":86.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1116038,
            "title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
            "url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for",
            "published":"2022-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for\/review\/?hl=80505"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2560,
        "row_id":54170,
        "rank":5,
        "Model":"CoCa",
        "mlmodel":{

        },
        "method_short":"CoCa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Accuracy":"86.1"
        },
        "raw_metrics":{
            "Accuracy":86.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=54170"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2560,
        "row_id":43950,
        "rank":6,
        "Model":"VLMo",
        "mlmodel":{

        },
        "method_short":"VLMo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-03",
        "metrics":{
            "Accuracy":"85.64"
        },
        "raw_metrics":{
            "Accuracy":85.64
        },
        "uses_additional_data":false,
        "paper":{
            "id":900016,
            "title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts",
            "url":"\/paper\/vlmo-unified-vision-language-pre-training",
            "published":"2021-11-03T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2560,
        "row_id":43427,
        "rank":7,
        "Model":"SimVLM",
        "mlmodel":{

        },
        "method_short":"SimVLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-08-24",
        "metrics":{
            "Accuracy":"84.53"
        },
        "raw_metrics":{
            "Accuracy":84.53
        },
        "uses_additional_data":false,
        "paper":{
            "id":856712,
            "title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
            "url":"\/paper\/simvlm-simple-visual-language-model",
            "published":"2021-08-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/simvlm-simple-visual-language-model\/review\/?hl=43427"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2560,
        "row_id":47857,
        "rank":8,
        "Model":"X-VLM (base)",
        "mlmodel":{

        },
        "method_short":"X-VLM ",
        "method_details":"base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-16",
        "metrics":{
            "Accuracy":"84.41"
        },
        "raw_metrics":{
            "Accuracy":84.41
        },
        "uses_additional_data":false,
        "paper":{
            "id":911012,
            "title":"Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts",
            "url":"\/paper\/multi-grained-vision-language-pre-training",
            "published":"2021-11-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multi-grained-vision-language-pre-training\/review\/?hl=47857"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2560,
        "row_id":110632,
        "rank":9,
        "Model":"VK-OOD",
        "mlmodel":{

        },
        "method_short":"VK-OOD",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-11",
        "metrics":{
            "Accuracy":"83.9"
        },
        "raw_metrics":{
            "Accuracy":83.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":1156677,
            "title":"Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis",
            "url":"\/paper\/differentiable-outlier-detection-enable",
            "published":"2023-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/differentiable-outlier-detection-enable\/review\/?hl=110632"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2560,
        "row_id":45363,
        "rank":10,
        "Model":"ALBEF (14M)",
        "mlmodel":{

        },
        "method_short":"ALBEF ",
        "method_details":"14M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-16",
        "metrics":{
            "Accuracy":"83.14"
        },
        "raw_metrics":{
            "Accuracy":83.14
        },
        "uses_additional_data":false,
        "paper":{
            "id":836928,
            "title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
            "url":"\/paper\/align-before-fuse-vision-and-language",
            "published":"2021-07-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/align-before-fuse-vision-and-language\/review\/?hl=45363"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2560,
        "row_id":50771,
        "rank":11,
        "Model":"SOHO",
        "mlmodel":{

        },
        "method_short":"SOHO",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-07",
        "metrics":{
            "Accuracy":"76.37"
        },
        "raw_metrics":{
            "Accuracy":76.37
        },
        "uses_additional_data":false,
        "paper":{
            "id":776622,
            "title":"Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning",
            "url":"\/paper\/seeing-out-of-the-box-end-to-end-pre-training",
            "published":"2021-04-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/seeing-out-of-the-box-end-to-end-pre-training\/review\/?hl=50771"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2560,
        "row_id":35250,
        "rank":12,
        "Model":"ViLT-B\/32",
        "mlmodel":{

        },
        "method_short":"ViLT-B\/32",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-05",
        "metrics":{
            "Accuracy":"75.7"
        },
        "raw_metrics":{
            "Accuracy":75.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":742872,
            "title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
            "url":"\/paper\/vilt-vision-and-language-transformer-without",
            "published":"2021-02-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vilt-vision-and-language-transformer-without\/review\/?hl=35250"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2560,
        "row_id":14817,
        "rank":13,
        "Model":"LXMERT (Pre-train + scratch)",
        "mlmodel":{

        },
        "method_short":"LXMERT ",
        "method_details":"Pre-train + scratch",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-20",
        "metrics":{
            "Accuracy":"74.9"
        },
        "raw_metrics":{
            "Accuracy":74.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":150646,
            "title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
            "url":"\/paper\/lxmert-learning-cross-modality-encoder",
            "published":"2019-08-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lxmert-learning-cross-modality-encoder\/review\/?hl=14817"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2560,
        "row_id":14818,
        "rank":14,
        "Model":"VisualBERT",
        "mlmodel":{

        },
        "method_short":"VisualBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-09",
        "metrics":{
            "Accuracy":"66.7"
        },
        "raw_metrics":{
            "Accuracy":66.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":149599,
            "title":"VisualBERT: A Simple and Performant Baseline for Vision and Language",
            "url":"\/paper\/visualbert-a-simple-and-performant-baseline",
            "published":"2019-08-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visualbert-a-simple-and-performant-baseline\/review\/?hl=14818"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]