[
    {
        "table_id":2562,
        "row_id":64303,
        "rank":1,
        "Model":"BEiT-3",
        "mlmodel":{

        },
        "method_short":"BEiT-3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-22",
        "metrics":{
            "Accuracy":"92.58"
        },
        "raw_metrics":{
            "Accuracy":92.58
        },
        "uses_additional_data":false,
        "paper":{
            "id":1062207,
            "title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
            "url":"\/paper\/image-as-a-foreign-language-beit-pretraining",
            "published":"2022-08-22T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2562,
        "row_id":80508,
        "rank":2,
        "Model":"X2-VLM (large)",
        "mlmodel":{

        },
        "method_short":"X2-VLM ",
        "method_details":"large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-22",
        "metrics":{
            "Accuracy":"89.4"
        },
        "raw_metrics":{
            "Accuracy":89.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1116038,
            "title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
            "url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for",
            "published":"2022-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for\/review\/?hl=80508"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2562,
        "row_id":103926,
        "rank":3,
        "Model":"XFM (base)",
        "mlmodel":{

        },
        "method_short":"XFM ",
        "method_details":"base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-12",
        "metrics":{
            "Accuracy":"88.4"
        },
        "raw_metrics":{
            "Accuracy":88.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1141541,
            "title":"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",
            "url":"\/paper\/toward-building-general-foundation-models-for",
            "published":"2023-01-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/toward-building-general-foundation-models-for\/review\/?hl=103926"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2562,
        "row_id":54171,
        "rank":4,
        "Model":"CoCa",
        "mlmodel":{

        },
        "method_short":"CoCa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Accuracy":"87.0"
        },
        "raw_metrics":{
            "Accuracy":87.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=54171"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2562,
        "row_id":80507,
        "rank":5,
        "Model":"X2-VLM (base)",
        "mlmodel":{

        },
        "method_short":"X2-VLM ",
        "method_details":"base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-22",
        "metrics":{
            "Accuracy":"87.0"
        },
        "raw_metrics":{
            "Accuracy":87.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1116038,
            "title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
            "url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for",
            "published":"2022-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for\/review\/?hl=80507"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2562,
        "row_id":43951,
        "rank":6,
        "Model":"VLMo",
        "mlmodel":{

        },
        "method_short":"VLMo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-03",
        "metrics":{
            "Accuracy":"86.86"
        },
        "raw_metrics":{
            "Accuracy":86.86
        },
        "uses_additional_data":false,
        "paper":{
            "id":900016,
            "title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts",
            "url":"\/paper\/vlmo-unified-vision-language-pre-training",
            "published":"2021-11-03T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2562,
        "row_id":43428,
        "rank":7,
        "Model":"SimVLM",
        "mlmodel":{

        },
        "method_short":"SimVLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-08-24",
        "metrics":{
            "Accuracy":"85.15"
        },
        "raw_metrics":{
            "Accuracy":85.15
        },
        "uses_additional_data":false,
        "paper":{
            "id":856712,
            "title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
            "url":"\/paper\/simvlm-simple-visual-language-model",
            "published":"2021-08-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/simvlm-simple-visual-language-model\/review\/?hl=43428"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2562,
        "row_id":47858,
        "rank":8,
        "Model":"X-VLM (base)",
        "mlmodel":{

        },
        "method_short":"X-VLM ",
        "method_details":"base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-16",
        "metrics":{
            "Accuracy":"84.76"
        },
        "raw_metrics":{
            "Accuracy":84.76
        },
        "uses_additional_data":false,
        "paper":{
            "id":911012,
            "title":"Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts",
            "url":"\/paper\/multi-grained-vision-language-pre-training",
            "published":"2021-11-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multi-grained-vision-language-pre-training\/review\/?hl=47858"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2562,
        "row_id":45362,
        "rank":9,
        "Model":"ALBEF (14M)",
        "mlmodel":{

        },
        "method_short":"ALBEF ",
        "method_details":"14M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-16",
        "metrics":{
            "Accuracy":"82.55"
        },
        "raw_metrics":{
            "Accuracy":82.55
        },
        "uses_additional_data":false,
        "paper":{
            "id":836928,
            "title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
            "url":"\/paper\/align-before-fuse-vision-and-language",
            "published":"2021-07-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/align-before-fuse-vision-and-language\/review\/?hl=45362"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2562,
        "row_id":14823,
        "rank":10,
        "Model":"UNITER (Large)",
        "mlmodel":{

        },
        "method_short":"UNITER ",
        "method_details":"Large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-25",
        "metrics":{
            "Accuracy":"79.5"
        },
        "raw_metrics":{
            "Accuracy":79.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":156206,
            "title":"UNITER: UNiversal Image-TExt Representation Learning",
            "url":"\/paper\/uniter-learning-universal-image-text-1",
            "published":"2019-09-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/uniter-learning-universal-image-text-1\/review\/?hl=14823"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2562,
        "row_id":50772,
        "rank":11,
        "Model":"SOHO",
        "mlmodel":{

        },
        "method_short":"SOHO",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-07",
        "metrics":{
            "Accuracy":"77.32"
        },
        "raw_metrics":{
            "Accuracy":77.32
        },
        "uses_additional_data":false,
        "paper":{
            "id":776622,
            "title":"Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning",
            "url":"\/paper\/seeing-out-of-the-box-end-to-end-pre-training",
            "published":"2021-04-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/seeing-out-of-the-box-end-to-end-pre-training\/review\/?hl=50772"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2562,
        "row_id":14824,
        "rank":12,
        "Model":"LXMERT",
        "mlmodel":{

        },
        "method_short":"LXMERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-20",
        "metrics":{
            "Accuracy":"76.2"
        },
        "raw_metrics":{
            "Accuracy":76.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":150646,
            "title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
            "url":"\/paper\/lxmert-learning-cross-modality-encoder",
            "published":"2019-08-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lxmert-learning-cross-modality-encoder\/review\/?hl=14824"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2562,
        "row_id":35251,
        "rank":13,
        "Model":"ViLT-B\/32",
        "mlmodel":{

        },
        "method_short":"ViLT-B\/32",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-05",
        "metrics":{
            "Accuracy":"76.13"
        },
        "raw_metrics":{
            "Accuracy":76.13
        },
        "uses_additional_data":false,
        "paper":{
            "id":742872,
            "title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
            "url":"\/paper\/vilt-vision-and-language-transformer-without",
            "published":"2021-02-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vilt-vision-and-language-transformer-without\/review\/?hl=35251"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]