[
    {
        "table_id":1234,
        "row_id":98797,
        "rank":1,
        "Model":"MVD (Kinetics400 pretrain, ViT-H, 16 frame)",
        "mlmodel":{

        },
        "method_short":"MVD ",
        "method_details":"Kinetics400 pretrain, ViT-H, 16 frame",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "Top-1 Accuracy":"77.3",
            "Top-5 Accuracy":"95.7",
            "Parameters":"633",
            "GFLOPs":"1192x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":77.3,
            "Top-5 Accuracy":95.7,
            "Parameters":633.0,
            "GFLOPs":1192.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1125592,
            "title":"Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning",
            "url":"\/paper\/masked-video-distillation-rethinking-masked",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-video-distillation-rethinking-masked\/review\/?hl=98797"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":86739,
        "rank":2,
        "Model":"InternVideo",
        "mlmodel":{

        },
        "method_short":"InternVideo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Top-1 Accuracy":"77.2",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":77.2,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124231,
            "title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
            "url":"\/paper\/internvideo-general-video-foundation-models",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internvideo-general-video-foundation-models\/review\/?hl=86739"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":100530,
        "rank":3,
        "Model":"VideoMAE V2-g",
        "mlmodel":{

        },
        "method_short":"VideoMAE V2-g",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-29",
        "metrics":{
            "Top-1 Accuracy":"77.0",
            "Top-5 Accuracy":"95.9",
            "Parameters":"1013",
            "GFLOPs":"2544x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":77.0,
            "Top-5 Accuracy":95.9,
            "Parameters":1013.0,
            "GFLOPs":2544.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1182705,
            "title":"VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
            "url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders",
            "published":"2023-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders\/review\/?hl=100530"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":98799,
        "rank":4,
        "Model":"MVD (Kinetics400 pretrain, ViT-L, 16 frame)",
        "mlmodel":{

        },
        "method_short":"MVD ",
        "method_details":"Kinetics400 pretrain, ViT-L, 16 frame",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "Top-1 Accuracy":"76.7",
            "Top-5 Accuracy":"95.5",
            "Parameters":"305",
            "GFLOPs":"597x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":76.7,
            "Top-5 Accuracy":95.5,
            "Parameters":305.0,
            "GFLOPs":597.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1125592,
            "title":"Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning",
            "url":"\/paper\/masked-video-distillation-rethinking-masked",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-video-distillation-rethinking-masked\/review\/?hl=98799"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":104386,
        "rank":5,
        "Model":"Hiera-L (no extra data)",
        "mlmodel":{

        },
        "method_short":"Hiera-L ",
        "method_details":"no extra data",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-01",
        "metrics":{
            "Top-1 Accuracy":"76.5",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":76.5,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1221231,
            "title":"Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
            "url":"\/paper\/hiera-a-hierarchical-vision-transformer",
            "published":"2023-06-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hiera-a-hierarchical-vision-transformer\/review\/?hl=104386"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":86968,
        "rank":6,
        "Model":"TubeViT-L",
        "mlmodel":{

        },
        "method_short":"TubeViT-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Top-1 Accuracy":"76.1",
            "Top-5 Accuracy":"95.2",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":76.1,
            "Top-5 Accuracy":95.2,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1124229,
            "title":"Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning",
            "url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for\/review\/?hl=86968"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":49993,
        "rank":7,
        "Model":"VideoMAE (no extra data, ViT-L, 32x2)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"no extra data, ViT-L, 32x2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "Top-1 Accuracy":"75.4",
            "Top-5 Accuracy":"95.2",
            "Parameters":"305",
            "GFLOPs":"1436x3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":75.4,
            "Top-5 Accuracy":95.2,
            "Parameters":305.0,
            "GFLOPs":1436.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=49993"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":112695,
        "rank":8,
        "Model":"Side4Video (EVA ViT-E\/14)",
        "mlmodel":{

        },
        "method_short":"Side4Video ",
        "method_details":"EVA ViT-E\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Top-1 Accuracy":"75.2",
            "Top-5 Accuracy":"94.0",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":75.2,
            "Top-5 Accuracy":94.0,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327957,
            "title":"Side4Video: Spatial-Temporal Side Network for Memory-Efficient Image-to-Video Transfer Learning",
            "url":"\/paper\/side4video-spatial-temporal-side-network-for",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/side4video-spatial-temporal-side-network-for\/review\/?hl=112695"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":44632,
        "rank":9,
        "Model":"MaskFeat (Kinetics600 pretrain, MViT-L)",
        "mlmodel":{

        },
        "method_short":"MaskFeat ",
        "method_details":"Kinetics600 pretrain, MViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-16",
        "metrics":{
            "Top-1 Accuracy":"75.0",
            "Top-5 Accuracy":"95.0",
            "Parameters":"218",
            "GFLOPs":"2828*3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":75.0,
            "Top-5 Accuracy":95.0,
            "Parameters":218.0,
            "GFLOPs":2828.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":932132,
            "title":"Masked Feature Prediction for Self-Supervised Visual Pre-Training",
            "url":"\/paper\/masked-feature-prediction-for-self-supervised",
            "published":"2021-12-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-feature-prediction-for-self-supervised\/review\/?hl=44632"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":154,
                "name":"MViT",
                "color":"#d327c5"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":60457,
        "rank":10,
        "Model":"MAR (50% mask, ViT-L, 16x4)",
        "mlmodel":{

        },
        "method_short":"MAR ",
        "method_details":"50% mask, ViT-L, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-24",
        "metrics":{
            "Top-1 Accuracy":"74.7",
            "Top-5 Accuracy":"94.9",
            "Parameters":"311",
            "GFLOPs":"276x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":74.7,
            "Top-5 Accuracy":94.9,
            "Parameters":311.0,
            "GFLOPs":276.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1048887,
            "title":"MAR: Masked Autoencoders for Efficient Action Recognition",
            "url":"\/paper\/mar-masked-autoencoders-for-efficient-action",
            "published":"2022-07-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mar-masked-autoencoders-for-efficient-action\/review\/?hl=60457"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":106200,
        "rank":11,
        "Model":"ATM",
        "mlmodel":{

        },
        "method_short":"ATM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Top-1 Accuracy":"74.6",
            "Top-5 Accuracy":"94.4",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":74.6,
            "Top-5 Accuracy":94.4,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248104,
            "title":"What Can Simple Arithmetic Operations Do for Temporal Modeling?",
            "url":"\/paper\/what-can-simple-arithmetic-operations-do-for",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/what-can-simple-arithmetic-operations-do-for\/review\/?hl=106200"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":100966,
        "rank":12,
        "Model":"MAWS (ViT-L)",
        "mlmodel":{

        },
        "method_short":"MAWS ",
        "method_details":"ViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "Top-1 Accuracy":"74.4",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":74.4,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1179156,
            "title":"The effectiveness of MAE pre-pretraining for billion-scale pretraining",
            "url":"\/paper\/the-effectiveness-of-mae-pre-pretraining-for",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":49994,
        "rank":13,
        "Model":"VideoMAE (no extra data, ViT-L, 16frame)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"no extra data, ViT-L, 16frame",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "Top-1 Accuracy":"74.3",
            "Top-5 Accuracy":"94.6",
            "Parameters":"305",
            "GFLOPs":"597x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":74.3,
            "Top-5 Accuracy":94.6,
            "Parameters":305.0,
            "GFLOPs":597.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=49994"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":60458,
        "rank":14,
        "Model":"MAR (75% mask, ViT-L, 16x4)",
        "mlmodel":{

        },
        "method_short":"MAR ",
        "method_details":"75% mask, ViT-L, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-24",
        "metrics":{
            "Top-1 Accuracy":"73.8",
            "Top-5 Accuracy":"94.4",
            "Parameters":"311",
            "GFLOPs":"131x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":73.8,
            "Top-5 Accuracy":94.4,
            "Parameters":311.0,
            "GFLOPs":131.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1048887,
            "title":"MAR: Masked Autoencoders for Efficient Action Recognition",
            "url":"\/paper\/mar-masked-autoencoders-for-efficient-action",
            "published":"2022-07-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mar-masked-autoencoders-for-efficient-action\/review\/?hl=60458"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":98800,
        "rank":15,
        "Model":"MVD (Kinetics400 pretrain, ViT-B, 16 frame)",
        "mlmodel":{

        },
        "method_short":"MVD ",
        "method_details":"Kinetics400 pretrain, ViT-B, 16 frame",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "Top-1 Accuracy":"73.7",
            "Top-5 Accuracy":"94.0",
            "Parameters":"87",
            "GFLOPs":"180x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":73.7,
            "Top-5 Accuracy":94.0,
            "Parameters":87.0,
            "GFLOPs":180.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1125592,
            "title":"Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning",
            "url":"\/paper\/masked-video-distillation-rethinking-masked",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-video-distillation-rethinking-masked\/review\/?hl=98800"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":112987,
        "rank":16,
        "Model":"ViC-MAE (ViT-L)",
        "mlmodel":{

        },
        "method_short":"ViC-MAE ",
        "method_details":"ViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-21",
        "metrics":{
            "Top-1 Accuracy":"73.7",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":73.7,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1177739,
            "title":"ViC-MAE: Self-Supervised Representation Learning from Images and Video with Contrastive Masked Autoencoders",
            "url":"\/paper\/visual-representation-learning-from-unlabeled",
            "published":"2023-03-21T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/visual-representation-learning-from-unlabeled\/review\/?hl=112987"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":107495,
        "rank":17,
        "Model":"TAdaFormer-L\/14",
        "mlmodel":{

        },
        "method_short":"TAdaFormer-L\/14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-10",
        "metrics":{
            "Top-1 Accuracy":"73.6",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":73.6,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1262390,
            "title":"Temporally-Adaptive Models for Efficient Video Understanding",
            "url":"\/paper\/temporally-adaptive-models-for-efficient",
            "published":"2023-08-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporally-adaptive-models-for-efficient\/review\/?hl=107495"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":53605,
        "rank":18,
        "Model":"MViTv2-L (IN-21K + Kinetics400 pretrain)",
        "mlmodel":{

        },
        "method_short":"MViTv2-L ",
        "method_details":"IN-21K + Kinetics400 pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top-1 Accuracy":"73.3",
            "Top-5 Accuracy":"94.1",
            "Parameters":"213.1",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":73.3,
            "Top-5 Accuracy":94.1,
            "Parameters":213.1,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=53605"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":77325,
        "rank":19,
        "Model":"UniFormerV2-L",
        "mlmodel":{

        },
        "method_short":"UniFormerV2-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-22",
        "metrics":{
            "Top-1 Accuracy":"73.0",
            "Top-5 Accuracy":"94.5",
            "Parameters":null,
            "GFLOPs":"5154"
        },
        "raw_metrics":{
            "Top-1 Accuracy":73.0,
            "Top-5 Accuracy":94.5,
            "Parameters":null,
            "GFLOPs":5154.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1087805,
            "title":"UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer",
            "url":"\/paper\/uniformerv2-spatiotemporal-learning-by-arming",
            "published":"2022-09-22T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":58242,
        "rank":20,
        "Model":"ST-Adapter (ViT-L, CLIP)",
        "mlmodel":{

        },
        "method_short":"ST-Adapter ",
        "method_details":"ViT-L, CLIP",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-27",
        "metrics":{
            "Top-1 Accuracy":"72.3",
            "Top-5 Accuracy":"93.9",
            "Parameters":null,
            "GFLOPs":"8248"
        },
        "raw_metrics":{
            "Top-1 Accuracy":72.3,
            "Top-5 Accuracy":93.9,
            "Parameters":null,
            "GFLOPs":8248.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1034453,
            "title":"ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning",
            "url":"\/paper\/parameter-efficient-image-to-video-transfer",
            "published":"2022-06-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/parameter-efficient-image-to-video-transfer\/review\/?hl=58242"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":145,
                "name":"ViT",
                "color":"#2771D3"
            },
            {
                "id":256,
                "name":"Efficient Adaptation",
                "color":"#a527d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":109642,
        "rank":21,
        "Model":"ZeroI2V ViT-L\/14",
        "mlmodel":{

        },
        "method_short":"ZeroI2V ViT-L\/14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-02",
        "metrics":{
            "Top-1 Accuracy":"72.2",
            "Top-5 Accuracy":"93.0",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":72.2,
            "Top-5 Accuracy":93.0,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1292264,
            "title":"ZeroI2V: Zero-Cost Adaptation of Pre-trained Transformers from Image to Video",
            "url":"\/paper\/zeroi2v-zero-cost-adaptation-of-pre-trained",
            "published":"2023-10-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/zeroi2v-zero-cost-adaptation-of-pre-trained\/review\/?hl=109642"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":43871,
        "rank":22,
        "Model":"MViT-B (IN-21K + Kinetics400 pretrain)",
        "mlmodel":{

        },
        "method_short":"MViT-B ",
        "method_details":"IN-21K + Kinetics400 pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top-1 Accuracy":"72.1",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":"225x3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":72.1,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":225.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=43871"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":154,
                "name":"MViT",
                "color":"#d327c5"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":114808,
        "rank":23,
        "Model":"CAST-B\/16",
        "mlmodel":{

        },
        "method_short":"CAST-B\/16",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-30",
        "metrics":{
            "Top-1 Accuracy":"71.6",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":71.6,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1333277,
            "title":"CAST: Cross-Attention in Space and Time for Video Action Recognition",
            "url":"\/paper\/cast-cross-attention-in-space-and-time-for-1",
            "published":"2023-11-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/cast-cross-attention-in-space-and-time-for-1\/review\/?hl=114808"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":45666,
        "rank":24,
        "Model":"OMNIVORE (Swin-B,  IN-21K+ Kinetics400 pretrain)",
        "mlmodel":{

        },
        "method_short":"OMNIVORE ",
        "method_details":"Swin-B,  IN-21K+ Kinetics400 pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-20",
        "metrics":{
            "Top-1 Accuracy":"71.4",
            "Top-5 Accuracy":"93.5",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":71.4,
            "Top-5 Accuracy":93.5,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":948292,
            "title":"Omnivore: A Single Model for Many Visual Modalities",
            "url":"\/paper\/omnivore-a-single-model-for-many-visual",
            "published":"2022-01-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/omnivore-a-single-model-for-many-visual\/review\/?hl=45666"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":44233,
        "rank":25,
        "Model":"BEVT (IN-1K + Kinetics400 pretrain)",
        "mlmodel":{

        },
        "method_short":"BEVT ",
        "method_details":"IN-1K + Kinetics400 pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top-1 Accuracy":"71.4",
            "Top-5 Accuracy":"-",
            "Parameters":"89",
            "GFLOPs":"321x3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":71.4,
            "Top-5 Accuracy":0,
            "Parameters":89.0,
            "GFLOPs":321.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":924690,
            "title":"BEVT: BERT Pretraining of Video Transformers",
            "url":"\/paper\/bevt-bert-pretraining-of-video-transformers",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":44420,
        "rank":26,
        "Model":"UniFormer-B (IN-1K + Kinetics400 pretrain)",
        "mlmodel":{

        },
        "method_short":"UniFormer-B ",
        "method_details":"IN-1K + Kinetics400 pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top-1 Accuracy":"71.2",
            "Top-5 Accuracy":"92.8",
            "Parameters":"50.1",
            "GFLOPs":"259x3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":71.2,
            "Top-5 Accuracy":92.8,
            "Parameters":50.1,
            "GFLOPs":259.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":883979,
            "title":"UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning",
            "url":"\/paper\/uniformer-unified-transformer-for-efficient",
            "published":"2021-09-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":173,
                "name":"UniFormer",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":107496,
        "rank":27,
        "Model":"TAdaConvNeXtV2-B",
        "mlmodel":{

        },
        "method_short":"TAdaConvNeXtV2-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-10",
        "metrics":{
            "Top-1 Accuracy":"71.1",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":71.1,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1262390,
            "title":"Temporally-Adaptive Models for Efficient Video Understanding",
            "url":"\/paper\/temporally-adaptive-models-for-efficient",
            "published":"2023-08-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporally-adaptive-models-for-efficient\/review\/?hl=107496"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":60459,
        "rank":28,
        "Model":"MAR (50% mask, ViT-B, 16x4)",
        "mlmodel":{

        },
        "method_short":"MAR ",
        "method_details":"50% mask, ViT-B, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-24",
        "metrics":{
            "Top-1 Accuracy":"71.0",
            "Top-5 Accuracy":"92.8",
            "Parameters":"94",
            "GFLOPs":"86x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":71.0,
            "Top-5 Accuracy":92.8,
            "Parameters":94.0,
            "GFLOPs":86.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1048887,
            "title":"MAR: Masked Autoencoders for Efficient Action Recognition",
            "url":"\/paper\/mar-masked-autoencoders-for-efficient-action",
            "published":"2022-07-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mar-masked-autoencoders-for-efficient-action\/review\/?hl=60459"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":98801,
        "rank":29,
        "Model":"MVD (Kinetics400 pretrain, ViT-S, 16 frame)",
        "mlmodel":{

        },
        "method_short":"MVD ",
        "method_details":"Kinetics400 pretrain, ViT-S, 16 frame",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "Top-1 Accuracy":"70.9",
            "Top-5 Accuracy":"92.8",
            "Parameters":"22",
            "GFLOPs":"57x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":70.9,
            "Top-5 Accuracy":92.8,
            "Parameters":22.0,
            "GFLOPs":57.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1125592,
            "title":"Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning",
            "url":"\/paper\/masked-video-distillation-rethinking-masked",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-video-distillation-rethinking-masked\/review\/?hl=98801"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":44477,
        "rank":30,
        "Model":"CoVeR(JFT-3B)",
        "mlmodel":{

        },
        "method_short":"CoVeR",
        "method_details":"JFT-3B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-14",
        "metrics":{
            "Top-1 Accuracy":"70.9",
            "Top-5 Accuracy":"92.5",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":70.9,
            "Top-5 Accuracy":92.5,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":931113,
            "title":"Co-training Transformer with Videos and Images Improves Action Recognition",
            "url":"\/paper\/co-training-transformer-with-videos-and",
            "published":"2021-12-14T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":49995,
        "rank":31,
        "Model":"VideoMAE (no extra data, ViT-B, 16frame)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"no extra data, ViT-B, 16frame",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "Top-1 Accuracy":"70.8",
            "Top-5 Accuracy":"92.4",
            "Parameters":"87",
            "GFLOPs":"180x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":70.8,
            "Top-5 Accuracy":92.4,
            "Parameters":87.0,
            "GFLOPs":180.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=49995"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":110512,
        "rank":32,
        "Model":"ILA (ViT-L\/14)",
        "mlmodel":{

        },
        "method_short":"ILA ",
        "method_details":"ViT-L\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-20",
        "metrics":{
            "Top-1 Accuracy":"70.2",
            "Top-5 Accuracy":"91.8",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":70.2,
            "Top-5 Accuracy":91.8,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1194756,
            "title":"Implicit Temporal Modeling with Learnable Alignment for Video Recognition",
            "url":"\/paper\/implicit-temporal-modeling-with-learnable",
            "published":"2023-04-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/implicit-temporal-modeling-with-learnable\/review\/?hl=110512"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":43446,
        "rank":33,
        "Model":"MorphMLP-B (IN-1K)",
        "mlmodel":{

        },
        "method_short":"MorphMLP-B ",
        "method_details":"IN-1K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-24",
        "metrics":{
            "Top-1 Accuracy":"70.1",
            "Top-5 Accuracy":"92.8",
            "Parameters":"68.5",
            "GFLOPs":"197x3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":70.1,
            "Top-5 Accuracy":92.8,
            "Parameters":68.5,
            "GFLOPs":197.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":916214,
            "title":"MorphMLP: An Efficient MLP-Like Backbone for Spatial-Temporal Representation Learning",
            "url":"\/paper\/morphmlp-a-self-attention-free-mlp-like",
            "published":"2021-11-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/morphmlp-a-self-attention-free-mlp-like\/review\/?hl=43446"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":18,
                "name":"MLP",
                "color":"#ffae00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":44479,
        "rank":34,
        "Model":"CoVeR(JFT-300M)",
        "mlmodel":{

        },
        "method_short":"CoVeR",
        "method_details":"JFT-300M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-14",
        "metrics":{
            "Top-1 Accuracy":"69.8",
            "Top-5 Accuracy":"91.9",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":69.8,
            "Top-5 Accuracy":91.9,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":931113,
            "title":"Co-training Transformer with Videos and Images Improves Action Recognition",
            "url":"\/paper\/co-training-transformer-with-videos-and",
            "published":"2021-12-14T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":79063,
        "rank":35,
        "Model":"TPS",
        "mlmodel":{

        },
        "method_short":"TPS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-27",
        "metrics":{
            "Top-1 Accuracy":"69.8",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":69.8,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1050620,
            "title":"Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition",
            "url":"\/paper\/spatiotemporal-self-attention-modeling-with",
            "published":"2022-07-27T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":79064,
        "rank":36,
        "Model":"SIFA",
        "mlmodel":{

        },
        "method_short":"SIFA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-14",
        "metrics":{
            "Top-1 Accuracy":"69.8",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":69.8,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1026933,
            "title":"Stand-Alone Inter-Frame Attention in Video Models",
            "url":"\/paper\/stand-alone-inter-frame-attention-in-video-1",
            "published":"2022-06-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/stand-alone-inter-frame-attention-in-video-1\/review\/?hl=79064"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":47,
                "name":"swin-transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":35528,
        "rank":37,
        "Model":"Swin-B (IN-21K + Kinetics400 pretrain)",
        "mlmodel":{

        },
        "method_short":"Swin-B ",
        "method_details":"IN-21K + Kinetics400 pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Top-1 Accuracy":"69.6",
            "Top-5 Accuracy":"92.7",
            "Parameters":"89",
            "GFLOPs":"321x3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":69.6,
            "Top-5 Accuracy":92.7,
            "Parameters":89.0,
            "GFLOPs":321.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":824241,
            "title":"Video Swin Transformer",
            "url":"\/paper\/video-swin-transformer",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-swin-transformer\/review\/?hl=35528"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":43596,
        "rank":38,
        "Model":"TDN ResNet101 (one clip, three crop, 8+16 ensemble, ImageNet pretrained, RGB only)",
        "mlmodel":{

        },
        "method_short":"TDN ResNet101 ",
        "method_details":"one clip, three crop, 8+16 ensemble, ImageNet pretrained, RGB only",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-18",
        "metrics":{
            "Top-1 Accuracy":"69.6",
            "Top-5 Accuracy":"92.2",
            "Parameters":null,
            "GFLOPs":"198x3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":69.6,
            "Top-5 Accuracy":92.2,
            "Parameters":null,
            "GFLOPs":198.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":729685,
            "title":"TDN: Temporal Difference Networks for Efficient Action Recognition",
            "url":"\/paper\/tdn-temporal-difference-networks-for",
            "published":"2020-12-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tdn-temporal-difference-networks-for\/review\/?hl=43596"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":60460,
        "rank":39,
        "Model":"MAR (75% mask, ViT-B, 16x4)",
        "mlmodel":{

        },
        "method_short":"MAR ",
        "method_details":"75% mask, ViT-B, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-24",
        "metrics":{
            "Top-1 Accuracy":"69.5",
            "Top-5 Accuracy":"91.9",
            "Parameters":"94",
            "GFLOPs":"41x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":69.5,
            "Top-5 Accuracy":91.9,
            "Parameters":94.0,
            "GFLOPs":41.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1048887,
            "title":"MAR: Masked Autoencoders for Efficient Action Recognition",
            "url":"\/paper\/mar-masked-autoencoders-for-efficient-action",
            "published":"2022-07-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mar-masked-autoencoders-for-efficient-action\/review\/?hl=60460"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":40967,
        "rank":40,
        "Model":"ORViT Mformer-L (ORViT blocks)",
        "mlmodel":{

        },
        "method_short":"ORViT Mformer-L ",
        "method_details":"ORViT blocks",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-13",
        "metrics":{
            "Top-1 Accuracy":"69.5",
            "Top-5 Accuracy":"91.5",
            "Parameters":"N\/A",
            "GFLOPs":"N\/A"
        },
        "raw_metrics":{
            "Top-1 Accuracy":69.5,
            "Top-5 Accuracy":91.5,
            "Parameters":0,
            "GFLOPs":0
        },
        "uses_additional_data":true,
        "paper":{
            "id":887844,
            "title":"Object-Region Video Transformers",
            "url":"\/paper\/object-region-video-transformers-1",
            "published":"2021-10-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/object-region-video-transformers-1\/review\/?hl=40967"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":44421,
        "rank":41,
        "Model":"UniFormer-S (IN-1K + Kinetics600 pretrain)",
        "mlmodel":{

        },
        "method_short":"UniFormer-S ",
        "method_details":"IN-1K + Kinetics600 pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top-1 Accuracy":"69.4",
            "Top-5 Accuracy":"92.1",
            "Parameters":"21.4",
            "GFLOPs":"41.8x3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":69.4,
            "Top-5 Accuracy":92.1,
            "Parameters":21.4,
            "GFLOPs":41.8
        },
        "uses_additional_data":true,
        "paper":{
            "id":883979,
            "title":"UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning",
            "url":"\/paper\/uniformer-unified-transformer-for-efficient",
            "published":"2021-09-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":173,
                "name":"UniFormer",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":21558,
        "rank":42,
        "Model":"MML (ensemble)",
        "mlmodel":{

        },
        "method_short":"MML ",
        "method_details":"ensemble",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-04",
        "metrics":{
            "Top-1 Accuracy":"69.02",
            "Top-5 Accuracy":"92.70",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":69.02,
            "Top-5 Accuracy":92.7,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":233064,
            "title":"Mutual Modality Learning for Video Action Classification",
            "url":"\/paper\/mutual-modality-learning-for-video-action",
            "published":"2020-11-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mutual-modality-learning-for-video-action\/review\/?hl=21558"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":30548,
        "rank":43,
        "Model":"MViT-B-24, 32x3",
        "mlmodel":{

        },
        "method_short":"MViT-B-24, 32x3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Top-1 Accuracy":"68.7",
            "Top-5 Accuracy":"91.5",
            "Parameters":"53.2M",
            "GFLOPs":"236x3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":68.7,
            "Top-5 Accuracy":91.5,
            "Parameters":53200000.0,
            "GFLOPs":236.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30548"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":154,
                "name":"MViT",
                "color":"#d327c5"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":45381,
        "rank":44,
        "Model":"MTV-B",
        "mlmodel":{

        },
        "method_short":"MTV-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-12",
        "metrics":{
            "Top-1 Accuracy":"68.5",
            "Top-5 Accuracy":"90.4",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":68.5,
            "Top-5 Accuracy":90.4,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":944453,
            "title":"Multiview Transformers for Video Recognition",
            "url":"\/paper\/multiview-transformers-for-video-recognition",
            "published":"2022-01-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiview-transformers-for-video-recognition\/review\/?hl=45381"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":172,
                "name":"MTV",
                "color":"#27b6d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":79067,
        "rank":45,
        "Model":"MLP-3D",
        "mlmodel":{

        },
        "method_short":"MLP-3D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-13",
        "metrics":{
            "Top-1 Accuracy":"68.5",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":68.5,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1025799,
            "title":"MLP-3D: A MLP-like 3D Architecture with Grouped Time Mixing",
            "url":"\/paper\/mlp-3d-a-mlp-like-3d-architecture-with-1",
            "published":"2022-06-13T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/mlp-3d-a-mlp-like-3d-architecture-with-1\/review\/?hl=79067"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":30758,
        "rank":46,
        "Model":"TDN ResNet101 (one clip, center crop, 8+16 ensemble, ImageNet pretrained, RGB only)",
        "mlmodel":{

        },
        "method_short":"TDN ResNet101 ",
        "method_details":"one clip, center crop, 8+16 ensemble, ImageNet pretrained, RGB only",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-18",
        "metrics":{
            "Top-1 Accuracy":"68.2",
            "Top-5 Accuracy":"91.6",
            "Parameters":null,
            "GFLOPs":"198x1"
        },
        "raw_metrics":{
            "Top-1 Accuracy":68.2,
            "Top-5 Accuracy":91.6,
            "Parameters":null,
            "GFLOPs":198.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":729685,
            "title":"TDN: Temporal Difference Networks for Efficient Action Recognition",
            "url":"\/paper\/tdn-temporal-difference-networks-for",
            "published":"2020-12-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tdn-temporal-difference-networks-for\/review\/?hl=30758"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":97430,
        "rank":47,
        "Model":"MSMA (8+16frames)",
        "mlmodel":{

        },
        "method_short":"MSMA ",
        "method_details":"8+16frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-19",
        "metrics":{
            "Top-1 Accuracy":"68.2",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":68.2,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1162721,
            "title":"Multi-scale Motion-Aware Module for Video Action Recognition",
            "url":"\/paper\/multi-scale-motion-aware-module-for-video",
            "published":"2023-02-19T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":34648,
        "rank":48,
        "Model":"Mformer-L",
        "mlmodel":{

        },
        "method_short":"Mformer-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-09",
        "metrics":{
            "Top-1 Accuracy":"68.1",
            "Top-5 Accuracy":"91.2",
            "Parameters":"N\/A",
            "GFLOPs":"1181x3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":68.1,
            "Top-5 Accuracy":91.2,
            "Parameters":0,
            "GFLOPs":1181.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":815328,
            "title":"Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers",
            "url":"\/paper\/keeping-your-eye-on-the-ball-trajectory",
            "published":"2021-06-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/keeping-your-eye-on-the-ball-trajectory\/review\/?hl=34648"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":35418,
        "rank":49,
        "Model":"VIMPAC",
        "mlmodel":{

        },
        "method_short":"VIMPAC",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-21",
        "metrics":{
            "Top-1 Accuracy":"68.1",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":68.1,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":821705,
            "title":"VIMPAC: Video Pre-Training via Masked Token Prediction and Contrastive Learning",
            "url":"\/paper\/vimpac-video-pre-training-via-masked-token",
            "published":"2021-06-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vimpac-video-pre-training-via-masked-token\/review\/?hl=35418"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":40966,
        "rank":50,
        "Model":"ORViT Mformer (ORViT blocks)",
        "mlmodel":{

        },
        "method_short":"ORViT Mformer ",
        "method_details":"ORViT blocks",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-13",
        "metrics":{
            "Top-1 Accuracy":"67.9",
            "Top-5 Accuracy":"90.5",
            "Parameters":"N\/A",
            "GFLOPs":"N\/A"
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.9,
            "Top-5 Accuracy":90.5,
            "Parameters":0,
            "GFLOPs":0
        },
        "uses_additional_data":true,
        "paper":{
            "id":887844,
            "title":"Object-Region Video Transformers",
            "url":"\/paper\/object-region-video-transformers-1",
            "published":"2021-10-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/object-region-video-transformers-1\/review\/?hl=40966"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":30547,
        "rank":51,
        "Model":"MViT-B, 32x3(Kinetics600 pretrain)",
        "mlmodel":{

        },
        "method_short":"MViT-B, 32x3",
        "method_details":"Kinetics600 pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Top-1 Accuracy":"67.8",
            "Top-5 Accuracy":"91.3",
            "Parameters":"36.6",
            "GFLOPs":"170x3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.8,
            "Top-5 Accuracy":91.3,
            "Parameters":36.6,
            "GFLOPs":170.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30547"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":154,
                "name":"MViT",
                "color":"#d327c5"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":98679,
        "rank":52,
        "Model":"GC-TDN Ensemble (R50,8+16)",
        "mlmodel":{

        },
        "method_short":"GC-TDN Ensemble ",
        "method_details":"R50,8+16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-18",
        "metrics":{
            "Top-1 Accuracy":"67.8",
            "Top-5 Accuracy":"91.2",
            "Parameters":"27.4",
            "GFLOPs":"110.1"
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.8,
            "Top-5 Accuracy":91.2,
            "Parameters":27.4,
            "GFLOPs":110.1
        },
        "uses_additional_data":true,
        "paper":{
            "id":979691,
            "title":"Group Contextualization for Video Recognition",
            "url":"\/paper\/group-contextualization-for-video-recognition",
            "published":"2022-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/group-contextualization-for-video-recognition\/review\/?hl=98679"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":33,
                "name":"ResNet-50",
                "color":"#c17b9b"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":36595,
        "rank":53,
        "Model":"CT-Net Ensemble (R50, 8+12+16+24)",
        "mlmodel":{

        },
        "method_short":"CT-Net Ensemble ",
        "method_details":"R50, 8+12+16+24",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-03",
        "metrics":{
            "Top-1 Accuracy":"67.8",
            "Top-5 Accuracy":"91.1",
            "Parameters":"83.8",
            "GFLOPs":"280"
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.8,
            "Top-5 Accuracy":91.1,
            "Parameters":83.8,
            "GFLOPs":280.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":810992,
            "title":"CT-Net: Channel Tensorization Network for Video Classification",
            "url":"\/paper\/ct-net-channel-tensorization-network-for-1",
            "published":"2021-06-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ct-net-channel-tensorization-network-for-1\/review\/?hl=36595"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":87528,
        "rank":54,
        "Model":"TCM (Ensemble)",
        "mlmodel":{

        },
        "method_short":"TCM ",
        "method_details":"Ensemble",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-24",
        "metrics":{
            "Top-1 Accuracy":"67.8",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.8,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":967245,
            "title":"Motion-driven Visual Tempo Learning for Video-based Action Recognition",
            "url":"\/paper\/slow-fast-visual-tempo-learning-for-video",
            "published":"2022-02-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slow-fast-visual-tempo-learning-for-video\/review\/?hl=87528"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":33,
                "name":"ResNet-50",
                "color":"#c17b9b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":38659,
        "rank":55,
        "Model":"SELFYNet-TSM-R50En (8+16 frames, ImageNet pretrained, 2 clips)",
        "mlmodel":{

        },
        "method_short":"SELFYNet-TSM-R50En ",
        "method_details":"8+16 frames, ImageNet pretrained, 2 clips",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-14",
        "metrics":{
            "Top-1 Accuracy":"67.7",
            "Top-5 Accuracy":"91.1",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.7,
            "Top-5 Accuracy":91.1,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":745413,
            "title":"Learning Self-Similarity in Space and Time as Generalized Motion for Video Action Recognition",
            "url":"\/paper\/learning-self-similarity-in-space-and-time-as-1",
            "published":"2021-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-self-similarity-in-space-and-time-as-1\/review\/?hl=38659"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":44006,
        "rank":56,
        "Model":"RSANet-R50 (8+16 frames, ImageNet pretrained, 2 clips",
        "mlmodel":{

        },
        "method_short":"RSANet-R50 ",
        "method_details":"8+16 frames, ImageNet pretrained, 2 clips",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-02",
        "metrics":{
            "Top-1 Accuracy":"67.7",
            "Top-5 Accuracy":"91.1",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.7,
            "Top-5 Accuracy":91.1,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":899314,
            "title":"Relational Self-Attention: What's Missing in Attention for Video Understanding",
            "url":"\/paper\/relational-self-attention-what-s-missing-in",
            "published":"2021-11-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/relational-self-attention-what-s-missing-in\/review\/?hl=44006"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":87532,
        "rank":57,
        "Model":"GTDNet",
        "mlmodel":{

        },
        "method_short":"GTDNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-23",
        "metrics":{
            "Top-1 Accuracy":"67.6",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.6,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1128067,
            "title":"Global Temporal Difference Network for Action Recognition",
            "url":"\/paper\/global-temporal-difference-network-for-action",
            "published":"2022-11-23T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":38657,
        "rank":58,
        "Model":"SELFYNet-TSM-R50En (8+16 frames, ImageNet pretrained, a single clip)",
        "mlmodel":{

        },
        "method_short":"SELFYNet-TSM-R50En ",
        "method_details":"8+16 frames, ImageNet pretrained, a single clip",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-14",
        "metrics":{
            "Top-1 Accuracy":"67.4",
            "Top-5 Accuracy":"91",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.4,
            "Top-5 Accuracy":91.0,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":745413,
            "title":"Learning Self-Similarity in Space and Time as Generalized Motion for Video Action Recognition",
            "url":"\/paper\/learning-self-similarity-in-space-and-time-as-1",
            "published":"2021-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-self-similarity-in-space-and-time-as-1\/review\/?hl=38657"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":22151,
        "rank":59,
        "Model":"VoV3D-L (32frames, Kinetics pretrained, single)",
        "mlmodel":{

        },
        "method_short":"VoV3D-L ",
        "method_details":"32frames, Kinetics pretrained, single",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Top-1 Accuracy":"67.35",
            "Top-5 Accuracy":"90.50",
            "Parameters":"5.8M",
            "GFLOPs":"20.9x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.35,
            "Top-5 Accuracy":90.5,
            "Parameters":5800000.0,
            "GFLOPs":20.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":238450,
            "title":"Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification",
            "url":"\/paper\/diverse-temporal-aggregation-and-depthwise",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diverse-temporal-aggregation-and-depthwise\/review\/?hl=22151"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":103730,
        "rank":60,
        "Model":"PLAR",
        "mlmodel":{

        },
        "method_short":"PLAR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-21",
        "metrics":{
            "Top-1 Accuracy":"67.3",
            "Top-5 Accuracy":"91",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.3,
            "Top-5 Accuracy":91.0,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1213055,
            "title":"PLAR: Prompt Learning for Action Recognition",
            "url":"\/paper\/prompt-learning-for-action-recognition",
            "published":"2023-05-21T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/prompt-learning-for-action-recognition\/review\/?hl=103730"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":44004,
        "rank":61,
        "Model":"RSANet-R50 (8+16 frames, ImageNet pretrained, a single clip)",
        "mlmodel":{

        },
        "method_short":"RSANet-R50 ",
        "method_details":"8+16 frames, ImageNet pretrained, a single clip",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-02",
        "metrics":{
            "Top-1 Accuracy":"67.3",
            "Top-5 Accuracy":"90.8",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.3,
            "Top-5 Accuracy":90.8,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":899314,
            "title":"Relational Self-Attention: What's Missing in Attention for Video Understanding",
            "url":"\/paper\/relational-self-attention-what-s-missing-in",
            "published":"2021-11-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/relational-self-attention-what-s-missing-in\/review\/?hl=44004"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":34830,
        "rank":62,
        "Model":"X-Vit (x16)",
        "mlmodel":{

        },
        "method_short":"X-Vit ",
        "method_details":"x16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-10",
        "metrics":{
            "Top-1 Accuracy":"67.2",
            "Top-5 Accuracy":"90.8",
            "Parameters":"N\/A",
            "GFLOPs":"850x1"
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.2,
            "Top-5 Accuracy":90.8,
            "Parameters":0,
            "GFLOPs":850.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":815239,
            "title":"Space-time Mixing Attention for Video Transformer",
            "url":"\/paper\/space-time-mixing-attention-for-video",
            "published":"2021-06-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/space-time-mixing-attention-for-video\/review\/?hl=34830"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":45711,
        "rank":63,
        "Model":"TAda2D-En (ResNet-50, 8+16 frames)",
        "mlmodel":{

        },
        "method_short":"TAda2D-En ",
        "method_details":"ResNet-50, 8+16 frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-12",
        "metrics":{
            "Top-1 Accuracy":"67.2",
            "Top-5 Accuracy":"89.8",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.2,
            "Top-5 Accuracy":89.8,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":887048,
            "title":"TAda! Temporally-Adaptive Convolutions for Video Understanding",
            "url":"\/paper\/tada-temporally-adaptive-convolutions-for-1",
            "published":"2021-10-12T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":34649,
        "rank":64,
        "Model":"Mformer-HR",
        "mlmodel":{

        },
        "method_short":"Mformer-HR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-09",
        "metrics":{
            "Top-1 Accuracy":"67.1",
            "Top-5 Accuracy":"90.6",
            "Parameters":"N\/A",
            "GFLOPs":"958.8x3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.1,
            "Top-5 Accuracy":90.6,
            "Parameters":0,
            "GFLOPs":958.8
        },
        "uses_additional_data":true,
        "paper":{
            "id":815328,
            "title":"Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers",
            "url":"\/paper\/keeping-your-eye-on-the-ball-trajectory",
            "published":"2021-06-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/keeping-your-eye-on-the-ball-trajectory\/review\/?hl=34649"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":51066,
        "rank":65,
        "Model":"TAdaConvNeXt-T",
        "mlmodel":{

        },
        "method_short":"TAdaConvNeXt-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-12",
        "metrics":{
            "Top-1 Accuracy":"67.1",
            "Top-5 Accuracy":"90.4",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.1,
            "Top-5 Accuracy":90.4,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":887048,
            "title":"TAda! Temporally-Adaptive Convolutions for Video Understanding",
            "url":"\/paper\/tada-temporally-adaptive-convolutions-for-1",
            "published":"2021-10-12T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":212,
                "name":"ConvNeXt",
                "color":"#77bb41"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":87530,
        "rank":66,
        "Model":"MoDS (8+16frames)",
        "mlmodel":{

        },
        "method_short":"MoDS ",
        "method_details":"8+16frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-15",
        "metrics":{
            "Top-1 Accuracy":"67.1",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.1,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1128066,
            "title":"Action Recognition With Motion Diversification and Dynamic Selection",
            "url":"\/paper\/action-recognition-with-motion",
            "published":"2022-07-15T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":33,
                "name":"ResNet-50",
                "color":"#c17b9b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":87525,
        "rank":67,
        "Model":"STPG (8+16frames)",
        "mlmodel":{

        },
        "method_short":"STPG ",
        "method_details":"8+16frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-09",
        "metrics":{
            "Top-1 Accuracy":"67.0",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.0,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1128057,
            "title":"Spatial-Temporal Pyramid Graph Reasoning for Action Recognition",
            "url":"\/paper\/spatial-temporal-pyramid-graph-reasoning-for",
            "published":"2022-08-09T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":33,
                "name":"ResNet-50",
                "color":"#c17b9b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":21559,
        "rank":68,
        "Model":"MML (single)",
        "mlmodel":{

        },
        "method_short":"MML ",
        "method_details":"single",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-04",
        "metrics":{
            "Top-1 Accuracy":"66.83",
            "Top-5 Accuracy":"91.30",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":66.83,
            "Top-5 Accuracy":91.3,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":233064,
            "title":"Mutual Modality Learning for Video Action Classification",
            "url":"\/paper\/mutual-modality-learning-for-video-action",
            "published":"2020-11-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mutual-modality-learning-for-video-action\/review\/?hl=21559"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":110513,
        "rank":69,
        "Model":"ILA (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"ILA ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-20",
        "metrics":{
            "Top-1 Accuracy":"66.8",
            "Top-5 Accuracy":"90.3",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":66.8,
            "Top-5 Accuracy":90.3,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1194756,
            "title":"Implicit Temporal Modeling with Learnable Alignment for Video Recognition",
            "url":"\/paper\/implicit-temporal-modeling-with-learnable",
            "published":"2023-04-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/implicit-temporal-modeling-with-learnable\/review\/?hl=110513"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":12953,
        "rank":70,
        "Model":"TSM (RGB + Flow)",
        "mlmodel":{

        },
        "method_short":"TSM ",
        "method_details":"RGB + Flow",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-11-20",
        "metrics":{
            "Top-1 Accuracy":"66.6",
            "Top-5 Accuracy":"91.3",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":66.6,
            "Top-5 Accuracy":91.3,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":62848,
            "title":"TSM: Temporal Shift Module for Efficient Video Understanding",
            "url":"\/paper\/temporal-shift-module-for-efficient-video",
            "published":"2018-11-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporal-shift-module-for-efficient-video\/review\/?hl=12953"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":20732,
        "rank":71,
        "Model":"MSNet-R50En (8+16 ensemble, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"MSNet-R50En ",
        "method_details":"8+16 ensemble, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-20",
        "metrics":{
            "Top-1 Accuracy":"66.6",
            "Top-5 Accuracy":"90.6",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":66.6,
            "Top-5 Accuracy":90.6,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":209431,
            "title":"MotionSqueeze: Neural Motion Feature Learning for Video Understanding",
            "url":"\/paper\/motionsqueeze-neural-motion-feature-learning",
            "published":"2020-07-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/motionsqueeze-neural-motion-feature-learning\/review\/?hl=20732"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":19045,
        "rank":72,
        "Model":"PAN ResNet101 (RGB only, no Flow)",
        "mlmodel":{

        },
        "method_short":"PAN ResNet101 ",
        "method_details":"RGB only, no Flow",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-08-08",
        "metrics":{
            "Top-1 Accuracy":"66.5",
            "Top-5 Accuracy":"90.6",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":66.5,
            "Top-5 Accuracy":90.6,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":212522,
            "title":"PAN: Towards Fast Action Recognition via Learning Persistence of Appearance",
            "url":"\/paper\/pan-towards-fast-action-recognition-via",
            "published":"2020-08-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pan-towards-fast-action-recognition-via\/review\/?hl=19045"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":15909,
        "rank":73,
        "Model":"TSM+W3 (16 frames, RGB ResNet-50)",
        "mlmodel":{

        },
        "method_short":"TSM+W3 ",
        "method_details":"16 frames, RGB ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-02",
        "metrics":{
            "Top-1 Accuracy":"66.5",
            "Top-5 Accuracy":"90.4",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":66.5,
            "Top-5 Accuracy":90.4,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":189470,
            "title":"Knowing What, Where and When to Look: Efficient Video Action Modeling with Attention",
            "url":"\/paper\/knowing-what-where-and-when-to-look-efficient",
            "published":"2020-04-02T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/knowing-what-where-and-when-to-look-efficient\/review\/?hl=15909"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":34647,
        "rank":74,
        "Model":"Mformer",
        "mlmodel":{

        },
        "method_short":"Mformer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-09",
        "metrics":{
            "Top-1 Accuracy":"66.5",
            "Top-5 Accuracy":"90.1",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":66.5,
            "Top-5 Accuracy":90.1,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":815328,
            "title":"Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers",
            "url":"\/paper\/keeping-your-eye-on-the-ball-trajectory",
            "published":"2021-06-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/keeping-your-eye-on-the-ball-trajectory\/review\/?hl=34647"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":34626,
        "rank":75,
        "Model":"MVFNet-ResNet50 (center crop, 8+16 ensemble, ImageNet pretrained, RGB only)",
        "mlmodel":{

        },
        "method_short":"MVFNet-ResNet50 ",
        "method_details":"center crop, 8+16 ensemble, ImageNet pretrained, RGB only",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-13",
        "metrics":{
            "Top-1 Accuracy":"66.3",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":66.3,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":727533,
            "title":"MVFNet: Multi-View Fusion Network for Efficient Video Recognition",
            "url":"\/paper\/mvfnet-multi-view-fusion-network-for",
            "published":"2020-12-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mvfnet-multi-view-fusion-network-for\/review\/?hl=34626"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":30546,
        "rank":76,
        "Model":"MViT-B, 16x4",
        "mlmodel":{

        },
        "method_short":"MViT-B, 16x4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Top-1 Accuracy":"66.2",
            "Top-5 Accuracy":"90.2",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":66.2,
            "Top-5 Accuracy":90.2,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30546"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":44003,
        "rank":77,
        "Model":"RSANet-R50 (16 frames, ImageNet pretrained, a single clip)",
        "mlmodel":{

        },
        "method_short":"RSANet-R50 ",
        "method_details":"16 frames, ImageNet pretrained, a single clip",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-02",
        "metrics":{
            "Top-1 Accuracy":"66",
            "Top-5 Accuracy":"89.8",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":66.0,
            "Top-5 Accuracy":89.8,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":899314,
            "title":"Relational Self-Attention: What's Missing in Attention for Video Understanding",
            "url":"\/paper\/relational-self-attention-what-s-missing-in",
            "published":"2021-11-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/relational-self-attention-what-s-missing-in\/review\/?hl=44003"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":22152,
        "rank":78,
        "Model":"VoV3D-L (32frames, from scratch, single)",
        "mlmodel":{

        },
        "method_short":"VoV3D-L ",
        "method_details":"32frames, from scratch, single",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Top-1 Accuracy":"65.8",
            "Top-5 Accuracy":"89.5",
            "Parameters":"5.8M",
            "GFLOPs":"20.9x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":65.8,
            "Top-5 Accuracy":89.5,
            "Parameters":5800000.0,
            "GFLOPs":20.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":238450,
            "title":"Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification",
            "url":"\/paper\/diverse-temporal-aggregation-and-depthwise",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diverse-temporal-aggregation-and-depthwise\/review\/?hl=22152"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":98906,
        "rank":79,
        "Model":"E3D-L",
        "mlmodel":{

        },
        "method_short":"E3D-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-05",
        "metrics":{
            "Top-1 Accuracy":"65.7",
            "Top-5 Accuracy":"89.8",
            "Parameters":null,
            "GFLOPs":"18.3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":65.7,
            "Top-5 Accuracy":89.8,
            "Parameters":null,
            "GFLOPs":18.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1168453,
            "title":"Maximizing Spatio-Temporal Entropy of Deep 3D CNNs for Efficient Video Recognition",
            "url":"\/paper\/maximizing-spatio-temporal-entropy-of-deep-3d",
            "published":"2023-03-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maximizing-spatio-temporal-entropy-of-deep-3d\/review\/?hl=98906"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":38655,
        "rank":80,
        "Model":"SELFYNet-TSM-R50 (16 frames, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"SELFYNet-TSM-R50 ",
        "method_details":"16 frames, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-14",
        "metrics":{
            "Top-1 Accuracy":"65.7",
            "Top-5 Accuracy":"89.8",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":65.7,
            "Top-5 Accuracy":89.8,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":745413,
            "title":"Learning Self-Similarity in Space and Time as Generalized Motion for Video Action Recognition",
            "url":"\/paper\/learning-self-similarity-in-space-and-time-as-1",
            "published":"2021-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-self-similarity-in-space-and-time-as-1\/review\/?hl=38655"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":45712,
        "rank":81,
        "Model":"TAda2D (ResNet-50, 16 frames)",
        "mlmodel":{

        },
        "method_short":"TAda2D ",
        "method_details":"ResNet-50, 16 frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-12",
        "metrics":{
            "Top-1 Accuracy":"65.6",
            "Top-5 Accuracy":"89.2",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":65.6,
            "Top-5 Accuracy":89.2,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":887048,
            "title":"TAda! Temporally-Adaptive Convolutions for Video Understanding",
            "url":"\/paper\/tada-temporally-adaptive-convolutions-for-1",
            "published":"2021-10-12T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":29014,
        "rank":82,
        "Model":"ViViT-L\/16x2 Fact. encoder",
        "mlmodel":{

        },
        "method_short":"ViViT-L\/16x2 Fact. encoder",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top-1 Accuracy":"65.4",
            "Top-5 Accuracy":"89.8",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":65.4,
            "Top-5 Accuracy":89.8,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":758440,
            "title":"ViViT: A Video Vision Transformer",
            "url":"\/paper\/2103-15691",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-15691\/review\/?hl=29014"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":22153,
        "rank":83,
        "Model":"VoV3D-M (32frames, Kinetics pretrained, single)",
        "mlmodel":{

        },
        "method_short":"VoV3D-M ",
        "method_details":"32frames, Kinetics pretrained, single",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Top-1 Accuracy":"65.24",
            "Top-5 Accuracy":"89.48",
            "Parameters":"3.3M",
            "GFLOPs":"11.5x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":65.24,
            "Top-5 Accuracy":89.48,
            "Parameters":3300000.0,
            "GFLOPs":11.5
        },
        "uses_additional_data":true,
        "paper":{
            "id":238450,
            "title":"Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification",
            "url":"\/paper\/diverse-temporal-aggregation-and-depthwise",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diverse-temporal-aggregation-and-depthwise\/review\/?hl=22153"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":25431,
        "rank":84,
        "Model":"bLVNet",
        "mlmodel":{

        },
        "method_short":"bLVNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-02",
        "metrics":{
            "Top-1 Accuracy":"65.2",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":65.2,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":175169,
            "title":"More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation",
            "url":"\/paper\/more-is-less-learning-efficient-video-1",
            "published":"2019-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/more-is-less-learning-efficient-video-1\/review\/?hl=25431"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":50632,
        "rank":85,
        "Model":"DirecFormer",
        "mlmodel":{

        },
        "method_short":"DirecFormer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-19",
        "metrics":{
            "Top-1 Accuracy":"64.94",
            "Top-5 Accuracy":"87.9",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":64.94,
            "Top-5 Accuracy":87.9,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":980094,
            "title":"DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition",
            "url":"\/paper\/direcformer-a-directed-attention-in",
            "published":"2022-03-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/direcformer-a-directed-attention-in\/review\/?hl=50632"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":44002,
        "rank":86,
        "Model":"RSANet-R50 (8 frames, ImageNet pretrained, a single clip)",
        "mlmodel":{

        },
        "method_short":"RSANet-R50 ",
        "method_details":"8 frames, ImageNet pretrained, a single clip",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-02",
        "metrics":{
            "Top-1 Accuracy":"64.8",
            "Top-5 Accuracy":"89.1",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":64.8,
            "Top-5 Accuracy":89.1,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":899314,
            "title":"Relational Self-Attention: What's Missing in Attention for Video Understanding",
            "url":"\/paper\/relational-self-attention-what-s-missing-in",
            "published":"2021-11-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/relational-self-attention-what-s-missing-in\/review\/?hl=44002"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":20731,
        "rank":87,
        "Model":"MSNet-R50 (16 frames, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"MSNet-R50 ",
        "method_details":"16 frames, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-20",
        "metrics":{
            "Top-1 Accuracy":"64.7",
            "Top-5 Accuracy":"89.4",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":64.7,
            "Top-5 Accuracy":89.4,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":209431,
            "title":"MotionSqueeze: Neural Motion Feature Learning for Video Understanding",
            "url":"\/paper\/motionsqueeze-neural-motion-feature-learning",
            "published":"2020-07-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/motionsqueeze-neural-motion-feature-learning\/review\/?hl=20731"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":78914,
        "rank":88,
        "Model":"AK-Net",
        "mlmodel":{

        },
        "method_short":"AK-Net",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-17",
        "metrics":{
            "Top-1 Accuracy":"64.3",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":64.3,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":946180,
            "title":"Action Keypoint Network for Efficient Video Recognition",
            "url":"\/paper\/action-keypoint-network-for-efficient-video",
            "published":"2022-01-17T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/action-keypoint-network-for-efficient-video\/review\/?hl=78914"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":22154,
        "rank":89,
        "Model":"VoV3D-M (32frames, from scratch, single)",
        "mlmodel":{

        },
        "method_short":"VoV3D-M ",
        "method_details":"32frames, from scratch, single",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Top-1 Accuracy":"64.2",
            "Top-5 Accuracy":"88.8",
            "Parameters":"3.3M",
            "GFLOPs":"11.5x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":64.2,
            "Top-5 Accuracy":88.8,
            "Parameters":3300000.0,
            "GFLOPs":11.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":238450,
            "title":"Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification",
            "url":"\/paper\/diverse-temporal-aggregation-and-depthwise",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diverse-temporal-aggregation-and-depthwise\/review\/?hl=22154"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":22155,
        "rank":90,
        "Model":"VoV3D-L (16frames, from scratch, single)",
        "mlmodel":{

        },
        "method_short":"VoV3D-L ",
        "method_details":"16frames, from scratch, single",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Top-1 Accuracy":"64.1",
            "Top-5 Accuracy":"88.6",
            "Parameters":"5.8M",
            "GFLOPs":"9.3x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":64.1,
            "Top-5 Accuracy":88.6,
            "Parameters":5800000.0,
            "GFLOPs":9.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":238450,
            "title":"Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification",
            "url":"\/paper\/diverse-temporal-aggregation-and-depthwise",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diverse-temporal-aggregation-and-depthwise\/review\/?hl=22155"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":45713,
        "rank":91,
        "Model":"TAda2D (ResNet-50, 8 frames)",
        "mlmodel":{

        },
        "method_short":"TAda2D ",
        "method_details":"ResNet-50, 8 frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-12",
        "metrics":{
            "Top-1 Accuracy":"64.0",
            "Top-5 Accuracy":"88.0",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":64.0,
            "Top-5 Accuracy":88.0,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":887048,
            "title":"TAda! Temporally-Adaptive Convolutions for Video Understanding",
            "url":"\/paper\/tada-temporally-adaptive-convolutions-for-1",
            "published":"2021-10-12T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":28472,
        "rank":92,
        "Model":"MoViNet-A2",
        "mlmodel":{

        },
        "method_short":"MoViNet-A2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"63.5",
            "Top-5 Accuracy":"89.0",
            "Parameters":"4.8M",
            "GFLOPs":"10.3x1"
        },
        "raw_metrics":{
            "Top-1 Accuracy":63.5,
            "Top-5 Accuracy":89.0,
            "Parameters":4800000.0,
            "GFLOPs":10.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28472"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":22156,
        "rank":93,
        "Model":"VoV3D-M (16frames, from scratch, single)",
        "mlmodel":{

        },
        "method_short":"VoV3D-M ",
        "method_details":"16frames, from scratch, single",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Top-1 Accuracy":"63.2",
            "Top-5 Accuracy":"88.2",
            "Parameters":"3.3M",
            "GFLOPs":"5.7x6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":63.2,
            "Top-5 Accuracy":88.2,
            "Parameters":3300000.0,
            "GFLOPs":5.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":238450,
            "title":"Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification",
            "url":"\/paper\/diverse-temporal-aggregation-and-depthwise",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diverse-temporal-aggregation-and-depthwise\/review\/?hl=22156"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":20730,
        "rank":94,
        "Model":"MSNet-R50 (8 frames, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"MSNet-R50 ",
        "method_details":"8 frames, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-20",
        "metrics":{
            "Top-1 Accuracy":"63",
            "Top-5 Accuracy":"88.4",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":63.0,
            "Top-5 Accuracy":88.4,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":209431,
            "title":"MotionSqueeze: Neural Motion Feature Learning for Video Understanding",
            "url":"\/paper\/motionsqueeze-neural-motion-feature-learning",
            "published":"2020-07-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/motionsqueeze-neural-motion-feature-learning\/review\/?hl=20730"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":30112,
        "rank":95,
        "Model":"MoViNet-A1",
        "mlmodel":{

        },
        "method_short":"MoViNet-A1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"62.7",
            "Top-5 Accuracy":"89.0",
            "Parameters":"4.6M",
            "GFLOPs":"6.0x1"
        },
        "raw_metrics":{
            "Top-1 Accuracy":62.7,
            "Top-5 Accuracy":89.0,
            "Parameters":4600000.0,
            "GFLOPs":6.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=30112"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":69048,
        "rank":96,
        "Model":"OmniVL",
        "mlmodel":{

        },
        "method_short":"OmniVL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-15",
        "metrics":{
            "Top-1 Accuracy":"62.5",
            "Top-5 Accuracy":"86.2",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":62.5,
            "Top-5 Accuracy":86.2,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1075042,
            "title":"OmniVL:One Foundation Model for Image-Language and Video-Language Tasks",
            "url":"\/paper\/omnivl-one-foundation-model-for-image",
            "published":"2022-09-15T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivl-one-foundation-model-for-image\/review\/?hl=69048"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":25424,
        "rank":97,
        "Model":"TimeSformer-HR",
        "mlmodel":{

        },
        "method_short":"TimeSformer-HR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-09",
        "metrics":{
            "Top-1 Accuracy":"62.5",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":62.5,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":744187,
            "title":"Is Space-Time Attention All You Need for Video Understanding?",
            "url":"\/paper\/is-space-time-attention-all-you-need-for",
            "published":"2021-02-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/is-space-time-attention-all-you-need-for\/review\/?hl=25424"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":25426,
        "rank":98,
        "Model":"TimeSformer-L",
        "mlmodel":{

        },
        "method_short":"TimeSformer-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-09",
        "metrics":{
            "Top-1 Accuracy":"62.3",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":62.3,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":744187,
            "title":"Is Space-Time Attention All You Need for Video Understanding?",
            "url":"\/paper\/is-space-time-attention-all-you-need-for",
            "published":"2021-02-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/is-space-time-attention-all-you-need-for\/review\/?hl=25426"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":12954,
        "rank":99,
        "Model":"TRG (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"TRG ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-27",
        "metrics":{
            "Top-1 Accuracy":"62.2",
            "Top-5 Accuracy":"90.3",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":62.2,
            "Top-5 Accuracy":90.3,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":151332,
            "title":"Temporal Reasoning Graph for Activity Recognition",
            "url":"\/paper\/temporal-reasoning-graph-for-activity",
            "published":"2019-08-27T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/temporal-reasoning-graph-for-activity\/review\/?hl=12954"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":10888,
        "rank":100,
        "Model":"TPN (TSM-50)",
        "mlmodel":{

        },
        "method_short":"TPN ",
        "method_details":"TSM-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-07",
        "metrics":{
            "Top-1 Accuracy":"62.0",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":62.0,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":189848,
            "title":"Temporal Pyramid Network for Action Recognition",
            "url":"\/paper\/temporal-pyramid-network-for-action",
            "published":"2020-04-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporal-pyramid-network-for-action\/review\/?hl=10888"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":17088,
        "rank":101,
        "Model":"Multigrid",
        "mlmodel":{

        },
        "method_short":"Multigrid",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-02",
        "metrics":{
            "Top-1 Accuracy":"61.7",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":61.7,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":175171,
            "title":"A Multigrid Method for Efficiently Training Video Models",
            "url":"\/paper\/a-multigrid-method-for-efficiently-training",
            "published":"2019-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-multigrid-method-for-efficiently-training\/review\/?hl=17088"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":25429,
        "rank":102,
        "Model":"SlowFast",
        "mlmodel":{

        },
        "method_short":"SlowFast",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "Top-1 Accuracy":"61.7",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":61.7,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=25429"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":12955,
        "rank":103,
        "Model":"TRG (Inception-V3)",
        "mlmodel":{

        },
        "method_short":"TRG ",
        "method_details":"Inception-V3",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-27",
        "metrics":{
            "Top-1 Accuracy":"61.3",
            "Top-5 Accuracy":"91.4",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":61.3,
            "Top-5 Accuracy":91.4,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":151332,
            "title":"Temporal Reasoning Graph for Activity Recognition",
            "url":"\/paper\/temporal-reasoning-graph-for-activity",
            "published":"2019-08-27T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/temporal-reasoning-graph-for-activity\/review\/?hl=12955"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":28473,
        "rank":104,
        "Model":"MoViNet-A0",
        "mlmodel":{

        },
        "method_short":"MoViNet-A0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"61.3",
            "Top-5 Accuracy":"88.2",
            "Parameters":"3.1M",
            "GFLOPs":"2.7x1"
        },
        "raw_metrics":{
            "Top-1 Accuracy":61.3,
            "Top-5 Accuracy":88.2,
            "Parameters":3100000.0,
            "GFLOPs":2.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28473"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":6690,
        "rank":105,
        "Model":"CCS + two-stream + TRN",
        "mlmodel":{

        },
        "method_short":"CCS + two-stream + TRN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-27",
        "metrics":{
            "Top-1 Accuracy":"61.2",
            "Top-5 Accuracy":"89.3",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":61.2,
            "Top-5 Accuracy":89.3,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":151253,
            "title":"Cooperative Cross-Stream Network for Discriminative Action Representation",
            "url":"\/paper\/cooperative-cross-stream-network-for",
            "published":"2019-08-27T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/cooperative-cross-stream-network-for\/review\/?hl=6690"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":31464,
        "rank":106,
        "Model":"VidTr-L",
        "mlmodel":{

        },
        "method_short":"VidTr-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-23",
        "metrics":{
            "Top-1 Accuracy":"60.2",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":60.2,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787687,
            "title":"VidTr: Video Transformer Without Convolutions",
            "url":"\/paper\/vidtr-video-transformer-without-convolutions",
            "published":"2021-04-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/vidtr-video-transformer-without-convolutions\/review\/?hl=31464"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":25422,
        "rank":107,
        "Model":"TimeSformer",
        "mlmodel":{

        },
        "method_short":"TimeSformer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-09",
        "metrics":{
            "Top-1 Accuracy":"59.5",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":59.5,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":744187,
            "title":"Is Space-Time Attention All You Need for Video Understanding?",
            "url":"\/paper\/is-space-time-attention-all-you-need-for",
            "published":"2021-02-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/is-space-time-attention-all-you-need-for\/review\/?hl=25422"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":89417,
        "rank":108,
        "Model":"SVT (finetune)",
        "mlmodel":{

        },
        "method_short":"SVT ",
        "method_details":"finetune",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top-1 Accuracy":"59.2",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":59.2,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924698,
            "title":"Self-supervised Video Transformer",
            "url":"\/paper\/self-supervised-video-transformer",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-supervised-video-transformer\/review\/?hl=89417"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":6583,
        "rank":109,
        "Model":"TAM (5-shot)",
        "mlmodel":{

        },
        "method_short":"TAM ",
        "method_details":"5-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-27",
        "metrics":{
            "Top-1 Accuracy":"52.3",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":52.3,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":144094,
            "title":"Few-Shot Video Classification via Temporal Alignment",
            "url":"\/paper\/few-shot-video-classification-via-temporal",
            "published":"2019-06-27T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/few-shot-video-classification-via-temporal\/review\/?hl=6583"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":6579,
        "rank":110,
        "Model":"model3D_1 with left-right augmentation and fps jitter",
        "mlmodel":{

        },
        "method_short":"model3D_1 with left-right augmentation and fps jitter",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-10-01",
        "metrics":{
            "Top-1 Accuracy":"51.33",
            "Top-5 Accuracy":"80.46",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":51.33,
            "Top-5 Accuracy":80.46,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":21519,
            "title":"The \"something something\" video database for learning and evaluating visual common sense",
            "url":"\/paper\/the-something-something-video-database-for",
            "published":"2017-06-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/the-something-something-video-database-for\/review\/?hl=6579"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":6586,
        "rank":111,
        "Model":"Prob-Distill",
        "mlmodel":{

        },
        "method_short":"Prob-Distill",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-05",
        "metrics":{
            "Top-1 Accuracy":"49.9",
            "Top-5 Accuracy":"79.1",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":49.9,
            "Top-5 Accuracy":79.1,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":111002,
            "title":"Attention Distillation for Learning Video Representations",
            "url":"\/paper\/paying-more-attention-to-motion-attention",
            "published":"2019-04-05T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/paying-more-attention-to-motion-attention\/review\/?hl=6586"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":8208,
        "rank":112,
        "Model":"STM + TRNMultiscale",
        "mlmodel":{

        },
        "method_short":"STM + TRNMultiscale",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-11",
        "metrics":{
            "Top-1 Accuracy":"47.73",
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":47.73,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":153171,
            "title":"Comparative Analysis of CNN-based Spatiotemporal Reasoning in Videos",
            "url":"\/paper\/comparative-analysis-of-cnn-based",
            "published":"2019-09-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/comparative-analysis-of-cnn-based\/review\/?hl=8208"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":53604,
        "rank":113,
        "Model":"MViTv2-B (IN-21K + Kinetics400 pretrain)",
        "mlmodel":{

        },
        "method_short":"MViTv2-B ",
        "method_details":"IN-21K + Kinetics400 pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top-1 Accuracy":null,
            "Top-5 Accuracy":"93.4",
            "Parameters":"51.1",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":null,
            "Top-5 Accuracy":93.4,
            "Parameters":51.1,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=53604"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":44005,
        "rank":114,
        "Model":"RSANet-R50 (8+16 frames, ImageNet pretrained, 2 clips)",
        "mlmodel":{

        },
        "method_short":"RSANet-R50 ",
        "method_details":"8+16 frames, ImageNet pretrained, 2 clips",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-02",
        "metrics":{
            "Top-1 Accuracy":null,
            "Top-5 Accuracy":"91.1",
            "Parameters":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":null,
            "Top-5 Accuracy":91.1,
            "Parameters":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":899314,
            "title":"Relational Self-Attention: What's Missing in Attention for Video Understanding",
            "url":"\/paper\/relational-self-attention-what-s-missing-in",
            "published":"2021-11-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/relational-self-attention-what-s-missing-in\/review\/?hl=44005"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":30116,
        "rank":115,
        "Model":"MoViNet-A3",
        "mlmodel":{

        },
        "method_short":"MoViNet-A3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":null,
            "Top-5 Accuracy":null,
            "Parameters":"5.3M",
            "GFLOPs":"23.7x1"
        },
        "raw_metrics":{
            "Top-1 Accuracy":null,
            "Top-5 Accuracy":null,
            "Parameters":5300000.0,
            "GFLOPs":23.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=30116"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1234,
        "row_id":43828,
        "rank":116,
        "Model":"MViT-L (IN-21K + Kinetics400 pretrain)",
        "mlmodel":{

        },
        "method_short":"MViT-L ",
        "method_details":"IN-21K + Kinetics400 pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top-1 Accuracy":null,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":"2828x3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":null,
            "Top-5 Accuracy":null,
            "Parameters":null,
            "GFLOPs":2828.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=43828"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":154,
                "name":"MViT",
                "color":"#d327c5"
            }
        ],
        "reports":[

        ]
    }
]