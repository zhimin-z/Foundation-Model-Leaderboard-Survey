[
    {
        "table_id":71,
        "row_id":88766,
        "rank":1,
        "Model":"SparseGPT (175B, 50% Sparsity)",
        "mlmodel":{

        },
        "method_short":"SparseGPT ",
        "method_details":"175B, 50% Sparsity",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Test perplexity":"8.21",
            "Validation perplexity":null,
            "Number of params":null
        },
        "raw_metrics":{
            "Test perplexity":8.21,
            "Validation perplexity":null,
            "Number of params":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1136959,
            "title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
            "url":"\/paper\/massive-language-models-can-be-accurately",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/massive-language-models-can-be-accurately\/review\/?hl=88766"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":88764,
        "rank":2,
        "Model":"OPT-175B",
        "mlmodel":{

        },
        "method_short":"OPT-175B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Test perplexity":"8.34",
            "Validation perplexity":null,
            "Number of params":null
        },
        "raw_metrics":{
            "Test perplexity":8.34,
            "Validation perplexity":null,
            "Number of params":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1136959,
            "title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
            "url":"\/paper\/massive-language-models-can-be-accurately",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/massive-language-models-can-be-accurately\/review\/?hl=88764"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":88767,
        "rank":3,
        "Model":"SparseGPT (175B, 4:8 Sparsity)",
        "mlmodel":{

        },
        "method_short":"SparseGPT ",
        "method_details":"175B, 4:8 Sparsity",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Test perplexity":"8.45",
            "Validation perplexity":null,
            "Number of params":null
        },
        "raw_metrics":{
            "Test perplexity":8.45,
            "Validation perplexity":null,
            "Number of params":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1136959,
            "title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
            "url":"\/paper\/massive-language-models-can-be-accurately",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/massive-language-models-can-be-accurately\/review\/?hl=88767"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":88768,
        "rank":4,
        "Model":"SparseGPT (175B, 2:4 Sparsity)",
        "mlmodel":{

        },
        "method_short":"SparseGPT ",
        "method_details":"175B, 2:4 Sparsity",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Test perplexity":"8.73",
            "Validation perplexity":null,
            "Number of params":null
        },
        "raw_metrics":{
            "Test perplexity":8.73,
            "Validation perplexity":null,
            "Number of params":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1136959,
            "title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
            "url":"\/paper\/massive-language-models-can-be-accurately",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/massive-language-models-can-be-accurately\/review\/?hl=88768"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":61188,
        "rank":5,
        "Model":"GPT-2 (fine-tuned)",
        "mlmodel":{

        },
        "method_short":"GPT-2 ",
        "method_details":"fine-tuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-16",
        "metrics":{
            "Test perplexity":"15.17",
            "Validation perplexity":"15.69",
            "Number of params":"1542M"
        },
        "raw_metrics":{
            "Test perplexity":15.17,
            "Validation perplexity":15.69,
            "Number of params":1542000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":890258,
            "title":"Hydra: A System for Large Multi-Model Deep Learning",
            "url":"\/paper\/hydra-a-system-for-large-multi-model-deep",
            "published":"2021-10-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hydra-a-system-for-large-multi-model-deep\/review\/?hl=61188"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":4040,
        "rank":6,
        "Model":"GPT-2",
        "mlmodel":{

        },
        "method_short":"GPT-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Test perplexity":"18.34",
            "Validation perplexity":null,
            "Number of params":"1542M"
        },
        "raw_metrics":{
            "Test perplexity":18.34,
            "Validation perplexity":null,
            "Number of params":1542000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":105884,
            "title":"Language Models are Unsupervised Multitask Learners",
            "url":"\/paper\/language-models-are-unsupervised-multitask",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":65434,
        "rank":7,
        "Model":"GPT-2 (large)",
        "mlmodel":{

        },
        "method_short":"GPT-2 ",
        "method_details":"large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Test perplexity":"19.93",
            "Validation perplexity":null,
            "Number of params":"762M"
        },
        "raw_metrics":{
            "Test perplexity":19.93,
            "Validation perplexity":null,
            "Number of params":762000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":105884,
            "title":"Language Models are Unsupervised Multitask Learners",
            "url":"\/paper\/language-models-are-unsupervised-multitask",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":65435,
        "rank":8,
        "Model":"GPT-2 (medium)",
        "mlmodel":{

        },
        "method_short":"GPT-2 ",
        "method_details":"medium",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Test perplexity":"22.76",
            "Validation perplexity":null,
            "Number of params":"345M"
        },
        "raw_metrics":{
            "Test perplexity":22.76,
            "Validation perplexity":null,
            "Number of params":345000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":105884,
            "title":"Language Models are Unsupervised Multitask Learners",
            "url":"\/paper\/language-models-are-unsupervised-multitask",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":65436,
        "rank":9,
        "Model":"GPT-2 (small)",
        "mlmodel":{

        },
        "method_short":"GPT-2 ",
        "method_details":"small",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Test perplexity":"29.41",
            "Validation perplexity":null,
            "Number of params":"117M"
        },
        "raw_metrics":{
            "Test perplexity":29.41,
            "Validation perplexity":null,
            "Number of params":117000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":105884,
            "title":"Language Models are Unsupervised Multitask Learners",
            "url":"\/paper\/language-models-are-unsupervised-multitask",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":18143,
        "rank":10,
        "Model":"BERT-Large-CAS",
        "mlmodel":{

        },
        "method_short":"BERT-Large-CAS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-20",
        "metrics":{
            "Test perplexity":"34.1",
            "Validation perplexity":"37.7",
            "Number of params":"395M"
        },
        "raw_metrics":{
            "Test perplexity":34.1,
            "Validation perplexity":37.7,
            "Number of params":395000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":112444,
            "title":"Language Models with Transformers",
            "url":"\/paper\/190409408",
            "published":"2019-04-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/190409408\/review\/?hl=18143"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":26085,
        "rank":11,
        "Model":"Mogrifier LSTM + dynamic eval",
        "mlmodel":{

        },
        "method_short":"Mogrifier LSTM + dynamic eval",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-04",
        "metrics":{
            "Test perplexity":"38.6",
            "Validation perplexity":"40.2",
            "Number of params":"35M"
        },
        "raw_metrics":{
            "Test perplexity":38.6,
            "Validation perplexity":40.2,
            "Number of params":35000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":152261,
            "title":"Mogrifier LSTM",
            "url":"\/paper\/mogrifier-lstm",
            "published":"2019-09-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mogrifier-lstm\/review\/?hl=26085"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":6161,
        "rank":12,
        "Model":"adversarial + AWD-LSTM-MoS + dynamic eval",
        "mlmodel":{

        },
        "method_short":"adversarial + AWD-LSTM-MoS + dynamic eval",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-10",
        "metrics":{
            "Test perplexity":"38.65",
            "Validation perplexity":"40.27",
            "Number of params":"35M"
        },
        "raw_metrics":{
            "Test perplexity":38.65,
            "Validation perplexity":40.27,
            "Number of params":35000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":142020,
            "title":"Improving Neural Language Modeling via Adversarial Training",
            "url":"\/paper\/improving-neural-language-modeling-via",
            "published":"2019-06-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improving-neural-language-modeling-via\/review\/?hl=6161"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":3479,
        "rank":13,
        "Model":"FRAGE + AWD-LSTM-MoS + dynamic eval",
        "mlmodel":{

        },
        "method_short":"FRAGE + AWD-LSTM-MoS + dynamic eval",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-09-18",
        "metrics":{
            "Test perplexity":"39.14",
            "Validation perplexity":"40.85",
            "Number of params":"35M"
        },
        "raw_metrics":{
            "Test perplexity":39.14,
            "Validation perplexity":40.85,
            "Number of params":35000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":57578,
            "title":"FRAGE: Frequency-Agnostic Word Representation",
            "url":"\/paper\/frage-frequency-agnostic-word-representation",
            "published":"2018-09-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/frage-frequency-agnostic-word-representation\/review\/?hl=3479"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":3958,
        "rank":14,
        "Model":"Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.",
        "mlmodel":{

        },
        "method_short":"Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-08-14",
        "metrics":{
            "Test perplexity":"40.3",
            "Validation perplexity":"42.0",
            "Number of params":"35M"
        },
        "raw_metrics":{
            "Test perplexity":40.3,
            "Validation perplexity":42.0,
            "Number of params":35000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":55080,
            "title":"Improved Language Modeling by Decoding the Past",
            "url":"\/paper\/improved-language-modeling-by-decoding-the",
            "published":"2018-08-14T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/improved-language-modeling-by-decoding-the\/review\/?hl=3958"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":19298,
        "rank":15,
        "Model":"GL-LWGC + AWD-MoS-LSTM + dynamic eval",
        "mlmodel":{

        },
        "method_short":"GL-LWGC + AWD-MoS-LSTM + dynamic eval",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-05-21",
        "metrics":{
            "Test perplexity":"40.46",
            "Validation perplexity":"42.19",
            "Number of params":"38M"
        },
        "raw_metrics":{
            "Test perplexity":40.46,
            "Validation perplexity":42.19,
            "Number of params":38000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":3186,
            "title":"Gradual Learning of Recurrent Neural Networks",
            "url":"\/paper\/gradual-learning-of-recurrent-neural-networks",
            "published":"2017-08-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/gradual-learning-of-recurrent-neural-networks\/review\/?hl=19298"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":227,
        "rank":16,
        "Model":"AWD-LSTM-MoS + dynamic eval",
        "mlmodel":{

        },
        "method_short":"AWD-LSTM-MoS + dynamic eval",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-10",
        "metrics":{
            "Test perplexity":"40.68",
            "Validation perplexity":"42.41",
            "Number of params":"35M"
        },
        "raw_metrics":{
            "Test perplexity":40.68,
            "Validation perplexity":42.41,
            "Number of params":35000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":9001,
            "title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model",
            "url":"\/paper\/breaking-the-softmax-bottleneck-a-high-rank",
            "published":"2017-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/breaking-the-softmax-bottleneck-a-high-rank\/review\/?hl=227"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":9803,
        "rank":17,
        "Model":"AWD-LSTM-DRILL + dynamic eval",
        "mlmodel":{

        },
        "method_short":"AWD-LSTM-DRILL + dynamic eval",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-14",
        "metrics":{
            "Test perplexity":"42.0",
            "Validation perplexity":"43.9",
            "Number of params":"34M"
        },
        "raw_metrics":{
            "Test perplexity":42.0,
            "Validation perplexity":43.9,
            "Number of params":34000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":114504,
            "title":"Deep Residual Output Layers for Neural Language Generation",
            "url":"\/paper\/deep-residual-output-layers-for-neural",
            "published":"2019-05-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deep-residual-output-layers-for-neural\/review\/?hl=9803"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":228,
        "rank":18,
        "Model":"AWD-LSTM + dynamic eval",
        "mlmodel":{

        },
        "method_short":"AWD-LSTM + dynamic eval",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-09-21",
        "metrics":{
            "Test perplexity":"44.3",
            "Validation perplexity":"46.4",
            "Number of params":"33M"
        },
        "raw_metrics":{
            "Test perplexity":44.3,
            "Validation perplexity":46.4,
            "Number of params":33000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":15803,
            "title":"Dynamic Evaluation of Neural Sequence Models",
            "url":"\/paper\/dynamic-evaluation-of-neural-sequence-models",
            "published":"2017-09-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dynamic-evaluation-of-neural-sequence-models\/review\/?hl=228"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":229,
        "rank":19,
        "Model":"AWD-LSTM + continuous cache pointer",
        "mlmodel":{

        },
        "method_short":"AWD-LSTM + continuous cache pointer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-08-07",
        "metrics":{
            "Test perplexity":"52.0",
            "Validation perplexity":"53.8",
            "Number of params":"33M"
        },
        "raw_metrics":{
            "Test perplexity":52.0,
            "Validation perplexity":53.8,
            "Number of params":33000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":19155,
            "title":"Regularizing and Optimizing LSTM Language Models",
            "url":"\/paper\/regularizing-and-optimizing-lstm-language",
            "published":"2017-08-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/regularizing-and-optimizing-lstm-language\/review\/?hl=229"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":230,
        "rank":20,
        "Model":"AWD-LSTM-DOC x5",
        "mlmodel":{

        },
        "method_short":"AWD-LSTM-DOC x5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-08-30",
        "metrics":{
            "Test perplexity":"53.09",
            "Validation perplexity":"54.19",
            "Number of params":"185M"
        },
        "raw_metrics":{
            "Test perplexity":53.09,
            "Validation perplexity":54.19,
            "Number of params":185000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":55908,
            "title":"Direct Output Connection for a High-Rank Language Model",
            "url":"\/paper\/direct-output-connection-for-a-high-rank",
            "published":"2018-08-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/direct-output-connection-for-a-high-rank\/review\/?hl=230"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":114229,
        "rank":21,
        "Model":"Ensemble of All",
        "mlmodel":{

        },
        "method_short":"Ensemble of All",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "Test perplexity":"53.73",
            "Validation perplexity":"55.4",
            "Number of params":null
        },
        "raw_metrics":{
            "Test perplexity":53.73,
            "Validation perplexity":55.4,
            "Number of params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1339408,
            "title":"Advancing State of the Art in Language Modeling",
            "url":"\/paper\/advancing-state-of-the-art-in-language",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/advancing-state-of-the-art-in-language\/review\/?hl=114229"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":26086,
        "rank":22,
        "Model":"Mogrifier LSTM",
        "mlmodel":{

        },
        "method_short":"Mogrifier LSTM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-04",
        "metrics":{
            "Test perplexity":"55.1",
            "Validation perplexity":"57.3",
            "Number of params":"35M"
        },
        "raw_metrics":{
            "Test perplexity":55.1,
            "Validation perplexity":57.3,
            "Number of params":35000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":152261,
            "title":"Mogrifier LSTM",
            "url":"\/paper\/mogrifier-lstm",
            "published":"2019-09-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mogrifier-lstm\/review\/?hl=26086"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":4288,
        "rank":23,
        "Model":"AWD-LSTM-DOC + Partial Shuffle",
        "mlmodel":{

        },
        "method_short":"AWD-LSTM-DOC + Partial Shuffle",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-03-11",
        "metrics":{
            "Test perplexity":"57.85",
            "Validation perplexity":"60.16",
            "Number of params":"37M"
        },
        "raw_metrics":{
            "Test perplexity":57.85,
            "Validation perplexity":60.16,
            "Number of params":37000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":108137,
            "title":"Partially Shuffling the Training Data to Improve Language Models",
            "url":"\/paper\/partially-shuffling-the-training-data-to-1",
            "published":"2019-03-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/partially-shuffling-the-training-data-to-1\/review\/?hl=4288"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":231,
        "rank":24,
        "Model":"AWD-LSTM-DOC",
        "mlmodel":{

        },
        "method_short":"AWD-LSTM-DOC",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-08-30",
        "metrics":{
            "Test perplexity":"58.03",
            "Validation perplexity":"60.29",
            "Number of params":"37M"
        },
        "raw_metrics":{
            "Test perplexity":58.03,
            "Validation perplexity":60.29,
            "Number of params":37000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":55908,
            "title":"Direct Output Connection for a High-Rank Language Model",
            "url":"\/paper\/direct-output-connection-for-a-high-rank",
            "published":"2018-08-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/direct-output-connection-for-a-high-rank\/review\/?hl=231"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":4312,
        "rank":25,
        "Model":"AWD-LSTM-MoS + Partial Shuffle",
        "mlmodel":{

        },
        "method_short":"AWD-LSTM-MoS + Partial Shuffle",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-03-11",
        "metrics":{
            "Test perplexity":"59.98",
            "Validation perplexity":"62.38",
            "Number of params":"35M"
        },
        "raw_metrics":{
            "Test perplexity":59.98,
            "Validation perplexity":62.38,
            "Number of params":35000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":108137,
            "title":"Partially Shuffling the Training Data to Improve Language Models",
            "url":"\/paper\/partially-shuffling-the-training-data-to-1",
            "published":"2019-03-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/partially-shuffling-the-training-data-to-1\/review\/?hl=4312"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":232,
        "rank":26,
        "Model":"AWD-LSTM-MoS",
        "mlmodel":{

        },
        "method_short":"AWD-LSTM-MoS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-10",
        "metrics":{
            "Test perplexity":"61.45",
            "Validation perplexity":"63.88",
            "Number of params":"35M"
        },
        "raw_metrics":{
            "Test perplexity":61.45,
            "Validation perplexity":63.88,
            "Number of params":35000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":9001,
            "title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model",
            "url":"\/paper\/breaking-the-softmax-bottleneck-a-high-rank",
            "published":"2017-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/breaking-the-softmax-bottleneck-a-high-rank\/review\/?hl=232"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":39175,
        "rank":27,
        "Model":"AWD-FWM Schlag et al. (2020)",
        "mlmodel":{

        },
        "method_short":"AWD-FWM Schlag et al. ",
        "method_details":"2020",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-16",
        "metrics":{
            "Test perplexity":"61.65",
            "Validation perplexity":"54.48",
            "Number of params":"37M"
        },
        "raw_metrics":{
            "Test perplexity":61.65,
            "Validation perplexity":54.48,
            "Number of params":37000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":235331,
            "title":"Learning Associative Inference Using Fast Weight Memory",
            "url":"\/paper\/learning-associative-inference-using-fast-1",
            "published":"2020-11-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-associative-inference-using-fast-1\/review\/?hl=39175"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":9804,
        "rank":28,
        "Model":"AWD-LSTM-DRILL",
        "mlmodel":{

        },
        "method_short":"AWD-LSTM-DRILL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-14",
        "metrics":{
            "Test perplexity":"61.9",
            "Validation perplexity":"64.9",
            "Number of params":"34M"
        },
        "raw_metrics":{
            "Test perplexity":61.9,
            "Validation perplexity":64.9,
            "Number of params":34000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":114504,
            "title":"Deep Residual Output Layers for Neural Language Generation",
            "url":"\/paper\/deep-residual-output-layers-for-neural",
            "published":"2019-05-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deep-residual-output-layers-for-neural\/review\/?hl=9804"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":233,
        "rank":29,
        "Model":"AWD-LSTM 3-layer with Fraternal dropout",
        "mlmodel":{

        },
        "method_short":"AWD-LSTM 3-layer with Fraternal dropout",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-10-31",
        "metrics":{
            "Test perplexity":"64.1",
            "Validation perplexity":"66.8",
            "Number of params":"34M"
        },
        "raw_metrics":{
            "Test perplexity":64.1,
            "Validation perplexity":66.8,
            "Number of params":34000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":7245,
            "title":"Fraternal Dropout",
            "url":"\/paper\/fraternal-dropout",
            "published":"2017-10-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fraternal-dropout\/review\/?hl=233"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":26087,
        "rank":30,
        "Model":"AWD-LSTM + ATOI",
        "mlmodel":{

        },
        "method_short":"AWD-LSTM + ATOI",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-18",
        "metrics":{
            "Test perplexity":"64.73",
            "Validation perplexity":"67.47",
            "Number of params":"33M"
        },
        "raw_metrics":{
            "Test perplexity":64.73,
            "Validation perplexity":67.47,
            "Number of params":33000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":154344,
            "title":"Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes",
            "url":"\/paper\/alleviating-sequence-information-loss-with",
            "published":"2019-09-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/alleviating-sequence-information-loss-with\/review\/?hl=26087"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":234,
        "rank":31,
        "Model":"AWD-LSTM",
        "mlmodel":{

        },
        "method_short":"AWD-LSTM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-08-07",
        "metrics":{
            "Test perplexity":"65.8",
            "Validation perplexity":"68.6",
            "Number of params":"33M"
        },
        "raw_metrics":{
            "Test perplexity":65.8,
            "Validation perplexity":68.6,
            "Number of params":33000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":19155,
            "title":"Regularizing and Optimizing LSTM Language Models",
            "url":"\/paper\/regularizing-and-optimizing-lstm-language",
            "published":"2017-08-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/regularizing-and-optimizing-lstm-language\/review\/?hl=234"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":11127,
        "rank":32,
        "Model":"Melis et al. (2017) - 1-layer LSTM (tied)",
        "mlmodel":{

        },
        "method_short":"Melis et al. ",
        "method_details":"2017",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-07-18",
        "metrics":{
            "Test perplexity":"65.9",
            "Validation perplexity":"69.3",
            "Number of params":"24M"
        },
        "raw_metrics":{
            "Test perplexity":65.9,
            "Validation perplexity":69.3,
            "Number of params":24000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":14433,
            "title":"On the State of the Art of Evaluation in Neural Language Models",
            "url":"\/paper\/on-the-state-of-the-art-of-evaluation-in",
            "published":"2017-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/on-the-state-of-the-art-of-evaluation-in\/review\/?hl=11127"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":11128,
        "rank":33,
        "Model":"Grave et al. (2016) - LSTM + continuous cache pointer",
        "mlmodel":{

        },
        "method_short":"Grave et al. ",
        "method_details":"2016",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-12-13",
        "metrics":{
            "Test perplexity":"68.9",
            "Validation perplexity":null,
            "Number of params":null
        },
        "raw_metrics":{
            "Test perplexity":68.9,
            "Validation perplexity":null,
            "Number of params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":27765,
            "title":"Improving Neural Language Models with a Continuous Cache",
            "url":"\/paper\/improving-neural-language-models-with-a",
            "published":"2016-12-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improving-neural-language-models-with-a\/review\/?hl=11128"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":99294,
        "rank":34,
        "Model":"EGRU",
        "mlmodel":{

        },
        "method_short":"EGRU",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-13",
        "metrics":{
            "Test perplexity":"68.9",
            "Validation perplexity":null,
            "Number of params":null
        },
        "raw_metrics":{
            "Test perplexity":68.9,
            "Validation perplexity":null,
            "Number of params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1025986,
            "title":"Efficient recurrent architectures through activity sparsity and sparse back-propagation through time",
            "url":"\/paper\/egru-event-based-gru-for-activity-sparse",
            "published":"2022-06-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/egru-event-based-gru-for-activity-sparse\/review\/?hl=99294"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":11129,
        "rank":35,
        "Model":"Inan et al. (2016) - Variational LSTM (tied) (h=650) + augmented loss",
        "mlmodel":{

        },
        "method_short":"Inan et al. ",
        "method_details":"2016",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-11-04",
        "metrics":{
            "Test perplexity":"87.0",
            "Validation perplexity":"91.5",
            "Number of params":null
        },
        "raw_metrics":{
            "Test perplexity":87.0,
            "Validation perplexity":91.5,
            "Number of params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":25343,
            "title":"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling",
            "url":"\/paper\/tying-word-vectors-and-word-classifiers-a",
            "published":"2016-11-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tying-word-vectors-and-word-classifiers-a\/review\/?hl=11129"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":11130,
        "rank":36,
        "Model":"Inan et al. (2016) - Variational LSTM (tied) (h=650)",
        "mlmodel":{

        },
        "method_short":"Inan et al. ",
        "method_details":"2016",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-11-04",
        "metrics":{
            "Test perplexity":"87.7",
            "Validation perplexity":"92.3",
            "Number of params":null
        },
        "raw_metrics":{
            "Test perplexity":87.7,
            "Validation perplexity":92.3,
            "Number of params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":25343,
            "title":"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling",
            "url":"\/paper\/tying-word-vectors-and-word-classifiers-a",
            "published":"2016-11-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tying-word-vectors-and-word-classifiers-a\/review\/?hl=11130"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":11131,
        "rank":37,
        "Model":"Grave et al. (2016) - LSTM",
        "mlmodel":{

        },
        "method_short":"Grave et al. ",
        "method_details":"2016",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-12-13",
        "metrics":{
            "Test perplexity":"99.3",
            "Validation perplexity":null,
            "Number of params":null
        },
        "raw_metrics":{
            "Test perplexity":99.3,
            "Validation perplexity":null,
            "Number of params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":27765,
            "title":"Improving Neural Language Models with a Continuous Cache",
            "url":"\/paper\/improving-neural-language-models-with-a",
            "published":"2016-12-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improving-neural-language-models-with-a\/review\/?hl=11131"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":71,
        "row_id":88765,
        "rank":38,
        "Model":"OPT-175B (50% Sparsity)",
        "mlmodel":{

        },
        "method_short":"OPT-175B ",
        "method_details":"50% Sparsity",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Test perplexity":"234.77",
            "Validation perplexity":null,
            "Number of params":null
        },
        "raw_metrics":{
            "Test perplexity":234.77,
            "Validation perplexity":null,
            "Number of params":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1136959,
            "title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
            "url":"\/paper\/massive-language-models-can-be-accurately",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/massive-language-models-can-be-accurately\/review\/?hl=88765"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]