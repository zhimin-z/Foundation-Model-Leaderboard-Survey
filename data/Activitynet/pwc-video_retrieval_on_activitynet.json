[
    {
        "table_id":1139,
        "row_id":104629,
        "rank":1,
        "Model":"VAST",
        "mlmodel":{

        },
        "method_short":"VAST",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "text-to-video R@1":"70.5",
            "text-to-video R@5":"90.9",
            "text-to-video R@10":"95.5",
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":70.5,
            "text-to-video R@5":90.9,
            "text-to-video R@10":95.5,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":101409,
        "rank":2,
        "Model":"VALOR",
        "mlmodel":{

        },
        "method_short":"VALOR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-17",
        "metrics":{
            "text-to-video R@1":"70.1",
            "text-to-video R@5":"90.8",
            "text-to-video R@10":"95.3",
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":70.1,
            "text-to-video R@5":90.8,
            "text-to-video R@10":95.3,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1191887,
            "title":"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset",
            "url":"\/paper\/valor-vision-audio-language-omni-perception",
            "published":"2023-04-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/valor-vision-audio-language-omni-perception\/review\/?hl=101409"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":100386,
        "rank":3,
        "Model":"UMT-L (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"UMT-L ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "text-to-video R@1":"66.8",
            "text-to-video R@5":"89.1",
            "text-to-video R@10":"94.9",
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":"64.4",
            "video-to-text R@5":"89.1",
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":"94.8",
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":66.8,
            "text-to-video R@5":89.1,
            "text-to-video R@10":94.9,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":64.4,
            "video-to-text R@5":89.1,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":94.8,
            "video-to-text R@50":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1181934,
            "title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
            "url":"\/paper\/unmasked-teacher-towards-training-efficient",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":86724,
        "rank":4,
        "Model":"InternVideo",
        "mlmodel":{

        },
        "method_short":"InternVideo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "text-to-video R@1":"62.2",
            "text-to-video R@5":null,
            "text-to-video R@10":null,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":"62.8",
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":62.2,
            "text-to-video R@5":null,
            "text-to-video R@10":null,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":62.8,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124231,
            "title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
            "url":"\/paper\/internvideo-general-video-foundation-models",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internvideo-general-video-foundation-models\/review\/?hl=86724"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":68570,
        "rank":5,
        "Model":"CLIP-ViP",
        "mlmodel":{

        },
        "method_short":"CLIP-ViP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-14",
        "metrics":{
            "text-to-video R@1":"61.4",
            "text-to-video R@5":"85.7",
            "text-to-video R@10":"92.6",
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":"1",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":61.4,
            "text-to-video R@5":85.7,
            "text-to-video R@10":92.6,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":1.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1074966,
            "title":"CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment",
            "url":"\/paper\/clip-vip-adapting-pre-trained-image-text",
            "published":"2022-09-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clip-vip-adapting-pre-trained-image-text\/review\/?hl=68570"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":51596,
        "rank":6,
        "Model":"HunYuan_tvr",
        "mlmodel":{

        },
        "method_short":"HunYuan_tvr",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "text-to-video R@1":"57.3",
            "text-to-video R@5":"84.8",
            "text-to-video R@10":"93.1",
            "text-to-video R@50":null,
            "text-to-video Mean Rank":"4.0",
            "text-to-video Median Rank":"1",
            "video-to-text R@1":"57.7",
            "video-to-text R@5":"85.7",
            "video-to-text Mean Rank":"3.4",
            "video-to-text Median Rank":"1",
            "video-to-text R@10":"93.9",
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":57.3,
            "text-to-video R@5":84.8,
            "text-to-video R@10":93.1,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":4.0,
            "text-to-video Median Rank":1.0,
            "video-to-text R@1":57.7,
            "video-to-text R@5":85.7,
            "video-to-text Mean Rank":3.4,
            "video-to-text Median Rank":1.0,
            "video-to-text R@10":93.9,
            "video-to-text R@50":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":990765,
            "title":"Tencent Text-Video Retrieval: Hierarchical Cross-Modal Interactions with Multi-Level Representations",
            "url":"\/paper\/hunyuan-tvr-for-text-video-retrivial",
            "published":"2022-04-07T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":111066,
        "rank":7,
        "Model":"TESTA (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"TESTA ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-29",
        "metrics":{
            "text-to-video R@1":"54.8",
            "text-to-video R@5":"80.8",
            "text-to-video R@10":"89.6",
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":54.8,
            "text-to-video R@5":80.8,
            "text-to-video R@10":89.6,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1310474,
            "title":"TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding",
            "url":"\/paper\/testa-temporal-spatial-token-aggregation-for",
            "published":"2023-10-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/testa-temporal-spatial-token-aggregation-for\/review\/?hl=111066"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":109146,
        "rank":8,
        "Model":"DMAE\n(ViT-B\/32)",
        "mlmodel":{

        },
        "method_short":"DMAE\n",
        "method_details":"ViT-B\/32",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-20",
        "metrics":{
            "text-to-video R@1":"53.4",
            "text-to-video R@5":"80.7",
            "text-to-video R@10":"89.2",
            "text-to-video R@50":null,
            "text-to-video Mean Rank":"5.3",
            "text-to-video Median Rank":"1.0",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":53.4,
            "text-to-video R@5":80.7,
            "text-to-video R@10":89.2,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":5.3,
            "text-to-video Median Rank":1.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1284092,
            "title":"Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning",
            "url":"\/paper\/dual-modal-attention-enhanced-text-video",
            "published":"2023-09-20T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/dual-modal-attention-enhanced-text-video\/review\/?hl=109146"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":31886,
        "rank":9,
        "Model":"CAMoE",
        "mlmodel":{

        },
        "method_short":"CAMoE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-09",
        "metrics":{
            "text-to-video R@1":"51.0",
            "text-to-video R@5":"77.7",
            "text-to-video R@10":"87.6",
            "text-to-video R@50":null,
            "text-to-video Mean Rank":"6.3",
            "text-to-video Median Rank":"1",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":51.0,
            "text-to-video R@5":77.7,
            "text-to-video R@10":87.6,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":6.3,
            "text-to-video Median Rank":1.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":864259,
            "title":"Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss",
            "url":"\/paper\/improving-video-text-retrieval-by-multi",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improving-video-text-retrieval-by-multi\/review\/?hl=31886"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":96063,
        "rank":10,
        "Model":"EMCL-Net++",
        "mlmodel":{

        },
        "method_short":"EMCL-Net++",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-21",
        "metrics":{
            "text-to-video R@1":"50.6",
            "text-to-video R@5":"78.7",
            "text-to-video R@10":null,
            "text-to-video R@50":"98.1",
            "text-to-video Mean Rank":"1",
            "text-to-video Median Rank":null,
            "video-to-text R@1":"50.6",
            "video-to-text R@5":"78.9",
            "video-to-text Mean Rank":"1",
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":"98.4"
        },
        "raw_metrics":{
            "text-to-video R@1":50.6,
            "text-to-video R@5":78.7,
            "text-to-video R@10":null,
            "text-to-video R@50":98.1,
            "text-to-video Mean Rank":1.0,
            "text-to-video Median Rank":null,
            "video-to-text R@1":50.6,
            "video-to-text R@5":78.9,
            "video-to-text Mean Rank":1.0,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":98.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1114877,
            "title":"Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations",
            "url":"\/paper\/expectation-maximization-contrastive-learning",
            "published":"2022-11-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/expectation-maximization-contrastive-learning\/review\/?hl=96063"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":88515,
        "rank":11,
        "Model":"HiTeA",
        "mlmodel":{

        },
        "method_short":"HiTeA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-30",
        "metrics":{
            "text-to-video R@1":"49.7",
            "text-to-video R@5":"77.1",
            "text-to-video R@10":"86.7",
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":49.7,
            "text-to-video R@5":77.1,
            "text-to-video R@10":86.7,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1136078,
            "title":"HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training",
            "url":"\/paper\/hitea-hierarchical-temporal-aware-video",
            "published":"2022-12-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/hitea-hierarchical-temporal-aware-video\/review\/?hl=88515"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":100106,
        "rank":12,
        "Model":"DiffusionRet+QB-Norm",
        "mlmodel":{

        },
        "method_short":"DiffusionRet+QB-Norm",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-17",
        "metrics":{
            "text-to-video R@1":"48.1",
            "text-to-video R@5":null,
            "text-to-video R@10":"85.7",
            "text-to-video R@50":null,
            "text-to-video Mean Rank":"6.8",
            "text-to-video Median Rank":"2.0",
            "video-to-text R@1":"47.4",
            "video-to-text R@5":"76.3",
            "video-to-text Mean Rank":"6.7",
            "video-to-text Median Rank":"2.0",
            "video-to-text R@10":"86.7",
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":48.1,
            "text-to-video R@5":null,
            "text-to-video R@10":85.7,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":6.8,
            "text-to-video Median Rank":2.0,
            "video-to-text R@1":47.4,
            "video-to-text R@5":76.3,
            "video-to-text Mean Rank":6.7,
            "video-to-text Median Rank":2.0,
            "video-to-text R@10":86.7,
            "video-to-text R@50":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1175924,
            "title":"DiffusionRet: Generative Text-Video Retrieval with Diffusion Model",
            "url":"\/paper\/diffusionret-generative-text-video-retrieval",
            "published":"2023-03-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diffusionret-generative-text-video-retrieval\/review\/?hl=100106"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":56544,
        "rank":13,
        "Model":"Singularity",
        "mlmodel":{

        },
        "method_short":"Singularity",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-07",
        "metrics":{
            "text-to-video R@1":"47.1",
            "text-to-video R@5":"75.5",
            "text-to-video R@10":"85.5",
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":47.1,
            "text-to-video R@5":75.5,
            "text-to-video R@10":85.5,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1023192,
            "title":"Revealing Single Frame Bias for Video-and-Language Learning",
            "url":"\/paper\/revealing-single-frame-bias-for-video-and",
            "published":"2022-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revealing-single-frame-bias-for-video-and\/review\/?hl=56544"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":53737,
        "rank":14,
        "Model":"CenterCLIP (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"CenterCLIP ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-02",
        "metrics":{
            "text-to-video R@1":"46.2",
            "text-to-video R@5":"77.0",
            "text-to-video R@10":"87.6",
            "text-to-video R@50":null,
            "text-to-video Mean Rank":"5.7",
            "text-to-video Median Rank":"2",
            "video-to-text R@1":"46.7",
            "video-to-text R@5":"77.1",
            "video-to-text Mean Rank":"5.5",
            "video-to-text Median Rank":"2",
            "video-to-text R@10":"88.0",
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":46.2,
            "text-to-video R@5":77.0,
            "text-to-video R@10":87.6,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":5.7,
            "text-to-video Median Rank":2.0,
            "video-to-text R@1":46.7,
            "video-to-text R@5":77.1,
            "video-to-text Mean Rank":5.5,
            "video-to-text Median Rank":2.0,
            "video-to-text R@10":88.0,
            "video-to-text R@50":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1002430,
            "title":"CenterCLIP: Token Clustering for Efficient Text-Video Retrieval",
            "url":"\/paper\/centerclip-token-clustering-for-efficient",
            "published":"2022-05-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/centerclip-token-clustering-for-efficient\/review\/?hl=53737"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":65463,
        "rank":15,
        "Model":"X-CLIP",
        "mlmodel":{

        },
        "method_short":"X-CLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-15",
        "metrics":{
            "text-to-video R@1":"46.2",
            "text-to-video R@5":"75.5",
            "text-to-video R@10":null,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":"6.8",
            "text-to-video Median Rank":null,
            "video-to-text R@1":"46.4",
            "video-to-text R@5":"75.9",
            "video-to-text Mean Rank":"6.4",
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":46.2,
            "text-to-video R@5":75.5,
            "text-to-video R@10":null,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":6.8,
            "text-to-video Median Rank":null,
            "video-to-text R@1":46.4,
            "video-to-text R@5":75.9,
            "video-to-text Mean Rank":6.4,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1044539,
            "title":"X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval",
            "url":"\/paper\/x-clip-end-to-end-multi-grained-contrastive",
            "published":"2022-07-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-clip-end-to-end-multi-grained-contrastive\/review\/?hl=65463"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":100105,
        "rank":16,
        "Model":"DiffusionRet",
        "mlmodel":{

        },
        "method_short":"DiffusionRet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-17",
        "metrics":{
            "text-to-video R@1":"45.8",
            "text-to-video R@5":"75.6",
            "text-to-video R@10":"86.3",
            "text-to-video R@50":null,
            "text-to-video Mean Rank":"6.5",
            "text-to-video Median Rank":"2.0",
            "video-to-text R@1":"43.8",
            "video-to-text R@5":"75.3",
            "video-to-text Mean Rank":"6.3",
            "video-to-text Median Rank":"2.0",
            "video-to-text R@10":"86.7",
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":45.8,
            "text-to-video R@5":75.6,
            "text-to-video R@10":86.3,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":6.5,
            "text-to-video Median Rank":2.0,
            "video-to-text R@1":43.8,
            "video-to-text R@5":75.3,
            "video-to-text Mean Rank":6.3,
            "video-to-text Median Rank":2.0,
            "video-to-text R@10":86.7,
            "video-to-text R@50":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1175924,
            "title":"DiffusionRet: Generative Text-Video Retrieval with Diffusion Model",
            "url":"\/paper\/diffusionret-generative-text-video-retrieval",
            "published":"2023-03-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diffusionret-generative-text-video-retrieval\/review\/?hl=100105"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":100336,
        "rank":17,
        "Model":"HBI",
        "mlmodel":{

        },
        "method_short":"HBI",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-25",
        "metrics":{
            "text-to-video R@1":"42.2",
            "text-to-video R@5":"73.0",
            "text-to-video R@10":"84.6",
            "text-to-video R@50":null,
            "text-to-video Mean Rank":"6.6",
            "text-to-video Median Rank":"2.0",
            "video-to-text R@1":"42.4",
            "video-to-text R@5":"73.0",
            "video-to-text Mean Rank":"6.5",
            "video-to-text Median Rank":"2.0",
            "video-to-text R@10":"86.0",
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":42.2,
            "text-to-video R@5":73.0,
            "text-to-video R@10":84.6,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":6.6,
            "text-to-video Median Rank":2.0,
            "video-to-text R@1":42.4,
            "video-to-text R@5":73.0,
            "video-to-text Mean Rank":6.5,
            "video-to-text Median Rank":2.0,
            "video-to-text R@10":86.0,
            "video-to-text R@50":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1180602,
            "title":"Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning",
            "url":"\/paper\/video-text-as-game-players-hierarchical",
            "published":"2023-03-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-text-as-game-players-hierarchical\/review\/?hl=100336"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":96061,
        "rank":18,
        "Model":"EMCL-Net",
        "mlmodel":{

        },
        "method_short":"EMCL-Net",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-21",
        "metrics":{
            "text-to-video R@1":"41.2",
            "text-to-video R@5":"72.7",
            "text-to-video R@10":null,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":"2",
            "text-to-video Median Rank":null,
            "video-to-text R@1":"42.7",
            "video-to-text R@5":"74",
            "video-to-text Mean Rank":"2",
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":"98.3"
        },
        "raw_metrics":{
            "text-to-video R@1":41.2,
            "text-to-video R@5":72.7,
            "text-to-video R@10":null,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":2.0,
            "text-to-video Median Rank":null,
            "video-to-text R@1":42.7,
            "video-to-text R@5":74.0,
            "video-to-text Mean Rank":2.0,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":98.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1114877,
            "title":"Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations",
            "url":"\/paper\/expectation-maximization-contrastive-learning",
            "published":"2022-11-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/expectation-maximization-contrastive-learning\/review\/?hl=96061"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":39885,
        "rank":19,
        "Model":"CLIP4Clip",
        "mlmodel":{

        },
        "method_short":"CLIP4Clip",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-18",
        "metrics":{
            "text-to-video R@1":"40.5",
            "text-to-video R@5":"73.4",
            "text-to-video R@10":null,
            "text-to-video R@50":"98.2",
            "text-to-video Mean Rank":"7.5",
            "text-to-video Median Rank":"2",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":40.5,
            "text-to-video R@5":73.4,
            "text-to-video R@10":null,
            "text-to-video R@50":98.2,
            "text-to-video Mean Rank":7.5,
            "text-to-video Median Rank":2.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":784398,
            "title":"CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval",
            "url":"\/paper\/clip4clip-an-empirical-study-of-clip-for-end",
            "published":"2021-04-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clip4clip-an-empirical-study-of-clip-for-end\/review\/?hl=39885"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":38604,
        "rank":20,
        "Model":"TACo",
        "mlmodel":{

        },
        "method_short":"TACo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-08-23",
        "metrics":{
            "text-to-video R@1":"30.4",
            "text-to-video R@5":"61.2",
            "text-to-video R@10":null,
            "text-to-video R@50":"93.4",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":"3.0",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":30.4,
            "text-to-video R@5":61.2,
            "text-to-video R@10":null,
            "text-to-video R@50":93.4,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":3.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":854963,
            "title":"TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment",
            "url":"\/paper\/taco-token-aware-cascade-contrastive-learning",
            "published":"2021-08-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/taco-token-aware-cascade-contrastive-learning\/review\/?hl=38604"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":22463,
        "rank":21,
        "Model":"MMT-Pretrained",
        "mlmodel":{

        },
        "method_short":"MMT-Pretrained",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-21",
        "metrics":{
            "text-to-video R@1":"28.7",
            "text-to-video R@5":"61.4",
            "text-to-video R@10":null,
            "text-to-video R@50":"94.5",
            "text-to-video Mean Rank":"16",
            "text-to-video Median Rank":"3.3",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":28.7,
            "text-to-video R@5":61.4,
            "text-to-video R@10":null,
            "text-to-video R@50":94.5,
            "text-to-video Mean Rank":16.0,
            "text-to-video Median Rank":3.3,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":209798,
            "title":"Multi-modal Transformer for Video Retrieval",
            "url":"\/paper\/multi-modal-transformer-for-video-retrieval",
            "published":"2020-07-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multi-modal-transformer-for-video-retrieval\/review\/?hl=22463"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":110585,
        "rank":22,
        "Model":"HD-VILA",
        "mlmodel":{

        },
        "method_short":"HD-VILA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-19",
        "metrics":{
            "text-to-video R@1":"28.5",
            "text-to-video R@5":"57.4",
            "text-to-video R@10":null,
            "text-to-video R@50":"94",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":"4",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":28.5,
            "text-to-video R@5":57.4,
            "text-to-video R@10":null,
            "text-to-video R@50":94.0,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":4.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912961,
            "title":"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions",
            "url":"\/paper\/advancing-high-resolution-video-language",
            "published":"2021-11-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/advancing-high-resolution-video-language\/review\/?hl=110585"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":48512,
        "rank":23,
        "Model":"Ours",
        "mlmodel":{

        },
        "method_short":"Ours",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-21",
        "metrics":{
            "text-to-video R@1":"25.4",
            "text-to-video R@5":"59.1",
            "text-to-video R@10":null,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":"26.1",
            "video-to-text R@5":"60",
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":25.4,
            "text-to-video R@5":59.1,
            "text-to-video R@10":null,
            "text-to-video R@50":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":26.1,
            "video-to-text R@5":60.0,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":892158,
            "title":"Video and Text Matching with Conditioned Embeddings",
            "url":"\/paper\/video-and-text-matching-with-conditioned",
            "published":"2021-10-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-and-text-matching-with-conditioned\/review\/?hl=48512"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":22462,
        "rank":24,
        "Model":"MMT",
        "mlmodel":{

        },
        "method_short":"MMT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-21",
        "metrics":{
            "text-to-video R@1":"22.7",
            "text-to-video R@5":"54.2",
            "text-to-video R@10":null,
            "text-to-video R@50":"93.2",
            "text-to-video Mean Rank":"20.8",
            "text-to-video Median Rank":"5",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":22.7,
            "text-to-video R@5":54.2,
            "text-to-video R@10":null,
            "text-to-video R@50":93.2,
            "text-to-video Mean Rank":20.8,
            "text-to-video Median Rank":5.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":209798,
            "title":"Multi-modal Transformer for Video Retrieval",
            "url":"\/paper\/multi-modal-transformer-for-video-retrieval",
            "published":"2020-07-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multi-modal-transformer-for-video-retrieval\/review\/?hl=22462"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1139,
        "row_id":12882,
        "rank":25,
        "Model":"Collaborative Experts",
        "mlmodel":{

        },
        "method_short":"Collaborative Experts",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-31",
        "metrics":{
            "text-to-video R@1":"20.5",
            "text-to-video R@5":"47.7",
            "text-to-video R@10":"63.9",
            "text-to-video R@50":"91.4",
            "text-to-video Mean Rank":"23.1",
            "text-to-video Median Rank":"6",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "raw_metrics":{
            "text-to-video R@1":20.5,
            "text-to-video R@5":47.7,
            "text-to-video R@10":63.9,
            "text-to-video R@50":91.4,
            "text-to-video Mean Rank":23.1,
            "text-to-video Median Rank":6.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text Mean Rank":null,
            "video-to-text Median Rank":null,
            "video-to-text R@10":null,
            "video-to-text R@50":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":148748,
            "title":"Use What You Have: Video Retrieval Using Representations From Collaborative Experts",
            "url":"\/paper\/use-what-you-have-video-retrieval-using",
            "published":"2019-07-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/use-what-you-have-video-retrieval-using\/review\/?hl=12882"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]