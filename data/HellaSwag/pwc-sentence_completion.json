[
    {
        "table_id":3156,
        "row_id":99240,
        "rank":1,
        "method":"GPT-4 (few-shot, k=10)",
        "mlmodel":{

        },
        "Model":"GPT-4 ",
        "method_details":"few-shot, k=10",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-15",
        "metrics":{
            "Accuracy":"95.3"
        },
        "raw_metrics":{
            "Accuracy":95.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1174373,
            "title":"GPT-4 Technical Report",
            "url":"\/paper\/gpt-4-technical-report-1",
            "published":"2023-03-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/gpt-4-technical-report-1\/review\/?hl=99240"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":106870,
        "rank":2,
        "method":"TheBloke\/llama-2-70b-Guanaco-QLoRA-fp16 (10-shot)",
        "mlmodel":{

        },
        "Model":"TheBloke\/llama-2-70b-Guanaco-QLoRA-fp16 ",
        "method_details":"10-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Accuracy":"88.3"
        },
        "raw_metrics":{
            "Accuracy":88.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":102519,
        "rank":3,
        "method":"PaLM 2-L (one-shot)",
        "mlmodel":{

        },
        "Model":"PaLM 2-L ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Accuracy":"87.4"
        },
        "raw_metrics":{
            "Accuracy":87.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102519"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":102518,
        "rank":4,
        "method":"PaLM 2-M (one-shot)",
        "mlmodel":{

        },
        "Model":"PaLM 2-M ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Accuracy":"86.7"
        },
        "raw_metrics":{
            "Accuracy":86.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102518"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":35865,
        "rank":5,
        "method":"MUPPET Roberta Large",
        "mlmodel":{

        },
        "Model":"MUPPET Roberta Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-26",
        "metrics":{
            "Accuracy":"86.4"
        },
        "raw_metrics":{
            "Accuracy":86.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":739728,
            "title":"Muppet: Massive Multi-task Representations with Pre-Finetuning",
            "url":"\/paper\/muppet-massive-multi-task-representations",
            "published":"2021-01-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/muppet-massive-multi-task-representations\/review\/?hl=35865"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":105777,
        "rank":6,
        "method":"LLaMA-65B+CFG (zero-shot)",
        "mlmodel":{

        },
        "Model":"LLaMA-65B+CFG ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-30",
        "metrics":{
            "Accuracy":"86.3"
        },
        "raw_metrics":{
            "Accuracy":86.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1238561,
            "title":"Stay on topic with Classifier-Free Guidance",
            "url":"\/paper\/stay-on-topic-with-classifier-free-guidance",
            "published":"2023-06-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/stay-on-topic-with-classifier-free-guidance\/review\/?hl=105777"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":102517,
        "rank":7,
        "method":"PaLM 2-S (one-shot)",
        "mlmodel":{

        },
        "Model":"PaLM 2-S ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Accuracy":"85.6"
        },
        "raw_metrics":{
            "Accuracy":85.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102517"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":99241,
        "rank":8,
        "method":"GPT-3.5 (few-shot, k=10)",
        "mlmodel":{

        },
        "Model":"GPT-3.5 ",
        "method_details":"few-shot, k=10",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-15",
        "metrics":{
            "Accuracy":"85.5"
        },
        "raw_metrics":{
            "Accuracy":85.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1174373,
            "title":"GPT-4 Technical Report",
            "url":"\/paper\/gpt-4-technical-report-1",
            "published":"2023-03-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/gpt-4-technical-report-1\/review\/?hl=99241"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":105776,
        "rank":9,
        "method":"LLaMA-30B+CFG (zero-shot)",
        "mlmodel":{

        },
        "Model":"LLaMA-30B+CFG ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-30",
        "metrics":{
            "Accuracy":"85.3"
        },
        "raw_metrics":{
            "Accuracy":85.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1238561,
            "title":"Stay on topic with Classifier-Free Guidance",
            "url":"\/paper\/stay-on-topic-with-classifier-free-guidance",
            "published":"2023-06-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/stay-on-topic-with-classifier-free-guidance\/review\/?hl=105776"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":106311,
        "rank":10,
        "method":"LLaMA 2 70B (zero-shot)",
        "mlmodel":{

        },
        "Model":"LLaMA 2 70B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Accuracy":"85.3"
        },
        "raw_metrics":{
            "Accuracy":85.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106311"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":97614,
        "rank":11,
        "method":"LLaMA 65B (zero-shot)",
        "mlmodel":{

        },
        "Model":"LLaMA 65B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"84.2"
        },
        "raw_metrics":{
            "Accuracy":84.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97614"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":51447,
        "rank":12,
        "method":"PaLM-540B (Few-Shot)",
        "mlmodel":{

        },
        "Model":"PaLM-540B ",
        "method_details":"Few-Shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-05",
        "metrics":{
            "Accuracy":"83.8"
        },
        "raw_metrics":{
            "Accuracy":83.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":989558,
            "title":"PaLM: Scaling Language Modeling with Pathways",
            "url":"\/paper\/palm-scaling-language-modeling-with-pathways-1",
            "published":"2022-04-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":51446,
        "rank":13,
        "method":"PaLM-540B (One-Shot)",
        "mlmodel":{

        },
        "Model":"PaLM-540B ",
        "method_details":"One-Shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-05",
        "metrics":{
            "Accuracy":"83.6"
        },
        "raw_metrics":{
            "Accuracy":83.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":989558,
            "title":"PaLM: Scaling Language Modeling with Pathways",
            "url":"\/paper\/palm-scaling-language-modeling-with-pathways-1",
            "published":"2022-04-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":214,
                "name":"one-shot",
                "color":"#ea9e57"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":51445,
        "rank":14,
        "method":"PaLM-540B (Zero-Shot)",
        "mlmodel":{

        },
        "Model":"PaLM-540B ",
        "method_details":"Zero-Shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-05",
        "metrics":{
            "Accuracy":"83.4"
        },
        "raw_metrics":{
            "Accuracy":83.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":989558,
            "title":"PaLM: Scaling Language Modeling with Pathways",
            "url":"\/paper\/palm-scaling-language-modeling-with-pathways-1",
            "published":"2022-04-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":106310,
        "rank":15,
        "method":"LLaMA 2 34B (zero-shot)",
        "mlmodel":{

        },
        "Model":"LLaMA 2 34B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Accuracy":"83.3"
        },
        "raw_metrics":{
            "Accuracy":83.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106310"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":97613,
        "rank":16,
        "method":"LLaMA 33B (zero-shot)",
        "mlmodel":{

        },
        "Model":"LLaMA 33B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"82.8"
        },
        "raw_metrics":{
            "Accuracy":82.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97613"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":51451,
        "rank":17,
        "method":"Megatron-Turing NLG 530B (Few-Shot)",
        "mlmodel":{

        },
        "Model":"Megatron-Turing NLG 530B ",
        "method_details":"Few-Shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-28",
        "metrics":{
            "Accuracy":"82.4"
        },
        "raw_metrics":{
            "Accuracy":82.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":952554,
            "title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
            "url":"\/paper\/using-deepspeed-and-megatron-to-train",
            "published":"2022-01-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/using-deepspeed-and-megatron-to-train\/review\/?hl=51451"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":105778,
        "rank":18,
        "method":"LLaMA-13B+CFG (zero-shot)",
        "mlmodel":{

        },
        "Model":"LLaMA-13B+CFG ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-30",
        "metrics":{
            "Accuracy":"82.1"
        },
        "raw_metrics":{
            "Accuracy":82.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1238561,
            "title":"Stay on topic with Classifier-Free Guidance",
            "url":"\/paper\/stay-on-topic-with-classifier-free-guidance",
            "published":"2023-06-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/stay-on-topic-with-classifier-free-guidance\/review\/?hl=105778"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":112814,
        "rank":19,
        "method":"Mistral 7B",
        "mlmodel":{

        },
        "Model":"Mistral 7B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-10",
        "metrics":{
            "Accuracy":"81.3"
        },
        "raw_metrics":{
            "Accuracy":81.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1297015,
            "title":"Mistral 7B",
            "url":"\/paper\/mistral-7b",
            "published":"2023-10-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mistral-7b\/review\/?hl=112814"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":51449,
        "rank":20,
        "method":"Chinchilla (Zero-Shot)",
        "mlmodel":{

        },
        "Model":"Chinchilla ",
        "method_details":"Zero-Shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-29",
        "metrics":{
            "Accuracy":"80.8"
        },
        "raw_metrics":{
            "Accuracy":80.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":985465,
            "title":"Training Compute-Optimal Large Language Models",
            "url":"\/paper\/training-compute-optimal-large-language",
            "published":"2022-03-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":106309,
        "rank":21,
        "method":"LLaMA 2 13B (zero-shot)",
        "mlmodel":{

        },
        "Model":"LLaMA 2 13B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Accuracy":"80.7"
        },
        "raw_metrics":{
            "Accuracy":80.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106309"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":51450,
        "rank":22,
        "method":"Megatron-Turing NLG 530B (One-Shot)",
        "mlmodel":{

        },
        "Model":"Megatron-Turing NLG 530B ",
        "method_details":"One-Shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-28",
        "metrics":{
            "Accuracy":"80.2"
        },
        "raw_metrics":{
            "Accuracy":80.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":952554,
            "title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
            "url":"\/paper\/using-deepspeed-and-megatron-to-train",
            "published":"2022-01-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/using-deepspeed-and-megatron-to-train\/review\/?hl=51450"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":214,
                "name":"one-shot",
                "color":"#ea9e57"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":16793,
        "rank":23,
        "method":"GPT-3 175B (Few-Shot)",
        "mlmodel":{

        },
        "Model":"GPT-3 175B ",
        "method_details":"Few-Shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "Accuracy":"79.3"
        },
        "raw_metrics":{
            "Accuracy":79.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=16793"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":66317,
        "rank":24,
        "method":"Gopher (zero-shot)",
        "mlmodel":{

        },
        "Model":"Gopher ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-08",
        "metrics":{
            "Accuracy":"79.2"
        },
        "raw_metrics":{
            "Accuracy":79.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":942590,
            "title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
            "url":"\/paper\/scaling-language-models-methods-analysis-1",
            "published":"2021-12-08T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":97612,
        "rank":25,
        "method":"LLaMA 13B (zero-shot)",
        "mlmodel":{

        },
        "Model":"LLaMA 13B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"79.2"
        },
        "raw_metrics":{
            "Accuracy":79.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97612"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":66316,
        "rank":26,
        "method":"GPT-3 (zero-shot)",
        "mlmodel":{

        },
        "Model":"GPT-3 ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "Accuracy":"78.9"
        },
        "raw_metrics":{
            "Accuracy":78.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=66316"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":106308,
        "rank":27,
        "method":"LLaMA 2 7B (zero-shot)",
        "mlmodel":{

        },
        "Model":"LLaMA 2 7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Accuracy":"77.2"
        },
        "raw_metrics":{
            "Accuracy":77.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106308"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":97611,
        "rank":28,
        "method":"LLaMA 7B (zero-shot)",
        "mlmodel":{

        },
        "Model":"LLaMA 7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"76.1"
        },
        "raw_metrics":{
            "Accuracy":76.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97611"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":100845,
        "rank":29,
        "method":"Blooomberg GPT (one-shot)",
        "mlmodel":{

        },
        "Model":"Blooomberg GPT ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"73.92"
        },
        "raw_metrics":{
            "Accuracy":73.92
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100845"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":100847,
        "rank":30,
        "method":"OPT 66B (one-shot)",
        "mlmodel":{

        },
        "Model":"OPT 66B ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"73.47"
        },
        "raw_metrics":{
            "Accuracy":73.47
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100847"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":100848,
        "rank":31,
        "method":"BLOOM 176B (one-shot)",
        "mlmodel":{

        },
        "Model":"BLOOM 176B ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"73.21"
        },
        "raw_metrics":{
            "Accuracy":73.21
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100848"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":112368,
        "rank":32,
        "method":"Sheared-LLaMA-2.7B (50B)",
        "mlmodel":{

        },
        "Model":"Sheared-LLaMA-2.7B ",
        "method_details":"50B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-10",
        "metrics":{
            "Accuracy":"70.8"
        },
        "raw_metrics":{
            "Accuracy":70.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1297030,
            "title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
            "url":"\/paper\/sheared-llama-accelerating-language-model-pre",
            "published":"2023-10-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sheared-llama-accelerating-language-model-pre\/review\/?hl=112368"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":100846,
        "rank":33,
        "method":"GPT-NeoX (one-shot)",
        "mlmodel":{

        },
        "Model":"GPT-NeoX ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"68.37"
        },
        "raw_metrics":{
            "Accuracy":68.37
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100846"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":112367,
        "rank":34,
        "method":"Open-LLaMA-3B-v2",
        "mlmodel":{

        },
        "Model":"Open-LLaMA-3B-v2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-10",
        "metrics":{
            "Accuracy":"67.6"
        },
        "raw_metrics":{
            "Accuracy":67.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1297030,
            "title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
            "url":"\/paper\/sheared-llama-accelerating-language-model-pre",
            "published":"2023-10-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sheared-llama-accelerating-language-model-pre\/review\/?hl=112367"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":113982,
        "rank":35,
        "method":"Mamba-2.8B",
        "mlmodel":{

        },
        "Model":"Mamba-2.8B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-12-01",
        "metrics":{
            "Accuracy":"66.1"
        },
        "raw_metrics":{
            "Accuracy":66.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1334747,
            "title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
            "url":"\/paper\/mamba-linear-time-sequence-modeling-with",
            "published":"2023-12-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":112366,
        "rank":36,
        "method":"Sheared-LLaMA-1.3B (50B)",
        "mlmodel":{

        },
        "Model":"Sheared-LLaMA-1.3B ",
        "method_details":"50B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-10",
        "metrics":{
            "Accuracy":"60.7"
        },
        "raw_metrics":{
            "Accuracy":60.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1297030,
            "title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
            "url":"\/paper\/sheared-llama-accelerating-language-model-pre",
            "published":"2023-10-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sheared-llama-accelerating-language-model-pre\/review\/?hl=112366"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":113983,
        "rank":37,
        "method":"Mamba-1.4B",
        "mlmodel":{

        },
        "Model":"Mamba-1.4B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-12-01",
        "metrics":{
            "Accuracy":"59.1"
        },
        "raw_metrics":{
            "Accuracy":59.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1334747,
            "title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
            "url":"\/paper\/mamba-linear-time-sequence-modeling-with",
            "published":"2023-12-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3156,
        "row_id":39037,
        "rank":38,
        "method":"FLAN 137B (zero-shot)",
        "mlmodel":{

        },
        "Model":"FLAN 137B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-03",
        "metrics":{
            "Accuracy":"56.7"
        },
        "raw_metrics":{
            "Accuracy":56.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":861409,
            "title":"Finetuned Language Models Are Zero-Shot Learners",
            "url":"\/paper\/finetuned-language-models-are-zero-shot",
            "published":"2021-09-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/finetuned-language-models-are-zero-shot\/review\/?hl=39037"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    }
]