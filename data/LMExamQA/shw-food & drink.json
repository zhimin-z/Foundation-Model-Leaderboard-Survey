[
    {
        "Model":"Vicuna-13B",
        "Accuracy":"2.9727272727272727",
        "Coherence":"3",
        "Factuality":"2.9681818181818183",
        "Comprehensiveness":"2.9909090909090907",
        "Overall":"4.9409090909090905"
    },
    {
        "Model":"ChatGPT",
        "Accuracy":"2.9954545454545456",
        "Coherence":"3",
        "Factuality":"3",
        "Comprehensiveness":"2.9954545454545456",
        "Overall":"4.990909090909091"
    },
    {
        "Model":"GLM-130B",
        "Accuracy":"2.5",
        "Coherence":"2.909090909090909",
        "Factuality":"2.672727272727273",
        "Comprehensiveness":"2.3181818181818183",
        "Overall":"3.8045454545454547"
    },
    {
        "Model":"LLaMA-13B",
        "Accuracy":"2.8136363636363635",
        "Coherence":"2.9909090909090907",
        "Factuality":"2.890909090909091",
        "Comprehensiveness":"2.536363636363636",
        "Overall":"4.340909090909091"
    },
    {
        "Model":"LLaMA-65B",
        "Accuracy":"2.8545454545454545",
        "Coherence":"3",
        "Factuality":"2.9363636363636365",
        "Comprehensiveness":"2.6136363636363638",
        "Overall":"4.468181818181818"
    },
    {
        "Model":"BLOOMZ",
        "Accuracy":"2.3545454545454545",
        "Coherence":"2.7363636363636363",
        "Factuality":"2.6363636363636362",
        "Comprehensiveness":"2.0318181818181817",
        "Overall":"3.4363636363636365"
    },
    {
        "Model":"Flan-UL2",
        "Accuracy":"2.481818181818182",
        "Coherence":"2.95",
        "Factuality":"2.786363636363636",
        "Comprehensiveness":"2.1772727272727272",
        "Overall":"3.7181818181818183"
    },
    {
        "Model":"Flan-T5",
        "Accuracy":"2.4454545454545453",
        "Coherence":"2.9318181818181817",
        "Factuality":"2.7",
        "Comprehensiveness":"2.209090909090909",
        "Overall":"3.65"
    }
]