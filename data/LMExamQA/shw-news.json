[
    {
        "Model":"Vicuna-13B",
        "Accuracy":"2.962962962962963",
        "Coherence":"3",
        "Factuality":"2.962962962962963",
        "Comprehensiveness":"2.9851851851851854",
        "Overall":"4.925925925925926"
    },
    {
        "Model":"ChatGPT",
        "Accuracy":"2.9925925925925925",
        "Coherence":"3",
        "Factuality":"2.9925925925925925",
        "Comprehensiveness":"2.9925925925925925",
        "Overall":"4.985185185185185"
    },
    {
        "Model":"GLM-130B",
        "Accuracy":"2.511111111111111",
        "Coherence":"2.8962962962962964",
        "Factuality":"2.714814814814815",
        "Comprehensiveness":"2.2925925925925927",
        "Overall":"3.7555555555555555"
    },
    {
        "Model":"LLaMA-13B",
        "Accuracy":"2.7777777777777777",
        "Coherence":"2.9925925925925925",
        "Factuality":"2.8851851851851853",
        "Comprehensiveness":"2.5185185185185186",
        "Overall":"4.2518518518518515"
    },
    {
        "Model":"LLaMA-65B",
        "Accuracy":"2.8518518518518516",
        "Coherence":"2.9962962962962965",
        "Factuality":"2.937037037037037",
        "Comprehensiveness":"2.5814814814814815",
        "Overall":"4.407407407407407"
    },
    {
        "Model":"BLOOMZ",
        "Accuracy":"2.314814814814815",
        "Coherence":"2.6814814814814816",
        "Factuality":"2.640740740740741",
        "Comprehensiveness":"1.974074074074074",
        "Overall":"3.3185185185185184"
    },
    {
        "Model":"Flan-UL2",
        "Accuracy":"2.414814814814815",
        "Coherence":"2.9444444444444446",
        "Factuality":"2.7962962962962963",
        "Comprehensiveness":"2.037037037037037",
        "Overall":"3.4703703703703703"
    },
    {
        "Model":"Flan-T5",
        "Accuracy":"2.4518518518518517",
        "Coherence":"2.9296296296296296",
        "Factuality":"2.7555555555555555",
        "Comprehensiveness":"2.1037037037037036",
        "Overall":"3.562962962962963"
    }
]