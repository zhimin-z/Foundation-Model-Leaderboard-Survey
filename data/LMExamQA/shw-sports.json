[
    {
        "Model":"Vicuna-13B",
        "Accuracy":"2.9363636363636365",
        "Coherence":"3",
        "Factuality":"2.9136363636363636",
        "Comprehensiveness":"2.9954545454545456",
        "Overall":"4.861363636363636"
    },
    {
        "Model":"ChatGPT",
        "Accuracy":"2.9795454545454545",
        "Coherence":"3",
        "Factuality":"2.9795454545454545",
        "Comprehensiveness":"2.9954545454545456",
        "Overall":"4.956818181818182"
    },
    {
        "Model":"GLM-130B",
        "Accuracy":"2.3795454545454544",
        "Coherence":"2.881818181818182",
        "Factuality":"2.559090909090909",
        "Comprehensiveness":"2.2590909090909093",
        "Overall":"3.5954545454545452"
    },
    {
        "Model":"LLaMA-13B",
        "Accuracy":"2.684090909090909",
        "Coherence":"2.9863636363636363",
        "Factuality":"2.8068181818181817",
        "Comprehensiveness":"2.463636363636364",
        "Overall":"4.122727272727273"
    },
    {
        "Model":"LLaMA-65B",
        "Accuracy":"2.7886363636363636",
        "Coherence":"2.9909090909090907",
        "Factuality":"2.9204545454545454",
        "Comprehensiveness":"2.5545454545454547",
        "Overall":"4.338636363636364"
    },
    {
        "Model":"BLOOMZ",
        "Accuracy":"2.202272727272727",
        "Coherence":"2.7522727272727274",
        "Factuality":"2.5568181818181817",
        "Comprehensiveness":"2.0090909090909093",
        "Overall":"3.2113636363636364"
    },
    {
        "Model":"Flan-UL2",
        "Accuracy":"2.327272727272727",
        "Coherence":"2.9681818181818183",
        "Factuality":"2.734090909090909",
        "Comprehensiveness":"2.0568181818181817",
        "Overall":"3.4272727272727272"
    },
    {
        "Model":"Flan-T5",
        "Accuracy":"2.3113636363636365",
        "Coherence":"2.890909090909091",
        "Factuality":"2.5954545454545452",
        "Comprehensiveness":"2.102272727272727",
        "Overall":"3.3954545454545455"
    }
]