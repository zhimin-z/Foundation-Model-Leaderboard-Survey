[
    {
        "Model":"Vicuna-13B",
        "Accuracy":"2.988888888888889",
        "Coherence":"3",
        "Factuality":"2.966666666666667",
        "Comprehensiveness":"3",
        "Overall":"4.955555555555556"
    },
    {
        "Model":"ChatGPT",
        "Accuracy":"2.988888888888889",
        "Coherence":"3",
        "Factuality":"2.988888888888889",
        "Comprehensiveness":"3",
        "Overall":"4.977777777777778"
    },
    {
        "Model":"GLM-130B",
        "Accuracy":"2.533333333333333",
        "Coherence":"2.9",
        "Factuality":"2.7222222222222223",
        "Comprehensiveness":"2.188888888888889",
        "Overall":"3.7111111111111112"
    },
    {
        "Model":"LLaMA-13B",
        "Accuracy":"2.7333333333333334",
        "Coherence":"2.9444444444444446",
        "Factuality":"2.9",
        "Comprehensiveness":"2.422222222222222",
        "Overall":"4.177777777777778"
    },
    {
        "Model":"LLaMA-65B",
        "Accuracy":"2.8444444444444446",
        "Coherence":"3",
        "Factuality":"2.933333333333333",
        "Comprehensiveness":"2.511111111111111",
        "Overall":"4.355555555555555"
    },
    {
        "Model":"BLOOMZ",
        "Accuracy":"2.2222222222222223",
        "Coherence":"2.588888888888889",
        "Factuality":"2.5555555555555554",
        "Comprehensiveness":"1.8222222222222222",
        "Overall":"3.111111111111111"
    },
    {
        "Model":"Flan-UL2",
        "Accuracy":"2.3444444444444446",
        "Coherence":"2.933333333333333",
        "Factuality":"2.7888888888888888",
        "Comprehensiveness":"1.8666666666666667",
        "Overall":"3.3222222222222224"
    },
    {
        "Model":"Flan-T5",
        "Accuracy":"2.311111111111111",
        "Coherence":"2.8666666666666667",
        "Factuality":"2.7222222222222223",
        "Comprehensiveness":"1.9444444444444444",
        "Overall":"3.3"
    }
]