[
    {
        "Model":"Vicuna-13B",
        "Accuracy":"2.9727272727272727",
        "Coherence":"3",
        "Factuality":"2.963636363636364",
        "Comprehensiveness":"2.993939393939394",
        "Overall":"4.9393939393939394"
    },
    {
        "Model":"ChatGPT",
        "Accuracy":"3",
        "Coherence":"3",
        "Factuality":"2.996969696969697",
        "Comprehensiveness":"3",
        "Overall":"4.996969696969697"
    },
    {
        "Model":"GLM-130B",
        "Accuracy":"2.4727272727272727",
        "Coherence":"2.9515151515151516",
        "Factuality":"2.6606060606060606",
        "Comprehensiveness":"2.2303030303030305",
        "Overall":"3.6818181818181817"
    },
    {
        "Model":"LLaMA-13B",
        "Accuracy":"2.8",
        "Coherence":"2.984848484848485",
        "Factuality":"2.890909090909091",
        "Comprehensiveness":"2.4909090909090907",
        "Overall":"4.281818181818182"
    },
    {
        "Model":"LLaMA-65B",
        "Accuracy":"2.890909090909091",
        "Coherence":"3",
        "Factuality":"2.9424242424242424",
        "Comprehensiveness":"2.581818181818182",
        "Overall":"4.46969696969697"
    },
    {
        "Model":"BLOOMZ",
        "Accuracy":"2.309090909090909",
        "Coherence":"2.757575757575758",
        "Factuality":"2.6666666666666665",
        "Comprehensiveness":"1.9666666666666666",
        "Overall":"3.3515151515151516"
    },
    {
        "Model":"Flan-UL2",
        "Accuracy":"2.4363636363636365",
        "Coherence":"2.9696969696969697",
        "Factuality":"2.793939393939394",
        "Comprehensiveness":"2.0454545454545454",
        "Overall":"3.5424242424242425"
    },
    {
        "Model":"Flan-T5",
        "Accuracy":"2.327272727272727",
        "Coherence":"2.9",
        "Factuality":"2.5939393939393938",
        "Comprehensiveness":"2.0393939393939395",
        "Overall":"3.4"
    }
]