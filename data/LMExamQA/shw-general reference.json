[
    {
        "Model":"Vicuna-13B",
        "Accuracy":"2.977777777777778",
        "Coherence":"3",
        "Factuality":"2.977777777777778",
        "Comprehensiveness":"2.988888888888889",
        "Overall":"4.966666666666667"
    },
    {
        "Model":"ChatGPT",
        "Accuracy":"3",
        "Coherence":"3",
        "Factuality":"3",
        "Comprehensiveness":"3",
        "Overall":"5"
    },
    {
        "Model":"GLM-130B",
        "Accuracy":"2.511111111111111",
        "Coherence":"2.888888888888889",
        "Factuality":"2.8222222222222224",
        "Comprehensiveness":"2.3222222222222224",
        "Overall":"3.7888888888888888"
    },
    {
        "Model":"LLaMA-13B",
        "Accuracy":"2.8444444444444446",
        "Coherence":"3",
        "Factuality":"2.966666666666667",
        "Comprehensiveness":"2.6222222222222222",
        "Overall":"4.477777777777778"
    },
    {
        "Model":"LLaMA-65B",
        "Accuracy":"2.888888888888889",
        "Coherence":"3",
        "Factuality":"2.9555555555555557",
        "Comprehensiveness":"2.6444444444444444",
        "Overall":"4.522222222222222"
    },
    {
        "Model":"BLOOMZ",
        "Accuracy":"2.3777777777777778",
        "Coherence":"2.7333333333333334",
        "Factuality":"2.7",
        "Comprehensiveness":"2.022222222222222",
        "Overall":"3.4444444444444446"
    },
    {
        "Model":"Flan-UL2",
        "Accuracy":"2.488888888888889",
        "Coherence":"2.966666666666667",
        "Factuality":"2.8444444444444446",
        "Comprehensiveness":"2.1444444444444444",
        "Overall":"3.6222222222222222"
    },
    {
        "Model":"Flan-T5",
        "Accuracy":"2.5",
        "Coherence":"2.9444444444444446",
        "Factuality":"2.7888888888888888",
        "Comprehensiveness":"2.2333333333333334",
        "Overall":"3.7444444444444445"
    }
]