[
    {
        "Model":"Falcon (40B)",
        "F1":"0.671",
        "Observed inference time (s)":"4.985",
        "# eval":"355",
        "# train":"2.023",
        "truncated":"0",
        "# prompt tokens":"1692.33",
        "# output tokens":"1"
    },
    {
        "Model":"Falcon (7B)",
        "F1":"0.621",
        "Observed inference time (s)":"1.141",
        "# eval":"355",
        "# train":"2.023",
        "truncated":"0",
        "# prompt tokens":"1692.33",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (13B)",
        "F1":"0.741",
        "Observed inference time (s)":"0.795",
        "# eval":"355",
        "# train":"4.408",
        "truncated":"0",
        "# prompt tokens":"3669.808",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (70B)",
        "F1":"0.763",
        "Observed inference time (s)":"1.871",
        "# eval":"355",
        "# train":"4.408",
        "truncated":"0",
        "# prompt tokens":"3669.808",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (7B)",
        "F1":"0.686",
        "Observed inference time (s)":"0.852",
        "# eval":"355",
        "# train":"4.408",
        "truncated":"0",
        "# prompt tokens":"3669.808",
        "# output tokens":"1"
    },
    {
        "Model":"LLaMA (65B)",
        "F1":"0.755",
        "Observed inference time (s)":"2.909",
        "# eval":"355",
        "# train":"1.434",
        "truncated":"0",
        "# prompt tokens":"1539.586",
        "# output tokens":"1"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "F1":"0.716",
        "Observed inference time (s)":"0.705",
        "# eval":"355",
        "# train":"4.575",
        "truncated":"0",
        "# prompt tokens":"3627.715",
        "# output tokens":"1"
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "F1":"0.767",
        "Observed inference time (s)":"0.65",
        "# eval":"355",
        "# train":"4.575",
        "truncated":"0",
        "# prompt tokens":"3627.715",
        "# output tokens":"1"
    },
    {
        "Model":"Phi-2",
        "F1":"0.703",
        "Observed inference time (s)":"0.493",
        "# eval":"355",
        "# train":"2.085",
        "truncated":"0",
        "# prompt tokens":"1705.006",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (34B)",
        "F1":"0.782",
        "Observed inference time (s)":"2.368",
        "# eval":"355",
        "# train":"4.868",
        "truncated":"0",
        "# prompt tokens":"3611.445",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (6B)",
        "F1":"0.702",
        "Observed inference time (s)":"1.404",
        "# eval":"355",
        "# train":"4.868",
        "truncated":"0",
        "# prompt tokens":"3611.445",
        "# output tokens":"1"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "F1":"0.728",
        "Observed inference time (s)":"1.82",
        "# eval":"355",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2534.434",
        "# output tokens":"6.583"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "F1":"0.744",
        "Observed inference time (s)":"1.179",
        "# eval":"355",
        "# train":"3.225",
        "truncated":"0",
        "# prompt tokens":"1700.741",
        "# output tokens":"5.039"
    },
    {
        "Model":"Luminous Base (13B)",
        "F1":"0.633",
        "Observed inference time (s)":"1.05",
        "# eval":"355",
        "# train":"2.037",
        "truncated":"0",
        "# prompt tokens":"1694.642",
        "# output tokens":"5.521"
    },
    {
        "Model":"Luminous Extended (30B)",
        "F1":"0.684",
        "Observed inference time (s)":"1.467",
        "# eval":"355",
        "# train":"2.037",
        "truncated":"0",
        "# prompt tokens":"1694.642",
        "# output tokens":"6.335"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "F1":"0.743",
        "Observed inference time (s)":"2.951",
        "# eval":"355",
        "# train":"2.037",
        "truncated":"0",
        "# prompt tokens":"1694.642",
        "# output tokens":"5.685"
    },
    {
        "Model":"Anthropic Claude v1.3",
        "F1":"0.723",
        "Observed inference time (s)":"6.114",
        "# eval":"355",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"3709.741",
        "# output tokens":"9.338"
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "F1":"0.616",
        "Observed inference time (s)":"1.491",
        "# eval":"355",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"3709.741",
        "# output tokens":"17.149"
    },
    {
        "Model":"Anthropic Claude 2.0",
        "F1":"0.718",
        "Observed inference time (s)":"4.811",
        "# eval":"355",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"3709.741",
        "# output tokens":"10.561"
    },
    {
        "Model":"Anthropic Claude 2.1",
        "F1":"0.677",
        "Observed inference time (s)":"5.376",
        "# eval":"355",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"3709.741",
        "# output tokens":"12.431"
    },
    {
        "Model":"Cohere Command",
        "F1":"0.749",
        "Observed inference time (s)":"1.783",
        "# eval":"355",
        "# train":"1.941",
        "truncated":"0",
        "# prompt tokens":"1660.485",
        "# output tokens":"7.442"
    },
    {
        "Model":"Cohere Command Light",
        "F1":"0.629",
        "Observed inference time (s)":"0.896",
        "# eval":"355",
        "# train":"1.941",
        "truncated":"0",
        "# prompt tokens":"1660.485",
        "# output tokens":"10.814"
    },
    {
        "Model":"PaLM-2 (Bison)",
        "F1":"0.718",
        "Observed inference time (s)":"1.031",
        "# eval":"355",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"4414.234",
        "# output tokens":"7.997"
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "F1":"0.583",
        "Observed inference time (s)":"3.283",
        "# eval":"355",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"4414.234",
        "# output tokens":"16.544"
    },
    {
        "Model":"Mistral Medium (2312)",
        "F1":"0.449",
        "Observed inference time (s)":"3.898",
        "# eval":"355",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"3955.33",
        "# output tokens":"24.885"
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "F1":"0.731",
        "Observed inference time (s)":"1.813",
        "# eval":"355",
        "# train":"4.955",
        "truncated":"0",
        "# prompt tokens":"3479.563",
        "# output tokens":"9.732"
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "F1":"0.719",
        "Observed inference time (s)":"1.226",
        "# eval":"355",
        "# train":"4.955",
        "truncated":"0",
        "# prompt tokens":"3479.563",
        "# output tokens":"8.448"
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "F1":"0.655",
        "Observed inference time (s)":"0.381",
        "# eval":"355",
        "# train":"4.946",
        "truncated":"0",
        "# prompt tokens":"3493.662",
        "# output tokens":"9.91"
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "F1":"0.727",
        "Observed inference time (s)":"1.068",
        "# eval":"355",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"3522.67",
        "# output tokens":"9.885"
    },
    {
        "Model":"GPT-4 (0613)",
        "F1":"0.768",
        "Observed inference time (s)":"0.976",
        "# eval":"355",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"3522.67",
        "# output tokens":"8.515"
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "F1":"0.752",
        "Observed inference time (s)":"1.202",
        "# eval":"355",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"3504.577",
        "# output tokens":"8.208"
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "F1":"0.706",
        "Observed inference time (s)":"2.849",
        "# eval":"355",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"3504.577",
        "# output tokens":"11.149"
    }
]