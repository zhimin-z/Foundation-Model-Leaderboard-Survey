[
    {
        "Model":"Falcon (40B)",
        "BLEU-4":0.162,
        "Observed inference time (s)":3.098,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":162.454,
        "# output tokens":1.0
    },
    {
        "Model":"Falcon (7B)",
        "BLEU-4":0.094,
        "Observed inference time (s)":1.604,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":162.454,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (13B)",
        "BLEU-4":0.167,
        "Observed inference time (s)":0.691,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":142.288,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (70B)",
        "BLEU-4":0.196,
        "Observed inference time (s)":1.074,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":142.288,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (7B)",
        "BLEU-4":0.144,
        "Observed inference time (s)":0.697,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":142.288,
        "# output tokens":1.0
    },
    {
        "Model":"LLaMA (65B)",
        "BLEU-4":0.189,
        "Observed inference time (s)":3.603,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":142.288,
        "# output tokens":1.0
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "BLEU-4":0.16,
        "Observed inference time (s)":0.561,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":144.433,
        "# output tokens":1.0
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "BLEU-4":0.19,
        "Observed inference time (s)":1.202,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":144.433,
        "# output tokens":0.999
    },
    {
        "Model":"Yi (34B)",
        "BLEU-4":0.172,
        "Observed inference time (s)":1.404,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":187.092,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (6B)",
        "BLEU-4":0.117,
        "Observed inference time (s)":0.626,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":187.092,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude 2.0",
        "BLEU-4":0.219,
        "Observed inference time (s)":1.995,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":218.573,
        "# output tokens":25.653
    },
    {
        "Model":"Anthropic Claude 2.1",
        "BLEU-4":0.204,
        "Observed inference time (s)":2.756,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":218.573,
        "# output tokens":25.235
    },
    {
        "Model":"Anthropic Claude v1.3",
        "BLEU-4":0.219,
        "Observed inference time (s)":2.232,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":218.573,
        "# output tokens":25.611
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "BLEU-4":0.194,
        "Observed inference time (s)":0.772,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":218.573,
        "# output tokens":25.579
    },
    {
        "Model":"Cohere Command",
        "BLEU-4":0.088,
        "Observed inference time (s)":2.894,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":149.459,
        "# output tokens":31.8
    },
    {
        "Model":"Cohere Command Light",
        "BLEU-4":0.023,
        "Observed inference time (s)":0.797,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":149.459,
        "# output tokens":39.885
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "BLEU-4":0.191,
        "Observed inference time (s)":0.8,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":181.694,
        "# output tokens":25.117
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "BLEU-4":0.174,
        "Observed inference time (s)":0.467,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":181.694,
        "# output tokens":24.862
    },
    {
        "Model":"GPT-4 (0613)",
        "BLEU-4":0.211,
        "Observed inference time (s)":1.58,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":193.043,
        "# output tokens":25.424
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "BLEU-4":0.205,
        "Observed inference time (s)":2.1,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":193.043,
        "# output tokens":26.996
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "BLEU-4":0.187,
        "Observed inference time (s)":0.394,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":193.043,
        "# output tokens":25.038
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "BLEU-4":0.239,
        "Observed inference time (s)":0.905,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":181.694,
        "# output tokens":25.142
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "BLEU-4":0.262,
        "Observed inference time (s)":1.406,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":181.694,
        "# output tokens":24.983
    },
    {
        "Model":"PaLM-2 (Bison)",
        "BLEU-4":0.241,
        "Observed inference time (s)":0.875,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":183.587,
        "# output tokens":29.981
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "BLEU-4":0.26,
        "Observed inference time (s)":1.801,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":183.587,
        "# output tokens":30.567
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "BLEU-4":0.114,
        "Observed inference time (s)":1.441,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":135.468,
        "# output tokens":24.063
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "BLEU-4":0.102,
        "Observed inference time (s)":0.759,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":135.468,
        "# output tokens":19.051
    },
    {
        "Model":"Luminous Base (13B)",
        "BLEU-4":0.066,
        "Observed inference time (s)":4.693,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":157.232,
        "# output tokens":99.974
    },
    {
        "Model":"Luminous Extended (30B)",
        "BLEU-4":0.083,
        "Observed inference time (s)":5.336,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":157.232,
        "# output tokens":100.0
    },
    {
        "Model":"Luminous Supreme (70B)",
        "BLEU-4":0.102,
        "Observed inference time (s)":11.052,
        "# eval":568.8,
        "# train":1,
        "truncated":0,
        "# prompt tokens":157.232,
        "# output tokens":100.0
    }
]