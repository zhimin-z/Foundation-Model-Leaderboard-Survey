[
    {
        "Model":"GPT-4 (0613)",
        "Mean win rate":"0.962",
        "NarrativeQA - F1":"0.768",
        "NaturalQuestions (open-book) - F1":"0.79",
        "NaturalQuestions (closed-book) - F1":"0.457",
        "OpenbookQA - EM":"0.96",
        "MMLU - EM":"0.735",
        "MATH - Equivalent (CoT)":"0.802",
        "GSM8K - EM":"0.932",
        "LegalBench - EM":"0.713",
        "MedQA - EM":"0.815",
        "WMT 2014 - BLEU-4":"0.211"
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "Mean win rate":"0.834",
        "NarrativeQA - F1":"0.727",
        "NaturalQuestions (open-book) - F1":"0.763",
        "NaturalQuestions (closed-book) - F1":"0.435",
        "OpenbookQA - EM":"0.95",
        "MMLU - EM":"0.699",
        "MATH - Equivalent (CoT)":"0.857",
        "GSM8K - EM":"0.668",
        "LegalBench - EM":"0.626",
        "MedQA - EM":"0.817",
        "WMT 2014 - BLEU-4":"0.205"
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "Mean win rate":"0.807",
        "NarrativeQA - F1":"0.706",
        "NaturalQuestions (open-book) - F1":"0.685",
        "NaturalQuestions (closed-book) - F1":"0.407",
        "OpenbookQA - EM":"0.938",
        "MMLU - EM":"0.702",
        "MATH - Equivalent (CoT)":"0.723",
        "GSM8K - EM":"0.831",
        "LegalBench - EM":"0.709",
        "MedQA - EM":"0.684",
        "WMT 2014 - BLEU-4":"0.262"
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "Mean win rate":"0.79",
        "NarrativeQA - F1":"0.583",
        "NaturalQuestions (open-book) - F1":"0.674",
        "NaturalQuestions (closed-book) - F1":"0.435",
        "OpenbookQA - EM":"0.938",
        "MMLU - EM":"0.702",
        "MATH - Equivalent (CoT)":"0.674",
        "GSM8K - EM":"0.831",
        "LegalBench - EM":"0.677",
        "MedQA - EM":"0.684",
        "WMT 2014 - BLEU-4":"0.26"
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "Mean win rate":"0.779",
        "NarrativeQA - F1":"0.752",
        "NaturalQuestions (open-book) - F1":"0.752",
        "NaturalQuestions (closed-book) - F1":"0.428",
        "OpenbookQA - EM":"0.878",
        "MMLU - EM":"0.621",
        "MATH - Equivalent (CoT)":"0.58",
        "GSM8K - EM":"0.735",
        "LegalBench - EM":"0.644",
        "MedQA - EM":"0.598",
        "WMT 2014 - BLEU-4":"0.239"
    },
    {
        "Model":"Yi (34B)",
        "Mean win rate":"0.772",
        "NarrativeQA - F1":"0.782",
        "NaturalQuestions (open-book) - F1":"0.775",
        "NaturalQuestions (closed-book) - F1":"0.443",
        "OpenbookQA - EM":"0.92",
        "MMLU - EM":"0.65",
        "MATH - Equivalent (CoT)":"0.375",
        "GSM8K - EM":"0.648",
        "LegalBench - EM":"0.618",
        "MedQA - EM":"0.656",
        "WMT 2014 - BLEU-4":"0.172"
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "Mean win rate":"0.728",
        "NarrativeQA - F1":"0.767",
        "NaturalQuestions (open-book) - F1":"0.699",
        "NaturalQuestions (closed-book) - F1":"0.427",
        "OpenbookQA - EM":"0.868",
        "MMLU - EM":"0.649",
        "MATH - Equivalent (CoT)":"0.494",
        "GSM8K - EM":"0.622",
        "LegalBench - EM":"0.63",
        "MedQA - EM":"0.652",
        "WMT 2014 - BLEU-4":"0.19"
    },
    {
        "Model":"Anthropic Claude v1.3",
        "Mean win rate":"0.724",
        "NarrativeQA - F1":"0.723",
        "NaturalQuestions (open-book) - F1":"0.699",
        "NaturalQuestions (closed-book) - F1":"0.409",
        "OpenbookQA - EM":"0.908",
        "MMLU - EM":"0.631",
        "MATH - Equivalent (CoT)":"0.54",
        "GSM8K - EM":"0.784",
        "LegalBench - EM":"0.629",
        "MedQA - EM":"0.618",
        "WMT 2014 - BLEU-4":"0.219"
    },
    {
        "Model":"PaLM-2 (Bison)",
        "Mean win rate":"0.69",
        "NarrativeQA - F1":"0.718",
        "NaturalQuestions (open-book) - F1":"0.813",
        "NaturalQuestions (closed-book) - F1":"0.39",
        "OpenbookQA - EM":"0.878",
        "MMLU - EM":"0.608",
        "MATH - Equivalent (CoT)":"0.421",
        "GSM8K - EM":"0.61",
        "LegalBench - EM":"0.645",
        "MedQA - EM":"0.547",
        "WMT 2014 - BLEU-4":"0.241"
    },
    {
        "Model":"Anthropic Claude 2.0",
        "Mean win rate":"0.679",
        "NarrativeQA - F1":"0.718",
        "NaturalQuestions (open-book) - F1":"0.67",
        "NaturalQuestions (closed-book) - F1":"0.428",
        "OpenbookQA - EM":"0.862",
        "MMLU - EM":"0.639",
        "MATH - Equivalent (CoT)":"0.603",
        "GSM8K - EM":"0.583",
        "LegalBench - EM":"0.643",
        "MedQA - EM":"0.652",
        "WMT 2014 - BLEU-4":"0.219"
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"0.659",
        "NarrativeQA - F1":"0.763",
        "NaturalQuestions (open-book) - F1":"0.674",
        "NaturalQuestions (closed-book) - F1":"0.46",
        "OpenbookQA - EM":"0.838",
        "MMLU - EM":"0.58",
        "MATH - Equivalent (CoT)":"0.323",
        "GSM8K - EM":"0.567",
        "LegalBench - EM":"0.673",
        "MedQA - EM":"0.618",
        "WMT 2014 - BLEU-4":"0.196"
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "Mean win rate":"0.621",
        "NarrativeQA - F1":"0.731",
        "NaturalQuestions (open-book) - F1":"0.77",
        "NaturalQuestions (closed-book) - F1":"0.413",
        "OpenbookQA - EM":"0.828",
        "MMLU - EM":"0.555",
        "MATH - Equivalent (CoT)":"0.449",
        "GSM8K - EM":"0.615",
        "LegalBench - EM":"0.622",
        "MedQA - EM":"0.531",
        "WMT 2014 - BLEU-4":"0.191"
    },
    {
        "Model":"Anthropic Claude 2.1",
        "Mean win rate":"0.593",
        "NarrativeQA - F1":"0.677",
        "NaturalQuestions (open-book) - F1":"0.611",
        "NaturalQuestions (closed-book) - F1":"0.375",
        "OpenbookQA - EM":"0.872",
        "MMLU - EM":"0.643",
        "MATH - Equivalent (CoT)":"0.632",
        "GSM8K - EM":"0.604",
        "LegalBench - EM":"0.643",
        "MedQA - EM":"0.644",
        "WMT 2014 - BLEU-4":"0.204"
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "Mean win rate":"0.562",
        "NarrativeQA - F1":"0.616",
        "NaturalQuestions (open-book) - F1":"0.731",
        "NaturalQuestions (closed-book) - F1":"0.343",
        "OpenbookQA - EM":"0.844",
        "MMLU - EM":"0.631",
        "MATH - Equivalent (CoT)":"0.499",
        "GSM8K - EM":"0.721",
        "LegalBench - EM":"0.586",
        "MedQA - EM":"0.559",
        "WMT 2014 - BLEU-4":"0.194"
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "Mean win rate":"0.514",
        "NarrativeQA - F1":"0.719",
        "NaturalQuestions (open-book) - F1":"0.71",
        "NaturalQuestions (closed-book) - F1":"0.394",
        "OpenbookQA - EM":"0.796",
        "MMLU - EM":"0.568",
        "MATH - Equivalent (CoT)":"0.428",
        "GSM8K - EM":"0.479",
        "LegalBench - EM":"0.58",
        "MedQA - EM":"0.525",
        "WMT 2014 - BLEU-4":"0.174"
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "Mean win rate":"0.507",
        "NarrativeQA - F1":"0.655",
        "NaturalQuestions (open-book) - F1":"0.678",
        "NaturalQuestions (closed-book) - F1":"0.335",
        "OpenbookQA - EM":"0.838",
        "MMLU - EM":"0.614",
        "MATH - Equivalent (CoT)":"0.667",
        "GSM8K - EM":"0.501",
        "LegalBench - EM":"0.528",
        "MedQA - EM":"0.622",
        "WMT 2014 - BLEU-4":"0.187"
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"0.503",
        "NarrativeQA - F1":"0.755",
        "NaturalQuestions (open-book) - F1":"0.672",
        "NaturalQuestions (closed-book) - F1":"0.433",
        "OpenbookQA - EM":"0.754",
        "MMLU - EM":"0.584",
        "MATH - Equivalent (CoT)":"0.257",
        "GSM8K - EM":"0.489",
        "LegalBench - EM":"0.48",
        "MedQA - EM":"0.507",
        "WMT 2014 - BLEU-4":"0.189"
    },
    {
        "Model":"Cohere Command",
        "Mean win rate":"0.462",
        "NarrativeQA - F1":"0.749",
        "NaturalQuestions (open-book) - F1":"0.777",
        "NaturalQuestions (closed-book) - F1":"0.391",
        "OpenbookQA - EM":"0.774",
        "MMLU - EM":"0.525",
        "MATH - Equivalent (CoT)":"0.236",
        "GSM8K - EM":"0.452",
        "LegalBench - EM":"0.578",
        "MedQA - EM":"0.445",
        "WMT 2014 - BLEU-4":"0.088"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"0.438",
        "NarrativeQA - F1":"0.716",
        "NaturalQuestions (open-book) - F1":"0.687",
        "NaturalQuestions (closed-book) - F1":"0.367",
        "OpenbookQA - EM":"0.776",
        "MMLU - EM":"0.584",
        "MATH - Equivalent (CoT)":"0.297",
        "GSM8K - EM":"0.377",
        "LegalBench - EM":"0.58",
        "MedQA - EM":"0.525",
        "WMT 2014 - BLEU-4":"0.16"
    },
    {
        "Model":"Yi (6B)",
        "Mean win rate":"0.376",
        "NarrativeQA - F1":"0.702",
        "NaturalQuestions (open-book) - F1":"0.748",
        "NaturalQuestions (closed-book) - F1":"0.31",
        "OpenbookQA - EM":"0.8",
        "MMLU - EM":"0.53",
        "MATH - Equivalent (CoT)":"0.126",
        "GSM8K - EM":"0.375",
        "LegalBench - EM":"0.519",
        "MedQA - EM":"0.497",
        "WMT 2014 - BLEU-4":"0.117"
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"0.348",
        "NarrativeQA - F1":"0.741",
        "NaturalQuestions (open-book) - F1":"0.64",
        "NaturalQuestions (closed-book) - F1":"0.371",
        "OpenbookQA - EM":"0.634",
        "MMLU - EM":"0.505",
        "MATH - Equivalent (CoT)":"0.102",
        "GSM8K - EM":"0.266",
        "LegalBench - EM":"0.591",
        "MedQA - EM":"0.392",
        "WMT 2014 - BLEU-4":"0.167"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"0.331",
        "NarrativeQA - F1":"0.728",
        "NaturalQuestions (open-book) - F1":"0.65",
        "NaturalQuestions (closed-book) - F1":"0.385",
        "OpenbookQA - EM":"0.688",
        "MMLU - EM":"0.483",
        "MATH - Equivalent (CoT)":"0.103",
        "GSM8K - EM":"0.239",
        "LegalBench - EM":"0.533",
        "MedQA - EM":"0.431",
        "WMT 2014 - BLEU-4":"0.114"
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"0.324",
        "NarrativeQA - F1":"0.671",
        "NaturalQuestions (open-book) - F1":"0.676",
        "NaturalQuestions (closed-book) - F1":"0.392",
        "OpenbookQA - EM":"0.662",
        "MMLU - EM":"0.507",
        "MATH - Equivalent (CoT)":"0.128",
        "GSM8K - EM":"0.267",
        "LegalBench - EM":"0.442",
        "MedQA - EM":"0.419",
        "WMT 2014 - BLEU-4":"0.162"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"0.252",
        "NarrativeQA - F1":"0.744",
        "NaturalQuestions (open-book) - F1":"0.627",
        "NaturalQuestions (closed-book) - F1":"0.35",
        "OpenbookQA - EM":"0.614",
        "MMLU - EM":"0.471",
        "MATH - Equivalent (CoT)":"0.064",
        "GSM8K - EM":"0.159",
        "LegalBench - EM":"0.468",
        "MedQA - EM":"0.39",
        "WMT 2014 - BLEU-4":"0.102"
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"0.217",
        "NarrativeQA - F1":"0.686",
        "NaturalQuestions (open-book) - F1":"0.612",
        "NaturalQuestions (closed-book) - F1":"0.333",
        "OpenbookQA - EM":"0.544",
        "MMLU - EM":"0.425",
        "MATH - Equivalent (CoT)":"0.097",
        "GSM8K - EM":"0.154",
        "LegalBench - EM":"0.502",
        "MedQA - EM":"0.392",
        "WMT 2014 - BLEU-4":"0.144"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"0.203",
        "NarrativeQA - F1":"0.743",
        "NaturalQuestions (open-book) - F1":"0.656",
        "NaturalQuestions (closed-book) - F1":"0.299",
        "OpenbookQA - EM":"0.284",
        "MMLU - EM":"0.316",
        "MATH - Equivalent (CoT)":"0.078",
        "GSM8K - EM":"0.137",
        "LegalBench - EM":"0.452",
        "MedQA - EM":"0.276",
        "WMT 2014 - BLEU-4":"0.102"
    },
    {
        "Model":"Cohere Command Light",
        "Mean win rate":"0.148",
        "NarrativeQA - F1":"0.629",
        "NaturalQuestions (open-book) - F1":"0.686",
        "NaturalQuestions (closed-book) - F1":"0.195",
        "OpenbookQA - EM":"0.398",
        "MMLU - EM":"0.386",
        "MATH - Equivalent (CoT)":"0.098",
        "GSM8K - EM":"0.149",
        "LegalBench - EM":"0.397",
        "MedQA - EM":"0.312",
        "WMT 2014 - BLEU-4":"0.023"
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"0.086",
        "NarrativeQA - F1":"0.684",
        "NaturalQuestions (open-book) - F1":"0.611",
        "NaturalQuestions (closed-book) - F1":"0.253",
        "OpenbookQA - EM":"0.272",
        "MMLU - EM":"0.248",
        "MATH - Equivalent (CoT)":"0.04",
        "GSM8K - EM":"0.075",
        "LegalBench - EM":"0.421",
        "MedQA - EM":"0.276",
        "WMT 2014 - BLEU-4":"0.083"
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"0.055",
        "NarrativeQA - F1":"0.621",
        "NaturalQuestions (open-book) - F1":"0.58",
        "NaturalQuestions (closed-book) - F1":"0.285",
        "OpenbookQA - EM":"0.26",
        "MMLU - EM":"0.288",
        "MATH - Equivalent (CoT)":"0.044",
        "GSM8K - EM":"0.055",
        "LegalBench - EM":"0.346",
        "MedQA - EM":"0.254",
        "WMT 2014 - BLEU-4":"0.094"
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"0.034",
        "NarrativeQA - F1":"0.633",
        "NaturalQuestions (open-book) - F1":"0.577",
        "NaturalQuestions (closed-book) - F1":"0.197",
        "OpenbookQA - EM":"0.286",
        "MMLU - EM":"0.243",
        "MATH - Equivalent (CoT)":"0.026",
        "GSM8K - EM":"0.028",
        "LegalBench - EM":"0.332",
        "MedQA - EM":"0.26",
        "WMT 2014 - BLEU-4":"0.066"
    }
]