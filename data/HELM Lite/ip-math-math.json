[
    {
        "Model":"Falcon (40B)",
        "Equivalent (CoT)":0.128,
        "Observed inference time (s)":11.414,
        "# eval":62.429,
        "# train":6.818,
        "truncated":0,
        "# prompt tokens":1150.049,
        "# output tokens":1.0
    },
    {
        "Model":"Falcon (7B)",
        "Equivalent (CoT)":0.044,
        "Observed inference time (s)":6.987,
        "# eval":62.429,
        "# train":6.818,
        "truncated":0,
        "# prompt tokens":1150.049,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (13B)",
        "Equivalent (CoT)":0.102,
        "Observed inference time (s)":1.516,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1438.636,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (70B)",
        "Equivalent (CoT)":0.323,
        "Observed inference time (s)":2.443,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1438.636,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (7B)",
        "Equivalent (CoT)":0.097,
        "Observed inference time (s)":2.66,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1438.636,
        "# output tokens":1.0
    },
    {
        "Model":"LLaMA (65B)",
        "Equivalent (CoT)":0.257,
        "Observed inference time (s)":20.79,
        "# eval":62.429,
        "# train":6.897,
        "truncated":0,
        "# prompt tokens":1214.707,
        "# output tokens":1.0
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Equivalent (CoT)":0.297,
        "Observed inference time (s)":1.159,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1455.266,
        "# output tokens":1.0
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "Equivalent (CoT)":0.494,
        "Observed inference time (s)":1.528,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1455.266,
        "# output tokens":1.0
    },
    {
        "Model":"Phi-2",
        "Equivalent (CoT)":0.255,
        "Observed inference time (s)":1.129,
        "# eval":62.429,
        "# train":6.916,
        "truncated":0,
        "# prompt tokens":1162.126,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (34B)",
        "Equivalent (CoT)":0.375,
        "Observed inference time (s)":3.809,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1468.935,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (6B)",
        "Equivalent (CoT)":0.126,
        "Observed inference time (s)":1.837,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1468.935,
        "# output tokens":1.0
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Equivalent (CoT)":0.103,
        "Observed inference time (s)":9.136,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1321.422,
        "# output tokens":136.538
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Equivalent (CoT)":0.064,
        "Observed inference time (s)":4.862,
        "# eval":62.429,
        "# train":6.778,
        "truncated":0,
        "# prompt tokens":943.419,
        "# output tokens":140.295
    },
    {
        "Model":"Luminous Base (13B)",
        "Equivalent (CoT)":0.026,
        "Observed inference time (s)":9.204,
        "# eval":62.429,
        "# train":6.916,
        "truncated":0,
        "# prompt tokens":1184.139,
        "# output tokens":139.637
    },
    {
        "Model":"Luminous Extended (30B)",
        "Equivalent (CoT)":0.04,
        "Observed inference time (s)":9.364,
        "# eval":62.429,
        "# train":6.916,
        "truncated":0,
        "# prompt tokens":1184.139,
        "# output tokens":142.866
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Equivalent (CoT)":0.078,
        "Observed inference time (s)":16.874,
        "# eval":62.429,
        "# train":6.916,
        "truncated":0,
        "# prompt tokens":1184.139,
        "# output tokens":127.587
    },
    {
        "Model":"Anthropic Claude v1.3",
        "Equivalent (CoT)":0.54,
        "Observed inference time (s)":6.109,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1361.814,
        "# output tokens":79.493
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "Equivalent (CoT)":0.499,
        "Observed inference time (s)":1.403,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1361.814,
        "# output tokens":65.956
    },
    {
        "Model":"Anthropic Claude 2.0",
        "Equivalent (CoT)":0.603,
        "Observed inference time (s)":6.211,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1361.814,
        "# output tokens":96.474
    },
    {
        "Model":"Anthropic Claude 2.1",
        "Equivalent (CoT)":0.632,
        "Observed inference time (s)":9.672,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1361.814,
        "# output tokens":96.72
    },
    {
        "Model":"Cohere Command",
        "Equivalent (CoT)":0.236,
        "Observed inference time (s)":5.762,
        "# eval":62.429,
        "# train":6.878,
        "truncated":0,
        "# prompt tokens":1177.329,
        "# output tokens":116.49
    },
    {
        "Model":"Cohere Command Light",
        "Equivalent (CoT)":0.098,
        "Observed inference time (s)":2.374,
        "# eval":62.429,
        "# train":6.878,
        "truncated":0,
        "# prompt tokens":1177.329,
        "# output tokens":106.589
    },
    {
        "Model":"PaLM-2 (Bison)",
        "Equivalent (CoT)":0.421,
        "Observed inference time (s)":1.614,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1439.843,
        "# output tokens":66.89
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "Equivalent (CoT)":0.674,
        "Observed inference time (s)":4.636,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1439.843,
        "# output tokens":80.458
    },
    {
        "Model":"Mistral Medium (2312)",
        "Equivalent (CoT)":0.565,
        "Observed inference time (s)":7.086,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1525.266,
        "# output tokens":113.328
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "Equivalent (CoT)":0.449,
        "Observed inference time (s)":4.334,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1375.735,
        "# output tokens":74.938
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "Equivalent (CoT)":0.428,
        "Observed inference time (s)":5.188,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1375.735,
        "# output tokens":136.822
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "Equivalent (CoT)":0.667,
        "Observed inference time (s)":0.813,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1323.911,
        "# output tokens":60.844
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "Equivalent (CoT)":0.857,
        "Observed inference time (s)":12.704,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1323.911,
        "# output tokens":161.876
    },
    {
        "Model":"GPT-4 (0613)",
        "Equivalent (CoT)":0.802,
        "Observed inference time (s)":3.472,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1323.911,
        "# output tokens":73.257
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "Equivalent (CoT)":0.58,
        "Observed inference time (s)":2.088,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1375.735,
        "# output tokens":87.032
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "Equivalent (CoT)":0.723,
        "Observed inference time (s)":4.259,
        "# eval":62.429,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1375.735,
        "# output tokens":83.135
    }
]