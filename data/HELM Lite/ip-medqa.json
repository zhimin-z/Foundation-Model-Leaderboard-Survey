[
    {
        "Model":"Falcon (40B)",
        "EM":0.419,
        "Observed inference time (s)":2.203,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1048.624,
        "# output tokens":1.0
    },
    {
        "Model":"Falcon (7B)",
        "EM":0.254,
        "Observed inference time (s)":0.735,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1048.624,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":0.392,
        "Observed inference time (s)":0.459,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1234.901,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":0.618,
        "Observed inference time (s)":0.971,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1234.901,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":0.392,
        "Observed inference time (s)":0.467,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1234.901,
        "# output tokens":1.0
    },
    {
        "Model":"LLaMA (65B)",
        "EM":0.507,
        "Observed inference time (s)":4.984,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1234.901,
        "# output tokens":1.0
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":0.525,
        "Observed inference time (s)":0.348,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1193.093,
        "# output tokens":1.0
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "EM":0.652,
        "Observed inference time (s)":0.353,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1193.093,
        "# output tokens":1.0
    },
    {
        "Model":"Phi-2",
        "EM":0.41,
        "Observed inference time (s)":0.275,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1038.833,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (34B)",
        "EM":0.656,
        "Observed inference time (s)":1.064,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1122.392,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (6B)",
        "EM":0.497,
        "Observed inference time (s)":0.405,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1122.392,
        "# output tokens":1.0
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":0.431,
        "Observed inference time (s)":1.535,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":758.622,
        "# output tokens":1.0
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":0.39,
        "Observed inference time (s)":0.914,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":758.622,
        "# output tokens":1.0
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":0.26,
        "Observed inference time (s)":0.726,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1005.229,
        "# output tokens":1.0
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":0.276,
        "Observed inference time (s)":0.895,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1005.229,
        "# output tokens":1.0
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":0.276,
        "Observed inference time (s)":1.326,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1005.229,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude v1.3",
        "EM":0.618,
        "Observed inference time (s)":3.39,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1092.437,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "EM":0.559,
        "Observed inference time (s)":0.763,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1092.437,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude 2.0",
        "EM":0.652,
        "Observed inference time (s)":2.254,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1092.437,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude 2.1",
        "EM":0.644,
        "Observed inference time (s)":2.482,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1092.437,
        "# output tokens":1.0
    },
    {
        "Model":"Cohere Command",
        "EM":0.445,
        "Observed inference time (s)":1.234,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1016.738,
        "# output tokens":1.0
    },
    {
        "Model":"Cohere Command Light",
        "EM":0.312,
        "Observed inference time (s)":0.896,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1016.738,
        "# output tokens":1.0
    },
    {
        "Model":"PaLM-2 (Bison)",
        "EM":0.547,
        "Observed inference time (s)":0.735,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1138.622,
        "# output tokens":1.0
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "EM":0.684,
        "Observed inference time (s)":1.178,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1138.622,
        "# output tokens":1.0
    },
    {
        "Model":"Mistral Medium (2312)",
        "EM":0.61,
        "Observed inference time (s)":2.813,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1193.093,
        "# output tokens":0.95
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "EM":0.531,
        "Observed inference time (s)":0.228,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1038.861,
        "# output tokens":1.0
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "EM":0.525,
        "Observed inference time (s)":0.206,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1038.861,
        "# output tokens":1.0
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "EM":0.622,
        "Observed inference time (s)":0.194,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1020.414,
        "# output tokens":1.0
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "EM":0.817,
        "Observed inference time (s)":0.392,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1020.414,
        "# output tokens":1.0
    },
    {
        "Model":"GPT-4 (0613)",
        "EM":0.815,
        "Observed inference time (s)":0.414,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1020.414,
        "# output tokens":1.0
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "EM":0.598,
        "Observed inference time (s)":0.605,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1038.861,
        "# output tokens":1.0
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "EM":0.684,
        "Observed inference time (s)":0.927,
        "# eval":503,
        "# train":5,
        "truncated":0,
        "# prompt tokens":1038.861,
        "# output tokens":1.0
    }
]