[
    {
        "Model":"Falcon (40B)",
        "EM":"0.597",
        "Observed inference time (s)":"1.333",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"251.171",
        "# output tokens":"1"
    },
    {
        "Model":"Falcon (7B)",
        "EM":"0.485",
        "Observed inference time (s)":"0.558",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"251.171",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":"0.618",
        "Observed inference time (s)":"0.331",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"276.825",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":"0.62",
        "Observed inference time (s)":"0.53",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"276.825",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":"0.566",
        "Observed inference time (s)":"0.344",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"276.825",
        "# output tokens":"1"
    },
    {
        "Model":"LLaMA (65B)",
        "EM":"0.561",
        "Observed inference time (s)":"6.264",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"276.825",
        "# output tokens":"1"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":"0.53",
        "Observed inference time (s)":"0.298",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"274.575",
        "# output tokens":"1"
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "EM":"0.612",
        "Observed inference time (s)":"0.373",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"274.575",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (34B)",
        "EM":"0.583",
        "Observed inference time (s)":"0.656",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"260.265",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (6B)",
        "EM":"0.506",
        "Observed inference time (s)":"0.409",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"260.265",
        "# output tokens":"1"
    },
    {
        "Model":"Anthropic Claude 2.0",
        "EM":"0.392",
        "Observed inference time (s)":"2.155",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"323.505",
        "# output tokens":"11.058"
    },
    {
        "Model":"Anthropic Claude 2.1",
        "EM":"0.459",
        "Observed inference time (s)":"2.23",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"323.505",
        "# output tokens":"1.522"
    },
    {
        "Model":"Anthropic Claude v1.3",
        "EM":"0.449",
        "Observed inference time (s)":"2.058",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"323.505",
        "# output tokens":"1"
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "EM":"0.348",
        "Observed inference time (s)":"0.64",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"323.505",
        "# output tokens":"2.219"
    },
    {
        "Model":"Cohere Command",
        "EM":"0.409",
        "Observed inference time (s)":"1.032",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"246.658",
        "# output tokens":"1.113"
    },
    {
        "Model":"Cohere Command Light",
        "EM":"0.567",
        "Observed inference time (s)":"0.475",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"246.658",
        "# output tokens":"3.328"
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "EM":"0.598",
        "Observed inference time (s)":"0.225",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"248.054",
        "# output tokens":"1.443"
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "EM":"0.57",
        "Observed inference time (s)":"0.182",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"248.054",
        "# output tokens":"1.238"
    },
    {
        "Model":"GPT-4 (0613)",
        "EM":"0.63",
        "Observed inference time (s)":"0.482",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"298.86",
        "# output tokens":"1"
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "EM":"0.54",
        "Observed inference time (s)":"0.468",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"298.86",
        "# output tokens":"1.035"
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "EM":"0.59",
        "Observed inference time (s)":"0.182",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"298.86",
        "# output tokens":"1.483"
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "EM":"0.511",
        "Observed inference time (s)":"0.425",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"248.054",
        "# output tokens":"1.043"
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "EM":"0.607",
        "Observed inference time (s)":"0.668",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"248.054",
        "# output tokens":"1"
    },
    {
        "Model":"PaLM-2 (Bison)",
        "EM":"0.484",
        "Observed inference time (s)":"0.646",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"358.853",
        "# output tokens":"1.019"
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "EM":"0.472",
        "Observed inference time (s)":"1.032",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"358.853",
        "# output tokens":"1"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":"0.425",
        "Observed inference time (s)":"1.126",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"183.837",
        "# output tokens":"2.043"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":"0.375",
        "Observed inference time (s)":"0.535",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"183.837",
        "# output tokens":"2.057"
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":"0.601",
        "Observed inference time (s)":"0.717",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"250.418",
        "# output tokens":"1.169"
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":"0.54",
        "Observed inference time (s)":"0.813",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"250.418",
        "# output tokens":"1.28"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":"0.59",
        "Observed inference time (s)":"0.866",
        "# eval":"1000",
        "# train":"4",
        "truncated":"0",
        "# prompt tokens":"250.418",
        "# output tokens":"1.031"
    }
]