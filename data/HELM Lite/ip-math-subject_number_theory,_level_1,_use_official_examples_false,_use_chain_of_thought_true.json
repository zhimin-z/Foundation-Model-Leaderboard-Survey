[
    {
        "Model":"Falcon (40B)",
        "Equivalent (CoT)":0.067,
        "Observed inference time (s)":13.883,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1101.667,
        "# output tokens":1.0
    },
    {
        "Model":"Falcon (7B)",
        "Equivalent (CoT)":0.0,
        "Observed inference time (s)":10.873,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1101.667,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (13B)",
        "Equivalent (CoT)":0.067,
        "Observed inference time (s)":1.771,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1121.633,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (70B)",
        "Equivalent (CoT)":0.4,
        "Observed inference time (s)":2.715,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1121.633,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (7B)",
        "Equivalent (CoT)":0.067,
        "Observed inference time (s)":5.271,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1121.633,
        "# output tokens":1.0
    },
    {
        "Model":"LLaMA (65B)",
        "Equivalent (CoT)":0.1,
        "Observed inference time (s)":24.145,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1121.633,
        "# output tokens":1.0
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Equivalent (CoT)":0.067,
        "Observed inference time (s)":1.112,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1148.8,
        "# output tokens":1.0
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "Equivalent (CoT)":0.367,
        "Observed inference time (s)":1.379,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1148.8,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (34B)",
        "Equivalent (CoT)":0.167,
        "Observed inference time (s)":4.599,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1128.2,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (6B)",
        "Equivalent (CoT)":0.1,
        "Observed inference time (s)":2.207,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1128.2,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude 2.0",
        "Equivalent (CoT)":0.533,
        "Observed inference time (s)":6.815,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1021.767,
        "# output tokens":104.4
    },
    {
        "Model":"Anthropic Claude 2.1",
        "Equivalent (CoT)":0.533,
        "Observed inference time (s)":9.573,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1021.767,
        "# output tokens":88.4
    },
    {
        "Model":"Anthropic Claude v1.3",
        "Equivalent (CoT)":0.433,
        "Observed inference time (s)":4.825,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1021.767,
        "# output tokens":53.133
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "Equivalent (CoT)":0.533,
        "Observed inference time (s)":1.271,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1021.767,
        "# output tokens":67.967
    },
    {
        "Model":"Cohere Command",
        "Equivalent (CoT)":0.1,
        "Observed inference time (s)":5.633,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1025.533,
        "# output tokens":124.833
    },
    {
        "Model":"Cohere Command Light",
        "Equivalent (CoT)":0.167,
        "Observed inference time (s)":2.198,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1025.533,
        "# output tokens":97.733
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "Equivalent (CoT)":0.3,
        "Observed inference time (s)":4.339,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1000.067,
        "# output tokens":78.1
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "Equivalent (CoT)":0.3,
        "Observed inference time (s)":3.628,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1000.067,
        "# output tokens":92.233
    },
    {
        "Model":"GPT-4 (0613)",
        "Equivalent (CoT)":0.833,
        "Observed inference time (s)":4.247,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1044.233,
        "# output tokens":81.1
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "Equivalent (CoT)":0.867,
        "Observed inference time (s)":15.09,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1044.233,
        "# output tokens":176.3
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "Equivalent (CoT)":0.533,
        "Observed inference time (s)":0.82,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1044.233,
        "# output tokens":59.667
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "Equivalent (CoT)":0.5,
        "Observed inference time (s)":2.285,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1000.067,
        "# output tokens":93.867
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "Equivalent (CoT)":0.6,
        "Observed inference time (s)":3.406,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1000.067,
        "# output tokens":62.2
    },
    {
        "Model":"PaLM-2 (Bison)",
        "Equivalent (CoT)":0.367,
        "Observed inference time (s)":1.593,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1120.2,
        "# output tokens":38.4
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "Equivalent (CoT)":0.533,
        "Observed inference time (s)":4.016,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1120.2,
        "# output tokens":59.9
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Equivalent (CoT)":0.033,
        "Observed inference time (s)":13.015,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":884.433,
        "# output tokens":220.133
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Equivalent (CoT)":0.033,
        "Observed inference time (s)":6.274,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":884.433,
        "# output tokens":209.933
    },
    {
        "Model":"Luminous Base (13B)",
        "Equivalent (CoT)":0.067,
        "Observed inference time (s)":8.214,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1010.833,
        "# output tokens":175.633
    },
    {
        "Model":"Luminous Extended (30B)",
        "Equivalent (CoT)":0.033,
        "Observed inference time (s)":11.104,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1010.833,
        "# output tokens":180.2
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Equivalent (CoT)":0.067,
        "Observed inference time (s)":18.837,
        "# eval":30,
        "# train":8,
        "truncated":0,
        "# prompt tokens":1010.833,
        "# output tokens":148.0
    }
]