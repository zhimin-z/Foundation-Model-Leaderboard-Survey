[
    {
        "Model":"Falcon (40B)",
        "BLEU-4":0.19,
        "Observed inference time (s)":2.468,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":124.726,
        "# output tokens":1.0
    },
    {
        "Model":"Falcon (7B)",
        "BLEU-4":0.137,
        "Observed inference time (s)":1.28,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":124.726,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (13B)",
        "BLEU-4":0.175,
        "Observed inference time (s)":0.557,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":130.384,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (70B)",
        "BLEU-4":0.189,
        "Observed inference time (s)":0.809,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":130.384,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (7B)",
        "BLEU-4":0.154,
        "Observed inference time (s)":0.625,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":130.384,
        "# output tokens":1.0
    },
    {
        "Model":"LLaMA (65B)",
        "BLEU-4":0.186,
        "Observed inference time (s)":3.71,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":130.384,
        "# output tokens":1.0
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "BLEU-4":0.175,
        "Observed inference time (s)":0.525,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":135.302,
        "# output tokens":1.0
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "BLEU-4":0.189,
        "Observed inference time (s)":1.115,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":135.302,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (34B)",
        "BLEU-4":0.181,
        "Observed inference time (s)":1.141,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":147.183,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (6B)",
        "BLEU-4":0.142,
        "Observed inference time (s)":0.619,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":147.183,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude 2.0",
        "BLEU-4":0.205,
        "Observed inference time (s)":1.692,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":201.853,
        "# output tokens":24.254
    },
    {
        "Model":"Anthropic Claude 2.1",
        "BLEU-4":0.193,
        "Observed inference time (s)":2.495,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":201.853,
        "# output tokens":24.439
    },
    {
        "Model":"Anthropic Claude v1.3",
        "BLEU-4":0.202,
        "Observed inference time (s)":1.391,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":201.853,
        "# output tokens":24.004
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "BLEU-4":0.184,
        "Observed inference time (s)":0.726,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":201.853,
        "# output tokens":24.177
    },
    {
        "Model":"Cohere Command",
        "BLEU-4":0.085,
        "Observed inference time (s)":3.028,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":129.757,
        "# output tokens":41.789
    },
    {
        "Model":"Cohere Command Light",
        "BLEU-4":0.036,
        "Observed inference time (s)":0.737,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":129.757,
        "# output tokens":40.406
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "BLEU-4":0.194,
        "Observed inference time (s)":0.756,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":141.443,
        "# output tokens":23.563
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "BLEU-4":0.186,
        "Observed inference time (s)":0.446,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":141.443,
        "# output tokens":23.557
    },
    {
        "Model":"GPT-4 (0613)",
        "BLEU-4":0.2,
        "Observed inference time (s)":1.448,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":178.231,
        "# output tokens":23.767
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "BLEU-4":0.181,
        "Observed inference time (s)":2.063,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":178.231,
        "# output tokens":28.59
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "BLEU-4":0.188,
        "Observed inference time (s)":0.39,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":178.231,
        "# output tokens":24.821
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "BLEU-4":0.226,
        "Observed inference time (s)":0.9,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":141.443,
        "# output tokens":23.829
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "BLEU-4":0.235,
        "Observed inference time (s)":1.32,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":141.443,
        "# output tokens":23.356
    },
    {
        "Model":"PaLM-2 (Bison)",
        "BLEU-4":0.222,
        "Observed inference time (s)":0.826,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":191.986,
        "# output tokens":28.076
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "BLEU-4":0.236,
        "Observed inference time (s)":1.706,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":191.986,
        "# output tokens":28.596
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "BLEU-4":0.122,
        "Observed inference time (s)":1.665,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":123.229,
        "# output tokens":30.439
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "BLEU-4":0.141,
        "Observed inference time (s)":0.723,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":123.229,
        "# output tokens":17.372
    },
    {
        "Model":"Luminous Base (13B)",
        "BLEU-4":0.142,
        "Observed inference time (s)":4.671,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":100.137,
        "# output tokens":100.0
    },
    {
        "Model":"Luminous Extended (30B)",
        "BLEU-4":0.163,
        "Observed inference time (s)":5.231,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":100.137,
        "# output tokens":100.0
    },
    {
        "Model":"Luminous Supreme (70B)",
        "BLEU-4":0.171,
        "Observed inference time (s)":10.924,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":100.137,
        "# output tokens":100.0
    }
]