[
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":2.023,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":1692.33,
        "NarrativeQA - # output tokens":1.0,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.598,
        "NaturalQuestions (open-book) - truncated":0.039,
        "NaturalQuestions (open-book) - # prompt tokens":1586.717,
        "NaturalQuestions (open-book) - # output tokens":0.991,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":124.246,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":251.174,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":500.12,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":6.818,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1150.049,
        "MATH - # output tokens":1.0,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1056.967,
        "GSM8K - # output tokens":1.0,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":3.853,
        "LegalBench - truncated":0.003,
        "LegalBench - # prompt tokens":566.694,
        "LegalBench - # output tokens":0.975,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1048.624,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":162.454,
        "WMT 2014 - # output tokens":1.0
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":2.023,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":1692.33,
        "NarrativeQA - # output tokens":1.0,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.598,
        "NaturalQuestions (open-book) - truncated":0.039,
        "NaturalQuestions (open-book) - # prompt tokens":1586.717,
        "NaturalQuestions (open-book) - # output tokens":0.99,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":124.246,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":251.174,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":500.12,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":6.818,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1150.049,
        "MATH - # output tokens":1.0,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1056.967,
        "GSM8K - # output tokens":1.0,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":3.853,
        "LegalBench - truncated":0.003,
        "LegalBench - # prompt tokens":566.694,
        "LegalBench - # output tokens":0.996,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1048.624,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":162.454,
        "WMT 2014 - # output tokens":1.0
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":4.408,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3669.808,
        "NarrativeQA - # output tokens":1.0,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.831,
        "NaturalQuestions (open-book) - truncated":0.026,
        "NaturalQuestions (open-book) - # prompt tokens":2289.357,
        "NaturalQuestions (open-book) - # output tokens":0.986,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":282.574,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1438.636,
        "MATH - # output tokens":1.0,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1207.746,
        "GSM8K - # output tokens":1.0,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.177,
        "LegalBench - truncated":0.001,
        "LegalBench - # prompt tokens":1027.35,
        "LegalBench - # output tokens":1.0,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1234.901,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":142.288,
        "WMT 2014 - # output tokens":1.0
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":4.408,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3669.808,
        "NarrativeQA - # output tokens":1.0,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.831,
        "NaturalQuestions (open-book) - truncated":0.026,
        "NaturalQuestions (open-book) - # prompt tokens":2289.357,
        "NaturalQuestions (open-book) - # output tokens":0.996,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":282.574,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1438.636,
        "MATH - # output tokens":1.0,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1207.746,
        "GSM8K - # output tokens":1.0,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.177,
        "LegalBench - truncated":0.001,
        "LegalBench - # prompt tokens":1027.35,
        "LegalBench - # output tokens":1.0,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1234.901,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":142.288,
        "WMT 2014 - # output tokens":1.0
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":4.408,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3669.808,
        "NarrativeQA - # output tokens":1.0,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.831,
        "NaturalQuestions (open-book) - truncated":0.026,
        "NaturalQuestions (open-book) - # prompt tokens":2289.357,
        "NaturalQuestions (open-book) - # output tokens":0.958,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":0.996,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":282.574,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1438.636,
        "MATH - # output tokens":1.0,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1207.746,
        "GSM8K - # output tokens":1.0,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.177,
        "LegalBench - truncated":0.001,
        "LegalBench - # prompt tokens":1027.35,
        "LegalBench - # output tokens":1.0,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1234.901,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":142.288,
        "WMT 2014 - # output tokens":1.0
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":1.434,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":1539.586,
        "NarrativeQA - # output tokens":1.0,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":3.722,
        "NaturalQuestions (open-book) - truncated":0.049,
        "NaturalQuestions (open-book) - # prompt tokens":1407.129,
        "NaturalQuestions (open-book) - # output tokens":0.985,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":282.574,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":6.897,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1214.707,
        "MATH - # output tokens":1.0,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1207.746,
        "GSM8K - # output tokens":1.0,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":3.805,
        "LegalBench - truncated":0.006,
        "LegalBench - # prompt tokens":595.161,
        "LegalBench - # output tokens":0.976,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1234.901,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":142.288,
        "WMT 2014 - # output tokens":1.0
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":4.575,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3627.715,
        "NarrativeQA - # output tokens":1.0,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.832,
        "NaturalQuestions (open-book) - truncated":0.026,
        "NaturalQuestions (open-book) - # prompt tokens":2268.728,
        "NaturalQuestions (open-book) - # output tokens":0.988,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":142.069,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":280.15,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":523.091,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1455.266,
        "MATH - # output tokens":1.0,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1187.268,
        "GSM8K - # output tokens":1.0,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.194,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":998.503,
        "LegalBench - # output tokens":0.998,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1193.093,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":144.433,
        "WMT 2014 - # output tokens":1.0
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":4.575,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3627.715,
        "NarrativeQA - # output tokens":1.0,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.832,
        "NaturalQuestions (open-book) - truncated":0.026,
        "NaturalQuestions (open-book) - # prompt tokens":2268.728,
        "NaturalQuestions (open-book) - # output tokens":0.991,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":142.069,
        "NaturalQuestions (closed-book) - # output tokens":0.999,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":280.15,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":523.091,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1455.266,
        "MATH - # output tokens":1.0,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1187.268,
        "GSM8K - # output tokens":1.0,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.194,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":998.503,
        "LegalBench - # output tokens":1.0,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1193.093,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":144.433,
        "WMT 2014 - # output tokens":0.999
    },
    {
        "Model":"Yi (34B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":4.868,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3611.445,
        "NarrativeQA - # output tokens":1.0,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.838,
        "NaturalQuestions (open-book) - truncated":0.026,
        "NaturalQuestions (open-book) - # prompt tokens":2171.698,
        "NaturalQuestions (open-book) - # output tokens":0.995,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":131.695,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":260.002,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":502.654,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1468.935,
        "MATH - # output tokens":1.0,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1170.814,
        "GSM8K - # output tokens":1.0,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.2,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":951.524,
        "LegalBench - # output tokens":1.0,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1122.392,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":187.092,
        "WMT 2014 - # output tokens":1.0
    },
    {
        "Model":"Yi (6B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":4.868,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3611.445,
        "NarrativeQA - # output tokens":1.0,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.838,
        "NaturalQuestions (open-book) - truncated":0.026,
        "NaturalQuestions (open-book) - # prompt tokens":2171.698,
        "NaturalQuestions (open-book) - # output tokens":0.995,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":131.695,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":260.002,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":502.654,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1468.935,
        "MATH - # output tokens":1.0,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1170.814,
        "GSM8K - # output tokens":1.0,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.2,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":951.524,
        "LegalBench - # output tokens":1.0,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1122.392,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":187.092,
        "WMT 2014 - # output tokens":1.0
    },
    {
        "Model":"Anthropic Claude 2.0",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":5.0,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3709.741,
        "NarrativeQA - # output tokens":10.561,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.964,
        "NaturalQuestions (open-book) - truncated":0.007,
        "NaturalQuestions (open-book) - # prompt tokens":1734.363,
        "NaturalQuestions (open-book) - # output tokens":7.605,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":189.259,
        "NaturalQuestions (closed-book) - # output tokens":7.206,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":328.79,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":543.747,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1361.814,
        "MATH - # output tokens":96.474,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1012.712,
        "GSM8K - # output tokens":78.704,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.798,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":1621.356,
        "LegalBench - # output tokens":3.338,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1092.437,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":218.573,
        "WMT 2014 - # output tokens":25.653
    },
    {
        "Model":"Anthropic Claude 2.1",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":5.0,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3709.741,
        "NarrativeQA - # output tokens":12.431,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.964,
        "NaturalQuestions (open-book) - truncated":0.007,
        "NaturalQuestions (open-book) - # prompt tokens":1734.363,
        "NaturalQuestions (open-book) - # output tokens":19.738,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":189.259,
        "NaturalQuestions (closed-book) - # output tokens":11.053,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":328.79,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":543.747,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1361.814,
        "MATH - # output tokens":96.72,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1012.712,
        "GSM8K - # output tokens":98.553,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.798,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":1621.356,
        "LegalBench - # output tokens":1.455,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1092.437,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":218.573,
        "WMT 2014 - # output tokens":25.235
    },
    {
        "Model":"Anthropic Claude v1.3",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":5.0,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3709.741,
        "NarrativeQA - # output tokens":9.338,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.964,
        "NaturalQuestions (open-book) - truncated":0.007,
        "NaturalQuestions (open-book) - # prompt tokens":1734.363,
        "NaturalQuestions (open-book) - # output tokens":4.973,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":189.259,
        "NaturalQuestions (closed-book) - # output tokens":3.722,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":328.79,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":543.747,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1361.814,
        "MATH - # output tokens":79.493,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1012.712,
        "GSM8K - # output tokens":104.726,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.798,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":1621.356,
        "LegalBench - # output tokens":1.354,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1092.437,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":218.573,
        "WMT 2014 - # output tokens":25.611
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":5.0,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3709.741,
        "NarrativeQA - # output tokens":17.149,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.964,
        "NaturalQuestions (open-book) - truncated":0.007,
        "NaturalQuestions (open-book) - # prompt tokens":1734.363,
        "NaturalQuestions (open-book) - # output tokens":8.217,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":189.259,
        "NaturalQuestions (closed-book) - # output tokens":5.113,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":328.79,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":543.747,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1361.814,
        "MATH - # output tokens":65.956,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1012.712,
        "GSM8K - # output tokens":105.998,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.798,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":1621.356,
        "LegalBench - # output tokens":1.646,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1092.437,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":218.573,
        "WMT 2014 - # output tokens":25.579
    },
    {
        "Model":"Cohere Command",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":1.941,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":1660.485,
        "NarrativeQA - # output tokens":7.442,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.617,
        "NaturalQuestions (open-book) - truncated":0.039,
        "NaturalQuestions (open-book) - # prompt tokens":1557.639,
        "NaturalQuestions (open-book) - # output tokens":8.461,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":115.191,
        "NaturalQuestions (closed-book) - # output tokens":5.679,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":246.682,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":6.878,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1177.329,
        "MATH - # output tokens":116.49,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":942.424,
        "GSM8K - # output tokens":94.43,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":3.878,
        "LegalBench - truncated":0.003,
        "LegalBench - # prompt tokens":566.501,
        "LegalBench - # output tokens":1.79,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1016.738,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":149.459,
        "WMT 2014 - # output tokens":31.8
    },
    {
        "Model":"Cohere Command Light",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":1.941,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":1660.485,
        "NarrativeQA - # output tokens":10.814,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.617,
        "NaturalQuestions (open-book) - truncated":0.039,
        "NaturalQuestions (open-book) - # prompt tokens":1557.639,
        "NaturalQuestions (open-book) - # output tokens":10.869,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":115.191,
        "NaturalQuestions (closed-book) - # output tokens":17.348,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":246.682,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":6.878,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1177.329,
        "MATH - # output tokens":106.589,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":942.424,
        "GSM8K - # output tokens":80.184,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":3.878,
        "LegalBench - truncated":0.003,
        "LegalBench - # prompt tokens":566.501,
        "LegalBench - # output tokens":6.64,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1016.738,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":149.459,
        "WMT 2014 - # output tokens":39.885
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":4.955,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3479.563,
        "NarrativeQA - # output tokens":9.732,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.885,
        "NaturalQuestions (open-book) - truncated":0.02,
        "NaturalQuestions (open-book) - # prompt tokens":1617.729,
        "NaturalQuestions (open-book) - # output tokens":6.8,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":116.254,
        "NaturalQuestions (closed-book) - # output tokens":7.074,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":254.21,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1375.735,
        "MATH - # output tokens":74.938,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":938.869,
        "GSM8K - # output tokens":93.717,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.211,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":907.387,
        "LegalBench - # output tokens":1.168,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1038.861,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":181.694,
        "WMT 2014 - # output tokens":25.117
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":4.955,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3479.563,
        "NarrativeQA - # output tokens":8.448,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.885,
        "NaturalQuestions (open-book) - truncated":0.02,
        "NaturalQuestions (open-book) - # prompt tokens":1617.729,
        "NaturalQuestions (open-book) - # output tokens":6.632,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":116.254,
        "NaturalQuestions (closed-book) - # output tokens":4.116,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":254.21,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1375.735,
        "MATH - # output tokens":136.822,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":938.869,
        "GSM8K - # output tokens":90.543,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.211,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":907.387,
        "LegalBench - # output tokens":1.099,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1038.861,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":181.694,
        "WMT 2014 - # output tokens":24.862
    },
    {
        "Model":"GPT-4 (0613)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":5.0,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3522.67,
        "NarrativeQA - # output tokens":8.515,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.964,
        "NaturalQuestions (open-book) - truncated":0.007,
        "NaturalQuestions (open-book) - # prompt tokens":1717.847,
        "NaturalQuestions (open-book) - # output tokens":8.055,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":173.127,
        "NaturalQuestions (closed-book) - # output tokens":3.832,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":242.782,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":460.72,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1323.911,
        "MATH - # output tokens":73.257,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1020.035,
        "GSM8K - # output tokens":111.209,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.798,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":1568.687,
        "LegalBench - # output tokens":1.34,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1020.414,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":193.043,
        "WMT 2014 - # output tokens":25.424
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":5.0,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3522.67,
        "NarrativeQA - # output tokens":9.885,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":5.0,
        "NaturalQuestions (open-book) - truncated":0.0,
        "NaturalQuestions (open-book) - # prompt tokens":1762.593,
        "NaturalQuestions (open-book) - # output tokens":8.753,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":173.127,
        "NaturalQuestions (closed-book) - # output tokens":14.157,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":242.782,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":460.72,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1323.911,
        "MATH - # output tokens":161.876,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1020.035,
        "GSM8K - # output tokens":98.073,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.8,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":1570.163,
        "LegalBench - # output tokens":1.458,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1020.414,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":193.043,
        "WMT 2014 - # output tokens":26.996
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":4.946,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3493.662,
        "NarrativeQA - # output tokens":9.91,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.884,
        "NaturalQuestions (open-book) - truncated":0.019,
        "NaturalQuestions (open-book) - # prompt tokens":1649.552,
        "NaturalQuestions (open-book) - # output tokens":9.389,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":173.127,
        "NaturalQuestions (closed-book) - # output tokens":5.576,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":242.782,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":460.72,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1323.911,
        "MATH - # output tokens":60.844,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1020.035,
        "GSM8K - # output tokens":77.29,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.218,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":949.517,
        "LegalBench - # output tokens":1.387,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1020.414,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":193.043,
        "WMT 2014 - # output tokens":25.038
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":5.0,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3504.577,
        "NarrativeQA - # output tokens":8.208,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.926,
        "NaturalQuestions (open-book) - truncated":0.013,
        "NaturalQuestions (open-book) - # prompt tokens":1662.782,
        "NaturalQuestions (open-book) - # output tokens":7.809,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":116.254,
        "NaturalQuestions (closed-book) - # output tokens":7.067,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":254.21,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1375.735,
        "MATH - # output tokens":87.032,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":938.869,
        "GSM8K - # output tokens":89.718,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.597,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":1355.759,
        "LegalBench - # output tokens":2.077,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1038.861,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":181.694,
        "WMT 2014 - # output tokens":25.142
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":5.0,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":3504.577,
        "NarrativeQA - # output tokens":11.149,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.885,
        "NaturalQuestions (open-book) - truncated":0.02,
        "NaturalQuestions (open-book) - # prompt tokens":1617.709,
        "NaturalQuestions (open-book) - # output tokens":12.864,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":116.254,
        "NaturalQuestions (closed-book) - # output tokens":19.113,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":254.21,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1375.735,
        "MATH - # output tokens":83.135,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":938.869,
        "GSM8K - # output tokens":89.919,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.597,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":1355.759,
        "LegalBench - # output tokens":1.078,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1038.861,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":181.694,
        "WMT 2014 - # output tokens":24.983
    },
    {
        "Model":"PaLM-2 (Bison)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":5.0,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":4414.234,
        "NarrativeQA - # output tokens":7.997,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.906,
        "NaturalQuestions (open-book) - truncated":0.015,
        "NaturalQuestions (open-book) - # prompt tokens":2124.565,
        "NaturalQuestions (open-book) - # output tokens":7.358,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":190.187,
        "NaturalQuestions (closed-book) - # output tokens":4.48,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":253.308,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":487.294,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1439.843,
        "MATH - # output tokens":66.89,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1109.549,
        "GSM8K - # output tokens":94.258,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.398,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":1387.966,
        "LegalBench - # output tokens":1.389,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1138.622,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":183.587,
        "WMT 2014 - # output tokens":29.981
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":5.0,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":4414.234,
        "NarrativeQA - # output tokens":16.544,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.906,
        "NaturalQuestions (open-book) - truncated":0.015,
        "NaturalQuestions (open-book) - # prompt tokens":2124.565,
        "NaturalQuestions (open-book) - # output tokens":13.327,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":190.187,
        "NaturalQuestions (closed-book) - # output tokens":9.803,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":253.308,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":487.294,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1439.843,
        "MATH - # output tokens":80.458,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":1109.549,
        "GSM8K - # output tokens":93.764,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.398,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":1387.966,
        "LegalBench - # output tokens":1.364,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1138.622,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":183.587,
        "WMT 2014 - # output tokens":30.567
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":5.0,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":2534.434,
        "NarrativeQA - # output tokens":6.583,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.931,
        "NaturalQuestions (open-book) - truncated":0.012,
        "NaturalQuestions (open-book) - # prompt tokens":1687.673,
        "NaturalQuestions (open-book) - # output tokens":4.785,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":102.377,
        "NaturalQuestions (closed-book) - # output tokens":5.79,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":188.75,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":8.0,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1321.422,
        "MATH - # output tokens":136.538,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":823.394,
        "GSM8K - # output tokens":102.036,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.798,
        "LegalBench - truncated":0.0,
        "LegalBench - # prompt tokens":1120.486,
        "LegalBench - # output tokens":2.028,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":758.622,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":135.468,
        "WMT 2014 - # output tokens":24.063
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":3.225,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":1700.741,
        "NarrativeQA - # output tokens":5.039,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.697,
        "NaturalQuestions (open-book) - truncated":0.038,
        "NaturalQuestions (open-book) - # prompt tokens":1522.929,
        "NaturalQuestions (open-book) - # output tokens":5.441,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":102.377,
        "NaturalQuestions (closed-book) - # output tokens":6.614,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":188.75,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":6.778,
        "MATH - truncated":0,
        "MATH - # prompt tokens":943.419,
        "MATH - # output tokens":140.295,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":823.394,
        "GSM8K - # output tokens":121.336,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":4.001,
        "LegalBench - truncated":0.002,
        "LegalBench - # prompt tokens":503.146,
        "LegalBench - # output tokens":2.056,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":758.622,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":135.468,
        "WMT 2014 - # output tokens":19.051
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":2.037,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":1694.642,
        "NarrativeQA - # output tokens":5.521,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.717,
        "NaturalQuestions (open-book) - truncated":0.038,
        "NaturalQuestions (open-book) - # prompt tokens":1488.14,
        "NaturalQuestions (open-book) - # output tokens":10.866,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":116.087,
        "NaturalQuestions (closed-book) - # output tokens":5.908,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":254.652,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":471.075,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":6.916,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1184.139,
        "MATH - # output tokens":139.637,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":943.121,
        "GSM8K - # output tokens":400.0,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":3.867,
        "LegalBench - truncated":0.133,
        "LegalBench - # prompt tokens":566.59,
        "LegalBench - # output tokens":1.639,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1005.229,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":157.232,
        "WMT 2014 - # output tokens":99.974
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":2.037,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":1694.642,
        "NarrativeQA - # output tokens":6.335,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.717,
        "NaturalQuestions (open-book) - truncated":0.038,
        "NaturalQuestions (open-book) - # prompt tokens":1488.14,
        "NaturalQuestions (open-book) - # output tokens":11.063,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":116.087,
        "NaturalQuestions (closed-book) - # output tokens":6.869,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":254.652,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":471.075,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":6.916,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1184.139,
        "MATH - # output tokens":142.866,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":943.121,
        "GSM8K - # output tokens":400.0,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":3.867,
        "LegalBench - truncated":0.133,
        "LegalBench - # prompt tokens":566.59,
        "LegalBench - # output tokens":1.548,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1005.229,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":157.232,
        "WMT 2014 - # output tokens":100.0
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"-",
        "NarrativeQA - # eval":355,
        "NarrativeQA - # train":2.037,
        "NarrativeQA - truncated":0,
        "NarrativeQA - # prompt tokens":1694.642,
        "NarrativeQA - # output tokens":5.685,
        "NaturalQuestions (open-book) - # eval":1000,
        "NaturalQuestions (open-book) - # train":4.717,
        "NaturalQuestions (open-book) - truncated":0.038,
        "NaturalQuestions (open-book) - # prompt tokens":1488.14,
        "NaturalQuestions (open-book) - # output tokens":6.864,
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":116.087,
        "NaturalQuestions (closed-book) - # output tokens":4.666,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":254.652,
        "OpenbookQA - # output tokens":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":471.075,
        "MMLU - # output tokens":1,
        "MATH - # eval":62.429,
        "MATH - # train":6.916,
        "MATH - truncated":0,
        "MATH - # prompt tokens":1184.139,
        "MATH - # output tokens":127.587,
        "GSM8K - # eval":1000,
        "GSM8K - # train":5,
        "GSM8K - truncated":0,
        "GSM8K - # prompt tokens":943.121,
        "GSM8K - # output tokens":400.0,
        "LegalBench - # eval":409.4,
        "LegalBench - # train":3.867,
        "LegalBench - truncated":0.133,
        "LegalBench - # prompt tokens":566.59,
        "LegalBench - # output tokens":1.266,
        "MedQA - # eval":503,
        "MedQA - # train":5,
        "MedQA - truncated":0,
        "MedQA - # prompt tokens":1005.229,
        "MedQA - # output tokens":1,
        "WMT 2014 - # eval":568.8,
        "WMT 2014 - # train":1,
        "WMT 2014 - truncated":0,
        "WMT 2014 - # prompt tokens":157.232,
        "WMT 2014 - # output tokens":100.0
    }
]