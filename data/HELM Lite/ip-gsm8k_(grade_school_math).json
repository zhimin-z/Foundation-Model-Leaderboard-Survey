[
    {
        "Model":"Falcon (40B)",
        "EM":"0.267",
        "Observed inference time (s)":"12.967",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1056.967",
        "# output tokens":"1"
    },
    {
        "Model":"Falcon (7B)",
        "EM":"0.055",
        "Observed inference time (s)":"6.94",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1056.967",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":"0.266",
        "Observed inference time (s)":"1.737",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1207.746",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":"0.567",
        "Observed inference time (s)":"3.737",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1207.746",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":"0.154",
        "Observed inference time (s)":"1.96",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1207.746",
        "# output tokens":"1"
    },
    {
        "Model":"LLaMA (65B)",
        "EM":"0.489",
        "Observed inference time (s)":"12.339",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1207.746",
        "# output tokens":"1"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":"0.377",
        "Observed inference time (s)":"1.632",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1187.268",
        "# output tokens":"1"
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "EM":"0.622",
        "Observed inference time (s)":"3.273",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1187.268",
        "# output tokens":"1"
    },
    {
        "Model":"Phi-2",
        "EM":"0.581",
        "Observed inference time (s)":"1.147",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"938.893",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (34B)",
        "EM":"0.648",
        "Observed inference time (s)":"4.887",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1170.814",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (6B)",
        "EM":"0.375",
        "Observed inference time (s)":"1.878",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1170.814",
        "# output tokens":"1"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":"0.239",
        "Observed inference time (s)":"5.176",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"823.394",
        "# output tokens":"102.036"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":"0.159",
        "Observed inference time (s)":"5.417",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"823.394",
        "# output tokens":"121.336"
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":"0.028",
        "Observed inference time (s)":"16.427",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"943.121",
        "# output tokens":"400"
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":"0.075",
        "Observed inference time (s)":"22.685",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"943.121",
        "# output tokens":"400"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":"0.137",
        "Observed inference time (s)":"48.242",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"943.121",
        "# output tokens":"400"
    },
    {
        "Model":"Anthropic Claude v1.3",
        "EM":"0.784",
        "Observed inference time (s)":"6.653",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1012.712",
        "# output tokens":"104.726"
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "EM":"0.721",
        "Observed inference time (s)":"1.474",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1012.712",
        "# output tokens":"105.998"
    },
    {
        "Model":"Anthropic Claude 2.0",
        "EM":"0.583",
        "Observed inference time (s)":"4.857",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1012.712",
        "# output tokens":"78.704"
    },
    {
        "Model":"Anthropic Claude 2.1",
        "EM":"0.604",
        "Observed inference time (s)":"7.706",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1012.712",
        "# output tokens":"98.553"
    },
    {
        "Model":"Cohere Command",
        "EM":"0.452",
        "Observed inference time (s)":"4.127",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"942.424",
        "# output tokens":"94.43"
    },
    {
        "Model":"Cohere Command Light",
        "EM":"0.149",
        "Observed inference time (s)":"1.751",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"942.424",
        "# output tokens":"80.184"
    },
    {
        "Model":"PaLM-2 (Bison)",
        "EM":"0.61",
        "Observed inference time (s)":"1.44",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1109.549",
        "# output tokens":"94.258"
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "EM":"0.831",
        "Observed inference time (s)":"5.437",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1109.549",
        "# output tokens":"93.764"
    },
    {
        "Model":"Mistral Medium (2312)",
        "EM":"0.706",
        "Observed inference time (s)":"9.719",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1255.268",
        "# output tokens":"137.554"
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "EM":"0.615",
        "Observed inference time (s)":"5.199",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"938.869",
        "# output tokens":"93.717"
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "EM":"0.479",
        "Observed inference time (s)":"3.762",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"938.869",
        "# output tokens":"90.543"
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "EM":"0.501",
        "Observed inference time (s)":"0.898",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1020.035",
        "# output tokens":"77.29"
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "EM":"0.668",
        "Observed inference time (s)":"5.738",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1020.035",
        "# output tokens":"98.073"
    },
    {
        "Model":"GPT-4 (0613)",
        "EM":"0.932",
        "Observed inference time (s)":"4.948",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"1020.035",
        "# output tokens":"111.209"
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "EM":"0.735",
        "Observed inference time (s)":"2.543",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"938.869",
        "# output tokens":"89.718"
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "EM":"0.831",
        "Observed inference time (s)":"5.07",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"938.869",
        "# output tokens":"89.919"
    }
]