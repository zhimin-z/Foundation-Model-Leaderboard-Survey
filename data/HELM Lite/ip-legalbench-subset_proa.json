[
    {
        "Model":"Falcon (40B)",
        "EM":0.737,
        "Observed inference time (s)":1.334,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":344.242,
        "# output tokens":1.0
    },
    {
        "Model":"Falcon (7B)",
        "EM":0.558,
        "Observed inference time (s)":0.493,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":344.242,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":0.779,
        "Observed inference time (s)":0.367,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":391.379,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":0.937,
        "Observed inference time (s)":0.448,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":391.379,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":0.747,
        "Observed inference time (s)":0.306,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":391.379,
        "# output tokens":1.0
    },
    {
        "Model":"LLaMA (65B)",
        "EM":0.863,
        "Observed inference time (s)":1.489,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":391.379,
        "# output tokens":1.0
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":0.789,
        "Observed inference time (s)":0.289,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":373.547,
        "# output tokens":1.0
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "EM":0.853,
        "Observed inference time (s)":0.369,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":373.547,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (34B)",
        "EM":0.8,
        "Observed inference time (s)":0.465,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":355.137,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (6B)",
        "EM":0.779,
        "Observed inference time (s)":0.379,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":355.137,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude 2.0",
        "EM":0.947,
        "Observed inference time (s)":1.856,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":415.2,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude 2.1",
        "EM":0.874,
        "Observed inference time (s)":2.325,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":415.2,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude v1.3",
        "EM":0.916,
        "Observed inference time (s)":1.69,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":415.2,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "EM":0.937,
        "Observed inference time (s)":0.645,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":415.2,
        "# output tokens":1.0
    },
    {
        "Model":"Cohere Command",
        "EM":0.884,
        "Observed inference time (s)":0.938,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":334.253,
        "# output tokens":1.0
    },
    {
        "Model":"Cohere Command Light",
        "EM":0.874,
        "Observed inference time (s)":0.767,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":334.253,
        "# output tokens":1.074
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "EM":0.947,
        "Observed inference time (s)":0.189,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":336.589,
        "# output tokens":1.0
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "EM":0.916,
        "Observed inference time (s)":0.167,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":336.589,
        "# output tokens":1.0
    },
    {
        "Model":"GPT-4 (0613)",
        "EM":0.905,
        "Observed inference time (s)":0.46,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":386.779,
        "# output tokens":1.0
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "EM":0.989,
        "Observed inference time (s)":0.445,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":386.779,
        "# output tokens":1.0
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "EM":0.747,
        "Observed inference time (s)":0.178,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":386.779,
        "# output tokens":1.0
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "EM":0.989,
        "Observed inference time (s)":0.469,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":336.589,
        "# output tokens":1.0
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "EM":0.926,
        "Observed inference time (s)":0.687,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":336.589,
        "# output tokens":1.0
    },
    {
        "Model":"PaLM-2 (Bison)",
        "EM":0.937,
        "Observed inference time (s)":0.53,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":471.8,
        "# output tokens":1.0
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "EM":0.926,
        "Observed inference time (s)":0.957,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":471.8,
        "# output tokens":1.0
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":0.821,
        "Observed inference time (s)":0.639,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":232.442,
        "# output tokens":2.0
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":0.842,
        "Observed inference time (s)":0.938,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":232.442,
        "# output tokens":2.0
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":0.505,
        "Observed inference time (s)":0.66,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":341.389,
        "# output tokens":1.0
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":0.632,
        "Observed inference time (s)":0.733,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":341.389,
        "# output tokens":1.0
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":0.768,
        "Observed inference time (s)":0.924,
        "# eval":95,
        "# train":5,
        "truncated":0,
        "# prompt tokens":341.389,
        "# output tokens":1.0
    }
]