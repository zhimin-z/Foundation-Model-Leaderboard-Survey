[
    {
        "Model":"Falcon (40B)",
        "Equivalent (CoT)":"0.154",
        "Observed inference time (s)":"12.843",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"1041.59",
        "# output tokens":"1"
    },
    {
        "Model":"Falcon (7B)",
        "Equivalent (CoT)":"0.026",
        "Observed inference time (s)":"5.453",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"1041.59",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (13B)",
        "Equivalent (CoT)":"0.077",
        "Observed inference time (s)":"1.543",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"1003.231",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (70B)",
        "Equivalent (CoT)":"0.205",
        "Observed inference time (s)":"2.177",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"1003.231",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (7B)",
        "Equivalent (CoT)":"0.103",
        "Observed inference time (s)":"1.399",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"1003.231",
        "# output tokens":"1"
    },
    {
        "Model":"LLaMA (65B)",
        "Equivalent (CoT)":"0.256",
        "Observed inference time (s)":"16.449",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"1003.231",
        "# output tokens":"1"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Equivalent (CoT)":"0.231",
        "Observed inference time (s)":"0.992",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"1033.513",
        "# output tokens":"1"
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "Equivalent (CoT)":"0.564",
        "Observed inference time (s)":"2.008",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"1033.513",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (34B)",
        "Equivalent (CoT)":"0.308",
        "Observed inference time (s)":"3.032",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"1011.205",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (6B)",
        "Equivalent (CoT)":"0.077",
        "Observed inference time (s)":"2.23",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"1011.205",
        "# output tokens":"1"
    },
    {
        "Model":"Anthropic Claude 2.0",
        "Equivalent (CoT)":"0.538",
        "Observed inference time (s)":"5.316",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"970.718",
        "# output tokens":"108.462"
    },
    {
        "Model":"Anthropic Claude 2.1",
        "Equivalent (CoT)":"0.641",
        "Observed inference time (s)":"9.563",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"970.718",
        "# output tokens":"103.564"
    },
    {
        "Model":"Anthropic Claude v1.3",
        "Equivalent (CoT)":"0.462",
        "Observed inference time (s)":"3.85",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"970.718",
        "# output tokens":"97.564"
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "Equivalent (CoT)":"0.436",
        "Observed inference time (s)":"1.247",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"970.718",
        "# output tokens":"76.513"
    },
    {
        "Model":"Cohere Command",
        "Equivalent (CoT)":"0.231",
        "Observed inference time (s)":"6.097",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"1000.949",
        "# output tokens":"113.769"
    },
    {
        "Model":"Cohere Command Light",
        "Equivalent (CoT)":"0.026",
        "Observed inference time (s)":"1.821",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"1000.949",
        "# output tokens":"120.769"
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "Equivalent (CoT)":"0.436",
        "Observed inference time (s)":"3.871",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"977.077",
        "# output tokens":"61.333"
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "Equivalent (CoT)":"0.41",
        "Observed inference time (s)":"3.467",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"977.077",
        "# output tokens":"83.128"
    },
    {
        "Model":"GPT-4 (0613)",
        "Equivalent (CoT)":"0.846",
        "Observed inference time (s)":"2.95",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"982.949",
        "# output tokens":"75.487"
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "Equivalent (CoT)":"0.923",
        "Observed inference time (s)":"11.526",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"982.949",
        "# output tokens":"178.231"
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "Equivalent (CoT)":"0.59",
        "Observed inference time (s)":"0.8",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"982.949",
        "# output tokens":"68.59"
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "Equivalent (CoT)":"0.538",
        "Observed inference time (s)":"2.011",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"977.077",
        "# output tokens":"80.795"
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "Equivalent (CoT)":"0.692",
        "Observed inference time (s)":"4.007",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"977.077",
        "# output tokens":"76.795"
    },
    {
        "Model":"PaLM-2 (Bison)",
        "Equivalent (CoT)":"0.41",
        "Observed inference time (s)":"1.161",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"1058.897",
        "# output tokens":"67.077"
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "Equivalent (CoT)":"0.667",
        "Observed inference time (s)":"4.315",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"1058.897",
        "# output tokens":"82.128"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Equivalent (CoT)":"0.077",
        "Observed inference time (s)":"8.82",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"796.795",
        "# output tokens":"133.641"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Equivalent (CoT)":"0.026",
        "Observed inference time (s)":"3.93",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"796.795",
        "# output tokens":"128.667"
    },
    {
        "Model":"Luminous Base (13B)",
        "Equivalent (CoT)":"0",
        "Observed inference time (s)":"5.6",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"985.256",
        "# output tokens":"114.077"
    },
    {
        "Model":"Luminous Extended (30B)",
        "Equivalent (CoT)":"0.051",
        "Observed inference time (s)":"9.287",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"985.256",
        "# output tokens":"154.923"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Equivalent (CoT)":"0.077",
        "Observed inference time (s)":"16.456",
        "# eval":"39",
        "# train":"8",
        "truncated":"0",
        "# prompt tokens":"985.256",
        "# output tokens":"128.615"
    }
]