[
    {
        "Model":"Falcon (40B)",
        "BLEU-4":0.208,
        "Observed inference time (s)":2.672,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":115.642,
        "# output tokens":1.0
    },
    {
        "Model":"Falcon (7B)",
        "BLEU-4":0.186,
        "Observed inference time (s)":1.158,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":115.642,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (13B)",
        "BLEU-4":0.191,
        "Observed inference time (s)":0.629,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":127.523,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (70B)",
        "BLEU-4":0.216,
        "Observed inference time (s)":0.811,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":127.523,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (7B)",
        "BLEU-4":0.175,
        "Observed inference time (s)":0.716,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":127.523,
        "# output tokens":1.0
    },
    {
        "Model":"LLaMA (65B)",
        "BLEU-4":0.219,
        "Observed inference time (s)":2.092,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":127.523,
        "# output tokens":1.0
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "BLEU-4":0.201,
        "Observed inference time (s)":0.52,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":130.306,
        "# output tokens":1.0
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "BLEU-4":0.23,
        "Observed inference time (s)":1.194,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":130.306,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (34B)",
        "BLEU-4":0.218,
        "Observed inference time (s)":1.184,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":139.298,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (6B)",
        "BLEU-4":0.182,
        "Observed inference time (s)":0.617,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":139.298,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude 2.0",
        "BLEU-4":0.23,
        "Observed inference time (s)":1.781,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":197.406,
        "# output tokens":26.374
    },
    {
        "Model":"Anthropic Claude 2.1",
        "BLEU-4":0.216,
        "Observed inference time (s)":2.492,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":197.406,
        "# output tokens":25.632
    },
    {
        "Model":"Anthropic Claude v1.3",
        "BLEU-4":0.228,
        "Observed inference time (s)":1.429,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":197.406,
        "# output tokens":26.28
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "BLEU-4":0.199,
        "Observed inference time (s)":0.759,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":197.406,
        "# output tokens":26.326
    },
    {
        "Model":"Cohere Command",
        "BLEU-4":0.151,
        "Observed inference time (s)":3.095,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":131.821,
        "# output tokens":27.71
    },
    {
        "Model":"Cohere Command Light",
        "BLEU-4":0.064,
        "Observed inference time (s)":0.735,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":131.821,
        "# output tokens":30.895
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "BLEU-4":0.225,
        "Observed inference time (s)":0.787,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":136.93,
        "# output tokens":25.604
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "BLEU-4":0.212,
        "Observed inference time (s)":0.465,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":136.93,
        "# output tokens":25.598
    },
    {
        "Model":"GPT-4 (0613)",
        "BLEU-4":0.225,
        "Observed inference time (s)":1.724,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":169.901,
        "# output tokens":26.121
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "BLEU-4":0.223,
        "Observed inference time (s)":1.962,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":169.901,
        "# output tokens":26.229
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "BLEU-4":0.21,
        "Observed inference time (s)":0.399,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":169.901,
        "# output tokens":26.352
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "BLEU-4":0.243,
        "Observed inference time (s)":0.948,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":136.93,
        "# output tokens":25.734
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "BLEU-4":0.277,
        "Observed inference time (s)":1.415,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":136.93,
        "# output tokens":25.417
    },
    {
        "Model":"PaLM-2 (Bison)",
        "BLEU-4":0.254,
        "Observed inference time (s)":0.894,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":195.686,
        "# output tokens":31.366
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "BLEU-4":0.274,
        "Observed inference time (s)":1.807,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":195.686,
        "# output tokens":31.734
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "BLEU-4":0.148,
        "Observed inference time (s)":1.413,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":128.296,
        "# output tokens":28.332
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "BLEU-4":0.149,
        "Observed inference time (s)":0.74,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":128.296,
        "# output tokens":19.083
    },
    {
        "Model":"Luminous Base (13B)",
        "BLEU-4":0.171,
        "Observed inference time (s)":4.683,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":99.111,
        "# output tokens":100.0
    },
    {
        "Model":"Luminous Extended (30B)",
        "BLEU-4":0.194,
        "Observed inference time (s)":5.384,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":99.111,
        "# output tokens":100.0
    },
    {
        "Model":"Luminous Supreme (70B)",
        "BLEU-4":0.193,
        "Observed inference time (s)":10.925,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":99.111,
        "# output tokens":100.0
    }
]