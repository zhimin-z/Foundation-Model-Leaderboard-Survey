[
    {
        "Model":"Falcon (40B)",
        "EM":0.325,
        "Observed inference time (s)":1.805,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":664.281,
        "# output tokens":1.0
    },
    {
        "Model":"Falcon (7B)",
        "EM":0.281,
        "Observed inference time (s)":0.475,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":664.281,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":0.307,
        "Observed inference time (s)":0.383,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":684.675,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":0.412,
        "Observed inference time (s)":0.56,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":684.675,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":0.325,
        "Observed inference time (s)":0.349,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":684.675,
        "# output tokens":1.0
    },
    {
        "Model":"LLaMA (65B)",
        "EM":0.439,
        "Observed inference time (s)":5.875,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":684.675,
        "# output tokens":1.0
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":0.518,
        "Observed inference time (s)":0.303,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":687.175,
        "# output tokens":1.0
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "EM":0.605,
        "Observed inference time (s)":0.363,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":687.175,
        "# output tokens":1.0
    },
    {
        "Model":"Phi-2",
        "EM":0.342,
        "Observed inference time (s)":0.295,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":624.07,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (34B)",
        "EM":0.588,
        "Observed inference time (s)":0.568,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":667.789,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (6B)",
        "EM":0.351,
        "Observed inference time (s)":0.368,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":667.789,
        "# output tokens":1.0
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":0.333,
        "Observed inference time (s)":0.9,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":552.719,
        "# output tokens":1.0
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":0.325,
        "Observed inference time (s)":0.755,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":552.719,
        "# output tokens":1.0
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":0.237,
        "Observed inference time (s)":0.648,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":618.447,
        "# output tokens":1.0
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":0.272,
        "Observed inference time (s)":0.754,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":618.447,
        "# output tokens":1.0
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":0.342,
        "Observed inference time (s)":1.009,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":618.447,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude v1.3",
        "EM":0.605,
        "Observed inference time (s)":1.619,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":684.596,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "EM":0.614,
        "Observed inference time (s)":0.633,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":684.596,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude 2.0",
        "EM":0.596,
        "Observed inference time (s)":1.936,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":684.596,
        "# output tokens":1.0
    },
    {
        "Model":"Anthropic Claude 2.1",
        "EM":0.596,
        "Observed inference time (s)":2.615,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":684.596,
        "# output tokens":1.0
    },
    {
        "Model":"Cohere Command",
        "EM":0.316,
        "Observed inference time (s)":1.384,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":628.421,
        "# output tokens":1.0
    },
    {
        "Model":"Cohere Command Light",
        "EM":0.298,
        "Observed inference time (s)":0.77,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":628.421,
        "# output tokens":1.0
    },
    {
        "Model":"PaLM-2 (Bison)",
        "EM":0.518,
        "Observed inference time (s)":1.047,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":638.088,
        "# output tokens":1.0
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "EM":0.649,
        "Observed inference time (s)":1.332,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":638.088,
        "# output tokens":1.0
    },
    {
        "Model":"Mistral Medium (2312)",
        "EM":0.579,
        "Observed inference time (s)":1.507,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":687.175,
        "# output tokens":0.991
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "EM":0.474,
        "Observed inference time (s)":0.2,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":624.07,
        "# output tokens":1.0
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "EM":0.491,
        "Observed inference time (s)":0.175,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":624.07,
        "# output tokens":1.0
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "EM":0.5,
        "Observed inference time (s)":0.176,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":607.43,
        "# output tokens":1.0
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "EM":0.675,
        "Observed inference time (s)":0.515,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":607.43,
        "# output tokens":1.0
    },
    {
        "Model":"GPT-4 (0613)",
        "EM":0.684,
        "Observed inference time (s)":0.364,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":607.43,
        "# output tokens":1.0
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "EM":0.526,
        "Observed inference time (s)":0.55,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":624.07,
        "# output tokens":1.0
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "EM":0.649,
        "Observed inference time (s)":0.783,
        "# eval":114,
        "# train":5,
        "truncated":0,
        "# prompt tokens":624.07,
        "# output tokens":1.0
    }
]