[
    {
        "Model":"Falcon (40B)",
        "EM":"0.662",
        "Observed inference time (s)":"1.268",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"251.174",
        "# output tokens":"1"
    },
    {
        "Model":"Falcon (7B)",
        "EM":"0.26",
        "Observed inference time (s)":"0.412",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"251.174",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":"0.634",
        "Observed inference time (s)":"0.347",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"282.574",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":"0.838",
        "Observed inference time (s)":"0.656",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"282.574",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":"0.544",
        "Observed inference time (s)":"0.393",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"282.574",
        "# output tokens":"1"
    },
    {
        "Model":"LLaMA (65B)",
        "EM":"0.754",
        "Observed inference time (s)":"4.49",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"282.574",
        "# output tokens":"1"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":"0.776",
        "Observed inference time (s)":"0.325",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"280.15",
        "# output tokens":"1"
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "EM":"0.868",
        "Observed inference time (s)":"0.354",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"280.15",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (34B)",
        "EM":"0.92",
        "Observed inference time (s)":"0.823",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"260.002",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (6B)",
        "EM":"0.8",
        "Observed inference time (s)":"0.354",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"260.002",
        "# output tokens":"1"
    },
    {
        "Model":"Anthropic Claude 2.0",
        "EM":"0.862",
        "Observed inference time (s)":"1.558",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"328.79",
        "# output tokens":"1"
    },
    {
        "Model":"Anthropic Claude 2.1",
        "EM":"0.872",
        "Observed inference time (s)":"1.809",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"328.79",
        "# output tokens":"1"
    },
    {
        "Model":"Anthropic Claude v1.3",
        "EM":"0.908",
        "Observed inference time (s)":"3.375",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"328.79",
        "# output tokens":"1"
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "EM":"0.844",
        "Observed inference time (s)":"0.597",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"328.79",
        "# output tokens":"1"
    },
    {
        "Model":"Cohere Command",
        "EM":"0.774",
        "Observed inference time (s)":"1.044",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"246.682",
        "# output tokens":"1"
    },
    {
        "Model":"Cohere Command Light",
        "EM":"0.398",
        "Observed inference time (s)":"0.705",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"246.682",
        "# output tokens":"1"
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)\u26a0",
        "EM":"0.828",
        "Observed inference time (s)":"0.204",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"254.21",
        "# output tokens":"1"
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)\u26a0",
        "EM":"0.796",
        "Observed inference time (s)":"0.174",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"254.21",
        "# output tokens":"1"
    },
    {
        "Model":"GPT-4 (0613)",
        "EM":"0.96",
        "Observed inference time (s)":"0.401",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"242.782",
        "# output tokens":"1"
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "EM":"0.95",
        "Observed inference time (s)":"0.512",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"242.782",
        "# output tokens":"1"
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "EM":"0.838",
        "Observed inference time (s)":"0.172",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"242.782",
        "# output tokens":"1"
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "EM":"0.878",
        "Observed inference time (s)":"0.42",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"254.21",
        "# output tokens":"1"
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "EM":"0.938",
        "Observed inference time (s)":"0.607",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"254.21",
        "# output tokens":"1"
    },
    {
        "Model":"PaLM-2 (Bison)",
        "EM":"0.878",
        "Observed inference time (s)":"0.788",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"253.308",
        "# output tokens":"1"
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "EM":"0.938",
        "Observed inference time (s)":"0.999",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"253.308",
        "# output tokens":"1"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":"0.688",
        "Observed inference time (s)":"0.998",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"188.75",
        "# output tokens":"1"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":"0.614",
        "Observed inference time (s)":"0.519",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"188.75",
        "# output tokens":"1"
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":"0.286",
        "Observed inference time (s)":"0.667",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"254.652",
        "# output tokens":"1"
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":"0.272",
        "Observed inference time (s)":"0.675",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"254.652",
        "# output tokens":"1"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":"0.284",
        "Observed inference time (s)":"0.779",
        "# eval":"500",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"254.652",
        "# output tokens":"1"
    }
]