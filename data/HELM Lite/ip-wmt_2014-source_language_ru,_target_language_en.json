[
    {
        "Model":"Falcon (40B)",
        "BLEU-4":0.188,
        "Observed inference time (s)":2.644,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":224.817,
        "# output tokens":1.0
    },
    {
        "Model":"Falcon (7B)",
        "BLEU-4":0.067,
        "Observed inference time (s)":1.05,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":224.817,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (13B)",
        "BLEU-4":0.209,
        "Observed inference time (s)":0.672,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":157.511,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (70B)",
        "BLEU-4":0.233,
        "Observed inference time (s)":0.871,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":157.511,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (7B)",
        "BLEU-4":0.189,
        "Observed inference time (s)":0.582,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":157.511,
        "# output tokens":1.0
    },
    {
        "Model":"LLaMA (65B)",
        "BLEU-4":0.239,
        "Observed inference time (s)":2.057,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":157.511,
        "# output tokens":1.0
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "BLEU-4":0.196,
        "Observed inference time (s)":0.521,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":163.018,
        "# output tokens":1.0
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "BLEU-4":0.224,
        "Observed inference time (s)":1.294,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":163.018,
        "# output tokens":1.0
    },
    {
        "Model":"Phi-2",
        "BLEU-4":0.006,
        "Observed inference time (s)":0.478,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":241.656,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (34B)",
        "BLEU-4":0.192,
        "Observed inference time (s)":1.071,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":173.66,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (6B)",
        "BLEU-4":0.085,
        "Observed inference time (s)":0.602,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":173.66,
        "# output tokens":1.0
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "BLEU-4":0.138,
        "Observed inference time (s)":1.236,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":148.278,
        "# output tokens":19.839
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "BLEU-4":0.111,
        "Observed inference time (s)":0.74,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":148.278,
        "# output tokens":18.626
    },
    {
        "Model":"Luminous Base (13B)",
        "BLEU-4":0.005,
        "Observed inference time (s)":4.703,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":181.893,
        "# output tokens":99.869
    },
    {
        "Model":"Luminous Extended (30B)",
        "BLEU-4":0.031,
        "Observed inference time (s)":5.365,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":181.893,
        "# output tokens":100.0
    },
    {
        "Model":"Luminous Supreme (70B)",
        "BLEU-4":0.093,
        "Observed inference time (s)":11.106,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":181.893,
        "# output tokens":100.0
    },
    {
        "Model":"Anthropic Claude v1.3",
        "BLEU-4":0.28,
        "Observed inference time (s)":2.605,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":240.974,
        "# output tokens":26.203
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "BLEU-4":0.24,
        "Observed inference time (s)":0.771,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":240.974,
        "# output tokens":25.612
    },
    {
        "Model":"Anthropic Claude 2.0",
        "BLEU-4":0.268,
        "Observed inference time (s)":2.145,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":240.974,
        "# output tokens":25.942
    },
    {
        "Model":"Anthropic Claude 2.1",
        "BLEU-4":0.233,
        "Observed inference time (s)":2.859,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":240.974,
        "# output tokens":25.119
    },
    {
        "Model":"Cohere Command",
        "BLEU-4":0.114,
        "Observed inference time (s)":2.376,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":178.821,
        "# output tokens":27.65
    },
    {
        "Model":"Cohere Command Light",
        "BLEU-4":0.008,
        "Observed inference time (s)":0.712,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":178.821,
        "# output tokens":44.899
    },
    {
        "Model":"PaLM-2 (Bison)",
        "BLEU-4":0.253,
        "Observed inference time (s)":0.952,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":206.169,
        "# output tokens":30.0
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "BLEU-4":0.279,
        "Observed inference time (s)":1.909,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":206.169,
        "# output tokens":30.875
    },
    {
        "Model":"Mistral Medium (2312)",
        "BLEU-4":0.22,
        "Observed inference time (s)":4.744,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":231.018,
        "# output tokens":27.573
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "BLEU-4":0.227,
        "Observed inference time (s)":0.822,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":241.662,
        "# output tokens":25.215
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "BLEU-4":0.2,
        "Observed inference time (s)":0.478,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":241.662,
        "# output tokens":24.813
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "BLEU-4":0.23,
        "Observed inference time (s)":0.409,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":211.841,
        "# output tokens":25.972
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "BLEU-4":0.241,
        "Observed inference time (s)":2.33,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":211.841,
        "# output tokens":26.503
    },
    {
        "Model":"GPT-4 (0613)",
        "BLEU-4":0.256,
        "Observed inference time (s)":1.699,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":211.841,
        "# output tokens":25.901
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "BLEU-4":0.27,
        "Observed inference time (s)":0.901,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":241.662,
        "# output tokens":25.549
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "BLEU-4":0.284,
        "Observed inference time (s)":1.465,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":241.662,
        "# output tokens":25.829
    }
]