[
    {
        "Model":"Falcon (40B)",
        "BLEU-4":0.017,
        "Observed inference time (s)":3.064,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":220.718,
        "# output tokens":1.0
    },
    {
        "Model":"Falcon (7B)",
        "BLEU-4":0.0,
        "Observed inference time (s)":3.055,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":220.718,
        "# output tokens":0.999
    },
    {
        "Model":"Llama 2 (13B)",
        "BLEU-4":0.074,
        "Observed inference time (s)":0.814,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":164.972,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (70B)",
        "BLEU-4":0.12,
        "Observed inference time (s)":1.399,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":164.972,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (7B)",
        "BLEU-4":0.046,
        "Observed inference time (s)":0.761,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":164.972,
        "# output tokens":1.0
    },
    {
        "Model":"LLaMA (65B)",
        "BLEU-4":0.102,
        "Observed inference time (s)":2.067,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":164.972,
        "# output tokens":1.0
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "BLEU-4":0.056,
        "Observed inference time (s)":0.537,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":159.482,
        "# output tokens":1.0
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "BLEU-4":0.099,
        "Observed inference time (s)":1.203,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":159.482,
        "# output tokens":0.994
    },
    {
        "Model":"Phi-2",
        "BLEU-4":0.0,
        "Observed inference time (s)":0.432,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":226.595,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (34B)",
        "BLEU-4":0.1,
        "Observed inference time (s)":1.118,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":317.56,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (6B)",
        "BLEU-4":0.055,
        "Observed inference time (s)":0.666,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":317.56,
        "# output tokens":1.0
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "BLEU-4":0.044,
        "Observed inference time (s)":1.317,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":143.141,
        "# output tokens":21.058
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "BLEU-4":0.021,
        "Observed inference time (s)":0.78,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":143.141,
        "# output tokens":21.34
    },
    {
        "Model":"Luminous Base (13B)",
        "BLEU-4":0.0,
        "Observed inference time (s)":4.731,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":255.504,
        "# output tokens":100.0
    },
    {
        "Model":"Luminous Extended (30B)",
        "BLEU-4":0.0,
        "Observed inference time (s)":5.406,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":255.504,
        "# output tokens":100.0
    },
    {
        "Model":"Luminous Supreme (70B)",
        "BLEU-4":0.0,
        "Observed inference time (s)":11.265,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":255.504,
        "# output tokens":100.0
    },
    {
        "Model":"Anthropic Claude v1.3",
        "BLEU-4":0.152,
        "Observed inference time (s)":1.982,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":238.013,
        "# output tokens":25.679
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "BLEU-4":0.138,
        "Observed inference time (s)":0.838,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":238.013,
        "# output tokens":25.569
    },
    {
        "Model":"Anthropic Claude 2.0",
        "BLEU-4":0.159,
        "Observed inference time (s)":2.443,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":238.013,
        "# output tokens":25.554
    },
    {
        "Model":"Anthropic Claude 2.1",
        "BLEU-4":0.148,
        "Observed inference time (s)":3.455,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":238.013,
        "# output tokens":24.927
    },
    {
        "Model":"Cohere Command",
        "BLEU-4":0.013,
        "Observed inference time (s)":2.836,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":162.637,
        "# output tokens":34.058
    },
    {
        "Model":"Cohere Command Light",
        "BLEU-4":0.0,
        "Observed inference time (s)":0.864,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":162.637,
        "# output tokens":47.65
    },
    {
        "Model":"PaLM-2 (Bison)",
        "BLEU-4":0.22,
        "Observed inference time (s)":0.878,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":145.755,
        "# output tokens":29.282
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "BLEU-4":0.238,
        "Observed inference time (s)":1.732,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":145.755,
        "# output tokens":29.898
    },
    {
        "Model":"Mistral Medium (2312)",
        "BLEU-4":0.07,
        "Observed inference time (s)":6.067,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":227.482,
        "# output tokens":30.692
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "BLEU-4":0.094,
        "Observed inference time (s)":0.817,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":226.599,
        "# output tokens":25.553
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "BLEU-4":0.077,
        "Observed inference time (s)":0.469,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":226.599,
        "# output tokens":24.704
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "BLEU-4":0.1,
        "Observed inference time (s)":0.367,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":213.185,
        "# output tokens":21.983
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "BLEU-4":0.156,
        "Observed inference time (s)":2.349,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":213.185,
        "# output tokens":27.159
    },
    {
        "Model":"GPT-4 (0613)",
        "BLEU-4":0.149,
        "Observed inference time (s)":1.566,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":213.185,
        "# output tokens":25.4
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "BLEU-4":0.2,
        "Observed inference time (s)":0.83,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":226.599,
        "# output tokens":24.639
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "BLEU-4":0.238,
        "Observed inference time (s)":1.354,
        "# eval":832,
        "# train":1,
        "truncated":0,
        "# prompt tokens":226.599,
        "# output tokens":24.546
    }
]