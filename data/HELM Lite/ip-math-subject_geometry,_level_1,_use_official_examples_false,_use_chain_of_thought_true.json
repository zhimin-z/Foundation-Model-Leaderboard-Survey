[
    {
        "Model":"Falcon (40B)",
        "Equivalent (CoT)":0.132,
        "Observed inference time (s)":7.555,
        "# eval":38,
        "# train":5.342,
        "truncated":0,
        "# prompt tokens":1495.447,
        "# output tokens":1.0
    },
    {
        "Model":"Falcon (7B)",
        "Equivalent (CoT)":0.026,
        "Observed inference time (s)":8.404,
        "# eval":38,
        "# train":5.342,
        "truncated":0,
        "# prompt tokens":1495.447,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (13B)",
        "Equivalent (CoT)":0.0,
        "Observed inference time (s)":1.685,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":2127.553,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (70B)",
        "Equivalent (CoT)":0.263,
        "Observed inference time (s)":2.678,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":2127.553,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (7B)",
        "Equivalent (CoT)":0.079,
        "Observed inference time (s)":3.618,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":2127.553,
        "# output tokens":1.0
    },
    {
        "Model":"LLaMA (65B)",
        "Equivalent (CoT)":0.211,
        "Observed inference time (s)":30.888,
        "# eval":38,
        "# train":5.316,
        "truncated":0,
        "# prompt tokens":1498.974,
        "# output tokens":1.0
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Equivalent (CoT)":0.263,
        "Observed inference time (s)":1.576,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":2141.289,
        "# output tokens":1.0
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "Equivalent (CoT)":0.289,
        "Observed inference time (s)":1.613,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":2141.289,
        "# output tokens":1.0
    },
    {
        "Model":"Phi-2",
        "Equivalent (CoT)":0.237,
        "Observed inference time (s)":1.13,
        "# eval":38,
        "# train":5.447,
        "truncated":0,
        "# prompt tokens":1482.5,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (34B)",
        "Equivalent (CoT)":0.316,
        "Observed inference time (s)":4.649,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":2195.395,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (6B)",
        "Equivalent (CoT)":0.132,
        "Observed inference time (s)":1.48,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":2195.395,
        "# output tokens":1.0
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Equivalent (CoT)":0.132,
        "Observed inference time (s)":6.226,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":2070.421,
        "# output tokens":87.263
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Equivalent (CoT)":0.079,
        "Observed inference time (s)":5.485,
        "# eval":38,
        "# train":5.447,
        "truncated":0,
        "# prompt tokens":1490.395,
        "# output tokens":116.658
    },
    {
        "Model":"Luminous Base (13B)",
        "Equivalent (CoT)":0.053,
        "Observed inference time (s)":5.774,
        "# eval":38,
        "# train":5.447,
        "truncated":0,
        "# prompt tokens":1515.184,
        "# output tokens":119.526
    },
    {
        "Model":"Luminous Extended (30B)",
        "Equivalent (CoT)":0.026,
        "Observed inference time (s)":8.41,
        "# eval":38,
        "# train":5.447,
        "truncated":0,
        "# prompt tokens":1515.184,
        "# output tokens":132.079
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Equivalent (CoT)":0.053,
        "Observed inference time (s)":13.143,
        "# eval":38,
        "# train":5.447,
        "truncated":0,
        "# prompt tokens":1515.184,
        "# output tokens":90.605
    },
    {
        "Model":"Anthropic Claude v1.3",
        "Equivalent (CoT)":0.553,
        "Observed inference time (s)":6.703,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1977.237,
        "# output tokens":87.553
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "Equivalent (CoT)":0.421,
        "Observed inference time (s)":1.526,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1977.237,
        "# output tokens":74.553
    },
    {
        "Model":"Anthropic Claude 2.0",
        "Equivalent (CoT)":0.5,
        "Observed inference time (s)":7.33,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1977.237,
        "# output tokens":102.5
    },
    {
        "Model":"Anthropic Claude 2.1",
        "Equivalent (CoT)":0.5,
        "Observed inference time (s)":10.062,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1977.237,
        "# output tokens":120.842
    },
    {
        "Model":"Cohere Command",
        "Equivalent (CoT)":0.263,
        "Observed inference time (s)":6.509,
        "# eval":38,
        "# train":5.184,
        "truncated":0,
        "# prompt tokens":1449.895,
        "# output tokens":129.474
    },
    {
        "Model":"Cohere Command Light",
        "Equivalent (CoT)":0.026,
        "Observed inference time (s)":2.426,
        "# eval":38,
        "# train":5.184,
        "truncated":0,
        "# prompt tokens":1449.895,
        "# output tokens":103.0
    },
    {
        "Model":"PaLM-2 (Bison)",
        "Equivalent (CoT)":0.474,
        "Observed inference time (s)":2.126,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":2129.237,
        "# output tokens":88.316
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "Equivalent (CoT)":0.526,
        "Observed inference time (s)":5.654,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":2129.237,
        "# output tokens":98.342
    },
    {
        "Model":"Mistral Medium (2312)",
        "Equivalent (CoT)":0.474,
        "Observed inference time (s)":6.667,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":2211.289,
        "# output tokens":118.158
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "Equivalent (CoT)":0.526,
        "Observed inference time (s)":4.356,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":2039.105,
        "# output tokens":73.974
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "Equivalent (CoT)":0.447,
        "Observed inference time (s)":6.636,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":2039.105,
        "# output tokens":184.947
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "Equivalent (CoT)":0.632,
        "Observed inference time (s)":0.819,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1887.395,
        "# output tokens":53.5
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "Equivalent (CoT)":0.711,
        "Observed inference time (s)":13.967,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1887.395,
        "# output tokens":171.211
    },
    {
        "Model":"GPT-4 (0613)",
        "Equivalent (CoT)":0.684,
        "Observed inference time (s)":4.044,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":1887.395,
        "# output tokens":78.5
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "Equivalent (CoT)":0.395,
        "Observed inference time (s)":2.31,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":2039.105,
        "# output tokens":100.368
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "Equivalent (CoT)":0.579,
        "Observed inference time (s)":5.139,
        "# eval":38,
        "# train":8.0,
        "truncated":0,
        "# prompt tokens":2039.105,
        "# output tokens":98.237
    }
]