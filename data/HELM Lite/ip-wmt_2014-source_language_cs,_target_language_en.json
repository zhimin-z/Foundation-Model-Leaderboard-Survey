[
    {
        "Model":"Falcon (40B)",
        "BLEU-4":0.207,
        "Observed inference time (s)":4.642,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":126.37,
        "# output tokens":1.0
    },
    {
        "Model":"Falcon (7B)",
        "BLEU-4":0.08,
        "Observed inference time (s)":1.476,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":126.37,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (13B)",
        "BLEU-4":0.187,
        "Observed inference time (s)":0.785,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":131.048,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (70B)",
        "BLEU-4":0.222,
        "Observed inference time (s)":1.477,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":131.048,
        "# output tokens":1.0
    },
    {
        "Model":"Llama 2 (7B)",
        "BLEU-4":0.156,
        "Observed inference time (s)":0.802,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":131.048,
        "# output tokens":1.0
    },
    {
        "Model":"LLaMA (65B)",
        "BLEU-4":0.199,
        "Observed inference time (s)":8.087,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":131.048,
        "# output tokens":1.0
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "BLEU-4":0.174,
        "Observed inference time (s)":0.701,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":134.058,
        "# output tokens":1.0
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "BLEU-4":0.207,
        "Observed inference time (s)":1.206,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":134.058,
        "# output tokens":1.0
    },
    {
        "Model":"Phi-2",
        "BLEU-4":0.008,
        "Observed inference time (s)":0.534,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":161.837,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (34B)",
        "BLEU-4":0.167,
        "Observed inference time (s)":2.506,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":157.759,
        "# output tokens":1.0
    },
    {
        "Model":"Yi (6B)",
        "BLEU-4":0.12,
        "Observed inference time (s)":0.624,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":157.759,
        "# output tokens":1.0
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "BLEU-4":0.12,
        "Observed inference time (s)":1.575,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":134.398,
        "# output tokens":20.646
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "BLEU-4":0.088,
        "Observed inference time (s)":0.81,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":134.398,
        "# output tokens":18.833
    },
    {
        "Model":"Luminous Base (13B)",
        "BLEU-4":0.012,
        "Observed inference time (s)":4.678,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":149.513,
        "# output tokens":100.0
    },
    {
        "Model":"Luminous Extended (30B)",
        "BLEU-4":0.027,
        "Observed inference time (s)":5.294,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":149.513,
        "# output tokens":100.0
    },
    {
        "Model":"Luminous Supreme (70B)",
        "BLEU-4":0.054,
        "Observed inference time (s)":11.04,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":149.513,
        "# output tokens":100.0
    },
    {
        "Model":"Anthropic Claude v1.3",
        "BLEU-4":0.231,
        "Observed inference time (s)":3.755,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":214.62,
        "# output tokens":25.891
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "BLEU-4":0.212,
        "Observed inference time (s)":0.764,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":214.62,
        "# output tokens":26.209
    },
    {
        "Model":"Anthropic Claude 2.0",
        "BLEU-4":0.234,
        "Observed inference time (s)":1.914,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":214.62,
        "# output tokens":26.141
    },
    {
        "Model":"Anthropic Claude 2.1",
        "BLEU-4":0.231,
        "Observed inference time (s)":2.478,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":214.62,
        "# output tokens":26.058
    },
    {
        "Model":"Cohere Command",
        "BLEU-4":0.078,
        "Observed inference time (s)":3.133,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":144.26,
        "# output tokens":27.795
    },
    {
        "Model":"Cohere Command Light",
        "BLEU-4":0.005,
        "Observed inference time (s)":0.934,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":144.26,
        "# output tokens":35.577
    },
    {
        "Model":"PaLM-2 (Bison)",
        "BLEU-4":0.255,
        "Observed inference time (s)":0.827,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":178.34,
        "# output tokens":31.181
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "BLEU-4":0.271,
        "Observed inference time (s)":1.851,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":178.34,
        "# output tokens":31.734
    },
    {
        "Model":"Mistral Medium (2312)",
        "BLEU-4":0.186,
        "Observed inference time (s)":3.982,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":202.058,
        "# output tokens":26.958
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "BLEU-4":0.215,
        "Observed inference time (s)":0.817,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":161.835,
        "# output tokens":25.652
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "BLEU-4":0.197,
        "Observed inference time (s)":0.478,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":161.835,
        "# output tokens":25.636
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "BLEU-4":0.209,
        "Observed inference time (s)":0.402,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":192.056,
        "# output tokens":26.064
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "BLEU-4":0.222,
        "Observed inference time (s)":1.797,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":192.056,
        "# output tokens":26.499
    },
    {
        "Model":"GPT-4 (0613)",
        "BLEU-4":0.222,
        "Observed inference time (s)":1.461,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":192.056,
        "# output tokens":25.932
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "BLEU-4":0.255,
        "Observed inference time (s)":0.945,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":161.835,
        "# output tokens":25.958
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "BLEU-4":0.275,
        "Observed inference time (s)":1.477,
        "# eval":503,
        "# train":1,
        "truncated":0,
        "# prompt tokens":161.835,
        "# output tokens":25.767
    }
]