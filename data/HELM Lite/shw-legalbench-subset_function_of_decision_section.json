[
    {
        "Model":"Falcon (40B)",
        "EM":"0.313",
        "Observed inference time (s)":"1.379",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"540.289",
        "# output tokens":"1"
    },
    {
        "Model":"Falcon (7B)",
        "EM":"0.12",
        "Observed inference time (s)":"0.593",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"540.289",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":"0.338",
        "Observed inference time (s)":"0.384",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"604.033",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":"0.444",
        "Observed inference time (s)":"0.621",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"604.033",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":"0.245",
        "Observed inference time (s)":"0.362",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"604.033",
        "# output tokens":"1"
    },
    {
        "Model":"LLaMA (65B)",
        "EM":"0.411",
        "Observed inference time (s)":"5.94",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"604.033",
        "# output tokens":"1"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":"0.433",
        "Observed inference time (s)":"0.313",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"590.678",
        "# output tokens":"1"
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "EM":"0.428",
        "Observed inference time (s)":"0.407",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"590.678",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (34B)",
        "EM":"0.311",
        "Observed inference time (s)":"0.612",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"570.894",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (6B)",
        "EM":"0.327",
        "Observed inference time (s)":"0.441",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"570.894",
        "# output tokens":"1"
    },
    {
        "Model":"Anthropic Claude 2.0",
        "EM":"0.387",
        "Observed inference time (s)":"1.997",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"602.452",
        "# output tokens":"1.54"
    },
    {
        "Model":"Anthropic Claude 2.1",
        "EM":"0.406",
        "Observed inference time (s)":"2.573",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"602.452",
        "# output tokens":"1.619"
    },
    {
        "Model":"Anthropic Claude v1.3",
        "EM":"0.417",
        "Observed inference time (s)":"4.237",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"602.452",
        "# output tokens":"1.54"
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "EM":"0.341",
        "Observed inference time (s)":"0.667",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"602.452",
        "# output tokens":"2.011"
    },
    {
        "Model":"Cohere Command",
        "EM":"0.365",
        "Observed inference time (s)":"1.154",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"516.975",
        "# output tokens":"2.18"
    },
    {
        "Model":"Cohere Command Light",
        "EM":"0.18",
        "Observed inference time (s)":"0.423",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"516.975",
        "# output tokens":"3.256"
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "EM":"0.324",
        "Observed inference time (s)":"0.21",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"521.341",
        "# output tokens":"1.06"
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "EM":"0.379",
        "Observed inference time (s)":"0.172",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"521.341",
        "# output tokens":"1.199"
    },
    {
        "Model":"GPT-4 (0613)",
        "EM":"0.452",
        "Observed inference time (s)":"0.477",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"554.346",
        "# output tokens":"1.635"
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "EM":"0.42",
        "Observed inference time (s)":"0.473",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"554.346",
        "# output tokens":"1.561"
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "EM":"0.302",
        "Observed inference time (s)":"0.189",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"554.346",
        "# output tokens":"1.42"
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "EM":"0.33",
        "Observed inference time (s)":"0.549",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"521.341",
        "# output tokens":"5.406"
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "EM":"0.439",
        "Observed inference time (s)":"0.752",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"521.341",
        "# output tokens":"1.188"
    },
    {
        "Model":"PaLM-2 (Bison)",
        "EM":"0.466",
        "Observed inference time (s)":"0.582",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"687.243",
        "# output tokens":"1.58"
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "EM":"0.452",
        "Observed inference time (s)":"1.141",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"687.243",
        "# output tokens":"1.621"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":"0.324",
        "Observed inference time (s)":"1.012",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"414.188",
        "# output tokens":"2.098"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":"0.199",
        "Observed inference time (s)":"0.601",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"414.188",
        "# output tokens":"2.008"
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":"0.166",
        "Observed inference time (s)":"0.681",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"520.869",
        "# output tokens":"1"
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":"0.204",
        "Observed inference time (s)":"0.785",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"520.869",
        "# output tokens":"1.011"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":"0.338",
        "Observed inference time (s)":"1.115",
        "# eval":"367",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"520.869",
        "# output tokens":"1.529"
    }
]