[
    {
        "Model":"Falcon (40B)",
        "F1":"0.392",
        "Observed inference time (s)":"2.849",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"124.246",
        "# output tokens":"1"
    },
    {
        "Model":"Falcon (7B)",
        "F1":"0.285",
        "Observed inference time (s)":"0.876",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"124.246",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (13B)",
        "F1":"0.371",
        "Observed inference time (s)":"0.384",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"137.383",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (70B)",
        "F1":"0.46",
        "Observed inference time (s)":"0.818",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"137.383",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (7B)",
        "F1":"0.333",
        "Observed inference time (s)":"0.479",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"137.383",
        "# output tokens":"0.996"
    },
    {
        "Model":"LLaMA (65B)",
        "F1":"0.433",
        "Observed inference time (s)":"4.704",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"137.383",
        "# output tokens":"1"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "F1":"0.367",
        "Observed inference time (s)":"0.462",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"142.069",
        "# output tokens":"1"
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "F1":"0.427",
        "Observed inference time (s)":"0.513",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"142.069",
        "# output tokens":"0.999"
    },
    {
        "Model":"Phi-2",
        "F1":"0.155",
        "Observed inference time (s)":"0.292",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"116.254",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (34B)",
        "F1":"0.443",
        "Observed inference time (s)":"1.458",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"131.695",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (6B)",
        "F1":"0.31",
        "Observed inference time (s)":"0.413",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"131.695",
        "# output tokens":"1"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "F1":"0.385",
        "Observed inference time (s)":"5.332",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"102.377",
        "# output tokens":"5.79"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "F1":"0.35",
        "Observed inference time (s)":"0.631",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"102.377",
        "# output tokens":"6.614"
    },
    {
        "Model":"Luminous Base (13B)",
        "F1":"0.197",
        "Observed inference time (s)":"0.802",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"116.087",
        "# output tokens":"5.908"
    },
    {
        "Model":"Luminous Extended (30B)",
        "F1":"0.253",
        "Observed inference time (s)":"0.98",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"116.087",
        "# output tokens":"6.869"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "F1":"0.299",
        "Observed inference time (s)":"1.272",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"116.087",
        "# output tokens":"4.666"
    },
    {
        "Model":"Anthropic Claude v1.3",
        "F1":"0.409",
        "Observed inference time (s)":"2.059",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"189.259",
        "# output tokens":"3.722"
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "F1":"0.343",
        "Observed inference time (s)":"0.674",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"189.259",
        "# output tokens":"5.113"
    },
    {
        "Model":"Anthropic Claude 2.0",
        "F1":"0.428",
        "Observed inference time (s)":"1.149",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"189.259",
        "# output tokens":"7.206"
    },
    {
        "Model":"Anthropic Claude 2.1",
        "F1":"0.375",
        "Observed inference time (s)":"1.753",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"189.259",
        "# output tokens":"11.053"
    },
    {
        "Model":"Cohere Command",
        "F1":"0.391",
        "Observed inference time (s)":"0.986",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"115.191",
        "# output tokens":"5.679"
    },
    {
        "Model":"Cohere Command Light",
        "F1":"0.195",
        "Observed inference time (s)":"0.696",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"115.191",
        "# output tokens":"17.348"
    },
    {
        "Model":"PaLM-2 (Bison)",
        "F1":"0.39",
        "Observed inference time (s)":"0.755",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"190.187",
        "# output tokens":"4.48"
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "F1":"0.435",
        "Observed inference time (s)":"1.56",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"190.187",
        "# output tokens":"9.803"
    },
    {
        "Model":"Mistral Medium (2312)",
        "F1":"0.29",
        "Observed inference time (s)":"6.588",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"211.069",
        "# output tokens":"34.263"
    },
    {
        "Model":"GPT-3.5 (text-davinci-003)",
        "F1":"0.413",
        "Observed inference time (s)":"0.996",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"116.254",
        "# output tokens":"7.074"
    },
    {
        "Model":"GPT-3.5 (text-davinci-002)",
        "F1":"0.394",
        "Observed inference time (s)":"0.683",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"116.254",
        "# output tokens":"4.116"
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "F1":"0.335",
        "Observed inference time (s)":"0.221",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"173.127",
        "# output tokens":"5.576"
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "F1":"0.435",
        "Observed inference time (s)":"1.131",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"173.127",
        "# output tokens":"14.157"
    },
    {
        "Model":"GPT-4 (0613)",
        "F1":"0.457",
        "Observed inference time (s)":"0.512",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"173.127",
        "# output tokens":"3.832"
    },
    {
        "Model":"Palmyra X V2 (33B)",
        "F1":"0.428",
        "Observed inference time (s)":"0.62",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"116.254",
        "# output tokens":"7.067"
    },
    {
        "Model":"Palmyra X V3 (72B)",
        "F1":"0.407",
        "Observed inference time (s)":"2.373",
        "# eval":"1000",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"116.254",
        "# output tokens":"19.113"
    }
]