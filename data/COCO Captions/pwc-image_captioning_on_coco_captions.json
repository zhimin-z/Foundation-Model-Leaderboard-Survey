[
    {
        "table_id":3446,
        "row_id":47703,
        "rank":1,
        "Model":"mPLUG",
        "mlmodel":{

        },
        "method_short":"mPLUG",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-24",
        "metrics":{
            "BLEU-4":"46.5",
            "CIDER":"155.1",
            "METEOR":"32.0",
            "SPICE":"26.0",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":46.5,
            "CIDER":155.1,
            "METEOR":32.0,
            "SPICE":26.0,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1015224,
            "title":"mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections",
            "url":"\/paper\/mplug-effective-and-efficient-vision-language",
            "published":"2022-05-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-effective-and-efficient-vision-language\/review\/?hl=47703"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":73798,
        "rank":2,
        "Model":"OFA",
        "mlmodel":{

        },
        "method_short":"OFA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-07",
        "metrics":{
            "BLEU-4":"44.9",
            "CIDER":"154.9",
            "METEOR":"32.5",
            "SPICE":"26.6",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":44.9,
            "CIDER":154.9,
            "METEOR":32.5,
            "SPICE":26.6,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":956719,
            "title":"OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework",
            "url":"\/paper\/unifying-architectures-tasks-and-modalities",
            "published":"2022-02-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unifying-architectures-tasks-and-modalities\/review\/?hl=73798"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":64351,
        "rank":3,
        "Model":"GIT",
        "mlmodel":{

        },
        "method_short":"GIT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "BLEU-4":"44.1",
            "CIDER":"151.1",
            "METEOR":" 32.2",
            "SPICE":"26.3",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":44.1,
            "CIDER":151.1,
            "METEOR":32.2,
            "SPICE":26.3,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":96176,
        "rank":4,
        "Model":"BLIP-2 ViT-G OPT 2.7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G OPT 2.7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "BLEU-4":"43.7",
            "CIDER":"145.8",
            "METEOR":null,
            "SPICE":null,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":43.7,
            "CIDER":145.8,
            "METEOR":null,
            "SPICE":null,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96176"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":96177,
        "rank":5,
        "Model":"BLIP-2 ViT-G OPT 6.7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G OPT 6.7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "BLEU-4":"43.5",
            "CIDER":"145.2",
            "METEOR":null,
            "SPICE":null,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":43.5,
            "CIDER":145.2,
            "METEOR":null,
            "SPICE":null,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96177"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":64707,
        "rank":6,
        "Model":"ExpansionNet v2 (No VL pretraining)",
        "mlmodel":{

        },
        "method_short":"ExpansionNet v2 ",
        "method_details":"No VL pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-02",
        "metrics":{
            "BLEU-4":"42.7",
            "CIDER":"143.7",
            "METEOR":"30.6",
            "SPICE":"24.7",
            "ROUGE-L":"61.1",
            "BLEU-1":"83.5",
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":42.7,
            "CIDER":143.7,
            "METEOR":30.6,
            "SPICE":24.7,
            "ROUGE-L":61.1,
            "BLEU-1":83.5,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1059095,
            "title":"Exploiting Multiple Sequence Lengths in Fast End to End Training for Image Captioning",
            "url":"\/paper\/expansionnet-v2-block-static-expansion-in",
            "published":"2022-08-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/expansionnet-v2-block-static-expansion-in\/review\/?hl=64707"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":48708,
        "rank":7,
        "Model":"LEMON",
        "mlmodel":{

        },
        "method_short":"LEMON",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-24",
        "metrics":{
            "BLEU-4":"42.6",
            "CIDER":"145.5",
            "METEOR":"31.4",
            "SPICE":"25.5",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":42.6,
            "CIDER":145.5,
            "METEOR":31.4,
            "SPICE":25.5,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":916259,
            "title":"Scaling Up Vision-Language Pre-training for Image Captioning",
            "url":"\/paper\/scaling-up-vision-language-pre-training-for",
            "published":"2021-11-24T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/scaling-up-vision-language-pre-training-for\/review\/?hl=48708"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":96178,
        "rank":8,
        "Model":"BLIP-2 ViT-G FlanT5 XL (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G FlanT5 XL ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "BLEU-4":"42.4",
            "CIDER":"144.5",
            "METEOR":null,
            "SPICE":null,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":42.4,
            "CIDER":144.5,
            "METEOR":null,
            "SPICE":null,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96178"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":60205,
        "rank":9,
        "Model":"GRIT (No VL pretraining - base)",
        "mlmodel":{

        },
        "method_short":"GRIT ",
        "method_details":"No VL pretraining - base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-20",
        "metrics":{
            "BLEU-4":"42.4",
            "CIDER":"144.2",
            "METEOR":"30.6",
            "SPICE":"24.3",
            "ROUGE-L":"60.7",
            "BLEU-1":"84.2",
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":42.4,
            "CIDER":144.2,
            "METEOR":30.6,
            "SPICE":24.3,
            "ROUGE-L":60.7,
            "BLEU-1":84.2,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1046916,
            "title":"GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features",
            "url":"\/paper\/grit-faster-and-better-image-captioning",
            "published":"2022-07-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/grit-faster-and-better-image-captioning\/review\/?hl=60205"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":62145,
        "rank":10,
        "Model":"Prompt Tuning",
        "mlmodel":{

        },
        "method_short":"Prompt Tuning",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-04",
        "metrics":{
            "BLEU-4":"41.81",
            "CIDER":"141.4",
            "METEOR":"31.51",
            "SPICE":"24.42",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":41.81,
            "CIDER":141.4,
            "METEOR":31.51,
            "SPICE":24.42,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1054811,
            "title":"Prompt Tuning for Generative Multimodal Pretrained Models",
            "url":"\/paper\/prompt-tuning-for-generative-multimodal",
            "published":"2022-08-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/prompt-tuning-for-generative-multimodal\/review\/?hl=62145"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":18974,
        "rank":11,
        "Model":"Oscar",
        "mlmodel":{

        },
        "method_short":"Oscar",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-13",
        "metrics":{
            "BLEU-4":"41.7",
            "CIDER":"140",
            "METEOR":"30.6",
            "SPICE":"24.5",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":41.7,
            "CIDER":140.0,
            "METEOR":30.6,
            "SPICE":24.5,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":190875,
            "title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
            "url":"\/paper\/oscar-object-semantics-aligned-pre-training",
            "published":"2020-04-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/oscar-object-semantics-aligned-pre-training\/review\/?hl=18974"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":54258,
        "rank":12,
        "Model":"Xmodal-Ctx",
        "mlmodel":{

        },
        "method_short":"Xmodal-Ctx",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-09",
        "metrics":{
            "BLEU-4":"41.4",
            "CIDER":"139.9",
            "METEOR":"30.4",
            "SPICE":"24.0",
            "ROUGE-L":"60.4",
            "BLEU-1":"83.4",
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":41.4,
            "CIDER":139.9,
            "METEOR":30.4,
            "SPICE":24.0,
            "ROUGE-L":60.4,
            "BLEU-1":83.4,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1005954,
            "title":"Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning",
            "url":"\/paper\/beyond-a-pre-trained-object-detector-cross",
            "published":"2022-05-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/beyond-a-pre-trained-object-detector-cross\/review\/?hl=54258"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":60520,
        "rank":13,
        "Model":"Xmodal-Ctx + OSCAR",
        "mlmodel":{

        },
        "method_short":"Xmodal-Ctx + OSCAR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-09",
        "metrics":{
            "BLEU-4":"41.3",
            "CIDER":"142.2",
            "METEOR":null,
            "SPICE":"24.9",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":41.3,
            "CIDER":142.2,
            "METEOR":null,
            "SPICE":24.9,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1005954,
            "title":"Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning",
            "url":"\/paper\/beyond-a-pre-trained-object-detector-cross",
            "published":"2022-05-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/beyond-a-pre-trained-object-detector-cross\/review\/?hl=60520"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":47861,
        "rank":14,
        "Model":"X-VLM (base)",
        "mlmodel":{

        },
        "method_short":"X-VLM ",
        "method_details":"base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-16",
        "metrics":{
            "BLEU-4":"41.3",
            "CIDER":"140.8",
            "METEOR":null,
            "SPICE":null,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":41.3,
            "CIDER":140.8,
            "METEOR":null,
            "SPICE":null,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":911012,
            "title":"Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts",
            "url":"\/paper\/multi-grained-vision-language-pre-training",
            "published":"2021-11-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multi-grained-vision-language-pre-training\/review\/?hl=47861"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":48707,
        "rank":15,
        "Model":"VinVL",
        "mlmodel":{

        },
        "method_short":"VinVL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-02",
        "metrics":{
            "BLEU-4":"41.0",
            "CIDER":"140.9",
            "METEOR":"31.1",
            "SPICE":"25.2",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":41.0,
            "CIDER":140.9,
            "METEOR":31.1,
            "SPICE":25.2,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":733129,
            "title":"VinVL: Revisiting Visual Representations in Vision-Language Models",
            "url":"\/paper\/vinvl-making-visual-representations-matter-in",
            "published":"2021-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vinvl-making-visual-representations-matter-in\/review\/?hl=48707"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":63196,
        "rank":16,
        "Model":"CoCa",
        "mlmodel":{

        },
        "method_short":"CoCa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "BLEU-4":"40.9",
            "CIDER":"143.6",
            "METEOR":"33.9",
            "SPICE":"24.7",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":40.9,
            "CIDER":143.6,
            "METEOR":33.9,
            "SPICE":24.7,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=63196"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":43429,
        "rank":17,
        "Model":"SimVLM",
        "mlmodel":{

        },
        "method_short":"SimVLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-08-24",
        "metrics":{
            "BLEU-4":"40.6",
            "CIDER":"143.3",
            "METEOR":"33.4",
            "SPICE":"25.4",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":40.6,
            "CIDER":143.3,
            "METEOR":33.4,
            "SPICE":25.4,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":856712,
            "title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
            "url":"\/paper\/simvlm-simple-visual-language-model",
            "published":"2021-08-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/simvlm-simple-visual-language-model\/review\/?hl=43429"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":98778,
        "rank":18,
        "Model":"Prismer",
        "mlmodel":{

        },
        "method_short":"Prismer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-04",
        "metrics":{
            "BLEU-4":"40.4",
            "CIDER":"136.5",
            "METEOR":"31.4",
            "SPICE":"24.4",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":40.4,
            "CIDER":136.5,
            "METEOR":31.4,
            "SPICE":24.4,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1168347,
            "title":"Prismer: A Vision-Language Model with Multi-Task Experts",
            "url":"\/paper\/prismer-a-vision-language-model-with-an",
            "published":"2023-03-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/prismer-a-vision-language-model-with-an\/review\/?hl=98778"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":87863,
        "rank":19,
        "Model":"PTP-BLIP (14M)",
        "mlmodel":{

        },
        "method_short":"PTP-BLIP ",
        "method_details":"14M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-19",
        "metrics":{
            "BLEU-4":"40.1",
            "CIDER":"135.0",
            "METEOR":"30.4",
            "SPICE":"23.7",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":40.1,
            "CIDER":135.0,
            "METEOR":30.4,
            "SPICE":23.7,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1130454,
            "title":"Position-guided Text Prompt for Vision-Language Pre-training",
            "url":"\/paper\/position-guided-text-prompt-for-vision",
            "published":"2022-12-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/position-guided-text-prompt-for-vision\/review\/?hl=87863"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":48653,
        "rank":20,
        "Model":"L-Verse",
        "mlmodel":{

        },
        "method_short":"L-Verse",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-22",
        "metrics":{
            "BLEU-4":"39.9",
            "CIDER":null,
            "METEOR":"31.4",
            "SPICE":"23.3",
            "ROUGE-L":"60.4",
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":39.9,
            "CIDER":null,
            "METEOR":31.4,
            "SPICE":23.3,
            "ROUGE-L":60.4,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":914416,
            "title":"L-Verse: Bidirectional Generation Between Image and Text",
            "url":"\/paper\/l-verse-bidirectional-generation-between",
            "published":"2021-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/l-verse-bidirectional-generation-between\/review\/?hl=48653"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":60516,
        "rank":21,
        "Model":"Xmodal-Ctx",
        "mlmodel":{

        },
        "method_short":"Xmodal-Ctx",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-09",
        "metrics":{
            "BLEU-4":"39.7",
            "CIDER":"135.9",
            "METEOR":"30.0",
            "SPICE":"23.7",
            "ROUGE-L":"59.5",
            "BLEU-1":"81.5",
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":39.7,
            "CIDER":135.9,
            "METEOR":30.0,
            "SPICE":23.7,
            "ROUGE-L":59.5,
            "BLEU-1":81.5,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1005954,
            "title":"Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning",
            "url":"\/paper\/beyond-a-pre-trained-object-detector-cross",
            "published":"2022-05-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/beyond-a-pre-trained-object-detector-cross\/review\/?hl=60516"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":44623,
        "rank":22,
        "Model":"X-Transformer",
        "mlmodel":{

        },
        "method_short":"X-Transformer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-31",
        "metrics":{
            "BLEU-4":"39.7",
            "CIDER":"132.8",
            "METEOR":"29.5",
            "SPICE":"23.4",
            "ROUGE-L":"59.1",
            "BLEU-1":"80.9",
            "BLEU-2":"65.8",
            "BLEU-3":"51.5",
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":39.7,
            "CIDER":132.8,
            "METEOR":29.5,
            "SPICE":23.4,
            "ROUGE-L":59.1,
            "BLEU-1":80.9,
            "BLEU-2":65.8,
            "BLEU-3":51.5,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188934,
            "title":"X-Linear Attention Networks for Image Captioning",
            "url":"\/paper\/x-linear-attention-networks-for-image",
            "published":"2020-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-linear-attention-networks-for-image\/review\/?hl=44623"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":18114,
        "rank":23,
        "Model":"AoANet + VC",
        "mlmodel":{

        },
        "method_short":"AoANet + VC",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-02-27",
        "metrics":{
            "BLEU-4":"39.5",
            "CIDER":null,
            "METEOR":"29.3",
            "SPICE":null,
            "ROUGE-L":"59.3",
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":39.5,
            "CIDER":null,
            "METEOR":29.3,
            "SPICE":null,
            "ROUGE-L":59.3,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":185038,
            "title":"Visual Commonsense R-CNN",
            "url":"\/paper\/visual-commonsense-r-cnn",
            "published":"2020-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-commonsense-r-cnn\/review\/?hl=18114"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":20774,
        "rank":24,
        "Model":"Transformer_NSC",
        "mlmodel":{

        },
        "method_short":"Transformer_NSC",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-22",
        "metrics":{
            "BLEU-4":"39.4",
            "CIDER":"129.6",
            "METEOR":"28.9",
            "SPICE":"22.8",
            "ROUGE-L":"58.7",
            "BLEU-1":"80.7",
            "BLEU-2":"65.6",
            "BLEU-3":"51.3",
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":39.4,
            "CIDER":129.6,
            "METEOR":28.9,
            "SPICE":22.8,
            "ROUGE-L":58.7,
            "BLEU-1":80.7,
            "BLEU-2":65.6,
            "BLEU-3":51.3,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":187913,
            "title":"A Better Variant of Self-Critical Sequence Training",
            "url":"\/paper\/a-better-variant-of-self-critical-sequence",
            "published":"2020-03-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-better-variant-of-self-critical-sequence\/review\/?hl=20774"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":38690,
        "rank":25,
        "Model":"Meshed-Memory Transformer",
        "mlmodel":{

        },
        "method_short":"Meshed-Memory Transformer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-17",
        "metrics":{
            "BLEU-4":"39.1",
            "CIDER":"131.2",
            "METEOR":"29.2",
            "SPICE":"22.6",
            "ROUGE-L":"58.6",
            "BLEU-1":"80.8",
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":39.1,
            "CIDER":131.2,
            "METEOR":29.2,
            "SPICE":22.6,
            "ROUGE-L":58.6,
            "BLEU-1":80.8,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":177027,
            "title":"Meshed-Memory Transformer for Image Captioning",
            "url":"\/paper\/m2-meshed-memory-transformer-for-image",
            "published":"2019-12-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/m2-meshed-memory-transformer-for-image\/review\/?hl=38690"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":97535,
        "rank":26,
        "Model":"CLIP Text Encoder (RL w\/ CIDEr-reward)",
        "mlmodel":{

        },
        "method_short":"CLIP Text Encoder ",
        "method_details":"RL w\/ CIDEr-reward",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "BLEU-4":"38.2",
            "CIDER":"124.9",
            "METEOR":"28.7",
            "SPICE":null,
            "ROUGE-L":"58.5",
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":38.2,
            "CIDER":124.9,
            "METEOR":28.7,
            "SPICE":null,
            "ROUGE-L":58.5,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016655,
            "title":"Fine-grained Image Captioning with CLIP Reward",
            "url":"\/paper\/fine-grained-image-captioning-with-clip",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":41998,
        "rank":27,
        "Model":"RefineCap (w\/ REINFORCE)",
        "mlmodel":{

        },
        "method_short":"RefineCap ",
        "method_details":"w\/ REINFORCE",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-01",
        "metrics":{
            "BLEU-4":"37.8",
            "CIDER":"127.2",
            "METEOR":"28.3",
            "SPICE":"22.5",
            "ROUGE-L":"58.0",
            "BLEU-1":"80.2",
            "BLEU-2":"64.5",
            "BLEU-3":"49.9",
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":37.8,
            "CIDER":127.2,
            "METEOR":28.3,
            "SPICE":22.5,
            "ROUGE-L":58.0,
            "BLEU-1":80.2,
            "BLEU-2":64.5,
            "BLEU-3":49.9,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":863700,
            "title":"RefineCap: Concept-Aware Refinement for Image Captioning",
            "url":"\/paper\/refinecap-concept-aware-refinement-for-image",
            "published":"2021-09-08T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/refinecap-concept-aware-refinement-for-image\/review\/?hl=41998"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":28176,
        "rank":28,
        "Model":"RDN",
        "mlmodel":{

        },
        "method_short":"RDN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-30",
        "metrics":{
            "BLEU-4":"37.3",
            "CIDER":"125.2",
            "METEOR":"28.1",
            "SPICE":null,
            "ROUGE-L":"57.4",
            "BLEU-1":"80.2",
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":37.3,
            "CIDER":125.2,
            "METEOR":28.1,
            "SPICE":null,
            "ROUGE-L":57.4,
            "BLEU-1":80.2,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":151750,
            "title":"Reflective Decoding Network for Image Captioning",
            "url":"\/paper\/reflective-decoding-network-for-image",
            "published":"2019-08-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/reflective-decoding-network-for-image\/review\/?hl=28176"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":49584,
        "rank":29,
        "Model":"ClipCap (Transformer)",
        "mlmodel":{

        },
        "method_short":"ClipCap ",
        "method_details":"Transformer",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-18",
        "metrics":{
            "BLEU-4":"33.53",
            "CIDER":"113.08",
            "METEOR":"27.45",
            "SPICE":"21.05",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":33.53,
            "CIDER":113.08,
            "METEOR":27.45,
            "SPICE":21.05,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912383,
            "title":"ClipCap: CLIP Prefix for Image Captioning",
            "url":"\/paper\/clipcap-clip-prefix-for-image-captioning",
            "published":"2021-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clipcap-clip-prefix-for-image-captioning\/review\/?hl=49584"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":49585,
        "rank":30,
        "Model":"ClipCap (MLP + GPT2 tuning)",
        "mlmodel":{

        },
        "method_short":"ClipCap ",
        "method_details":"MLP + GPT2 tuning",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-18",
        "metrics":{
            "BLEU-4":"32.15",
            "CIDER":"108.35",
            "METEOR":"27.1",
            "SPICE":"20.12",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":32.15,
            "CIDER":108.35,
            "METEOR":27.1,
            "SPICE":20.12,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912383,
            "title":"ClipCap: CLIP Prefix for Image Captioning",
            "url":"\/paper\/clipcap-clip-prefix-for-image-captioning",
            "published":"2021-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clipcap-clip-prefix-for-image-captioning\/review\/?hl=49585"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":96181,
        "rank":31,
        "Model":"CapDec",
        "mlmodel":{

        },
        "method_short":"CapDec",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-31",
        "metrics":{
            "BLEU-4":"26.4",
            "CIDER":"91.8",
            "METEOR":"25.1",
            "SPICE":null,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":26.4,
            "CIDER":91.8,
            "METEOR":25.1,
            "SPICE":null,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1103921,
            "title":"Text-Only Training for Image Captioning using Noise-Injected CLIP",
            "url":"\/paper\/text-only-training-for-image-captioning-using",
            "published":"2022-11-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":54270,
        "rank":32,
        "Model":"From Captions to Visual Concepts and Back",
        "mlmodel":{

        },
        "method_short":"From Captions to Visual Concepts and Back",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2014-11-18",
        "metrics":{
            "BLEU-4":"25.7",
            "CIDER":null,
            "METEOR":"23.6",
            "SPICE":null,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":25.7,
            "CIDER":null,
            "METEOR":23.6,
            "SPICE":null,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":41234,
            "title":"From Captions to Visual Concepts and Back",
            "url":"\/paper\/from-captions-to-visual-concepts-and-back",
            "published":"2014-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/from-captions-to-visual-concepts-and-back\/review\/?hl=54270"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":96182,
        "rank":33,
        "Model":"VLKD (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"VLKD ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-16",
        "metrics":{
            "BLEU-4":"16.7",
            "CIDER":"58.3",
            "METEOR":"19.7",
            "SPICE":"13.4",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":16.7,
            "CIDER":58.3,
            "METEOR":19.7,
            "SPICE":13.4,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":920364,
            "title":"Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation",
            "url":"\/paper\/enabling-multimodal-generation-on-clip-via",
            "published":"2021-11-16T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":101426,
        "rank":34,
        "Model":"VALOR",
        "mlmodel":{

        },
        "method_short":"VALOR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-17",
        "metrics":{
            "BLEU-4":null,
            "CIDER":"152.5",
            "METEOR":null,
            "SPICE":"25.7",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":null,
            "CIDER":152.5,
            "METEOR":null,
            "SPICE":25.7,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1191887,
            "title":"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset",
            "url":"\/paper\/valor-vision-audio-language-omni-perception",
            "published":"2023-04-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/valor-vision-audio-language-omni-perception\/review\/?hl=101426"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":104613,
        "rank":35,
        "Model":"VAST",
        "mlmodel":{

        },
        "method_short":"VAST",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-29",
        "metrics":{
            "BLEU-4":null,
            "CIDER":"149.0",
            "METEOR":null,
            "SPICE":"27.0",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":null,
            "CIDER":149.0,
            "METEOR":null,
            "SPICE":27.0,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1352653,
            "title":"VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset",
            "url":"\/paper\/vast-a-vision-audio-subtitle-text-omni-1",
            "published":"2023-05-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":27549,
        "rank":36,
        "Model":"Virtex (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"Virtex ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-11",
        "metrics":{
            "BLEU-4":null,
            "CIDER":"94",
            "METEOR":null,
            "SPICE":"18.5",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":null,
            "CIDER":94.0,
            "METEOR":null,
            "SPICE":18.5,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":201701,
            "title":"VirTex: Learning Visual Representations from Textual Annotations",
            "url":"\/paper\/virtex-learning-visual-representations-from",
            "published":"2020-06-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/virtex-learning-visual-representations-from\/review\/?hl=27549"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":98385,
        "rank":37,
        "Model":"KOSMOS-1 (1.6B) (zero-shot)",
        "mlmodel":{

        },
        "method_short":"KOSMOS-1 ",
        "method_details":"1.6B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "BLEU-4":null,
            "CIDER":"84.7",
            "METEOR":null,
            "SPICE":"16.8",
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "raw_metrics":{
            "BLEU-4":null,
            "CIDER":84.7,
            "METEOR":null,
            "SPICE":16.8,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3446,
        "row_id":105067,
        "rank":38,
        "Model":"BLIP-FuseCap",
        "mlmodel":{

        },
        "method_short":"BLIP-FuseCap",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-28",
        "metrics":{
            "BLEU-4":null,
            "CIDER":null,
            "METEOR":null,
            "SPICE":null,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":"78.5"
        },
        "raw_metrics":{
            "BLEU-4":null,
            "CIDER":null,
            "METEOR":null,
            "SPICE":null,
            "ROUGE-L":null,
            "BLEU-1":null,
            "BLEU-2":null,
            "BLEU-3":null,
            "CLIPScore":78.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1218184,
            "title":"FuseCap: Leveraging Large Language Models for Enriched Fused Image Captions",
            "url":"\/paper\/fusecap-leveraging-large-language-models-to",
            "published":"2023-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fusecap-leveraging-large-language-models-to\/review\/?hl=105067"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]