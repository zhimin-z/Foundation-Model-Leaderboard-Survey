[
    {
        "table_id":4301,
        "row_id":113021,
        "rank":1,
        "method":"OmniVec",
        "mlmodel":{

        },
        "Model":"OmniVec",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-07",
        "metrics":{
            "Top-1 Accuracy":"93.8"
        },
        "raw_metrics":{
            "Top-1 Accuracy":93.8
        },
        "uses_additional_data":true,
        "paper":{
            "id":1319233,
            "title":"OmniVec: Learning robust representations with cross modal sharing",
            "url":"\/paper\/omnivec-learning-robust-representations-with",
            "published":"2023-11-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivec-learning-robust-representations-with\/review\/?hl=113021"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":98771,
        "rank":2,
        "method":"InternImage-H",
        "mlmodel":{

        },
        "Model":"InternImage-H",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-10",
        "metrics":{
            "Top-1 Accuracy":"92.6%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":92.6
        },
        "uses_additional_data":true,
        "paper":{
            "id":1109218,
            "title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
            "url":"\/paper\/internimage-exploring-large-scale-vision",
            "published":"2022-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internimage-exploring-large-scale-vision\/review\/?hl=98771"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":100960,
        "rank":3,
        "method":"MAWS (ViT-2B)",
        "mlmodel":{

        },
        "Model":"MAWS ",
        "method_details":"ViT-2B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "Top-1 Accuracy":"91.3%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":91.3
        },
        "uses_additional_data":true,
        "paper":{
            "id":1179156,
            "title":"The effectiveness of MAE pre-pretraining for billion-scale pretraining",
            "url":"\/paper\/the-effectiveness-of-mae-pre-pretraining-for",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":48880,
        "rank":4,
        "method":"MetaFormer\n(MetaFormer-2,384,extra_info)",
        "mlmodel":{

        },
        "Model":"MetaFormer\n",
        "method_details":"MetaFormer-2,384,extra_info",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-05",
        "metrics":{
            "Top-1 Accuracy":"88.7%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":88.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":972429,
            "title":"MetaFormer: A Unified Meta Framework for Fine-Grained Recognition",
            "url":"\/paper\/metaformer-a-unified-meta-framework-for-fine",
            "published":"2022-03-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-a-unified-meta-framework-for-fine\/review\/?hl=48880"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":104375,
        "rank":5,
        "method":"Hiera-H (448px)",
        "mlmodel":{

        },
        "Model":"Hiera-H ",
        "method_details":"448px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-01",
        "metrics":{
            "Top-1 Accuracy":"87.3%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":87.3
        },
        "uses_additional_data":true,
        "paper":{
            "id":1221231,
            "title":"Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
            "url":"\/paper\/hiera-a-hierarchical-vision-transformer",
            "published":"2023-06-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hiera-a-hierarchical-vision-transformer\/review\/?hl=104375"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":44099,
        "rank":6,
        "method":"MAE (ViT-H, 448)",
        "mlmodel":{

        },
        "Model":"MAE ",
        "method_details":"ViT-H, 448",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-11",
        "metrics":{
            "Top-1 Accuracy":"86.8%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":86.8
        },
        "uses_additional_data":true,
        "paper":{
            "id":908690,
            "title":"Masked Autoencoders Are Scalable Vision Learners",
            "url":"\/paper\/masked-autoencoders-are-scalable-vision",
            "published":"2021-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-autoencoders-are-scalable-vision\/review\/?hl=44099"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":45650,
        "rank":7,
        "method":"SWAG (ViT H\/14)",
        "mlmodel":{

        },
        "Model":"SWAG ",
        "method_details":"ViT H\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-20",
        "metrics":{
            "Top-1 Accuracy":"86.0%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":86.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":948291,
            "title":"Revisiting Weakly Supervised Pre-Training of Visual Perception Models",
            "url":"\/paper\/revisiting-weakly-supervised-pre-training-of",
            "published":"2022-01-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revisiting-weakly-supervised-pre-training-of\/review\/?hl=45650"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":47957,
        "rank":8,
        "method":"SEER (RegNet10B - finetuned - 384px)",
        "mlmodel":{

        },
        "Model":"SEER ",
        "method_details":"RegNet10B - finetuned - 384px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-16",
        "metrics":{
            "Top-1 Accuracy":"84.7%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":84.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":963673,
            "title":"Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision",
            "url":"\/paper\/vision-models-are-more-robust-and-fair-when",
            "published":"2022-02-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-models-are-more-robust-and-fair-when\/review\/?hl=47957"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":107,
                "name":"IG-1B",
                "color":"#2771D3"
            },
            {
                "id":108,
                "name":"RegNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":48881,
        "rank":9,
        "method":"MetaFormer\n(MetaFormer-2,384)",
        "mlmodel":{

        },
        "Model":"MetaFormer\n",
        "method_details":"MetaFormer-2,384",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-05",
        "metrics":{
            "Top-1 Accuracy":"84.3%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":84.3
        },
        "uses_additional_data":true,
        "paper":{
            "id":972429,
            "title":"MetaFormer: A Unified Meta Framework for Fine-Grained Recognition",
            "url":"\/paper\/metaformer-a-unified-meta-framework-for-fine",
            "published":"2022-03-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-a-unified-meta-framework-for-fine\/review\/?hl=48881"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":45672,
        "rank":10,
        "method":"OMNIVORE (Swin-L)",
        "mlmodel":{

        },
        "Model":"OMNIVORE ",
        "method_details":"Swin-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-20",
        "metrics":{
            "Top-1 Accuracy":"84.1%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":84.1
        },
        "uses_additional_data":true,
        "paper":{
            "id":948292,
            "title":"Omnivore: A Single Model for Many Visual Modalities",
            "url":"\/paper\/omnivore-a-single-model-for-many-visual",
            "published":"2022-01-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/omnivore-a-single-model-for-many-visual\/review\/?hl=45672"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":22075,
        "rank":11,
        "method":"RegNet-8GF",
        "mlmodel":{

        },
        "Model":"RegNet-8GF",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-25",
        "metrics":{
            "Top-1 Accuracy":"81.2%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":81.2
        },
        "uses_additional_data":true,
        "paper":{
            "id":237887,
            "title":"Grafit: Learning fine-grained image representations with coarse labels",
            "url":"\/paper\/grafit-learning-fine-grained-image",
            "published":"2020-11-25T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/grafit-learning-fine-grained-image\/review\/?hl=22075"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":43489,
        "rank":12,
        "method":"VL-LTR (ViT-B-16)",
        "mlmodel":{

        },
        "Model":"VL-LTR ",
        "method_details":"ViT-B-16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-26",
        "metrics":{
            "Top-1 Accuracy":"81.0%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":81.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":920831,
            "title":"VL-LTR: Learning Class-wise Visual-Linguistic Representation for Long-Tailed Visual Recognition",
            "url":"\/paper\/vl-ltr-learning-class-wise-visual-linguistic",
            "published":"2021-11-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vl-ltr-learning-class-wise-visual-linguistic\/review\/?hl=43489"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":70798,
        "rank":13,
        "method":"\u00b52Net+ (ViT-L\/16)",
        "mlmodel":{

        },
        "Model":"\u00b52Net+ ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-15",
        "metrics":{
            "Top-1 Accuracy":"80.97%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":80.97
        },
        "uses_additional_data":false,
        "paper":{
            "id":1075007,
            "title":"A Continual Development Methodology for Large-scale Multitask Dynamic ML Systems",
            "url":"\/paper\/a-continual-development-methodology-for-large",
            "published":"2022-09-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-continual-development-methodology-for-large\/review\/?hl=70798"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":57836,
        "rank":14,
        "method":"MixMIM-L",
        "mlmodel":{

        },
        "Model":"MixMIM-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top-1 Accuracy":"80.3%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":80.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016710,
            "title":"MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of Hierarchical Vision Transformers",
            "url":"\/paper\/mixmim-mixed-and-masked-image-modeling-for",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mixmim-mixed-and-masked-image-modeling-for\/review\/?hl=57836"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":23837,
        "rank":15,
        "method":"DeiT-B",
        "mlmodel":{

        },
        "Model":"DeiT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-23",
        "metrics":{
            "Top-1 Accuracy":"79.5%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":79.5
        },
        "uses_additional_data":true,
        "paper":{
            "id":731001,
            "title":"Training data-efficient image transformers & distillation through attention",
            "url":"\/paper\/training-data-efficient-image-transformers",
            "published":"2020-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/training-data-efficient-image-transformers\/review\/?hl=23837"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":28514,
        "rank":16,
        "method":"CeiT-S (384 finetune resolution)",
        "mlmodel":{

        },
        "Model":"CeiT-S ",
        "method_details":"384 finetune resolution",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-22",
        "metrics":{
            "Top-1 Accuracy":"79.4%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":79.4
        },
        "uses_additional_data":true,
        "paper":{
            "id":755729,
            "title":"Incorporating Convolution Designs into Visual Transformers",
            "url":"\/paper\/incorporating-convolution-designs-into-visual",
            "published":"2021-03-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/incorporating-convolution-designs-into-visual\/review\/?hl=28514"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":73970,
        "rank":17,
        "method":"GPaCo (ResNet-152)",
        "mlmodel":{

        },
        "Model":"GPaCo ",
        "method_details":"ResNet-152",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-26",
        "metrics":{
            "Top-1 Accuracy":"78.1%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":78.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1080045,
            "title":"Generalized Parametric Contrastive Learning",
            "url":"\/paper\/generalized-parametric-contrastive-learning",
            "published":"2022-09-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/generalized-parametric-contrastive-learning\/review\/?hl=73970"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":29447,
        "rank":18,
        "method":"CaiT-M-36 U 224",
        "mlmodel":{

        },
        "Model":"CaiT-M-36 U 224",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-31",
        "metrics":{
            "Top-1 Accuracy":"78%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":78.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":772635,
            "title":"Going deeper with Image Transformers",
            "url":"\/paper\/going-deeper-with-image-transformers",
            "published":"2021-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/going-deeper-with-image-transformers\/review\/?hl=29447"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":57833,
        "rank":19,
        "method":"MixMIM-B",
        "mlmodel":{

        },
        "Model":"MixMIM-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top-1 Accuracy":"77.5%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":77.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016710,
            "title":"MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of Hierarchical Vision Transformers",
            "url":"\/paper\/mixmim-mixed-and-masked-image-modeling-for",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mixmim-mixed-and-masked-image-modeling-for\/review\/?hl=57833"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":70349,
        "rank":20,
        "method":"GPaCo (ResNet-50)",
        "mlmodel":{

        },
        "Model":"GPaCo ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-26",
        "metrics":{
            "Top-1 Accuracy":"75.4%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":75.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1080045,
            "title":"Generalized Parametric Contrastive Learning",
            "url":"\/paper\/generalized-parametric-contrastive-learning",
            "published":"2022-09-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/generalized-parametric-contrastive-learning\/review\/?hl=70349"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":45374,
        "rank":21,
        "method":"CBD-ENS (ResNet-101)",
        "mlmodel":{

        },
        "Model":"CBD-ENS ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-12",
        "metrics":{
            "Top-1 Accuracy":"75.3%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":75.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":778816,
            "title":"Class-Balanced Distillation for Long-Tailed Visual Recognition",
            "url":"\/paper\/class-balanced-distillation-for-long-tailed",
            "published":"2021-04-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/class-balanced-distillation-for-long-tailed\/review\/?hl=45374"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":49545,
        "rank":22,
        "method":"ViT-L (attn finetune)",
        "mlmodel":{

        },
        "Model":"ViT-L ",
        "method_details":"attn finetune",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-18",
        "metrics":{
            "Top-1 Accuracy":"75.3%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":75.3
        },
        "uses_additional_data":true,
        "paper":{
            "id":979672,
            "title":"Three things everyone should know about Vision Transformers",
            "url":"\/paper\/three-things-everyone-should-know-about",
            "published":"2022-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/three-things-everyone-should-know-about\/review\/?hl=49545"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":37505,
        "rank":23,
        "method":"PaCo(ResNet-152)",
        "mlmodel":{

        },
        "Model":"PaCo",
        "method_details":"ResNet-152",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-26",
        "metrics":{
            "Top-1 Accuracy":"75.2%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":75.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":841133,
            "title":"Parametric Contrastive Learning",
            "url":"\/paper\/parametric-contrastive-learning",
            "published":"2021-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/parametric-contrastive-learning\/review\/?hl=37505"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":43488,
        "rank":24,
        "method":"VL-LTR (ResNet-50)",
        "mlmodel":{

        },
        "Model":"VL-LTR ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-26",
        "metrics":{
            "Top-1 Accuracy":"74.6%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":74.6
        },
        "uses_additional_data":true,
        "paper":{
            "id":920831,
            "title":"VL-LTR: Learning Class-wise Visual-Linguistic Representation for Long-Tailed Visual Recognition",
            "url":"\/paper\/vl-ltr-learning-class-wise-visual-linguistic",
            "published":"2021-11-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vl-ltr-learning-class-wise-visual-linguistic\/review\/?hl=43488"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":55998,
        "rank":25,
        "method":"BS-CMO (ResNet-50)",
        "mlmodel":{

        },
        "Model":"BS-CMO ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-01",
        "metrics":{
            "Top-1 Accuracy":"74.0%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":74.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":923852,
            "title":"The Majority Can Help The Minority: Context-rich Minority Oversampling for Long-tailed Classification",
            "url":"\/paper\/the-majority-can-help-the-minority-context",
            "published":"2021-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/the-majority-can-help-the-minority-context\/review\/?hl=55998"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":33,
                "name":"ResNet-50",
                "color":"#c17b9b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":45375,
        "rank":26,
        "method":"CBD-ENS (ResNet-50)",
        "mlmodel":{

        },
        "Model":"CBD-ENS ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-12",
        "metrics":{
            "Top-1 Accuracy":"73.6%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":73.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":778816,
            "title":"Class-Balanced Distillation for Long-Tailed Visual Recognition",
            "url":"\/paper\/class-balanced-distillation-for-long-tailed",
            "published":"2021-04-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/class-balanced-distillation-for-long-tailed\/review\/?hl=45375"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":28507,
        "rank":27,
        "method":"CeiT-S",
        "mlmodel":{

        },
        "Model":"CeiT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-22",
        "metrics":{
            "Top-1 Accuracy":"73.3%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":73.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":755729,
            "title":"Incorporating Convolution Designs into Visual Transformers",
            "url":"\/paper\/incorporating-convolution-designs-into-visual",
            "published":"2021-03-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/incorporating-convolution-designs-into-visual\/review\/?hl=28507"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":36931,
        "rank":28,
        "method":"TADE (ResNet-50)",
        "mlmodel":{

        },
        "Model":"TADE ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-20",
        "metrics":{
            "Top-1 Accuracy":"72.9%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":72.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":838586,
            "title":"Self-Supervised Aggregation of Diverse Experts for Test-Agnostic Long-Tailed Recognition",
            "url":"\/paper\/test-agnostic-long-tailed-recognition-by-test",
            "published":"2021-07-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/test-agnostic-long-tailed-recognition-by-test\/review\/?hl=36931"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":28500,
        "rank":29,
        "method":"CeiT-T (384 finetune resolution)",
        "mlmodel":{

        },
        "Model":"CeiT-T ",
        "method_details":"384 finetune resolution",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-22",
        "metrics":{
            "Top-1 Accuracy":"72.2%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":72.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":755729,
            "title":"Incorporating Convolution Designs into Visual Transformers",
            "url":"\/paper\/incorporating-convolution-designs-into-visual",
            "published":"2021-03-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/incorporating-convolution-designs-into-visual\/review\/?hl=28500"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":29820,
        "rank":30,
        "method":"RIDE (ResNet-50)",
        "mlmodel":{

        },
        "Model":"RIDE ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-10-05",
        "metrics":{
            "Top-1 Accuracy":"72.2%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":72.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":225058,
            "title":"Long-tailed Recognition by Routing Diverse Distribution-Aware Experts",
            "url":"\/paper\/long-tailed-recognition-by-routing-diverse-1",
            "published":"2020-10-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/long-tailed-recognition-by-routing-diverse-1\/review\/?hl=29820"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":45005,
        "rank":31,
        "method":"ResNeXt-101 (SAMix)",
        "mlmodel":{

        },
        "Model":"ResNeXt-101 ",
        "method_details":"SAMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-30",
        "metrics":{
            "Top-1 Accuracy":"70.54%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":70.54
        },
        "uses_additional_data":false,
        "paper":{
            "id":923048,
            "title":"Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup",
            "url":"\/paper\/boosting-discriminative-visual-representation",
            "published":"2021-11-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/boosting-discriminative-visual-representation\/review\/?hl=45005"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":45006,
        "rank":32,
        "method":"ResNeXt-101 (AutoMix)",
        "mlmodel":{

        },
        "Model":"ResNeXt-101 ",
        "method_details":"AutoMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-24",
        "metrics":{
            "Top-1 Accuracy":"70.49%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":70.49
        },
        "uses_additional_data":false,
        "paper":{
            "id":756879,
            "title":"AutoMix: Unveiling the Power of Mixup for Stronger Classifiers",
            "url":"\/paper\/automix-unveiling-the-power-of-mixup",
            "published":"2021-03-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/automix-unveiling-the-power-of-mixup\/review\/?hl=45006"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":34237,
        "rank":33,
        "method":"LADE",
        "mlmodel":{

        },
        "Model":"LADE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Top-1 Accuracy":"70.0%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":70.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":238449,
            "title":"Disentangling Label Distribution for Long-tailed Visual Recognition",
            "url":"\/paper\/disentangling-label-distribution-for-long",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/disentangling-label-distribution-for-long\/review\/?hl=34237"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":22074,
        "rank":34,
        "method":"ResNet-50",
        "mlmodel":{

        },
        "Model":"ResNet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-25",
        "metrics":{
            "Top-1 Accuracy":"69.8%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":69.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":237887,
            "title":"Grafit: Learning fine-grained image representations with coarse labels",
            "url":"\/paper\/grafit-learning-fine-grained-image",
            "published":"2020-11-25T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/grafit-learning-fine-grained-image\/review\/?hl=22074"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":22078,
        "rank":35,
        "method":"ResNet-152",
        "mlmodel":{

        },
        "Model":"ResNet-152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-08-09",
        "metrics":{
            "Top-1 Accuracy":"69.08%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":69.08
        },
        "uses_additional_data":false,
        "paper":{
            "id":212449,
            "title":"Feature Space Augmentation for Long-Tailed Data",
            "url":"\/paper\/feature-space-augmentation-for-long-tailed",
            "published":"2020-08-09T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/feature-space-augmentation-for-long-tailed\/review\/?hl=22078"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":22081,
        "rank":36,
        "method":"ResNet-152",
        "mlmodel":{

        },
        "Model":"ResNet-152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-01-16",
        "metrics":{
            "Top-1 Accuracy":"69.05%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":69.05
        },
        "uses_additional_data":false,
        "paper":{
            "id":87629,
            "title":"Class-Balanced Loss Based on Effective Number of Samples",
            "url":"\/paper\/class-balanced-loss-based-on-effective-number",
            "published":"2019-01-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/class-balanced-loss-based-on-effective-number\/review\/?hl=22081"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":87056,
        "rank":37,
        "method":"MetaSAug",
        "mlmodel":{

        },
        "Model":"MetaSAug",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-23",
        "metrics":{
            "Top-1 Accuracy":"68.75%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":68.75
        },
        "uses_additional_data":false,
        "paper":{
            "id":756338,
            "title":"MetaSAug: Meta Semantic Augmentation for Long-Tailed Visual Recognition",
            "url":"\/paper\/metasaug-meta-semantic-augmentation-for-long",
            "published":"2021-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metasaug-meta-semantic-augmentation-for-long\/review\/?hl=87056"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":22077,
        "rank":38,
        "method":"ResNet-101",
        "mlmodel":{

        },
        "Model":"ResNet-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-08-09",
        "metrics":{
            "Top-1 Accuracy":"68.39%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":68.39
        },
        "uses_additional_data":false,
        "paper":{
            "id":212449,
            "title":"Feature Space Augmentation for Long-Tailed Data",
            "url":"\/paper\/feature-space-augmentation-for-long-tailed",
            "published":"2020-08-09T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/feature-space-augmentation-for-long-tailed\/review\/?hl=22077"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":22080,
        "rank":39,
        "method":"ResNet-101",
        "mlmodel":{

        },
        "Model":"ResNet-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-01-16",
        "metrics":{
            "Top-1 Accuracy":"67.98%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.98
        },
        "uses_additional_data":false,
        "paper":{
            "id":87629,
            "title":"Class-Balanced Loss Based on Effective Number of Samples",
            "url":"\/paper\/class-balanced-loss-based-on-effective-number",
            "published":"2019-01-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/class-balanced-loss-based-on-effective-number\/review\/?hl=22080"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":29468,
        "rank":40,
        "method":"LeViT-384",
        "mlmodel":{

        },
        "Model":"LeViT-384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-02",
        "metrics":{
            "Top-1 Accuracy":"66.9%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":66.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":774248,
            "title":"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
            "url":"\/paper\/levit-a-vision-transformer-in-convnet-s",
            "published":"2021-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/levit-a-vision-transformer-in-convnet-s\/review\/?hl=29468"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":29477,
        "rank":41,
        "method":"LeViT-256",
        "mlmodel":{

        },
        "Model":"LeViT-256",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-02",
        "metrics":{
            "Top-1 Accuracy":"66.2%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":66.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":774248,
            "title":"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
            "url":"\/paper\/levit-a-vision-transformer-in-convnet-s",
            "published":"2021-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/levit-a-vision-transformer-in-convnet-s\/review\/?hl=29477"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":22076,
        "rank":42,
        "method":"ResNet-50",
        "mlmodel":{

        },
        "Model":"ResNet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-08-09",
        "metrics":{
            "Top-1 Accuracy":"65.91%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":65.91
        },
        "uses_additional_data":false,
        "paper":{
            "id":212449,
            "title":"Feature Space Augmentation for Long-Tailed Data",
            "url":"\/paper\/feature-space-augmentation-for-long-tailed",
            "published":"2020-08-09T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/feature-space-augmentation-for-long-tailed\/review\/?hl=22076"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":45007,
        "rank":43,
        "method":"ResNet-50 (SAMix)",
        "mlmodel":{

        },
        "Model":"ResNet-50 ",
        "method_details":"SAMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-30",
        "metrics":{
            "Top-1 Accuracy":"64.84%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":64.84
        },
        "uses_additional_data":false,
        "paper":{
            "id":923048,
            "title":"Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup",
            "url":"\/paper\/boosting-discriminative-visual-representation",
            "published":"2021-11-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/boosting-discriminative-visual-representation\/review\/?hl=45007"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":45008,
        "rank":44,
        "method":"ResNet-50 (AutoMix)",
        "mlmodel":{

        },
        "Model":"ResNet-50 ",
        "method_details":"AutoMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-24",
        "metrics":{
            "Top-1 Accuracy":"64.73%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":64.73
        },
        "uses_additional_data":false,
        "paper":{
            "id":756879,
            "title":"AutoMix: Unveiling the Power of Mixup for Stronger Classifiers",
            "url":"\/paper\/automix-unveiling-the-power-of-mixup",
            "published":"2021-03-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/automix-unveiling-the-power-of-mixup\/review\/?hl=45008"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":28493,
        "rank":45,
        "method":"CeiT-T",
        "mlmodel":{

        },
        "Model":"CeiT-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-22",
        "metrics":{
            "Top-1 Accuracy":"64.3%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":64.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":755729,
            "title":"Incorporating Convolution Designs into Visual Transformers",
            "url":"\/paper\/incorporating-convolution-designs-into-visual",
            "published":"2021-03-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/incorporating-convolution-designs-into-visual\/review\/?hl=28493"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":34932,
        "rank":46,
        "method":"ResMLP-24",
        "mlmodel":{

        },
        "Model":"ResMLP-24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-07",
        "metrics":{
            "Top-1 Accuracy":"64.3%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":64.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":795412,
            "title":"ResMLP: Feedforward networks for image classification with data-efficient training",
            "url":"\/paper\/resmlp-feedforward-networks-for-image",
            "published":"2021-05-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resmlp-feedforward-networks-for-image\/review\/?hl=34932"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":22079,
        "rank":47,
        "method":"ResNet-50",
        "mlmodel":{

        },
        "Model":"ResNet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-01-16",
        "metrics":{
            "Top-1 Accuracy":"64.16%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":64.16
        },
        "uses_additional_data":false,
        "paper":{
            "id":87629,
            "title":"Class-Balanced Loss Based on Effective Number of Samples",
            "url":"\/paper\/class-balanced-loss-based-on-effective-number",
            "published":"2019-01-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/class-balanced-loss-based-on-effective-number\/review\/?hl=22079"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":29486,
        "rank":48,
        "method":"LeViT-192",
        "mlmodel":{

        },
        "Model":"LeViT-192",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-02",
        "metrics":{
            "Top-1 Accuracy":"60.4%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":60.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":774248,
            "title":"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
            "url":"\/paper\/levit-a-vision-transformer-in-convnet-s",
            "published":"2021-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/levit-a-vision-transformer-in-convnet-s\/review\/?hl=29486"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":22071,
        "rank":49,
        "method":"Inception-V3",
        "mlmodel":{

        },
        "Model":"Inception-V3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-07-20",
        "metrics":{
            "Top-1 Accuracy":"60.20%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":60.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":6311,
            "title":"The iNaturalist Species Classification and Detection Dataset",
            "url":"\/paper\/the-inaturalist-species-classification-and",
            "published":"2017-07-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/the-inaturalist-species-classification-and\/review\/?hl=22071"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":34931,
        "rank":50,
        "method":"ResMLP-12",
        "mlmodel":{

        },
        "Model":"ResMLP-12",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-07",
        "metrics":{
            "Top-1 Accuracy":"60.2%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":60.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":795412,
            "title":"ResMLP: Feedforward networks for image classification with data-efficient training",
            "url":"\/paper\/resmlp-feedforward-networks-for-image",
            "published":"2021-05-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resmlp-feedforward-networks-for-image\/review\/?hl=34931"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":29504,
        "rank":51,
        "method":"LeViT-128S",
        "mlmodel":{

        },
        "Model":"LeViT-128S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-02",
        "metrics":{
            "Top-1 Accuracy":"55.2%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":55.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":774248,
            "title":"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
            "url":"\/paper\/levit-a-vision-transformer-in-convnet-s",
            "published":"2021-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/levit-a-vision-transformer-in-convnet-s\/review\/?hl=29504"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":29495,
        "rank":52,
        "method":"LeViT-128",
        "mlmodel":{

        },
        "Model":"LeViT-128",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-02",
        "metrics":{
            "Top-1 Accuracy":"54%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":54.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":774248,
            "title":"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
            "url":"\/paper\/levit-a-vision-transformer-in-convnet-s",
            "published":"2021-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/levit-a-vision-transformer-in-convnet-s\/review\/?hl=29495"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":22072,
        "rank":53,
        "method":"ResNet-50",
        "mlmodel":{

        },
        "Model":"ResNet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-06",
        "metrics":{
            "Top-1 Accuracy":"49.7%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":49.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":175999,
            "title":"ClusterFit: Improving Generalization of Visual Representations",
            "url":"\/paper\/clusterfit-improving-generalization-of-visual",
            "published":"2019-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clusterfit-improving-generalization-of-visual\/review\/?hl=22072"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":22073,
        "rank":54,
        "method":"ResNet-50",
        "mlmodel":{

        },
        "Model":"ResNet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-17",
        "metrics":{
            "Top-1 Accuracy":"48.6%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":48.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":202916,
            "title":"Unsupervised Learning of Visual Features by Contrasting Cluster Assignments",
            "url":"\/paper\/unsupervised-learning-of-visual-features-by",
            "published":"2020-06-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unsupervised-learning-of-visual-features-by\/review\/?hl=22073"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":4301,
        "row_id":28373,
        "rank":55,
        "method":"Barlow Twins (ResNet-50)",
        "mlmodel":{

        },
        "Model":"Barlow Twins ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-04",
        "metrics":{
            "Top-1 Accuracy":"46.5%"
        },
        "raw_metrics":{
            "Top-1 Accuracy":46.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":750717,
            "title":"Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
            "url":"\/paper\/barlow-twins-self-supervised-learning-via",
            "published":"2021-03-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/barlow-twins-self-supervised-learning-via\/review\/?hl=28373"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    }
]