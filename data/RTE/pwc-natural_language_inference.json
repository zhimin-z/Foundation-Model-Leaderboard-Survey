[
    {
        "table_id":963,
        "row_id":51240,
        "rank":1,
        "method":"PaLM 540B (finetuned)",
        "mlmodel":{

        },
        "Model":"PaLM 540B ",
        "method_details":"finetuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-05",
        "metrics":{
            "Accuracy":"95.7%"
        },
        "raw_metrics":{
            "Accuracy":95.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":989558,
            "title":"PaLM: Scaling Language Modeling with Pathways",
            "url":"\/paper\/palm-scaling-language-modeling-with-pathways-1",
            "published":"2022-04-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":24574,
        "rank":2,
        "method":"DeBERTa-1.5B",
        "mlmodel":{

        },
        "Model":"DeBERTa-1.5B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-05",
        "metrics":{
            "Accuracy":"93.2%"
        },
        "raw_metrics":{
            "Accuracy":93.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":201217,
            "title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
            "url":"\/paper\/deberta-decoding-enhanced-bert-with",
            "published":"2020-06-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deberta-decoding-enhanced-bert-with\/review\/?hl=24574"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":35862,
        "rank":3,
        "method":"MUPPET Roberta Large",
        "mlmodel":{

        },
        "Model":"MUPPET Roberta Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-26",
        "metrics":{
            "Accuracy":"92.8%"
        },
        "raw_metrics":{
            "Accuracy":92.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":739728,
            "title":"Muppet: Massive Multi-task Representations with Pre-Finetuning",
            "url":"\/paper\/muppet-massive-multi-task-representations",
            "published":"2021-01-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/muppet-massive-multi-task-representations\/review\/?hl=35862"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":60700,
        "rank":4,
        "method":"DeBERTaV3large",
        "mlmodel":{

        },
        "Model":"DeBERTaV3large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-18",
        "metrics":{
            "Accuracy":"92.7%"
        },
        "raw_metrics":{
            "Accuracy":92.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":912443,
            "title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing",
            "url":"\/paper\/debertav3-improving-deberta-using-electra",
            "published":"2021-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/debertav3-improving-deberta-using-electra\/review\/?hl=60700"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":8358,
        "rank":5,
        "method":"T5-11B",
        "mlmodel":{

        },
        "Model":"T5-11B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "Accuracy":"92.5%"
        },
        "raw_metrics":{
            "Accuracy":92.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=8358"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":86573,
        "rank":6,
        "method":"T5",
        "mlmodel":{

        },
        "Model":"T5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-08",
        "metrics":{
            "Accuracy":"92.5%"
        },
        "raw_metrics":{
            "Accuracy":92.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":169701,
            "title":"SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization",
            "url":"\/paper\/smart-robust-and-efficient-fine-tuning-for",
            "published":"2019-11-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/smart-robust-and-efficient-fine-tuning-for\/review\/?hl=86573"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":86539,
        "rank":7,
        "method":"SMARTRoBERTa",
        "mlmodel":{

        },
        "Model":"SMARTRoBERTa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-08",
        "metrics":{
            "Accuracy":"92.0%"
        },
        "raw_metrics":{
            "Accuracy":92.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":169701,
            "title":"SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization",
            "url":"\/paper\/smart-robust-and-efficient-fine-tuning-for",
            "published":"2019-11-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/smart-robust-and-efficient-fine-tuning-for\/review\/?hl=86539"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":8359,
        "rank":8,
        "method":"T5-3B",
        "mlmodel":{

        },
        "Model":"T5-3B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "Accuracy":"91.1%"
        },
        "raw_metrics":{
            "Accuracy":91.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=8359"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":31482,
        "rank":9,
        "method":"EFL",
        "mlmodel":{

        },
        "Model":"EFL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-29",
        "metrics":{
            "Accuracy":"90.5%"
        },
        "raw_metrics":{
            "Accuracy":90.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":791525,
            "title":"Entailment as Few-Shot Learner",
            "url":"\/paper\/entailment-as-few-shot-learner",
            "published":"2021-04-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/entailment-as-few-shot-learner\/review\/?hl=31482"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":7989,
        "rank":10,
        "method":"ALBERT",
        "mlmodel":{

        },
        "Model":"ALBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-26",
        "metrics":{
            "Accuracy":"89.2%"
        },
        "raw_metrics":{
            "Accuracy":89.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":156146,
            "title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
            "url":"\/paper\/albert-a-lite-bert-for-self-supervised",
            "published":"2019-09-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/albert-a-lite-bert-for-self-supervised\/review\/?hl=7989"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":30632,
        "rank":11,
        "method":"Adv-RoBERTa ensemble",
        "mlmodel":{

        },
        "Model":"Adv-RoBERTa ensemble",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-13",
        "metrics":{
            "Accuracy":"88.7%"
        },
        "raw_metrics":{
            "Accuracy":88.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":149844,
            "title":"StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding",
            "url":"\/paper\/structbert-incorporating-language-structures",
            "published":"2019-08-13T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/structbert-incorporating-language-structures\/review\/?hl=30632"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":6022,
        "rank":12,
        "method":"RoBERTa",
        "mlmodel":{

        },
        "Model":"RoBERTa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-26",
        "metrics":{
            "Accuracy":"88.2%"
        },
        "raw_metrics":{
            "Accuracy":88.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":148282,
            "title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "url":"\/paper\/roberta-a-robustly-optimized-bert-pretraining",
            "published":"2019-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/roberta-a-robustly-optimized-bert-pretraining\/review\/?hl=6022"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":8360,
        "rank":13,
        "method":"T5-Large",
        "mlmodel":{

        },
        "Model":"T5-Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "Accuracy":"87.2%"
        },
        "raw_metrics":{
            "Accuracy":87.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=8360"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":12719,
        "rank":14,
        "method":"XLNet (single model)",
        "mlmodel":{

        },
        "Model":"XLNet ",
        "method_details":"single model",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-19",
        "metrics":{
            "Accuracy":"85.9%"
        },
        "raw_metrics":{
            "Accuracy":85.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":143172,
            "title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding",
            "url":"\/paper\/xlnet-generalized-autoregressive-pretraining",
            "published":"2019-06-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/xlnet-generalized-autoregressive-pretraining\/review\/?hl=12719"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":63590,
        "rank":15,
        "method":"Vector-wise",
        "mlmodel":{

        },
        "Model":"Vector-wise",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-15",
        "metrics":{
            "Accuracy":"85.4%"
        },
        "raw_metrics":{
            "Accuracy":85.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1058964,
            "title":"LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
            "url":"\/paper\/llm-int8-8-bit-matrix-multiplication-for",
            "published":"2022-08-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llm-int8-8-bit-matrix-multiplication-for\/review\/?hl=63590"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":97047,
        "rank":16,
        "method":"OPT-IML 175B",
        "mlmodel":{

        },
        "Model":"OPT-IML 175B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-22",
        "metrics":{
            "Accuracy":"84.8%"
        },
        "raw_metrics":{
            "Accuracy":84.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1133915,
            "title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "url":"\/paper\/opt-iml-scaling-language-model-instruction",
            "published":"2022-12-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/opt-iml-scaling-language-model-instruction\/review\/?hl=97047"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":39028,
        "rank":17,
        "method":"FLAN 137B zero-shot",
        "mlmodel":{

        },
        "Model":"FLAN 137B zero-shot",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-03",
        "metrics":{
            "Accuracy":"84.1%"
        },
        "raw_metrics":{
            "Accuracy":84.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":861409,
            "title":"Finetuned Language Models Are Zero-Shot Learners",
            "url":"\/paper\/finetuned-language-models-are-zero-shot",
            "published":"2021-09-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/finetuned-language-models-are-zero-shot\/review\/?hl=39028"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":97045,
        "rank":18,
        "method":"OPT-IML 30B",
        "mlmodel":{

        },
        "Model":"OPT-IML 30B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-22",
        "metrics":{
            "Accuracy":"83.8%"
        },
        "raw_metrics":{
            "Accuracy":83.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1133915,
            "title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "url":"\/paper\/opt-iml-scaling-language-model-instruction",
            "published":"2022-12-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/opt-iml-scaling-language-model-instruction\/review\/?hl=97045"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":7935,
        "rank":19,
        "method":"ELECTRA",
        "mlmodel":{

        },
        "Model":"ELECTRA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Accuracy":"83.6%"
        },
        "raw_metrics":{
            "Accuracy":83.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":102567,
        "rank":20,
        "method":"PaLM 2-M (one-shot)",
        "mlmodel":{

        },
        "Model":"PaLM 2-M ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Accuracy":"81.9"
        },
        "raw_metrics":{
            "Accuracy":81.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102567"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":6081,
        "rank":21,
        "method":"ERNIE 2.0 Large",
        "mlmodel":{

        },
        "Model":"ERNIE 2.0 Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-29",
        "metrics":{
            "Accuracy":"80.2%"
        },
        "raw_metrics":{
            "Accuracy":80.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":148407,
            "title":"ERNIE 2.0: A Continual Pre-training Framework for Language Understanding",
            "url":"\/paper\/ernie-20-a-continual-pre-training-framework",
            "published":"2019-07-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ernie-20-a-continual-pre-training-framework\/review\/?hl=6081"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":8361,
        "rank":22,
        "method":"T5-Base",
        "mlmodel":{

        },
        "Model":"T5-Base",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "Accuracy":"80.1%"
        },
        "raw_metrics":{
            "Accuracy":80.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=8361"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":24200,
        "rank":23,
        "method":"MLM+ del-span",
        "mlmodel":{

        },
        "Model":"MLM+ del-span",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-31",
        "metrics":{
            "Accuracy":"79.8%"
        },
        "raw_metrics":{
            "Accuracy":79.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":732544,
            "title":"CLEAR: Contrastive Learning for Sentence Representation",
            "url":"\/paper\/clear-contrastive-learning-for-sentence",
            "published":"2020-12-31T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/clear-contrastive-learning-for-sentence\/review\/?hl=24200"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":102568,
        "rank":24,
        "method":"PaLM 2-L (one-shot)",
        "mlmodel":{

        },
        "Model":"PaLM 2-L ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Accuracy":"79.3"
        },
        "raw_metrics":{
            "Accuracy":79.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102568"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":7934,
        "rank":25,
        "method":"SpanBERT",
        "mlmodel":{

        },
        "Model":"SpanBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-24",
        "metrics":{
            "Accuracy":"79.0%"
        },
        "raw_metrics":{
            "Accuracy":79.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":146696,
            "title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans",
            "url":"\/paper\/spanbert-improving-pre-training-by",
            "published":"2019-07-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/spanbert-improving-pre-training-by\/review\/?hl=7934"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":102566,
        "rank":26,
        "method":"PaLM 2-S (one-shot)",
        "mlmodel":{

        },
        "Model":"PaLM 2-S ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Accuracy":"78.7"
        },
        "raw_metrics":{
            "Accuracy":78.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102566"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":18895,
        "rank":27,
        "method":"BigBird",
        "mlmodel":{

        },
        "Model":"BigBird",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-28",
        "metrics":{
            "Accuracy":"75.0%"
        },
        "raw_metrics":{
            "Accuracy":75.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":210706,
            "title":"Big Bird: Transformers for Longer Sequences",
            "url":"\/paper\/big-bird-transformers-for-longer-sequences",
            "published":"2020-07-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/big-bird-transformers-for-longer-sequences\/review\/?hl=18895"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":6080,
        "rank":28,
        "method":"ERNIE 2.0 Base",
        "mlmodel":{

        },
        "Model":"ERNIE 2.0 Base",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-29",
        "metrics":{
            "Accuracy":"74.8%"
        },
        "raw_metrics":{
            "Accuracy":74.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":148407,
            "title":"ERNIE 2.0: A Continual Pre-training Framework for Language Understanding",
            "url":"\/paper\/ernie-20-a-continual-pre-training-framework",
            "published":"2019-07-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ernie-20-a-continual-pre-training-framework\/review\/?hl=6080"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":71017,
        "rank":29,
        "method":"Neo-6B (QA + WS)",
        "mlmodel":{

        },
        "Model":"Neo-6B ",
        "method_details":"QA + WS",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-05",
        "metrics":{
            "Accuracy":"74.7%"
        },
        "raw_metrics":{
            "Accuracy":74.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087115,
            "title":"Ask Me Anything: A simple strategy for prompting language models",
            "url":"\/paper\/ask-me-anything-a-simple-strategy-for",
            "published":"2022-10-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ask-me-anything-a-simple-strategy-for\/review\/?hl=71017"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":30630,
        "rank":30,
        "method":"RealFormer",
        "mlmodel":{

        },
        "Model":"RealFormer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-21",
        "metrics":{
            "Accuracy":"73.65%"
        },
        "raw_metrics":{
            "Accuracy":73.65
        },
        "uses_additional_data":false,
        "paper":{
            "id":730853,
            "title":"RealFormer: Transformer Likes Residual Attention",
            "url":"\/paper\/informer-transformer-likes-informed-attention",
            "published":"2020-12-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/informer-transformer-likes-informed-attention\/review\/?hl=30630"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":17975,
        "rank":31,
        "method":"SqueezeBERT",
        "mlmodel":{

        },
        "Model":"SqueezeBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-19",
        "metrics":{
            "Accuracy":"73.2%"
        },
        "raw_metrics":{
            "Accuracy":73.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":205166,
            "title":"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?",
            "url":"\/paper\/squeezebert-what-can-computer-vision-teach",
            "published":"2020-06-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/squeezebert-what-can-computer-vision-teach\/review\/?hl=17975"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":86530,
        "rank":32,
        "method":"SMART-BERT",
        "mlmodel":{

        },
        "Model":"SMART-BERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-08",
        "metrics":{
            "Accuracy":"71.2%"
        },
        "raw_metrics":{
            "Accuracy":71.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":169701,
            "title":"SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization",
            "url":"\/paper\/smart-robust-and-efficient-fine-tuning-for",
            "published":"2019-11-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/smart-robust-and-efficient-fine-tuning-for\/review\/?hl=86530"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":105191,
        "rank":33,
        "method":"SMART",
        "mlmodel":{

        },
        "Model":"SMART",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-08",
        "metrics":{
            "Accuracy":"71.2%"
        },
        "raw_metrics":{
            "Accuracy":71.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":169701,
            "title":"SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization",
            "url":"\/paper\/smart-robust-and-efficient-fine-tuning-for",
            "published":"2019-11-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/smart-robust-and-efficient-fine-tuning-for\/review\/?hl=105191"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":86510,
        "rank":34,
        "method":"BERT-LARGE",
        "mlmodel":{

        },
        "Model":"BERT-LARGE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-10-11",
        "metrics":{
            "Accuracy":"70.1%"
        },
        "raw_metrics":{
            "Accuracy":70.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":59204,
            "title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "url":"\/paper\/bert-pre-training-of-deep-bidirectional",
            "published":"2018-10-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bert-pre-training-of-deep-bidirectional\/review\/?hl=86510"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":8362,
        "rank":35,
        "method":"T5-Small",
        "mlmodel":{

        },
        "Model":"T5-Small",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "Accuracy":"69.9%"
        },
        "raw_metrics":{
            "Accuracy":69.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=8362"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":52592,
        "rank":36,
        "method":"data2vec",
        "mlmodel":{

        },
        "Model":"data2vec",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-07",
        "metrics":{
            "Accuracy":"69.9%"
        },
        "raw_metrics":{
            "Accuracy":69.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":957898,
            "title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
            "url":"\/paper\/data2vec-a-general-framework-for-self-1",
            "published":"2022-02-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/data2vec-a-general-framework-for-self-1\/review\/?hl=52592"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":100805,
        "rank":37,
        "method":"Bloomberg GPT (one-shot)",
        "mlmodel":{

        },
        "Model":"Bloomberg GPT ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"69.31%"
        },
        "raw_metrics":{
            "Accuracy":69.31
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100805"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":16816,
        "rank":38,
        "method":"GPT-3 175B (Few-Shot)",
        "mlmodel":{

        },
        "Model":"GPT-3 175B ",
        "method_details":"Few-Shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "Accuracy":"69%"
        },
        "raw_metrics":{
            "Accuracy":69.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=16816"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":33090,
        "rank":39,
        "method":"FNet-Large",
        "mlmodel":{

        },
        "Model":"FNet-Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-09",
        "metrics":{
            "Accuracy":"69%"
        },
        "raw_metrics":{
            "Accuracy":69.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":796253,
            "title":"FNet: Mixing Tokens with Fourier Transforms",
            "url":"\/paper\/fnet-mixing-tokens-with-fourier-transforms",
            "published":"2021-05-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fnet-mixing-tokens-with-fourier-transforms\/review\/?hl=33090"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":12720,
        "rank":40,
        "method":"ERNIE",
        "mlmodel":{

        },
        "Model":"ERNIE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-17",
        "metrics":{
            "Accuracy":"68.8%"
        },
        "raw_metrics":{
            "Accuracy":68.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":114976,
            "title":"ERNIE: Enhanced Language Representation with Informative Entities",
            "url":"\/paper\/ernie-enhanced-language-representation-with",
            "published":"2019-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ernie-enhanced-language-representation-with\/review\/?hl=12720"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":96237,
        "rank":41,
        "method":"AlexaTM 20B",
        "mlmodel":{

        },
        "Model":"AlexaTM 20B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-02",
        "metrics":{
            "Accuracy":"68.59%"
        },
        "raw_metrics":{
            "Accuracy":68.59
        },
        "uses_additional_data":false,
        "paper":{
            "id":1053502,
            "title":"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
            "url":"\/paper\/alexatm-20b-few-shot-learning-using-a-large",
            "published":"2022-08-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/alexatm-20b-few-shot-learning-using-a-large\/review\/?hl=96237"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":97043,
        "rank":42,
        "method":"OPT-IML 1.3B",
        "mlmodel":{

        },
        "Model":"OPT-IML 1.3B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-22",
        "metrics":{
            "Accuracy":"66.8%"
        },
        "raw_metrics":{
            "Accuracy":66.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1133915,
            "title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "url":"\/paper\/opt-iml-scaling-language-model-instruction",
            "published":"2022-12-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/opt-iml-scaling-language-model-instruction\/review\/?hl=97043"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":8116,
        "rank":43,
        "method":"DistilBERT",
        "mlmodel":{

        },
        "Model":"DistilBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-02",
        "metrics":{
            "Accuracy":"62.9%"
        },
        "raw_metrics":{
            "Accuracy":62.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":156821,
            "title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "url":"\/paper\/distilbert-a-distilled-version-of-bert",
            "published":"2019-10-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/distilbert-a-distilled-version-of-bert\/review\/?hl=8116"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":12721,
        "rank":44,
        "method":"TinyBERT",
        "mlmodel":{

        },
        "Model":"TinyBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-23",
        "metrics":{
            "Accuracy":"62.9%"
        },
        "raw_metrics":{
            "Accuracy":62.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":154692,
            "title":"TinyBERT: Distilling BERT for Natural Language Understanding",
            "url":"\/paper\/190910351",
            "published":"2019-09-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/190910351\/review\/?hl=12721"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":71018,
        "rank":45,
        "method":"Neo-6B (QA)",
        "mlmodel":{

        },
        "Model":"Neo-6B ",
        "method_details":"QA",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-05",
        "metrics":{
            "Accuracy":"61.7%"
        },
        "raw_metrics":{
            "Accuracy":61.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087115,
            "title":"Ask Me Anything: A simple strategy for prompting language models",
            "url":"\/paper\/ask-me-anything-a-simple-strategy-for",
            "published":"2022-10-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ask-me-anything-a-simple-strategy-for\/review\/?hl=71018"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":97046,
        "rank":46,
        "method":"OPT 175B",
        "mlmodel":{

        },
        "Model":"OPT 175B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-22",
        "metrics":{
            "Accuracy":"60.3%"
        },
        "raw_metrics":{
            "Accuracy":60.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1133915,
            "title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "url":"\/paper\/opt-iml-scaling-language-model-instruction",
            "published":"2022-12-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/opt-iml-scaling-language-model-instruction\/review\/?hl=97046"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":60040,
        "rank":47,
        "method":"N-Grammer",
        "mlmodel":{

        },
        "Model":"N-Grammer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-13",
        "metrics":{
            "Accuracy":"59.21%"
        },
        "raw_metrics":{
            "Accuracy":59.21
        },
        "uses_additional_data":false,
        "paper":{
            "id":1043167,
            "title":"N-Grammer: Augmenting Transformers with latent n-grams",
            "url":"\/paper\/n-grammer-augmenting-transformers-with-latent-1",
            "published":"2022-07-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/n-grammer-augmenting-transformers-with-latent-1\/review\/?hl=60040"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":71019,
        "rank":48,
        "method":"Neo-6B (few-shot)",
        "mlmodel":{

        },
        "Model":"Neo-6B ",
        "method_details":"few-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-05",
        "metrics":{
            "Accuracy":"58.8%"
        },
        "raw_metrics":{
            "Accuracy":58.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087115,
            "title":"Ask Me Anything: A simple strategy for prompting language models",
            "url":"\/paper\/ask-me-anything-a-simple-strategy-for",
            "published":"2022-10-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ask-me-anything-a-simple-strategy-for\/review\/?hl=71019"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":97044,
        "rank":49,
        "method":"OPT 30B",
        "mlmodel":{

        },
        "Model":"OPT 30B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-22",
        "metrics":{
            "Accuracy":"58.1%"
        },
        "raw_metrics":{
            "Accuracy":58.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1133915,
            "title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "url":"\/paper\/opt-iml-scaling-language-model-instruction",
            "published":"2022-12-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/opt-iml-scaling-language-model-instruction\/review\/?hl=97044"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":30348,
        "rank":50,
        "method":"24hBERT",
        "mlmodel":{

        },
        "Model":"24hBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-15",
        "metrics":{
            "Accuracy":"57.7%"
        },
        "raw_metrics":{
            "Accuracy":57.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":783560,
            "title":"How to Train BERT with an Academic Budget",
            "url":"\/paper\/how-to-train-bert-with-an-academic-budget",
            "published":"2021-04-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":100808,
        "rank":51,
        "method":"BLOOM 176B (one-shot)",
        "mlmodel":{

        },
        "Model":"BLOOM 176B ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"57.40%"
        },
        "raw_metrics":{
            "Accuracy":57.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100808"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":100807,
        "rank":52,
        "method":"OPT 66B (one-shot)",
        "mlmodel":{

        },
        "Model":"OPT 66B ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"54.87%"
        },
        "raw_metrics":{
            "Accuracy":54.87
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100807"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":97041,
        "rank":53,
        "method":"OPT 1.3B",
        "mlmodel":{

        },
        "Model":"OPT 1.3B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-22",
        "metrics":{
            "Accuracy":"54.2%"
        },
        "raw_metrics":{
            "Accuracy":54.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1133915,
            "title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "url":"\/paper\/opt-iml-scaling-language-model-instruction",
            "published":"2022-12-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/opt-iml-scaling-language-model-instruction\/review\/?hl=97041"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":963,
        "row_id":100806,
        "rank":54,
        "method":"GPT-NeoX (one-shot)",
        "mlmodel":{

        },
        "Model":"GPT-NeoX ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"53.79%"
        },
        "raw_metrics":{
            "Accuracy":53.79
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100806"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]