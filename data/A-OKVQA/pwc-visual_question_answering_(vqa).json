[
    {
        "table_id":20297,
        "row_id":113808,
        "rank":1,
        "method":"PaLI-X-VPD",
        "mlmodel":{

        },
        "Model":"PaLI-X-VPD",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-12-05",
        "metrics":{
            "MC Accuracy":"80.4",
            "DA VQA Score":"68.2"
        },
        "raw_metrics":{
            "MC Accuracy":80.4,
            "DA VQA Score":68.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1337954,
            "title":"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models",
            "url":"\/paper\/visual-program-distillation-distilling-tools",
            "published":"2023-12-05T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/visual-program-distillation-distilling-tools\/review\/?hl=113808"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20297,
        "row_id":98911,
        "rank":2,
        "method":"Prophet",
        "mlmodel":{

        },
        "Model":"Prophet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-03",
        "metrics":{
            "MC Accuracy":"73.6",
            "DA VQA Score":"55.7"
        },
        "raw_metrics":{
            "MC Accuracy":73.6,
            "DA VQA Score":55.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1167694,
            "title":"Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering",
            "url":"\/paper\/prompting-large-language-models-with-answer",
            "published":"2023-03-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/prompting-large-language-models-with-answer\/review\/?hl=98911"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20297,
        "row_id":107593,
        "rank":3,
        "method":"PromptCap",
        "mlmodel":{

        },
        "Model":"PromptCap",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-15",
        "metrics":{
            "MC Accuracy":"73.2",
            "DA VQA Score":"59.6"
        },
        "raw_metrics":{
            "MC Accuracy":73.2,
            "DA VQA Score":59.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1113358,
            "title":"PromptCap: Prompt-Guided Task-Aware Image Captioning",
            "url":"\/paper\/promptcap-prompt-guided-task-aware-image",
            "published":"2022-11-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/promptcap-prompt-guided-task-aware-image\/review\/?hl=107593"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20297,
        "row_id":66326,
        "rank":4,
        "method":"GPV-2",
        "mlmodel":{

        },
        "Model":"GPV-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-04",
        "metrics":{
            "MC Accuracy":"53.7",
            "DA VQA Score":"40.7"
        },
        "raw_metrics":{
            "MC Accuracy":53.7,
            "DA VQA Score":40.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":956061,
            "title":"Webly Supervised Concept Expansion for General Purpose Vision Models",
            "url":"\/paper\/webly-supervised-concept-expansion-for",
            "published":"2022-02-04T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/webly-supervised-concept-expansion-for\/review\/?hl=66326"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20297,
        "row_id":66325,
        "rank":5,
        "method":"KRISP",
        "mlmodel":{

        },
        "Model":"KRISP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-20",
        "metrics":{
            "MC Accuracy":"42.2",
            "DA VQA Score":"42.2"
        },
        "raw_metrics":{
            "MC Accuracy":42.2,
            "DA VQA Score":42.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":730166,
            "title":"KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA",
            "url":"\/paper\/krisp-integrating-implicit-and-symbolic",
            "published":"2020-12-20T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20297,
        "row_id":66329,
        "rank":6,
        "method":"ViLBERT - VQA",
        "mlmodel":{

        },
        "Model":"ViLBERT - VQA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-06",
        "metrics":{
            "MC Accuracy":"42.1",
            "DA VQA Score":"12.0"
        },
        "raw_metrics":{
            "MC Accuracy":42.1,
            "DA VQA Score":12.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":149243,
            "title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
            "url":"\/paper\/vilbert-pretraining-task-agnostic",
            "published":"2019-08-06T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20297,
        "row_id":66327,
        "rank":7,
        "method":"LXMERT",
        "mlmodel":{

        },
        "Model":"LXMERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-20",
        "metrics":{
            "MC Accuracy":"41.6",
            "DA VQA Score":"25.9"
        },
        "raw_metrics":{
            "MC Accuracy":41.6,
            "DA VQA Score":25.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":150646,
            "title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
            "url":"\/paper\/lxmert-learning-cross-modality-encoder",
            "published":"2019-08-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lxmert-learning-cross-modality-encoder\/review\/?hl=66327"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20297,
        "row_id":66328,
        "rank":8,
        "method":"ViLBERT",
        "mlmodel":{

        },
        "Model":"ViLBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-06",
        "metrics":{
            "MC Accuracy":"41.5",
            "DA VQA Score":"25.9"
        },
        "raw_metrics":{
            "MC Accuracy":41.5,
            "DA VQA Score":25.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":149243,
            "title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
            "url":"\/paper\/vilbert-pretraining-task-agnostic",
            "published":"2019-08-06T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20297,
        "row_id":66331,
        "rank":9,
        "method":"Pythia",
        "mlmodel":{

        },
        "Model":"Pythia",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-07-26",
        "metrics":{
            "MC Accuracy":"40.1",
            "DA VQA Score":"21.9"
        },
        "raw_metrics":{
            "MC Accuracy":40.1,
            "DA VQA Score":21.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":53591,
            "title":"Pythia v0.1: the Winning Entry to the VQA Challenge 2018",
            "url":"\/paper\/pythia-v01-the-winning-entry-to-the-vqa",
            "published":"2018-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pythia-v01-the-winning-entry-to-the-vqa\/review\/?hl=66331"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20297,
        "row_id":66330,
        "rank":10,
        "method":"ViLBERT - OK-VQA",
        "mlmodel":{

        },
        "Model":"ViLBERT - OK-VQA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-06",
        "metrics":{
            "MC Accuracy":"34.1",
            "DA VQA Score":"9.2"
        },
        "raw_metrics":{
            "MC Accuracy":34.1,
            "DA VQA Score":9.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":149243,
            "title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
            "url":"\/paper\/vilbert-pretraining-task-agnostic",
            "published":"2019-08-06T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]