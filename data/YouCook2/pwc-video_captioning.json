[
    {
        "table_id":2490,
        "row_id":104643,
        "rank":1,
        "method":"VAST",
        "mlmodel":{

        },
        "method_short":"VAST",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "BLEU-4":"18.2",
            "BLEU-3":null,
            "CIDEr":"1.99",
            "ROUGE-L":null,
            "METEOR":null
        },
        "raw_metrics":{
            "BLEU-4":18.2,
            "BLEU-3":null,
            "CIDEr":1.99,
            "ROUGE-L":null,
            "METEOR":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2490,
        "row_id":99977,
        "rank":2,
        "method":"UniVL + MELTR",
        "mlmodel":{

        },
        "method_short":"UniVL + MELTR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "BLEU-4":"17.92",
            "BLEU-3":"24.12",
            "CIDEr":"1.90",
            "ROUGE-L":"47.04",
            "METEOR":"22.56"
        },
        "raw_metrics":{
            "BLEU-4":17.92,
            "BLEU-3":24.12,
            "CIDEr":1.9,
            "ROUGE-L":47.04,
            "METEOR":22.56
        },
        "uses_additional_data":false,
        "paper":{
            "id":1179150,
            "title":"MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models",
            "url":"\/paper\/meltr-meta-loss-transformer-for-learning-to",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meltr-meta-loss-transformer-for-learning-to\/review\/?hl=99977"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2490,
        "row_id":28778,
        "rank":3,
        "method":"UniVL",
        "mlmodel":{

        },
        "method_short":"UniVL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-02-15",
        "metrics":{
            "BLEU-4":"17.35",
            "BLEU-3":"23.87",
            "CIDEr":"1.81",
            "ROUGE-L":"46.52",
            "METEOR":"22.35"
        },
        "raw_metrics":{
            "BLEU-4":17.35,
            "BLEU-3":23.87,
            "CIDEr":1.81,
            "ROUGE-L":46.52,
            "METEOR":22.35
        },
        "uses_additional_data":true,
        "paper":{
            "id":183645,
            "title":"UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation",
            "url":"\/paper\/univilm-a-unified-video-and-language-pre",
            "published":"2020-02-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/univilm-a-unified-video-and-language-pre\/review\/?hl=28778"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2490,
        "row_id":87220,
        "rank":4,
        "method":"VideoCoCa",
        "mlmodel":{

        },
        "method_short":"VideoCoCa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "BLEU-4":"14.2",
            "BLEU-3":null,
            "CIDEr":"1.28",
            "ROUGE-L":"37.7",
            "METEOR":null
        },
        "raw_metrics":{
            "BLEU-4":14.2,
            "BLEU-3":null,
            "CIDEr":1.28,
            "ROUGE-L":37.7,
            "METEOR":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1126258,
            "title":"VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners",
            "url":"\/paper\/video-text-modeling-with-zero-shot-transfer",
            "published":"2022-12-09T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/video-text-modeling-with-zero-shot-transfer\/review\/?hl=87220"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2490,
        "row_id":56694,
        "rank":5,
        "method":"VLM",
        "mlmodel":{

        },
        "method_short":"VLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-20",
        "metrics":{
            "BLEU-4":"12.27",
            "BLEU-3":"17.78",
            "CIDEr":"1.3869",
            "ROUGE-L":"41.51",
            "METEOR":"18.22"
        },
        "raw_metrics":{
            "BLEU-4":12.27,
            "BLEU-3":17.78,
            "CIDEr":1.3869,
            "ROUGE-L":41.51,
            "METEOR":18.22
        },
        "uses_additional_data":true,
        "paper":{
            "id":803509,
            "title":"VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding",
            "url":"\/paper\/vlm-task-agnostic-video-language-model-pre",
            "published":"2021-05-20T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2490,
        "row_id":22248,
        "rank":6,
        "method":"E2vidD6-MASSvid-BiD",
        "mlmodel":{

        },
        "method_short":"E2vidD6-MASSvid-BiD",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-30",
        "metrics":{
            "BLEU-4":"12.04",
            "BLEU-3":null,
            "CIDEr":"1.22",
            "ROUGE-L":"39.03",
            "METEOR":"18.32"
        },
        "raw_metrics":{
            "BLEU-4":12.04,
            "BLEU-3":null,
            "CIDEr":1.22,
            "ROUGE-L":39.03,
            "METEOR":18.32
        },
        "uses_additional_data":true,
        "paper":{
            "id":237275,
            "title":"Multimodal Pretraining for Dense Video Captioning",
            "url":"\/paper\/multimodal-pretraining-for-dense-video",
            "published":"2020-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multimodal-pretraining-for-dense-video\/review\/?hl=22248"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2490,
        "row_id":109119,
        "rank":7,
        "method":"TextKG",
        "mlmodel":{

        },
        "method_short":"TextKG",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-22",
        "metrics":{
            "BLEU-4":"11.7",
            "BLEU-3":null,
            "CIDEr":"1.33",
            "ROUGE-L":"40.2",
            "METEOR":"14.8"
        },
        "raw_metrics":{
            "BLEU-4":11.7,
            "BLEU-3":null,
            "CIDEr":1.33,
            "ROUGE-L":40.2,
            "METEOR":14.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1178511,
            "title":"Text with Knowledge Graph Augmented Transformer for Video Captioning",
            "url":"\/paper\/text-with-knowledge-graph-augmented",
            "published":"2023-03-22T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/text-with-knowledge-graph-augmented\/review\/?hl=109119"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2490,
        "row_id":21527,
        "rank":8,
        "method":"COOT",
        "mlmodel":{

        },
        "method_short":"COOT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-01",
        "metrics":{
            "BLEU-4":"11.30",
            "BLEU-3":"17.97",
            "CIDEr":"0.57",
            "ROUGE-L":"37.94",
            "METEOR":"19.85"
        },
        "raw_metrics":{
            "BLEU-4":11.3,
            "BLEU-3":17.97,
            "CIDEr":0.57,
            "ROUGE-L":37.94,
            "METEOR":19.85
        },
        "uses_additional_data":true,
        "paper":{
            "id":232154,
            "title":"COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning",
            "url":"\/paper\/coot-cooperative-hierarchical-transformer-for",
            "published":"2020-11-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coot-cooperative-hierarchical-transformer-for\/review\/?hl=21527"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2490,
        "row_id":69057,
        "rank":9,
        "method":"OmniVL",
        "mlmodel":{

        },
        "method_short":"OmniVL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-15",
        "metrics":{
            "BLEU-4":"8.72",
            "BLEU-3":"12.87",
            "CIDEr":"1.16",
            "ROUGE-L":"36.09",
            "METEOR":"14.83"
        },
        "raw_metrics":{
            "BLEU-4":8.72,
            "BLEU-3":12.87,
            "CIDEr":1.16,
            "ROUGE-L":36.09,
            "METEOR":14.83
        },
        "uses_additional_data":false,
        "paper":{
            "id":1075042,
            "title":"OmniVL:One Foundation Model for Image-Language and Video-Language Tasks",
            "url":"\/paper\/omnivl-one-foundation-model-for-image",
            "published":"2022-09-15T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivl-one-foundation-model-for-image\/review\/?hl=69057"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2490,
        "row_id":14647,
        "rank":10,
        "method":"Zhou",
        "mlmodel":{

        },
        "method_short":"Zhou",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-04-03",
        "metrics":{
            "BLEU-4":"4.38",
            "BLEU-3":"7.53",
            "CIDEr":"0.38",
            "ROUGE-L":"27.44",
            "METEOR":"11.55"
        },
        "raw_metrics":{
            "BLEU-4":4.38,
            "BLEU-3":7.53,
            "CIDEr":0.38,
            "ROUGE-L":27.44,
            "METEOR":11.55
        },
        "uses_additional_data":false,
        "paper":{
            "id":6858,
            "title":"End-to-End Dense Video Captioning with Masked Transformer",
            "url":"\/paper\/end-to-end-dense-video-captioning-with-masked",
            "published":"2018-04-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/end-to-end-dense-video-captioning-with-masked\/review\/?hl=14647"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2490,
        "row_id":14646,
        "rank":11,
        "method":"VideoBERT + S3D",
        "mlmodel":{

        },
        "method_short":"VideoBERT + S3D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-03",
        "metrics":{
            "BLEU-4":"4.33",
            "BLEU-3":"7.59",
            "CIDEr":"0.55",
            "ROUGE-L":"28.80",
            "METEOR":"11.94"
        },
        "raw_metrics":{
            "BLEU-4":4.33,
            "BLEU-3":7.59,
            "CIDEr":0.55,
            "ROUGE-L":28.8,
            "METEOR":11.94
        },
        "uses_additional_data":false,
        "paper":{
            "id":110403,
            "title":"VideoBERT: A Joint Model for Video and Language Representation Learning",
            "url":"\/paper\/videobert-a-joint-model-for-video-and",
            "published":"2019-04-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videobert-a-joint-model-for-video-and\/review\/?hl=14646"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]