[
    {
        "table_id":2379,
        "row_id":104633,
        "rank":1,
        "Model":"VAST",
        "mlmodel":{

        },
        "method_short":"VAST",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "text-to-video R@1":"50.4",
            "text-to-video R@5":"74.3",
            "text-to-video R@10":"80.8",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":50.4,
            "text-to-video R@5":74.3,
            "text-to-video R@10":80.8,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2379,
        "row_id":99976,
        "rank":2,
        "Model":"UniVL + MELTR",
        "mlmodel":{

        },
        "method_short":"UniVL + MELTR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "text-to-video R@1":"33.7",
            "text-to-video R@5":"63.1",
            "text-to-video R@10":"74.8",
            "text-to-video Median Rank":"3",
            "text-to-video Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":33.7,
            "text-to-video R@5":63.1,
            "text-to-video R@10":74.8,
            "text-to-video Median Rank":3.0,
            "text-to-video Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1179150,
            "title":"MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models",
            "url":"\/paper\/meltr-meta-loss-transformer-for-learning-to",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meltr-meta-loss-transformer-for-learning-to\/review\/?hl=99976"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2379,
        "row_id":56689,
        "rank":3,
        "Model":"VideoCLIP",
        "mlmodel":{

        },
        "method_short":"VideoCLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-28",
        "metrics":{
            "text-to-video R@1":"32.2",
            "text-to-video R@5":"62.6",
            "text-to-video R@10":"75.0",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":32.2,
            "text-to-video R@5":62.6,
            "text-to-video R@10":75.0,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":875851,
            "title":"VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding",
            "url":"\/paper\/videoclip-contrastive-pre-training-for-zero",
            "published":"2021-09-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2379,
        "row_id":49162,
        "rank":4,
        "Model":"MDMMT-2",
        "mlmodel":{

        },
        "method_short":"MDMMT-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-14",
        "metrics":{
            "text-to-video R@1":"32.0",
            "text-to-video R@5":"64.0",
            "text-to-video R@10":"74.8",
            "text-to-video Median Rank":"3.0",
            "text-to-video Mean Rank":"12.7"
        },
        "raw_metrics":{
            "text-to-video R@1":32.0,
            "text-to-video R@5":64.0,
            "text-to-video R@10":74.8,
            "text-to-video Median Rank":3.0,
            "text-to-video Mean Rank":12.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":976160,
            "title":"MDMMT-2: Multidomain Multimodal Transformer for Video Retrieval, One More Step Towards Generalization",
            "url":"\/paper\/mdmmt-2-multidomain-multimodal-transformer",
            "published":"2022-03-14T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/mdmmt-2-multidomain-multimodal-transformer\/review\/?hl=49162"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2379,
        "row_id":38605,
        "rank":5,
        "Model":"TACo",
        "mlmodel":{

        },
        "method_short":"TACo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-08-23",
        "metrics":{
            "text-to-video R@1":"29.6",
            "text-to-video R@5":"59.7",
            "text-to-video R@10":"72.7",
            "text-to-video Median Rank":"4",
            "text-to-video Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":29.6,
            "text-to-video R@5":59.7,
            "text-to-video R@10":72.7,
            "text-to-video Median Rank":4.0,
            "text-to-video Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":854963,
            "title":"TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment",
            "url":"\/paper\/taco-token-aware-cascade-contrastive-learning",
            "published":"2021-08-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/taco-token-aware-cascade-contrastive-learning\/review\/?hl=38605"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2379,
        "row_id":28776,
        "rank":6,
        "Model":"UniVL",
        "mlmodel":{

        },
        "method_short":"UniVL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-02-15",
        "metrics":{
            "text-to-video R@1":"28.9",
            "text-to-video R@5":"57.6",
            "text-to-video R@10":"70.0",
            "text-to-video Median Rank":"4",
            "text-to-video Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":28.9,
            "text-to-video R@5":57.6,
            "text-to-video R@10":70.0,
            "text-to-video Median Rank":4.0,
            "text-to-video Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":183645,
            "title":"UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation",
            "url":"\/paper\/univilm-a-unified-video-and-language-pre",
            "published":"2020-02-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/univilm-a-unified-video-and-language-pre\/review\/?hl=28776"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2379,
        "row_id":56695,
        "rank":7,
        "Model":"VLM",
        "mlmodel":{

        },
        "method_short":"VLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-20",
        "metrics":{
            "text-to-video R@1":"27.05",
            "text-to-video R@5":"56.88",
            "text-to-video R@10":"69.38",
            "text-to-video Median Rank":"4",
            "text-to-video Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":27.05,
            "text-to-video R@5":56.88,
            "text-to-video R@10":69.38,
            "text-to-video Median Rank":4.0,
            "text-to-video Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":803509,
            "title":"VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding",
            "url":"\/paper\/vlm-task-agnostic-video-language-model-pre",
            "published":"2021-05-20T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2379,
        "row_id":56690,
        "rank":8,
        "Model":"VideoCLIP (zero-shot)",
        "mlmodel":{

        },
        "method_short":"VideoCLIP ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-28",
        "metrics":{
            "text-to-video R@1":"22.7",
            "text-to-video R@5":"50.4",
            "text-to-video R@10":"63.1",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":22.7,
            "text-to-video R@5":50.4,
            "text-to-video R@10":63.1,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":875851,
            "title":"VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding",
            "url":"\/paper\/videoclip-contrastive-pre-training-for-zero",
            "published":"2021-09-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2379,
        "row_id":87213,
        "rank":9,
        "Model":"VideoCoCa (zero-shot)",
        "mlmodel":{

        },
        "method_short":"VideoCoCa ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "text-to-video R@1":"21.7",
            "text-to-video R@5":"43.9",
            "text-to-video R@10":"55.2",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":21.7,
            "text-to-video R@5":43.9,
            "text-to-video R@10":55.2,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1126258,
            "title":"VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners",
            "url":"\/paper\/video-text-modeling-with-zero-shot-transfer",
            "published":"2022-12-09T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/video-text-modeling-with-zero-shot-transfer\/review\/?hl=87213"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2379,
        "row_id":21526,
        "rank":10,
        "Model":"COOT",
        "mlmodel":{

        },
        "method_short":"COOT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-01",
        "metrics":{
            "text-to-video R@1":"16.7",
            "text-to-video R@5":null,
            "text-to-video R@10":"52.3",
            "text-to-video Median Rank":"9",
            "text-to-video Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":16.7,
            "text-to-video R@5":null,
            "text-to-video R@10":52.3,
            "text-to-video Median Rank":9.0,
            "text-to-video Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":232154,
            "title":"COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning",
            "url":"\/paper\/coot-cooperative-hierarchical-transformer-for",
            "published":"2020-11-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coot-cooperative-hierarchical-transformer-for\/review\/?hl=21526"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2379,
        "row_id":14471,
        "rank":11,
        "Model":"Text-Video Embedding",
        "mlmodel":{

        },
        "method_short":"Text-Video Embedding",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-07",
        "metrics":{
            "text-to-video R@1":"8.2",
            "text-to-video R@5":"24.5",
            "text-to-video R@10":"35.3",
            "text-to-video Median Rank":"24",
            "text-to-video Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":8.2,
            "text-to-video R@5":24.5,
            "text-to-video R@10":35.3,
            "text-to-video Median Rank":24.0,
            "text-to-video Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142178,
            "title":"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips",
            "url":"\/paper\/howto100m-learning-a-text-video-embedding-by",
            "published":"2019-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/howto100m-learning-a-text-video-embedding-by\/review\/?hl=14471"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2379,
        "row_id":60070,
        "rank":12,
        "Model":"RoME",
        "mlmodel":{

        },
        "method_short":"RoME",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-26",
        "metrics":{
            "text-to-video R@1":"6.3",
            "text-to-video R@5":"16.9",
            "text-to-video R@10":"25.2",
            "text-to-video Median Rank":"53",
            "text-to-video Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":6.3,
            "text-to-video R@5":16.9,
            "text-to-video R@10":25.2,
            "text-to-video Median Rank":53.0,
            "text-to-video Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1033365,
            "title":"RoME: Role-aware Mixture-of-Expert Transformer for Text-to-Video Retrieval",
            "url":"\/paper\/rome-role-aware-mixture-of-expert-transformer",
            "published":"2022-06-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rome-role-aware-mixture-of-expert-transformer\/review\/?hl=60070"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2379,
        "row_id":60071,
        "rank":13,
        "Model":"Satar et al.",
        "mlmodel":{

        },
        "method_short":"Satar et al.",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-26",
        "metrics":{
            "text-to-video R@1":"5.3",
            "text-to-video R@5":"14.5",
            "text-to-video R@10":"20.8",
            "text-to-video Median Rank":"77",
            "text-to-video Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":5.3,
            "text-to-video R@5":14.5,
            "text-to-video R@10":20.8,
            "text-to-video Median Rank":77.0,
            "text-to-video Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1033364,
            "title":"Semantic Role Aware Correlation Transformer for Text to Video Retrieval",
            "url":"\/paper\/semantic-role-aware-correlation-transformer",
            "published":"2022-06-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/semantic-role-aware-correlation-transformer\/review\/?hl=60071"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2379,
        "row_id":14472,
        "rank":14,
        "Model":"HGLMM FV CCA",
        "mlmodel":{

        },
        "method_short":"HGLMM FV CCA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2015-06-01",
        "metrics":{
            "text-to-video R@1":"4.6",
            "text-to-video R@5":"14.3",
            "text-to-video R@10":"21.6",
            "text-to-video Median Rank":"75",
            "text-to-video Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":4.6,
            "text-to-video R@5":14.3,
            "text-to-video R@10":21.6,
            "text-to-video Median Rank":75.0,
            "text-to-video Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":89967,
            "title":"Associating Neural Word Embeddings With Deep Image Representations Using Fisher Vectors",
            "url":"\/paper\/associating-neural-word-embeddings-with-deep",
            "published":"2015-06-01T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2379,
        "row_id":113029,
        "rank":15,
        "Model":"OmniVec",
        "mlmodel":{

        },
        "method_short":"OmniVec",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-07",
        "metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":null,
            "text-to-video R@10":"70.8",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":null,
            "text-to-video R@10":70.8,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1319233,
            "title":"OmniVec: Learning robust representations with cross modal sharing",
            "url":"\/paper\/omnivec-learning-robust-representations-with",
            "published":"2023-11-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivec-learning-robust-representations-with\/review\/?hl=113029"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2379,
        "row_id":113028,
        "rank":16,
        "Model":"OmniVec (pretrained)",
        "mlmodel":{

        },
        "method_short":"OmniVec ",
        "method_details":"pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-07",
        "metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":null,
            "text-to-video R@10":"64.2",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":null,
            "text-to-video R@10":64.2,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1319233,
            "title":"OmniVec: Learning robust representations with cross modal sharing",
            "url":"\/paper\/omnivec-learning-robust-representations-with",
            "published":"2023-11-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivec-learning-robust-representations-with\/review\/?hl=113028"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]