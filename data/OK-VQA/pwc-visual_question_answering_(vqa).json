[
    {
        "table_id":18035,
        "row_id":113807,
        "rank":1,
        "method":"PaLI-X-VPD",
        "mlmodel":{

        },
        "method_short":"PaLI-X-VPD",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-12-05",
        "metrics":{
            "Accuracy":"66.8",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":66.8,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1337954,
            "title":"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models",
            "url":"\/paper\/visual-program-distillation-distilling-tools",
            "published":"2023-12-05T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/visual-program-distillation-distilling-tools\/review\/?hl=113807"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":98910,
        "rank":2,
        "method":"PaLM-E-562B",
        "mlmodel":{

        },
        "method_short":"PaLM-E-562B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-06",
        "metrics":{
            "Accuracy":"66.1",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":66.1,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1168280,
            "title":"PaLM-E: An Embodied Multimodal Language Model",
            "url":"\/paper\/palm-e-an-embodied-multimodal-language-model",
            "published":"2023-03-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-e-an-embodied-multimodal-language-model\/review\/?hl=98910"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":110120,
        "rank":3,
        "method":"PaLI-X (Single-task FT)",
        "mlmodel":{

        },
        "method_short":"PaLI-X ",
        "method_details":"Single-task FT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-29",
        "metrics":{
            "Accuracy":"66.1",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":66.1,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1219485,
            "title":"PaLI-X: On Scaling up a Multilingual Vision and Language Model",
            "url":"\/paper\/pali-x-on-scaling-up-a-multilingual-vision",
            "published":"2023-05-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pali-x-on-scaling-up-a-multilingual-vision\/review\/?hl=110120"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":105261,
        "rank":4,
        "method":"PaLI 17B",
        "mlmodel":{

        },
        "method_short":"PaLI 17B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-14",
        "metrics":{
            "Accuracy":"64.5",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":64.5,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1074922,
            "title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model",
            "url":"\/paper\/pali-a-jointly-scaled-multilingual-language",
            "published":"2022-09-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pali-a-jointly-scaled-multilingual-language\/review\/?hl=105261"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":111032,
        "rank":5,
        "method":"RA-VQA-v2 (BLIP 2)",
        "mlmodel":{

        },
        "method_short":"RA-VQA-v2 ",
        "method_details":"BLIP 2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Accuracy":"62.08",
            "Exact Match (EM)":"62.01",
            "Recall@5":"89.32"
        },
        "raw_metrics":{
            "Accuracy":62.08,
            "Exact Match (EM)":62.01,
            "Recall@5":89.32
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":100127,
        "rank":6,
        "method":"Prophet",
        "mlmodel":{

        },
        "method_short":"Prophet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-03",
        "metrics":{
            "Accuracy":"61.1",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":61.1,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1167694,
            "title":"Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering",
            "url":"\/paper\/prompting-large-language-models-with-answer",
            "published":"2023-03-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/prompting-large-language-models-with-answer\/review\/?hl=100127"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":79371,
        "rank":7,
        "method":"PromptCap",
        "mlmodel":{

        },
        "method_short":"PromptCap",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-15",
        "metrics":{
            "Accuracy":"60.4",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":60.4,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1113358,
            "title":"PromptCap: Prompt-Guided Task-Aware Image Captioning",
            "url":"\/paper\/promptcap-prompt-guided-task-aware-image",
            "published":"2022-11-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/promptcap-prompt-guided-task-aware-image\/review\/?hl=79371"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":108869,
        "rank":8,
        "method":"ReVeaL WIT + CC12M + Wikidata + VQA-2",
        "mlmodel":{

        },
        "method_short":"ReVeaL WIT + CC12M + Wikidata + VQA-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-10",
        "metrics":{
            "Accuracy":"59.1",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":59.1,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1126831,
            "title":"REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory",
            "url":"\/paper\/reveal-retrieval-augmented-visual-language",
            "published":"2022-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/reveal-retrieval-augmented-visual-language\/review\/?hl=108869"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":73823,
        "rank":9,
        "method":"REVIVE (Ensemble)",
        "mlmodel":{

        },
        "method_short":"REVIVE ",
        "method_details":"Ensemble",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-02",
        "metrics":{
            "Accuracy":"58.0",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":58.0,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1020544,
            "title":"REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering",
            "url":"\/paper\/revive-regional-visual-representation-matters",
            "published":"2022-06-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revive-regional-visual-representation-matters\/review\/?hl=73823"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":73394,
        "rank":10,
        "method":"REVIVE (Single)",
        "mlmodel":{

        },
        "method_short":"REVIVE ",
        "method_details":"Single",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-02",
        "metrics":{
            "Accuracy":"56.6",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":56.6,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1020544,
            "title":"REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering",
            "url":"\/paper\/revive-regional-visual-representation-matters",
            "published":"2022-06-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revive-regional-visual-representation-matters\/review\/?hl=73394"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":111034,
        "rank":11,
        "method":"RA-VQA-v2 (T5-large)",
        "mlmodel":{

        },
        "method_short":"RA-VQA-v2 ",
        "method_details":"T5-large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Accuracy":"54.85",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":54.85,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":88500,
        "rank":12,
        "method":"RA-VQA (T5-large)",
        "mlmodel":{

        },
        "method_short":"RA-VQA ",
        "method_details":"T5-large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-07",
        "metrics":{
            "Accuracy":"54.48",
            "Exact Match (EM)":"59.41",
            "Recall@5":"82.84"
        },
        "raw_metrics":{
            "Accuracy":54.48,
            "Exact Match (EM)":59.41,
            "Recall@5":82.84
        },
        "uses_additional_data":false,
        "paper":{
            "id":1089388,
            "title":"Retrieval Augmented Visual Question Answering with Outside Knowledge",
            "url":"\/paper\/retrieval-augmented-visual-question-answering",
            "published":"2022-10-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/retrieval-augmented-visual-question-answering\/review\/?hl=88500"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":110629,
        "rank":13,
        "method":"VK-OOD",
        "mlmodel":{

        },
        "method_short":"VK-OOD",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-11",
        "metrics":{
            "Accuracy":"52.4",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":52.4,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1156677,
            "title":"Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis",
            "url":"\/paper\/differentiable-outlier-detection-enable",
            "published":"2023-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/differentiable-outlier-detection-enable\/review\/?hl=110629"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":88501,
        "rank":14,
        "method":"RA-VQA-FrDPR (T5-large)",
        "mlmodel":{

        },
        "method_short":"RA-VQA-FrDPR ",
        "method_details":"T5-large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-07",
        "metrics":{
            "Accuracy":"51.22",
            "Exact Match (EM)":"55.77",
            "Recall@5":"81.25"
        },
        "raw_metrics":{
            "Accuracy":51.22,
            "Exact Match (EM)":55.77,
            "Recall@5":81.25
        },
        "uses_additional_data":false,
        "paper":{
            "id":1089388,
            "title":"Retrieval Augmented Visual Question Answering with Outside Knowledge",
            "url":"\/paper\/retrieval-augmented-visual-question-answering",
            "published":"2022-10-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/retrieval-augmented-visual-question-answering\/review\/?hl=88501"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":96148,
        "rank":15,
        "method":"Flamingo80B",
        "mlmodel":{

        },
        "method_short":"Flamingo80B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-29",
        "metrics":{
            "Accuracy":"50.6",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":50.6,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1001838,
            "title":"Flamingo: a Visual Language Model for Few-Shot Learning",
            "url":"\/paper\/flamingo-a-visual-language-model-for-few-shot-1",
            "published":"2022-04-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":97408,
        "rank":16,
        "method":"TRiG (T5-Large)",
        "mlmodel":{

        },
        "method_short":"TRiG ",
        "method_details":"T5-Large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-01",
        "metrics":{
            "Accuracy":"50.50",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":50.5,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1021811,
            "title":"Transform-Retrieve-Generate: Natural Language-Centric Outside-Knowledge Visual Question Answering",
            "url":"\/paper\/transform-retrieve-generate-natural-language",
            "published":"2022-01-01T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":97431,
        "rank":17,
        "method":"PICa",
        "mlmodel":{

        },
        "method_short":"PICa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-10",
        "metrics":{
            "Accuracy":"48.0",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":48.0,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":864918,
            "title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA",
            "url":"\/paper\/an-empirical-study-of-gpt-3-for-few-shot",
            "published":"2021-09-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-empirical-study-of-gpt-3-for-few-shot\/review\/?hl=97431"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":99561,
        "rank":18,
        "method":"LaKo",
        "mlmodel":{

        },
        "method_short":"LaKo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-26",
        "metrics":{
            "Accuracy":"47.01",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":47.01,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1049899,
            "title":"LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection",
            "url":"\/paper\/lako-knowledge-driven-visual-question",
            "published":"2022-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lako-knowledge-driven-visual-question\/review\/?hl=99561"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":96147,
        "rank":19,
        "method":"BLIP-2 ViT-G FlanT5 XXL (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G FlanT5 XXL ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"45.9",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":45.9,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96147"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":96149,
        "rank":20,
        "method":"Flamingo9B",
        "mlmodel":{

        },
        "method_short":"Flamingo9B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-29",
        "metrics":{
            "Accuracy":"44.7",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":44.7,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1001838,
            "title":"Flamingo: a Visual Language Model for Few-Shot Learning",
            "url":"\/paper\/flamingo-a-visual-language-model-for-few-shot-1",
            "published":"2022-04-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":73822,
        "rank":21,
        "method":"VLC-BERT",
        "mlmodel":{

        },
        "method_short":"VLC-BERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Accuracy":"43.1",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":43.1,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1099773,
            "title":"VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge",
            "url":"\/paper\/vlc-bert-visual-question-answering-with",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vlc-bert-visual-question-answering-with\/review\/?hl=73822"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":99560,
        "rank":22,
        "method":"T5(Tan and Bansal, 2019) + Prefixes",
        "mlmodel":{

        },
        "method_short":"T5",
        "method_details":"Tan and Bansal, 2019",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-26",
        "metrics":{
            "Accuracy":"42.03",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":42.03,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1049899,
            "title":"LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection",
            "url":"\/paper\/lako-knowledge-driven-visual-question",
            "published":"2022-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lako-knowledge-driven-visual-question\/review\/?hl=99560"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":96150,
        "rank":23,
        "method":"Flamingo3B",
        "mlmodel":{

        },
        "method_short":"Flamingo3B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-29",
        "metrics":{
            "Accuracy":"41.2",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":41.2,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1001838,
            "title":"Flamingo: a Visual Language Model for Few-Shot Learning",
            "url":"\/paper\/flamingo-a-visual-language-model-for-few-shot-1",
            "published":"2022-04-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":96146,
        "rank":24,
        "method":"BLIP-2 ViT-G FlanT5 XL (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G FlanT5 XL ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"40.7",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":40.7,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96146"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":96145,
        "rank":25,
        "method":"BLIP-2 ViT-L FlanT5 XL (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-L FlanT5 XL ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"39.4",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":39.4,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96145"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":96144,
        "rank":26,
        "method":"BLIP-2 ViT-G OPT 6.7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G OPT 6.7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"36.4",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":36.4,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96144"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":73806,
        "rank":27,
        "method":"PNP-VQA",
        "mlmodel":{

        },
        "method_short":"PNP-VQA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-17",
        "metrics":{
            "Accuracy":"35.9",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":35.9,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1094335,
            "title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training",
            "url":"\/paper\/plug-and-play-vqa-zero-shot-vqa-by-conjoining",
            "published":"2022-10-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/plug-and-play-vqa-zero-shot-vqa-by-conjoining\/review\/?hl=73806"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":96143,
        "rank":28,
        "method":"BLIP-2 ViT-G OPT 2.7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G OPT 2.7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"31.7",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":31.7,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96143"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":96142,
        "rank":29,
        "method":"BLIP-2 ViT-L OPT 2.7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-L OPT 2.7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"30.2",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":30.2,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96142"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":96151,
        "rank":30,
        "method":"FewVLM",
        "mlmodel":{

        },
        "method_short":"FewVLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-16",
        "metrics":{
            "Accuracy":"16.5",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":16.5,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":890167,
            "title":"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models",
            "url":"\/paper\/a-good-prompt-is-worth-millions-of-parameters",
            "published":"2021-10-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-good-prompt-is-worth-millions-of-parameters\/review\/?hl=96151"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":56662,
        "rank":31,
        "method":"MetaLM",
        "mlmodel":{

        },
        "method_short":"MetaLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-13",
        "metrics":{
            "Accuracy":"11.4",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":11.4,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1026079,
            "title":"Language Models are General-Purpose Interfaces",
            "url":"\/paper\/language-models-are-general-purpose",
            "published":"2022-06-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-general-purpose\/review\/?hl=56662"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":56663,
        "rank":32,
        "method":"VLKD(ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"VLKD",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-16",
        "metrics":{
            "Accuracy":"10.5",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":10.5,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":920364,
            "title":"Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation",
            "url":"\/paper\/enabling-multimodal-generation-on-clip-via",
            "published":"2021-11-16T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":18035,
        "row_id":56664,
        "rank":33,
        "method":"Frozen",
        "mlmodel":{

        },
        "method_short":"Frozen",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-25",
        "metrics":{
            "Accuracy":" 5.9",
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "raw_metrics":{
            "Accuracy":5.9,
            "Exact Match (EM)":null,
            "Recall@5":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":825770,
            "title":"Multimodal Few-Shot Learning with Frozen Language Models",
            "url":"\/paper\/multimodal-few-shot-learning-with-frozen",
            "published":"2021-06-25T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/multimodal-few-shot-learning-with-frozen\/review\/?hl=56664"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    }
]