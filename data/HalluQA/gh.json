[
    {
        "Model":"ERNIE-Bot",
        "Misleading":70.86,
        "Misleading-hard":46.38,
        "Knowledge":75.73,
        "Total":69.33
    },
    {
        "Model":"Baichuan2-53B",
        "Misleading":59.43,
        "Misleading-hard":43.48,
        "Knowledge":83.98,
        "Total":68.22
    },
    {
        "Model":"ChatGLM-Pro",
        "Misleading":64.0,
        "Misleading-hard":34.78,
        "Knowledge":67.96,
        "Total":61.33
    },
    {
        "Model":"SparkDesk",
        "Misleading":59.43,
        "Misleading-hard":27.54,
        "Knowledge":71.36,
        "Total":60.0
    },
    {
        "Model":"abab5.5-chat",
        "Misleading":60.57,
        "Misleading-hard":39.13,
        "Knowledge":57.77,
        "Total":56.0
    },
    {
        "Model":"gpt-4-0613",
        "Misleading":76.0,
        "Misleading-hard":57.97,
        "Knowledge":32.04,
        "Total":53.11
    },
    {
        "Model":"Qwen-14B-chat",
        "Misleading":75.43,
        "Misleading-hard":23.19,
        "Knowledge":30.58,
        "Total":46.89
    },
    {
        "Model":"Baichuan2-13B-chat",
        "Misleading":61.71,
        "Misleading-hard":24.64,
        "Knowledge":32.04,
        "Total":42.44
    },
    {
        "Model":"Baichuan2-7B-chat",
        "Misleading":54.86,
        "Misleading-hard":28.99,
        "Knowledge":32.52,
        "Total":40.67
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Misleading":66.29,
        "Misleading-hard":30.43,
        "Knowledge":19.42,
        "Total":39.33
    },
    {
        "Model":"Xverse-13B-chat",
        "Misleading":65.14,
        "Misleading-hard":23.19,
        "Knowledge":22.33,
        "Total":39.11
    },
    {
        "Model":"Xverse-7B-chat",
        "Misleading":64.0,
        "Misleading-hard":13.04,
        "Knowledge":21.84,
        "Total":36.89
    },
    {
        "Model":"ChatGLM2-6B",
        "Misleading":55.43,
        "Misleading-hard":23.19,
        "Knowledge":21.36,
        "Total":34.89
    },
    {
        "Model":"Qwen-7B-chat",
        "Misleading":55.43,
        "Misleading-hard":14.49,
        "Knowledge":17.48,
        "Total":31.78
    },
    {
        "Model":"Baichuan-13B-chat",
        "Misleading":49.71,
        "Misleading-hard":8.7,
        "Knowledge":23.3,
        "Total":31.33
    },
    {
        "Model":"ChatGLM-6b",
        "Misleading":52.57,
        "Misleading-hard":20.29,
        "Knowledge":15.05,
        "Total":30.44
    },
    {
        "Model":"Qwen-14B",
        "Misleading":54.86,
        "Misleading-hard":23.19,
        "Knowledge":24.76,
        "Total":36.22
    },
    {
        "Model":"Baichuan2-13B-base",
        "Misleading":23.43,
        "Misleading-hard":24.64,
        "Knowledge":45.63,
        "Total":33.78
    },
    {
        "Model":"Qwen-7B",
        "Misleading":48.57,
        "Misleading-hard":20.29,
        "Knowledge":16.99,
        "Total":29.78
    },
    {
        "Model":"Xverse-13B",
        "Misleading":18.86,
        "Misleading-hard":24.64,
        "Knowledge":32.52,
        "Total":27.33
    },
    {
        "Model":"Baichuan-13B-base",
        "Misleading":9.71,
        "Misleading-hard":18.84,
        "Knowledge":40.78,
        "Total":25.33
    },
    {
        "Model":"Baichuan2-7B-base",
        "Misleading":8.0,
        "Misleading-hard":21.74,
        "Knowledge":41.26,
        "Total":25.33
    },
    {
        "Model":"Baichuan-7B-base",
        "Misleading":6.86,
        "Misleading-hard":15.94,
        "Knowledge":37.38,
        "Total":22.22
    },
    {
        "Model":"Xverse-7B",
        "Misleading":12.0,
        "Misleading-hard":13.04,
        "Knowledge":29.61,
        "Total":20.22
    }
]