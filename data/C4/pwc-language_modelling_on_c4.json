[
    {
        "table_id":12548,
        "row_id":39670,
        "rank":1,
        "Model":"Primer",
        "mlmodel":{

        },
        "method_short":"Primer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-17",
        "metrics":{
            "Perplexity":"12.35",
            "TPUv3 Hours":"17.3K",
            "Steps":"1M"
        },
        "raw_metrics":{
            "Perplexity":12.35,
            "TPUv3 Hours":17.3,
            "Steps":1000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":869676,
            "title":"Primer: Searching for Efficient Transformers for Language Modeling",
            "url":"\/paper\/primer-searching-for-efficient-transformers",
            "published":"2021-09-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/primer-searching-for-efficient-transformers\/review\/?hl=39670"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":12548,
        "row_id":118391,
        "rank":2,
        "Model":"Zeropoint LLM.int8 13B (vector-wise + decomp)",
        "mlmodel":{

        },
        "method_short":"Zeropoint LLM.int8 13B ",
        "method_details":"vector-wise + decomp",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-15",
        "metrics":{
            "Perplexity":"12.45",
            "TPUv3 Hours":null,
            "Steps":null
        },
        "raw_metrics":{
            "Perplexity":12.45,
            "TPUv3 Hours":null,
            "Steps":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1058964,
            "title":"LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
            "url":"\/paper\/llm-int8-8-bit-matrix-multiplication-for",
            "published":"2022-08-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llm-int8-8-bit-matrix-multiplication-for\/review\/?hl=118391"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":12548,
        "row_id":39671,
        "rank":3,
        "Model":"T5++",
        "mlmodel":{

        },
        "method_short":"T5++",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-17",
        "metrics":{
            "Perplexity":"12.69",
            "TPUv3 Hours":"16.5K",
            "Steps":"1M"
        },
        "raw_metrics":{
            "Perplexity":12.69,
            "TPUv3 Hours":16.5,
            "Steps":1000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":869676,
            "title":"Primer: Searching for Efficient Transformers for Language Modeling",
            "url":"\/paper\/primer-searching-for-efficient-transformers",
            "published":"2021-09-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/primer-searching-for-efficient-transformers\/review\/?hl=39671"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":12548,
        "row_id":39672,
        "rank":4,
        "Model":"Original T5",
        "mlmodel":{

        },
        "method_short":"Original T5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-17",
        "metrics":{
            "Perplexity":"13.25",
            "TPUv3 Hours":"15.7K",
            "Steps":"1M"
        },
        "raw_metrics":{
            "Perplexity":13.25,
            "TPUv3 Hours":15.7,
            "Steps":1000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":869676,
            "title":"Primer: Searching for Efficient Transformers for Language Modeling",
            "url":"\/paper\/primer-searching-for-efficient-transformers",
            "published":"2021-09-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/primer-searching-for-efficient-transformers\/review\/?hl=39672"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":12548,
        "row_id":118387,
        "rank":5,
        "Model":"LLM.float32 6.7B",
        "mlmodel":{

        },
        "method_short":"LLM.float32 6.7B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-15",
        "metrics":{
            "Perplexity":"13.3",
            "TPUv3 Hours":null,
            "Steps":null
        },
        "raw_metrics":{
            "Perplexity":13.3,
            "TPUv3 Hours":null,
            "Steps":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1058964,
            "title":"LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
            "url":"\/paper\/llm-int8-8-bit-matrix-multiplication-for",
            "published":"2022-08-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llm-int8-8-bit-matrix-multiplication-for\/review\/?hl=118387"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":12548,
        "row_id":118386,
        "rank":6,
        "Model":"LLM.float32 2.7B",
        "mlmodel":{

        },
        "method_short":"LLM.float32 2.7B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-15",
        "metrics":{
            "Perplexity":"14.43",
            "TPUv3 Hours":null,
            "Steps":null
        },
        "raw_metrics":{
            "Perplexity":14.43,
            "TPUv3 Hours":null,
            "Steps":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1058964,
            "title":"LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
            "url":"\/paper\/llm-int8-8-bit-matrix-multiplication-for",
            "published":"2022-08-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llm-int8-8-bit-matrix-multiplication-for\/review\/?hl=118386"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":12548,
        "row_id":118245,
        "rank":7,
        "Model":"N-Grammer 343M",
        "mlmodel":{

        },
        "method_short":"N-Grammer 343M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-13",
        "metrics":{
            "Perplexity":"14.79",
            "TPUv3 Hours":null,
            "Steps":null
        },
        "raw_metrics":{
            "Perplexity":14.79,
            "TPUv3 Hours":null,
            "Steps":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1043167,
            "title":"N-Grammer: Augmenting Transformers with latent n-grams",
            "url":"\/paper\/n-grammer-augmenting-transformers-with-latent-1",
            "published":"2022-07-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/n-grammer-augmenting-transformers-with-latent-1\/review\/?hl=118245"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":12548,
        "row_id":118243,
        "rank":8,
        "Model":"N-Grammer 288M",
        "mlmodel":{

        },
        "method_short":"N-Grammer 288M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-13",
        "metrics":{
            "Perplexity":"15.01",
            "TPUv3 Hours":null,
            "Steps":null
        },
        "raw_metrics":{
            "Perplexity":15.01,
            "TPUv3 Hours":null,
            "Steps":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1043167,
            "title":"N-Grammer: Augmenting Transformers with latent n-grams",
            "url":"\/paper\/n-grammer-augmenting-transformers-with-latent-1",
            "published":"2022-07-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/n-grammer-augmenting-transformers-with-latent-1\/review\/?hl=118243"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":12548,
        "row_id":118385,
        "rank":9,
        "Model":"LLM.float32 1.3B",
        "mlmodel":{

        },
        "method_short":"LLM.float32 1.3B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-15",
        "metrics":{
            "Perplexity":"15.91",
            "TPUv3 Hours":null,
            "Steps":null
        },
        "raw_metrics":{
            "Perplexity":15.91,
            "TPUv3 Hours":null,
            "Steps":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1058964,
            "title":"LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
            "url":"\/paper\/llm-int8-8-bit-matrix-multiplication-for",
            "published":"2022-08-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llm-int8-8-bit-matrix-multiplication-for\/review\/?hl=118385"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]