[
    {
        "table_id":1226,
        "row_id":39892,
        "rank":1,
        "Model":"VideoMAE V2-g",
        "mlmodel":{

        },
        "method_short":"VideoMAE V2-g",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-29",
        "metrics":{
            "Average accuracy of 3 splits":"88.1"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":88.1
        },
        "uses_additional_data":true,
        "paper":{
            "id":1182705,
            "title":"VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
            "url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders",
            "published":"2023-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders\/review\/?hl=39892"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":101056,
        "rank":2,
        "Model":"DEEP-HAL with ODF+SDF(I3D)",
        "mlmodel":{

        },
        "method_short":"DEEP-HAL with ODF+SDF",
        "method_details":"I3D",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-01-14",
        "metrics":{
            "Average accuracy of 3 splits":"87.56"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":87.56
        },
        "uses_additional_data":true,
        "paper":{
            "id":179803,
            "title":"Self-supervising Action Recognition by Statistical Moment and Subspace Descriptors",
            "url":"\/paper\/hallucinating-statistical-moment-and-subspace",
            "published":"2020-01-14T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":41689,
        "rank":3,
        "Model":"TO+MaxExp+IDT",
        "mlmodel":{

        },
        "method_short":"TO+MaxExp+IDT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-11",
        "metrics":{
            "Average accuracy of 3 splits":"87.21"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":87.21
        },
        "uses_additional_data":true,
        "paper":{
            "id":885936,
            "title":"High-order Tensor Pooling with Attention for Action Recognition",
            "url":"\/paper\/high-order-tensor-pooling-with-attention-for",
            "published":"2021-10-11T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":43654,
        "rank":4,
        "Model":"SCK\u2295(I3D)+IDT",
        "mlmodel":{

        },
        "method_short":"SCK\u2295",
        "method_details":"I3D",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-28",
        "metrics":{
            "Average accuracy of 3 splits":"86.11"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":86.11
        },
        "uses_additional_data":true,
        "paper":{
            "id":731870,
            "title":"Tensor Representations for Action Recognition",
            "url":"\/paper\/tensor-representations-for-action-recognition",
            "published":"2020-12-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tensor-representations-for-action-recognition\/review\/?hl=43654"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":41690,
        "rank":5,
        "Model":"SO+MaxExp+IDT",
        "mlmodel":{

        },
        "method_short":"SO+MaxExp+IDT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-11",
        "metrics":{
            "Average accuracy of 3 splits":"85.70"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":85.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":885936,
            "title":"High-order Tensor Pooling with Attention for Action Recognition",
            "url":"\/paper\/high-order-tensor-pooling-with-attention-for",
            "published":"2021-10-11T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":19271,
        "rank":6,
        "Model":"R2+1D-BERT",
        "mlmodel":{

        },
        "method_short":"R2+1D-BERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-08-03",
        "metrics":{
            "Average accuracy of 3 splits":"85.10"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":85.1
        },
        "uses_additional_data":true,
        "paper":{
            "id":211801,
            "title":"Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition",
            "url":"\/paper\/late-temporal-modeling-in-3d-cnn",
            "published":"2020-08-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/late-temporal-modeling-in-3d-cnn\/review\/?hl=19271"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":28,
                "name":"BERT",
                "color":"#850000"
            },
            {
                "id":19,
                "name":"R(2+1)D",
                "color":"#65a701"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":53212,
        "rank":7,
        "Model":"Ours + ResNext101 BERT",
        "mlmodel":{

        },
        "method_short":"Ours + ResNext101 BERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-10-16",
        "metrics":{
            "Average accuracy of 3 splits":"84.53"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":84.53
        },
        "uses_additional_data":false,
        "paper":{
            "id":228489,
            "title":"Pose And Joint-Aware Action Recognition",
            "url":"\/paper\/pose-and-joint-aware-action-recognition",
            "published":"2020-10-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pose-and-joint-aware-action-recognition\/review\/?hl=53212"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":24020,
        "rank":8,
        "Model":"SMART",
        "mlmodel":{

        },
        "method_short":"SMART",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-19",
        "metrics":{
            "Average accuracy of 3 splits":"84.36"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":84.36
        },
        "uses_additional_data":false,
        "paper":{
            "id":730142,
            "title":"SMART Frame Selection for Action Recognition",
            "url":"\/paper\/smart-frame-selection-for-action-recognition",
            "published":"2020-12-19T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/smart-frame-selection-for-action-recognition\/review\/?hl=24020"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":100257,
        "rank":9,
        "Model":"BIKE",
        "mlmodel":{

        },
        "method_short":"BIKE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-31",
        "metrics":{
            "Average accuracy of 3 splits":"84.31"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":84.31
        },
        "uses_additional_data":true,
        "paper":{
            "id":1136846,
            "title":"Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained Vision-Language Models",
            "url":"\/paper\/bidirectional-cross-modal-knowledge",
            "published":"2022-12-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bidirectional-cross-modal-knowledge\/review\/?hl=100257"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":21648,
        "rank":10,
        "Model":"OmniSource (SlowOnly-8x8-R101-RGB + I3D Flow)",
        "mlmodel":{

        },
        "method_short":"OmniSource ",
        "method_details":"SlowOnly-8x8-R101-RGB + I3D Flow",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-29",
        "metrics":{
            "Average accuracy of 3 splits":"83.8"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":83.8
        },
        "uses_additional_data":true,
        "paper":{
            "id":188764,
            "title":"Omni-sourced Webly-supervised Learning for Video Recognition",
            "url":"\/paper\/omni-sourced-webly-supervised-learning-for",
            "published":"2020-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/omni-sourced-webly-supervised-learning-for\/review\/?hl=21648"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":109645,
        "rank":11,
        "Model":"ZeroI2V ViT-L\/14",
        "mlmodel":{

        },
        "method_short":"ZeroI2V ViT-L\/14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-02",
        "metrics":{
            "Average accuracy of 3 splits":"83.4"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":83.4
        },
        "uses_additional_data":true,
        "paper":{
            "id":1292264,
            "title":"ZeroI2V: Zero-Cost Adaptation of Pre-trained Transformers from Image to Video",
            "url":"\/paper\/zeroi2v-zero-cost-adaptation-of-pre-trained",
            "published":"2023-10-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/zeroi2v-zero-cost-adaptation-of-pre-trained\/review\/?hl=109645"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":23856,
        "rank":12,
        "Model":"PERF-Net (distilled S3D-G)",
        "mlmodel":{

        },
        "method_short":"PERF-Net ",
        "method_details":"distilled S3D-G",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-09-28",
        "metrics":{
            "Average accuracy of 3 splits":"83.2"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":83.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":219654,
            "title":"PERF-Net: Pose Empowered RGB-Flow Net",
            "url":"\/paper\/perf-net-pose-empowered-rgb-flow-net",
            "published":"2020-09-28T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/perf-net-pose-empowered-rgb-flow-net\/review\/?hl=23856"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":21899,
        "rank":13,
        "Model":"BubbleNET",
        "mlmodel":{

        },
        "method_short":"BubbleNET",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-10-30",
        "metrics":{
            "Average accuracy of 3 splits":"82.60"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":82.6
        },
        "uses_additional_data":true,
        "paper":{
            "id":743563,
            "title":"Bubblenet: A Disperse Recurrent Structure To Recognize Activities",
            "url":"\/paper\/bubblenet-a-disperse-recurrent-structure-to",
            "published":"2020-10-30T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":12941,
        "rank":14,
        "Model":"HAF+BoW\/FV halluc",
        "mlmodel":{

        },
        "method_short":"HAF+BoW\/FV halluc",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-13",
        "metrics":{
            "Average accuracy of 3 splits":"82.48"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":82.48
        },
        "uses_additional_data":true,
        "paper":{
            "id":142792,
            "title":"Hallucinating IDT Descriptors and I3D Optical Flow Features for Action Recognition with CNNs",
            "url":"\/paper\/hallucinating-bag-of-words-and-fisher-vector",
            "published":"2019-06-13T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":6691,
        "rank":15,
        "Model":"CCS + TSN (ImageNet+Kinetics pretrained)",
        "mlmodel":{

        },
        "method_short":"CCS + TSN ",
        "method_details":"ImageNet+Kinetics pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-27",
        "metrics":{
            "Average accuracy of 3 splits":"81.9"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":81.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":151253,
            "title":"Cooperative Cross-Stream Network for Discriminative Action Representation",
            "url":"\/paper\/cooperative-cross-stream-network-for",
            "published":"2019-08-27T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/cooperative-cross-stream-network-for\/review\/?hl=6691"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27782,
        "rank":16,
        "Model":"RepFlow-50 ([2+1]D CNN, FcF, Non-local block)",
        "mlmodel":{

        },
        "method_short":"RepFlow-50 ",
        "method_details":"[2+1]D CNN, FcF, Non-local block",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-10-02",
        "metrics":{
            "Average accuracy of 3 splits":"81.1"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":81.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":58715,
            "title":"Representation Flow for Action Recognition",
            "url":"\/paper\/representation-flow-for-action-recognition",
            "published":"2018-10-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/representation-flow-for-action-recognition\/review\/?hl=27782"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":19,
                "name":"R(2+1)D",
                "color":"#65a701"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":6658,
        "rank":17,
        "Model":"Multi-stream I3D ",
        "mlmodel":{

        },
        "method_short":"Multi-stream I3D ",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-03-20",
        "metrics":{
            "Average accuracy of 3 splits":"80.92"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":80.92
        },
        "uses_additional_data":false,
        "paper":{
            "id":151908,
            "title":"Contextual Action Cues from Camera Sensor for Multi-Stream Action Recognition",
            "url":"\/paper\/contextual-action-cues-from-camera-sensor-for",
            "published":"2019-03-20T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":6678,
        "rank":18,
        "Model":"MARS+RGB+FLow (64 frames, Kinetics pretrained)",
        "mlmodel":{

        },
        "method_short":"MARS+RGB+FLow ",
        "method_details":"64 frames, Kinetics pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-01",
        "metrics":{
            "Average accuracy of 3 splits":"80.9"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":80.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":116002,
            "title":"MARS: Motion-Augmented RGB Stream for Action Recognition",
            "url":"\/paper\/mars-motion-augmented-rgb-stream-for-action",
            "published":"2019-06-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":12943,
        "rank":19,
        "Model":"Two-stream I3D",
        "mlmodel":{

        },
        "method_short":"Two-stream I3D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-05-22",
        "metrics":{
            "Average accuracy of 3 splits":"80.9"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":80.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":10430,
            "title":"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
            "url":"\/paper\/quo-vadis-action-recognition-a-new-model-and",
            "published":"2017-05-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/quo-vadis-action-recognition-a-new-model-and\/review\/?hl=12943"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27754,
        "rank":20,
        "Model":"Two-Stream I3D (Imagenet+Kinetics pre-training)",
        "mlmodel":{

        },
        "method_short":"Two-Stream I3D ",
        "method_details":"Imagenet+Kinetics pre-training",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-05-22",
        "metrics":{
            "Average accuracy of 3 splits":"80.7"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":80.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":10430,
            "title":"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
            "url":"\/paper\/quo-vadis-action-recognition-a-new-model-and",
            "published":"2017-05-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/quo-vadis-action-recognition-a-new-model-and\/review\/?hl=27754"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":12944,
        "rank":21,
        "Model":"LGD-3D Two-stream",
        "mlmodel":{

        },
        "method_short":"LGD-3D Two-stream",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-13",
        "metrics":{
            "Average accuracy of 3 splits":"80.5"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":80.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":142601,
            "title":"Learning Spatio-Temporal Representation with Local and Global Diffusion",
            "url":"\/paper\/learning-spatio-temporal-representation-with-3",
            "published":"2019-06-13T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/learning-spatio-temporal-representation-with-3\/review\/?hl=12944"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27821,
        "rank":22,
        "Model":"D3D + D3D",
        "mlmodel":{

        },
        "method_short":"D3D + D3D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-19",
        "metrics":{
            "Average accuracy of 3 splits":"80.5"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":80.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":65765,
            "title":"D3D: Distilled 3D Networks for Video Action Recognition",
            "url":"\/paper\/d3d-distilled-3d-networks-for-video-action",
            "published":"2018-12-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/d3d-distilled-3d-networks-for-video-action\/review\/?hl=27821"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27819,
        "rank":23,
        "Model":"D3D (Kinetics-600 pretraining)",
        "mlmodel":{

        },
        "method_short":"D3D ",
        "method_details":"Kinetics-600 pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-19",
        "metrics":{
            "Average accuracy of 3 splits":"79.3"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":79.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":65765,
            "title":"D3D: Distilled 3D Networks for Video Action Recognition",
            "url":"\/paper\/d3d-distilled-3d-networks-for-video-action",
            "published":"2018-12-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/d3d-distilled-3d-networks-for-video-action\/review\/?hl=27819"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27873,
        "rank":24,
        "Model":"LGD-3D Flow",
        "mlmodel":{

        },
        "method_short":"LGD-3D Flow",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-13",
        "metrics":{
            "Average accuracy of 3 splits":"78.9"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":78.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":142601,
            "title":"Learning Spatio-Temporal Representation with Local and Global Diffusion",
            "url":"\/paper\/learning-spatio-temporal-representation-with-3",
            "published":"2019-06-13T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/learning-spatio-temporal-representation-with-3\/review\/?hl=27873"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":8210,
        "rank":25,
        "Model":"Hidden Two-Stream",
        "mlmodel":{

        },
        "method_short":"Hidden Two-Stream",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-04-02",
        "metrics":{
            "Average accuracy of 3 splits":"78.7"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":78.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":15944,
            "title":"Hidden Two-Stream Convolutional Networks for Action Recognition",
            "url":"\/paper\/hidden-two-stream-convolutional-networks-for",
            "published":"2017-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hidden-two-stream-convolutional-networks-for\/review\/?hl=8210"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27748,
        "rank":26,
        "Model":"R[2+1]D-TwoStream (Kinetics pretrained)",
        "mlmodel":{

        },
        "method_short":"R[2+1]D-TwoStream ",
        "method_details":"Kinetics pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-30",
        "metrics":{
            "Average accuracy of 3 splits":"78.7"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":78.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":6204,
            "title":"A Closer Look at Spatiotemporal Convolutions for Action Recognition",
            "url":"\/paper\/a-closer-look-at-spatiotemporal-convolutions",
            "published":"2017-11-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":19,
                "name":"R(2+1)D",
                "color":"#65a701"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27817,
        "rank":27,
        "Model":"D3D (Kinetics-400 pretraining)",
        "mlmodel":{

        },
        "method_short":"D3D ",
        "method_details":"Kinetics-400 pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-19",
        "metrics":{
            "Average accuracy of 3 splits":"78.7"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":78.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":65765,
            "title":"D3D: Distilled 3D Networks for Video Action Recognition",
            "url":"\/paper\/d3d-distilled-3d-networks-for-video-action",
            "published":"2018-12-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/d3d-distilled-3d-networks-for-video-action\/review\/?hl=27817"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":49520,
        "rank":28,
        "Model":"I3D RGB + DMC-Net (I3D)",
        "mlmodel":{

        },
        "method_short":"I3D RGB + DMC-Net ",
        "method_details":"I3D",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-01-11",
        "metrics":{
            "Average accuracy of 3 splits":"77.8"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":77.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":87278,
            "title":"DMC-Net: Generating Discriminative Motion Cues for Fast Compressed Video Action Recognition",
            "url":"\/paper\/dmc-net-generating-discriminative-motion-cues",
            "published":"2019-01-11T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/dmc-net-generating-discriminative-motion-cues\/review\/?hl=49520"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":40704,
        "rank":29,
        "Model":"BQN",
        "mlmodel":{

        },
        "method_short":"BQN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Average accuracy of 3 splits":"77.6"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":77.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":758504,
            "title":"Busy-Quiet Video Disentangling for Video Classification",
            "url":"\/paper\/video-classification-with-finecoarse-networks",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-classification-with-finecoarse-networks\/review\/?hl=40704"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":20734,
        "rank":30,
        "Model":"MSNet-R50 (16 frames, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"MSNet-R50 ",
        "method_details":"16 frames, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-20",
        "metrics":{
            "Average accuracy of 3 splits":"77.4"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":77.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":209431,
            "title":"MotionSqueeze: Neural Motion Feature Learning for Video Understanding",
            "url":"\/paper\/motionsqueeze-neural-motion-feature-learning",
            "published":"2020-07-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/motionsqueeze-neural-motion-feature-learning\/review\/?hl=20734"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27758,
        "rank":31,
        "Model":"Flow-I3D (Kinetics pre-training)",
        "mlmodel":{

        },
        "method_short":"Flow-I3D ",
        "method_details":"Kinetics pre-training",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-05-22",
        "metrics":{
            "Average accuracy of 3 splits":"77.3"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":77.3
        },
        "uses_additional_data":true,
        "paper":{
            "id":10430,
            "title":"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
            "url":"\/paper\/quo-vadis-action-recognition-a-new-model-and",
            "published":"2017-05-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/quo-vadis-action-recognition-a-new-model-and\/review\/?hl=27758"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27752,
        "rank":32,
        "Model":"Flow-I3D (Imagenet+Kinetics pre-training)",
        "mlmodel":{

        },
        "method_short":"Flow-I3D ",
        "method_details":"Imagenet+Kinetics pre-training",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-05-22",
        "metrics":{
            "Average accuracy of 3 splits":"77.1"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":77.1
        },
        "uses_additional_data":true,
        "paper":{
            "id":10430,
            "title":"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
            "url":"\/paper\/quo-vadis-action-recognition-a-new-model-and",
            "published":"2017-05-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/quo-vadis-action-recognition-a-new-model-and\/review\/?hl=27752"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":12945,
        "rank":33,
        "Model":"HATNet (32 frames)",
        "mlmodel":{

        },
        "method_short":"HATNet ",
        "method_details":"32 frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-25",
        "metrics":{
            "Average accuracy of 3 splits":"76.5"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":76.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":112715,
            "title":"Large Scale Holistic Video Understanding",
            "url":"\/paper\/holistic-large-scale-video-understanding",
            "published":"2019-04-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/holistic-large-scale-video-understanding\/review\/?hl=12945"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27747,
        "rank":34,
        "Model":"R[2+1]D-Flow (Kinetics pretrained)",
        "mlmodel":{

        },
        "method_short":"R[2+1]D-Flow ",
        "method_details":"Kinetics pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-30",
        "metrics":{
            "Average accuracy of 3 splits":"76.4"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":76.4
        },
        "uses_additional_data":true,
        "paper":{
            "id":6204,
            "title":"A Closer Look at Spatiotemporal Convolutions for Action Recognition",
            "url":"\/paper\/a-closer-look-at-spatiotemporal-convolutions",
            "published":"2017-11-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":19,
                "name":"R(2+1)D",
                "color":"#65a701"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27773,
        "rank":35,
        "Model":"S3D-G (ImageNet, Kinetics-400 pretrained)",
        "mlmodel":{

        },
        "method_short":"S3D-G ",
        "method_details":"ImageNet, Kinetics-400 pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-12-13",
        "metrics":{
            "Average accuracy of 3 splits":"75.9"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":75.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":13041,
            "title":"Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
            "url":"\/paper\/rethinking-spatiotemporal-feature-learning",
            "published":"2017-12-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-spatiotemporal-feature-learning\/review\/?hl=27773"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":6639,
        "rank":36,
        "Model":"FASTER32 (Kinetics pretrain)",
        "mlmodel":{

        },
        "method_short":"FASTER32 ",
        "method_details":"Kinetics pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-10",
        "metrics":{
            "Average accuracy of 3 splits":"75.7"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":75.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":142365,
            "title":"FASTER Recurrent Networks for Efficient Video Classification",
            "url":"\/paper\/faster-recurrent-networks-for-video",
            "published":"2019-06-10T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/faster-recurrent-networks-for-video\/review\/?hl=6639"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27871,
        "rank":37,
        "Model":"LGD-3D RGB",
        "mlmodel":{

        },
        "method_short":"LGD-3D RGB",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-13",
        "metrics":{
            "Average accuracy of 3 splits":"75.7"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":75.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":142601,
            "title":"Learning Spatio-Temporal Representation with Local and Global Diffusion",
            "url":"\/paper\/learning-spatio-temporal-representation-with-3",
            "published":"2019-06-13T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/learning-spatio-temporal-representation-with-3\/review\/?hl=27871"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27750,
        "rank":38,
        "Model":"RGB-I3D (Imagenet+Kinetics pre-training)",
        "mlmodel":{

        },
        "method_short":"RGB-I3D ",
        "method_details":"Imagenet+Kinetics pre-training",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-05-22",
        "metrics":{
            "Average accuracy of 3 splits":"74.8"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":74.8
        },
        "uses_additional_data":true,
        "paper":{
            "id":10430,
            "title":"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
            "url":"\/paper\/quo-vadis-action-recognition-a-new-model-and",
            "published":"2017-05-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/quo-vadis-action-recognition-a-new-model-and\/review\/?hl=27750"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27746,
        "rank":39,
        "Model":"R[2+1]D-RGB (Kinetics pretrained)",
        "mlmodel":{

        },
        "method_short":"R[2+1]D-RGB ",
        "method_details":"Kinetics pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-30",
        "metrics":{
            "Average accuracy of 3 splits":"74.5"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":74.5
        },
        "uses_additional_data":true,
        "paper":{
            "id":6204,
            "title":"A Closer Look at Spatiotemporal Convolutions for Action Recognition",
            "url":"\/paper\/a-closer-look-at-spatiotemporal-convolutions",
            "published":"2017-11-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":19,
                "name":"R(2+1)D",
                "color":"#65a701"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":31466,
        "rank":40,
        "Model":"VidTr-L",
        "mlmodel":{

        },
        "method_short":"VidTr-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-23",
        "metrics":{
            "Average accuracy of 3 splits":"74.4"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":74.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":787687,
            "title":"VidTr: Video Transformer Without Convolutions",
            "url":"\/paper\/vidtr-video-transformer-without-convolutions",
            "published":"2021-04-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/vidtr-video-transformer-without-convolutions\/review\/?hl=31466"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":6557,
        "rank":41,
        "Model":"ADL+ResNet+IDT",
        "mlmodel":{

        },
        "method_short":"ADL+ResNet+IDT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-07-24",
        "metrics":{
            "Average accuracy of 3 splits":"74.3"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":74.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":53500,
            "title":"Contrastive Video Representation Learning via Adversarial Perturbations",
            "url":"\/paper\/learning-discriminative-video-representations",
            "published":"2018-07-24T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27756,
        "rank":42,
        "Model":"RGB-I3D (Kinetics pre-training)",
        "mlmodel":{

        },
        "method_short":"RGB-I3D ",
        "method_details":"Kinetics pre-training",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-05-22",
        "metrics":{
            "Average accuracy of 3 splits":"74.3"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":74.3
        },
        "uses_additional_data":true,
        "paper":{
            "id":10430,
            "title":"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
            "url":"\/paper\/quo-vadis-action-recognition-a-new-model-and",
            "published":"2017-05-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/quo-vadis-action-recognition-a-new-model-and\/review\/?hl=27756"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":12946,
        "rank":43,
        "Model":"Optical Flow Guided Feature",
        "mlmodel":{

        },
        "method_short":"Optical Flow Guided Feature",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-29",
        "metrics":{
            "Average accuracy of 3 splits":"74.2"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":74.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":13814,
            "title":"Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition",
            "url":"\/paper\/optical-flow-guided-feature-a-fast-and-robust",
            "published":"2017-11-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/optical-flow-guided-feature-a-fast-and-robust\/review\/?hl=12946"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27745,
        "rank":44,
        "Model":"R[2+1D]D-TwoStream (Sports1M pretrained)",
        "mlmodel":{

        },
        "method_short":"R[2+1D]D-TwoStream ",
        "method_details":"Sports1M pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-30",
        "metrics":{
            "Average accuracy of 3 splits":"72.7"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":72.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":6204,
            "title":"A Closer Look at Spatiotemporal Convolutions for Action Recognition",
            "url":"\/paper\/a-closer-look-at-spatiotemporal-convolutions",
            "published":"2017-11-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27833,
        "rank":45,
        "Model":"TVNet+IDT",
        "mlmodel":{

        },
        "method_short":"TVNet+IDT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-04-02",
        "metrics":{
            "Average accuracy of 3 splits":"72.6"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":72.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":6928,
            "title":"End-to-End Learning of Motion Representation for Video Understanding",
            "url":"\/paper\/end-to-end-learning-of-motion-representation",
            "published":"2018-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/end-to-end-learning-of-motion-representation\/review\/?hl=27833"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":6556,
        "rank":46,
        "Model":"STM Network+IDT",
        "mlmodel":{

        },
        "method_short":"STM Network+IDT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-07-01",
        "metrics":{
            "Average accuracy of 3 splits":"72.2"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":72.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":91216,
            "title":"Spatiotemporal Multiplier Networks for Video Action Recognition",
            "url":"\/paper\/spatiotemporal-multiplier-networks-for-video",
            "published":"2017-07-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":6588,
        "rank":47,
        "Model":"Prob-Distill",
        "mlmodel":{

        },
        "method_short":"Prob-Distill",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-05",
        "metrics":{
            "Average accuracy of 3 splits":"72.0"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":72.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":111002,
            "title":"Attention Distillation for Learning Video Representations",
            "url":"\/paper\/paying-more-attention-to-motion-attention",
            "published":"2019-04-05T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/paying-more-attention-to-motion-attention\/review\/?hl=6588"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":49519,
        "rank":48,
        "Model":"DMC-Net (I3D)",
        "mlmodel":{

        },
        "method_short":"DMC-Net ",
        "method_details":"I3D",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-01-11",
        "metrics":{
            "Average accuracy of 3 splits":"71.8"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":71.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":87278,
            "title":"DMC-Net: Generating Discriminative Motion Cues for Fast Compressed Video Action Recognition",
            "url":"\/paper\/dmc-net-generating-discriminative-motion-cues",
            "published":"2019-01-11T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/dmc-net-generating-discriminative-motion-cues\/review\/?hl=49519"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":18467,
        "rank":49,
        "Model":"TesNet (ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"TesNet ",
        "method_details":"ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-02-11",
        "metrics":{
            "Average accuracy of 3 splits":"71.5"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":71.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":183114,
            "title":"Learning spatio-temporal representations with temporal squeeze pooling",
            "url":"\/paper\/learning-spatio-temporal-representations-with",
            "published":"2020-02-11T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/learning-spatio-temporal-representations-with\/review\/?hl=18467"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":6638,
        "rank":50,
        "Model":"HF-ECOLite (ImageNet+Kinetics pretrain)",
        "mlmodel":{

        },
        "method_short":"HF-ECOLite ",
        "method_details":"ImageNet+Kinetics pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-29",
        "metrics":{
            "Average accuracy of 3 splits":"71.13"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":71.13
        },
        "uses_additional_data":true,
        "paper":{
            "id":117681,
            "title":"Hierarchical Feature Aggregation Networks for Video Action Recognition",
            "url":"\/paper\/hierarchical-feature-aggregation-networks-for",
            "published":"2019-05-29T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/hierarchical-feature-aggregation-networks-for\/review\/?hl=6638"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27779,
        "rank":51,
        "Model":"ARTNet w\/ TSN",
        "mlmodel":{

        },
        "method_short":"ARTNet w\/ TSN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-24",
        "metrics":{
            "Average accuracy of 3 splits":"70.9"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":70.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":4408,
            "title":"Appearance-and-Relation Networks for Video Classification",
            "url":"\/paper\/appearance-and-relation-networks-for-video",
            "published":"2017-11-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/appearance-and-relation-networks-for-video\/review\/?hl=27779"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27767,
        "rank":52,
        "Model":"ST-ResNet + IDT",
        "mlmodel":{

        },
        "method_short":"ST-ResNet + IDT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-11-07",
        "metrics":{
            "Average accuracy of 3 splits":"70.3"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":70.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":28966,
            "title":"Spatiotemporal Residual Networks for Video Action Recognition",
            "url":"\/paper\/spatiotemporal-residual-networks-for-video",
            "published":"2016-11-07T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27744,
        "rank":53,
        "Model":"R[2+1]D-Flow (Sports1M pretrained)",
        "mlmodel":{

        },
        "method_short":"R[2+1]D-Flow ",
        "method_details":"Sports1M pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-30",
        "metrics":{
            "Average accuracy of 3 splits":"70.1"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":70.1
        },
        "uses_additional_data":true,
        "paper":{
            "id":6204,
            "title":"A Closer Look at Spatiotemporal Convolutions for Action Recognition",
            "url":"\/paper\/a-closer-look-at-spatiotemporal-convolutions",
            "published":"2017-11-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":19,
                "name":"R(2+1)D",
                "color":"#65a701"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27765,
        "rank":54,
        "Model":"Temporal Segment Networks",
        "mlmodel":{

        },
        "method_short":"Temporal Segment Networks",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-08-02",
        "metrics":{
            "Average accuracy of 3 splits":"69.4"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":69.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":31692,
            "title":"Temporal Segment Networks: Towards Good Practices for Deep Action Recognition",
            "url":"\/paper\/temporal-segment-networks-towards-good",
            "published":"2016-08-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporal-segment-networks-towards-good\/review\/?hl=27765"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":12947,
        "rank":55,
        "Model":"TS-LSTM",
        "mlmodel":{

        },
        "method_short":"TS-LSTM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-03-30",
        "metrics":{
            "Average accuracy of 3 splits":"69"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":69.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":24666,
            "title":"TS-LSTM and Temporal-Inception: Exploiting Spatiotemporal Dynamics for Activity Recognition",
            "url":"\/paper\/ts-lstm-and-temporal-inception-exploiting",
            "published":"2017-03-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ts-lstm-and-temporal-inception-exploiting\/review\/?hl=12947"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":89416,
        "rank":56,
        "Model":"SVT (finetune)",
        "mlmodel":{

        },
        "method_short":"SVT ",
        "method_details":"finetune",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Average accuracy of 3 splits":"67.2"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":67.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":924698,
            "title":"Self-supervised Video Transformer",
            "url":"\/paper\/self-supervised-video-transformer",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-supervised-video-transformer\/review\/?hl=89416"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27743,
        "rank":57,
        "Model":"R[2+1]D-RGB (Sports1M pretrained)",
        "mlmodel":{

        },
        "method_short":"R[2+1]D-RGB ",
        "method_details":"Sports1M pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-30",
        "metrics":{
            "Average accuracy of 3 splits":"66.6"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":66.6
        },
        "uses_additional_data":true,
        "paper":{
            "id":6204,
            "title":"A Closer Look at Spatiotemporal Convolutions for Action Recognition",
            "url":"\/paper\/a-closer-look-at-spatiotemporal-convolutions",
            "published":"2017-11-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27763,
        "rank":58,
        "Model":"TDD + IDT",
        "mlmodel":{

        },
        "method_short":"TDD + IDT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2015-05-19",
        "metrics":{
            "Average accuracy of 3 splits":"65.9"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":65.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":40603,
            "title":"Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors",
            "url":"\/paper\/action-recognition-with-trajectory-pooled",
            "published":"2015-05-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/action-recognition-with-trajectory-pooled\/review\/?hl=27763"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":35421,
        "rank":59,
        "Model":"VIMPAC",
        "mlmodel":{

        },
        "method_short":"VIMPAC",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-21",
        "metrics":{
            "Average accuracy of 3 splits":"65.9"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":65.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":821705,
            "title":"VIMPAC: Video Pre-Training via Masked Token Prediction and Contrastive Learning",
            "url":"\/paper\/vimpac-video-pre-training-via-masked-token",
            "published":"2021-06-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vimpac-video-pre-training-via-masked-token\/review\/?hl=35421"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":12949,
        "rank":60,
        "Model":"S:VGG-16, T:VGG-16 (ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"S:VGG-16, T:VGG-16 ",
        "method_details":"ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-04-22",
        "metrics":{
            "Average accuracy of 3 splits":"65.4"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":65.4
        },
        "uses_additional_data":true,
        "paper":{
            "id":30270,
            "title":"Convolutional Two-Stream Network Fusion for Video Action Recognition",
            "url":"\/paper\/convolutional-two-stream-network-fusion-for",
            "published":"2016-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convolutional-two-stream-network-fusion-for\/review\/?hl=12949"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27761,
        "rank":61,
        "Model":"Dynamic Image Networks + IDT",
        "mlmodel":{

        },
        "method_short":"Dynamic Image Networks + IDT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-06-01",
        "metrics":{
            "Average accuracy of 3 splits":"65.2"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":65.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":90406,
            "title":"Dynamic Image Networks for Action Recognition",
            "url":"\/paper\/dynamic-image-networks-for-action-recognition",
            "published":"2016-06-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":12950,
        "rank":62,
        "Model":"LTC",
        "mlmodel":{

        },
        "method_short":"LTC",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-04-15",
        "metrics":{
            "Average accuracy of 3 splits":"64.8"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":64.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":22103,
            "title":"Long-term Temporal Convolutions for Action Recognition",
            "url":"\/paper\/long-term-temporal-convolutions-for-action",
            "published":"2016-04-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/long-term-temporal-convolutions-for-action\/review\/?hl=12950"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":6657,
        "rank":63,
        "Model":"R-STAN-50",
        "mlmodel":{

        },
        "method_short":"R-STAN-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-19",
        "metrics":{
            "Average accuracy of 3 splits":"62.8"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":62.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":151907,
            "title":"R-STAN: Residual Spatial-Temporal Attention Network for Action Recognition",
            "url":"\/paper\/r-stan-residual-spatial-temporal-attention",
            "published":"2019-06-19T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":49518,
        "rank":64,
        "Model":"DMC-Net (ResNet-18)",
        "mlmodel":{

        },
        "method_short":"DMC-Net ",
        "method_details":"ResNet-18",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-01-11",
        "metrics":{
            "Average accuracy of 3 splits":"62.8"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":62.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":87278,
            "title":"DMC-Net: Generating Discriminative Motion Cues for Fast Compressed Video Action Recognition",
            "url":"\/paper\/dmc-net-generating-discriminative-motion-cues",
            "published":"2019-01-11T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/dmc-net-generating-discriminative-motion-cues\/review\/?hl=49518"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":6635,
        "rank":65,
        "Model":"SUSiNet (multi, Kinetics pretrained)",
        "mlmodel":{

        },
        "method_short":"SUSiNet ",
        "method_details":"multi, Kinetics pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-03",
        "metrics":{
            "Average accuracy of 3 splits":"62.7"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":62.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":64166,
            "title":"SUSiNet: See, Understand and Summarize it",
            "url":"\/paper\/susinet-see-understand-and-summarize-it",
            "published":"2018-12-03T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/susinet-see-understand-and-summarize-it\/review\/?hl=6635"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":12951,
        "rank":66,
        "Model":"Two-Stream (ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"Two-Stream ",
        "method_details":"ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2014-06-09",
        "metrics":{
            "Average accuracy of 3 splits":"59.4"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":59.4
        },
        "uses_additional_data":true,
        "paper":{
            "id":43357,
            "title":"Two-Stream Convolutional Networks for Action Recognition in Videos",
            "url":"\/paper\/two-stream-convolutional-networks-for-action",
            "published":"2014-06-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/two-stream-convolutional-networks-for-action\/review\/?hl=12951"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":89415,
        "rank":67,
        "Model":"SVT (linear)",
        "mlmodel":{

        },
        "method_short":"SVT ",
        "method_details":"linear",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Average accuracy of 3 splits":"57.8"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":57.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":924698,
            "title":"Self-supervised Video Transformer",
            "url":"\/paper\/self-supervised-video-transformer",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-supervised-video-transformer\/review\/?hl=89415"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27830,
        "rank":68,
        "Model":"ActionFlowNet",
        "mlmodel":{

        },
        "method_short":"ActionFlowNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-12-09",
        "metrics":{
            "Average accuracy of 3 splits":"56.4"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":56.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":10070,
            "title":"ActionFlowNet: Learning Motion Representation for Action Recognition",
            "url":"\/paper\/actionflownet-learning-motion-representation",
            "published":"2016-12-09T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/actionflownet-learning-motion-representation\/review\/?hl=27830"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":6656,
        "rank":69,
        "Model":"R-STAN-152",
        "mlmodel":{

        },
        "method_short":"R-STAN-152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-19",
        "metrics":{
            "Average accuracy of 3 splits":"55.16"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":55.16
        },
        "uses_additional_data":false,
        "paper":{
            "id":151907,
            "title":"R-STAN: Residual Spatial-Temporal Attention Network for Action Recognition",
            "url":"\/paper\/r-stan-residual-spatial-temporal-attention",
            "published":"2019-06-19T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27777,
        "rank":70,
        "Model":"Res3D",
        "mlmodel":{

        },
        "method_short":"Res3D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-08-16",
        "metrics":{
            "Average accuracy of 3 splits":"54.9"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":54.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":18716,
            "title":"ConvNet Architecture Search for Spatiotemporal Feature Learning",
            "url":"\/paper\/convnet-architecture-search-for",
            "published":"2017-08-16T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":6609,
        "rank":71,
        "Model":"R(2+1)D-18 (DistInit pretraining)",
        "mlmodel":{

        },
        "method_short":"R",
        "method_details":"2+1",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-01-26",
        "metrics":{
            "Average accuracy of 3 splits":"54.8"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":54.8
        },
        "uses_additional_data":true,
        "paper":{
            "id":88322,
            "title":"DistInit: Learning Video Representations Without a Single Labeled Video",
            "url":"\/paper\/distinit-learning-video-representations",
            "published":"2019-01-26T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/distinit-learning-video-representations\/review\/?hl=6609"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":19,
                "name":"R(2+1)D",
                "color":"#65a701"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":53204,
        "rank":72,
        "Model":"JRMN",
        "mlmodel":{

        },
        "method_short":"JRMN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-10-16",
        "metrics":{
            "Average accuracy of 3 splits":"54.2"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":54.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":228489,
            "title":"Pose And Joint-Aware Action Recognition",
            "url":"\/paper\/pose-and-joint-aware-action-recognition",
            "published":"2020-10-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pose-and-joint-aware-action-recognition\/review\/?hl=53204"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":230,
                "name":"Pose",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":6633,
        "rank":73,
        "Model":"CD-UAR",
        "mlmodel":{

        },
        "method_short":"CD-UAR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-03-22",
        "metrics":{
            "Average accuracy of 3 splits":"51.8"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":51.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":7719,
            "title":"Towards Universal Representation for Unseen Action Recognition",
            "url":"\/paper\/towards-universal-representation-for-unseen",
            "published":"2018-03-22T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/towards-universal-representation-for-unseen\/review\/?hl=6633"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27828,
        "rank":74,
        "Model":"C3D",
        "mlmodel":{

        },
        "method_short":"C3D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2014-12-02",
        "metrics":{
            "Average accuracy of 3 splits":"51.6"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":51.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":38344,
            "title":"Learning Spatiotemporal Features with 3D Convolutional Networks",
            "url":"\/paper\/learning-spatiotemporal-features-with-3d",
            "published":"2014-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-spatiotemporal-features-with-3d\/review\/?hl=27828"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27944,
        "rank":75,
        "Model":"R[2+1]D (VideoMoCo)",
        "mlmodel":{

        },
        "method_short":"R[2+1]D ",
        "method_details":"VideoMoCo",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-10",
        "metrics":{
            "Average accuracy of 3 splits":"49.2"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":49.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":752909,
            "title":"VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples",
            "url":"\/paper\/videomoco-contrastive-video-representation",
            "published":"2021-03-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomoco-contrastive-video-representation\/review\/?hl=27944"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":19,
                "name":"R(2+1)D",
                "color":"#65a701"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1226,
        "row_id":27942,
        "rank":76,
        "Model":"3D-ResNet-18 (VideoMoCo)",
        "mlmodel":{

        },
        "method_short":"3D-ResNet-18 ",
        "method_details":"VideoMoCo",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-10",
        "metrics":{
            "Average accuracy of 3 splits":"43.6"
        },
        "raw_metrics":{
            "Average accuracy of 3 splits":43.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":752909,
            "title":"VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples",
            "url":"\/paper\/videomoco-contrastive-video-representation",
            "published":"2021-03-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomoco-contrastive-video-representation\/review\/?hl=27942"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]