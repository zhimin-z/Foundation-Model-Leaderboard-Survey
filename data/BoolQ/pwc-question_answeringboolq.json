[
    {
        "table_id":1462,
        "row_id":60411,
        "rank":1,
        "method":"ST-MoE-32B",
        "mlmodel":{

        },
        "method_short":"ST-MoE-32B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-17",
        "metrics":{
            "Accuracy":"92.4"
        },
        "raw_metrics":{
            "Accuracy":92.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":964307,
            "title":"ST-MoE: Designing Stable and Transferable Sparse Expert Models",
            "url":"\/paper\/designing-effective-sparse-expert-models",
            "published":"2022-02-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-effective-sparse-expert-models\/review\/?hl=60411"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":51235,
        "rank":2,
        "method":"PaLM 540B (finetuned)",
        "mlmodel":{

        },
        "method_short":"PaLM 540B ",
        "method_details":"finetuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-05",
        "metrics":{
            "Accuracy":"92.2"
        },
        "raw_metrics":{
            "Accuracy":92.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":989558,
            "title":"PaLM: Scaling Language Modeling with Pathways",
            "url":"\/paper\/palm-scaling-language-modeling-with-pathways-1",
            "published":"2022-04-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":8375,
        "rank":3,
        "method":"T5-11B",
        "mlmodel":{

        },
        "method_short":"T5-11B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "Accuracy":"91.2"
        },
        "raw_metrics":{
            "Accuracy":91.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=8375"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":102562,
        "rank":4,
        "method":"PaLM 2-L (one-shot)",
        "mlmodel":{

        },
        "method_short":"PaLM 2-L ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Accuracy":"90.9"
        },
        "raw_metrics":{
            "Accuracy":90.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102562"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":24567,
        "rank":5,
        "method":"DeBERTa-1.5B",
        "mlmodel":{

        },
        "method_short":"DeBERTa-1.5B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-05",
        "metrics":{
            "Accuracy":"90.4"
        },
        "raw_metrics":{
            "Accuracy":90.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":201217,
            "title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
            "url":"\/paper\/deberta-decoding-enhanced-bert-with",
            "published":"2020-06-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deberta-decoding-enhanced-bert-with\/review\/?hl=24567"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":102561,
        "rank":6,
        "method":"PaLM 2-M (one-shot)",
        "mlmodel":{

        },
        "method_short":"PaLM 2-M ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Accuracy":"88.6"
        },
        "raw_metrics":{
            "Accuracy":88.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102561"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":102560,
        "rank":7,
        "method":"PaLM 2-S (one-shot)",
        "mlmodel":{

        },
        "method_short":"PaLM 2-S ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Accuracy":"88.1"
        },
        "raw_metrics":{
            "Accuracy":88.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102560"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":35861,
        "rank":8,
        "method":"MUPPET Roberta Large",
        "mlmodel":{

        },
        "method_short":"MUPPET Roberta Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-26",
        "metrics":{
            "Accuracy":"87.5"
        },
        "raw_metrics":{
            "Accuracy":87.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":739728,
            "title":"Muppet: Massive Multi-task Representations with Pre-Finetuning",
            "url":"\/paper\/muppet-massive-multi-task-representations",
            "published":"2021-01-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/muppet-massive-multi-task-representations\/review\/?hl=35861"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":31486,
        "rank":9,
        "method":"EFL",
        "mlmodel":{

        },
        "method_short":"EFL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-29",
        "metrics":{
            "Accuracy":"86.0"
        },
        "raw_metrics":{
            "Accuracy":86.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":791525,
            "title":"Entailment as Few-Shot Learner",
            "url":"\/paper\/entailment-as-few-shot-learner",
            "published":"2021-04-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/entailment-as-few-shot-learner\/review\/?hl=31486"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":97493,
        "rank":10,
        "method":"T5-Large",
        "mlmodel":{

        },
        "method_short":"T5-Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "Accuracy":"85.4"
        },
        "raw_metrics":{
            "Accuracy":85.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=97493"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":97602,
        "rank":11,
        "method":"LLaMA 65B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 65B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"85.3"
        },
        "raw_metrics":{
            "Accuracy":85.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97602"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":106303,
        "rank":12,
        "method":"LLaMA 2 70B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 2 70B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Accuracy":"85"
        },
        "raw_metrics":{
            "Accuracy":85.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106303"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":35860,
        "rank":13,
        "method":"MUPPET Roberta Base",
        "mlmodel":{

        },
        "method_short":"MUPPET Roberta Base",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-26",
        "metrics":{
            "Accuracy":"83.8"
        },
        "raw_metrics":{
            "Accuracy":83.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":739728,
            "title":"Muppet: Massive Multi-task Representations with Pre-Finetuning",
            "url":"\/paper\/muppet-massive-multi-task-representations",
            "published":"2021-01-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/muppet-massive-multi-task-representations\/review\/?hl=35860"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":66320,
        "rank":14,
        "method":"Chinchilla (zero-shot)",
        "mlmodel":{

        },
        "method_short":"Chinchilla ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-29",
        "metrics":{
            "Accuracy":"83.7"
        },
        "raw_metrics":{
            "Accuracy":83.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":985465,
            "title":"Training Compute-Optimal Large Language Models",
            "url":"\/paper\/training-compute-optimal-large-language",
            "published":"2022-03-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":106302,
        "rank":15,
        "method":"LLaMA 2 34B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 2 34B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Accuracy":"83.7"
        },
        "raw_metrics":{
            "Accuracy":83.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106302"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":97601,
        "rank":16,
        "method":"LLaMA 33B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 33B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"83.1"
        },
        "raw_metrics":{
            "Accuracy":83.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97601"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":39029,
        "rank":17,
        "method":"FLAN 137B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"FLAN 137B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-03",
        "metrics":{
            "Accuracy":"82.9"
        },
        "raw_metrics":{
            "Accuracy":82.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":861409,
            "title":"Finetuned Language Models Are Zero-Shot Learners",
            "url":"\/paper\/finetuned-language-models-are-zero-shot",
            "published":"2021-09-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/finetuned-language-models-are-zero-shot\/review\/?hl=39029"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":106301,
        "rank":18,
        "method":"LLaMA 2 13B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 2 13B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Accuracy":"81.7"
        },
        "raw_metrics":{
            "Accuracy":81.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106301"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":97492,
        "rank":19,
        "method":"T5-Base",
        "mlmodel":{

        },
        "method_short":"T5-Base",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "Accuracy":"81.4"
        },
        "raw_metrics":{
            "Accuracy":81.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=97492"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":66319,
        "rank":20,
        "method":"Gopher (zero-shot)",
        "mlmodel":{

        },
        "method_short":"Gopher ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-08",
        "metrics":{
            "Accuracy":"79.3"
        },
        "raw_metrics":{
            "Accuracy":79.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":942590,
            "title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
            "url":"\/paper\/scaling-language-models-methods-analysis-1",
            "published":"2021-12-08T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":97600,
        "rank":21,
        "method":"LLaMA 13B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 13B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"78.1"
        },
        "raw_metrics":{
            "Accuracy":78.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97600"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":106300,
        "rank":22,
        "method":"LLaMA 2 7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 2 7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Accuracy":"77.4"
        },
        "raw_metrics":{
            "Accuracy":77.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106300"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":97599,
        "rank":23,
        "method":"LLaMA 7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"76.5"
        },
        "raw_metrics":{
            "Accuracy":76.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97599"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":16813,
        "rank":24,
        "method":"GPT-3 175B (few-shot)",
        "mlmodel":{

        },
        "method_short":"GPT-3 175B ",
        "method_details":"few-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "Accuracy":"76.4"
        },
        "raw_metrics":{
            "Accuracy":76.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=16813"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":97491,
        "rank":25,
        "method":"T5-Small",
        "mlmodel":{

        },
        "method_short":"T5-Small",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "Accuracy":"76.4"
        },
        "raw_metrics":{
            "Accuracy":76.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=97491"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":100784,
        "rank":26,
        "method":"Bloomberg GPT (one-shot)",
        "mlmodel":{

        },
        "method_short":"Bloomberg GPT ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"74.59"
        },
        "raw_metrics":{
            "Accuracy":74.59
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100784"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":97042,
        "rank":27,
        "method":"OPT-IML 175B",
        "mlmodel":{

        },
        "method_short":"OPT-IML 175B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-22",
        "metrics":{
            "Accuracy":"71.4"
        },
        "raw_metrics":{
            "Accuracy":71.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1133915,
            "title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "url":"\/paper\/opt-iml-scaling-language-model-instruction",
            "published":"2022-12-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/opt-iml-scaling-language-model-instruction\/review\/?hl=97042"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":96233,
        "rank":28,
        "method":"AlexaTM 20B",
        "mlmodel":{

        },
        "method_short":"AlexaTM 20B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-02",
        "metrics":{
            "Accuracy":"69.44"
        },
        "raw_metrics":{
            "Accuracy":69.44
        },
        "uses_additional_data":false,
        "paper":{
            "id":1053502,
            "title":"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
            "url":"\/paper\/alexatm-20b-few-shot-learning-using-a-large",
            "published":"2022-08-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/alexatm-20b-few-shot-learning-using-a-large\/review\/?hl=96233"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":70998,
        "rank":29,
        "method":"Neo-6B (QA + WS)",
        "mlmodel":{

        },
        "method_short":"Neo-6B ",
        "method_details":"QA + WS",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-05",
        "metrics":{
            "Accuracy":" 67.2"
        },
        "raw_metrics":{
            "Accuracy":67.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087115,
            "title":"Ask Me Anything: A simple strategy for prompting language models",
            "url":"\/paper\/ask-me-anything-a-simple-strategy-for",
            "published":"2022-10-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ask-me-anything-a-simple-strategy-for\/review\/?hl=70998"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":97039,
        "rank":30,
        "method":"OPT-IML 30B",
        "mlmodel":{

        },
        "method_short":"OPT-IML 30B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-22",
        "metrics":{
            "Accuracy":"66.9"
        },
        "raw_metrics":{
            "Accuracy":66.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":1133915,
            "title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "url":"\/paper\/opt-iml-scaling-language-model-instruction",
            "published":"2022-12-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/opt-iml-scaling-language-model-instruction\/review\/?hl=97039"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":70996,
        "rank":31,
        "method":"Neo-6B (few-shot)",
        "mlmodel":{

        },
        "method_short":"Neo-6B ",
        "method_details":"few-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-05",
        "metrics":{
            "Accuracy":"66.5"
        },
        "raw_metrics":{
            "Accuracy":66.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087115,
            "title":"Ask Me Anything: A simple strategy for prompting language models",
            "url":"\/paper\/ask-me-anything-a-simple-strategy-for",
            "published":"2022-10-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ask-me-anything-a-simple-strategy-for\/review\/?hl=70996"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":60035,
        "rank":32,
        "method":"N-Grammer",
        "mlmodel":{

        },
        "method_short":"N-Grammer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-13",
        "metrics":{
            "Accuracy":"64.98"
        },
        "raw_metrics":{
            "Accuracy":64.98
        },
        "uses_additional_data":false,
        "paper":{
            "id":1043167,
            "title":"N-Grammer: Augmenting Transformers with latent n-grams",
            "url":"\/paper\/n-grammer-augmenting-transformers-with-latent-1",
            "published":"2022-07-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/n-grammer-augmenting-transformers-with-latent-1\/review\/?hl=60035"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":70997,
        "rank":33,
        "method":"Neo-6B (QA)",
        "mlmodel":{

        },
        "method_short":"Neo-6B ",
        "method_details":"QA",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-05",
        "metrics":{
            "Accuracy":"64.9"
        },
        "raw_metrics":{
            "Accuracy":64.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087115,
            "title":"Ask Me Anything: A simple strategy for prompting language models",
            "url":"\/paper\/ask-me-anything-a-simple-strategy-for",
            "published":"2022-10-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ask-me-anything-a-simple-strategy-for\/review\/?hl=70997"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":97032,
        "rank":34,
        "method":"OPT 30B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"OPT 30B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-22",
        "metrics":{
            "Accuracy":"64"
        },
        "raw_metrics":{
            "Accuracy":64.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1133915,
            "title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "url":"\/paper\/opt-iml-scaling-language-model-instruction",
            "published":"2022-12-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/opt-iml-scaling-language-model-instruction\/review\/?hl=97032"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":97031,
        "rank":35,
        "method":"OPT-IML 1.3B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"OPT-IML 1.3B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-22",
        "metrics":{
            "Accuracy":"61.5"
        },
        "raw_metrics":{
            "Accuracy":61.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1133915,
            "title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "url":"\/paper\/opt-iml-scaling-language-model-instruction",
            "published":"2022-12-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/opt-iml-scaling-language-model-instruction\/review\/?hl=97031"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":66318,
        "rank":36,
        "method":"GPT-3 (zero-shot)",
        "mlmodel":{

        },
        "method_short":"GPT-3 ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "Accuracy":"60.5"
        },
        "raw_metrics":{
            "Accuracy":60.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=66318"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":97030,
        "rank":37,
        "method":"OPT 1.3B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"OPT 1.3B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-22",
        "metrics":{
            "Accuracy":"60.5"
        },
        "raw_metrics":{
            "Accuracy":60.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1133915,
            "title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "url":"\/paper\/opt-iml-scaling-language-model-instruction",
            "published":"2022-12-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/opt-iml-scaling-language-model-instruction\/review\/?hl=97030"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":97040,
        "rank":38,
        "method":"OPT 175B",
        "mlmodel":{

        },
        "method_short":"OPT 175B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-22",
        "metrics":{
            "Accuracy":"60.1"
        },
        "raw_metrics":{
            "Accuracy":60.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1133915,
            "title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "url":"\/paper\/opt-iml-scaling-language-model-instruction",
            "published":"2022-12-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/opt-iml-scaling-language-model-instruction\/review\/?hl=97040"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":100786,
        "rank":39,
        "method":"OPT 66B (one-shot)",
        "mlmodel":{

        },
        "method_short":"OPT 66B ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"57.46"
        },
        "raw_metrics":{
            "Accuracy":57.46
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100786"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":100787,
        "rank":40,
        "method":"BLOOM 176B (one-shot)",
        "mlmodel":{

        },
        "method_short":"BLOOM 176B ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"52.94"
        },
        "raw_metrics":{
            "Accuracy":52.94
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100787"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":99161,
        "rank":41,
        "method":"Hyena",
        "mlmodel":{

        },
        "method_short":"Hyena",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-21",
        "metrics":{
            "Accuracy":"51.8"
        },
        "raw_metrics":{
            "Accuracy":51.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1161437,
            "title":"Hyena Hierarchy: Towards Larger Convolutional Language Models",
            "url":"\/paper\/hyena-hierarchy-towards-larger-convolutional",
            "published":"2023-02-21T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1462,
        "row_id":100785,
        "rank":42,
        "method":"GPT-NeoX (one-shot)",
        "mlmodel":{

        },
        "method_short":"GPT-NeoX ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"46.36"
        },
        "raw_metrics":{
            "Accuracy":46.36
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100785"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]