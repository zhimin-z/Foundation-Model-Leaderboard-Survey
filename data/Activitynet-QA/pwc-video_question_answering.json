[
    {
        "table_id":11042,
        "row_id":114014,
        "rank":1,
        "Model":"VideoChat2",
        "mlmodel":{

        },
        "method_short":"VideoChat2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "Accuracy":"49.1",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":49.1,
            "Confidence score":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1329478,
            "title":"MVBench: A Comprehensive Multi-modal Video Understanding Benchmark",
            "url":"\/paper\/mvbench-a-comprehensive-multi-modal-video",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":114015,
        "rank":2,
        "Model":"LLaMA-VID-13B (2 Token)",
        "mlmodel":{

        },
        "method_short":"LLaMA-VID-13B ",
        "method_details":"2 Token",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "Accuracy":"47.5",
            "Confidence score":"3.3"
        },
        "raw_metrics":{
            "Accuracy":47.5,
            "Confidence score":3.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1329473,
            "title":"LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
            "url":"\/paper\/llama-vid-an-image-is-worth-2-tokens-in-large",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":114016,
        "rank":3,
        "Model":"LLaMA-VID-7B (2 Token)",
        "mlmodel":{

        },
        "method_short":"LLaMA-VID-7B ",
        "method_details":"2 Token",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "Accuracy":"47.4",
            "Confidence score":"3.3"
        },
        "raw_metrics":{
            "Accuracy":47.4,
            "Confidence score":3.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1329473,
            "title":"LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
            "url":"\/paper\/llama-vid-an-image-is-worth-2-tokens-in-large",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":114017,
        "rank":4,
        "Model":"Chat-UniVi-13B",
        "mlmodel":{

        },
        "method_short":"Chat-UniVi-13B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-14",
        "metrics":{
            "Accuracy":"46.4",
            "Confidence score":"3.3"
        },
        "raw_metrics":{
            "Accuracy":46.4,
            "Confidence score":3.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1320706,
            "title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding",
            "url":"\/paper\/chat-univi-unified-visual-representation",
            "published":"2023-11-14T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":114018,
        "rank":5,
        "Model":"BT-Adapter (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BT-Adapter ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-27",
        "metrics":{
            "Accuracy":"46.1",
            "Confidence score":"3.6"
        },
        "raw_metrics":{
            "Accuracy":46.1,
            "Confidence score":3.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1289477,
            "title":"One For All: Video Conversation is Feasible Without Video Instruction Tuning",
            "url":"\/paper\/one-for-all-video-conversation-is-feasible",
            "published":"2023-09-27T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":114019,
        "rank":6,
        "Model":"MovieChat",
        "mlmodel":{

        },
        "method_short":"MovieChat",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-31",
        "metrics":{
            "Accuracy":"45.7",
            "Confidence score":"3.1"
        },
        "raw_metrics":{
            "Accuracy":45.7,
            "Confidence score":3.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1255388,
            "title":"MovieChat: From Dense Token to Sparse Memory for Long Video Understanding",
            "url":"\/paper\/moviechat-from-dense-token-to-sparse-memory",
            "published":"2023-07-31T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":114020,
        "rank":7,
        "Model":"Video-LLaVA",
        "mlmodel":{

        },
        "method_short":"Video-LLaVA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-16",
        "metrics":{
            "Accuracy":"45.3",
            "Confidence score":"3.3"
        },
        "raw_metrics":{
            "Accuracy":45.3,
            "Confidence score":3.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1323228,
            "title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
            "url":"\/paper\/video-llava-learning-united-visual-1",
            "published":"2023-11-16T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":114021,
        "rank":8,
        "Model":"Video-ChatGPT",
        "mlmodel":{

        },
        "method_short":"Video-ChatGPT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-08",
        "metrics":{
            "Accuracy":"35.2",
            "Confidence score":"2.7"
        },
        "raw_metrics":{
            "Accuracy":35.2,
            "Confidence score":2.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1225920,
            "title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
            "url":"\/paper\/video-chatgpt-towards-detailed-video",
            "published":"2023-06-08T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":114022,
        "rank":9,
        "Model":"LLaMA Adapter V2",
        "mlmodel":{

        },
        "method_short":"LLaMA Adapter V2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-28",
        "metrics":{
            "Accuracy":"34.2",
            "Confidence score":"2.7"
        },
        "raw_metrics":{
            "Accuracy":34.2,
            "Confidence score":2.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1199360,
            "title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
            "url":"\/paper\/llama-adapter-v2-parameter-efficient-visual",
            "published":"2023-04-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":114023,
        "rank":10,
        "Model":"Video Chat",
        "mlmodel":{

        },
        "method_short":"Video Chat",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-10",
        "metrics":{
            "Accuracy":"26.5",
            "Confidence score":"2.2"
        },
        "raw_metrics":{
            "Accuracy":26.5,
            "Confidence score":2.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1205778,
            "title":"VideoChat: Chat-Centric Video Understanding",
            "url":"\/paper\/videochat-chat-centric-video-understanding",
            "published":"2023-05-10T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":73322,
        "rank":11,
        "Model":"GPT-2 + CLIP-14 + CLIP-multilingual (Zero-Shot)",
        "mlmodel":{

        },
        "method_short":"GPT-2 + CLIP-14 + CLIP-multilingual ",
        "method_details":"Zero-Shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Accuracy":"0.612",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.612,
            "Confidence score":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097787,
            "title":"Composing Ensembles of Pre-trained Models via Iterative Consensus",
            "url":"\/paper\/composing-ensembles-of-pre-trained-models-via",
            "published":"2022-10-20T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/composing-ensembles-of-pre-trained-models-via\/review\/?hl=73322"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":73321,
        "rank":12,
        "Model":"GPT-2 + CLIP-32 (Zero-Shot)",
        "mlmodel":{

        },
        "method_short":"GPT-2 + CLIP-32 ",
        "method_details":"Zero-Shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Accuracy":"0.584",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.584,
            "Confidence score":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097787,
            "title":"Composing Ensembles of Pre-trained Models via Iterative Consensus",
            "url":"\/paper\/composing-ensembles-of-pre-trained-models-via",
            "published":"2022-10-20T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/composing-ensembles-of-pre-trained-models-via\/review\/?hl=73321"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":96474,
        "rank":13,
        "Model":"VideoCoCa",
        "mlmodel":{

        },
        "method_short":"VideoCoCa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Accuracy":"0.561",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.561,
            "Confidence score":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1126258,
            "title":"VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners",
            "url":"\/paper\/video-text-modeling-with-zero-shot-transfer",
            "published":"2022-12-09T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/video-text-modeling-with-zero-shot-transfer\/review\/?hl=96474"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":101423,
        "rank":14,
        "Model":"VALOR",
        "mlmodel":{

        },
        "method_short":"VALOR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-17",
        "metrics":{
            "Accuracy":"0.486",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.486,
            "Confidence score":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1191887,
            "title":"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset",
            "url":"\/paper\/valor-vision-audio-language-omni-perception",
            "published":"2023-04-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/valor-vision-audio-language-omni-perception\/review\/?hl=101423"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":100397,
        "rank":15,
        "Model":"UMT-L (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"UMT-L ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "Accuracy":"0.479",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.479,
            "Confidence score":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1181934,
            "title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
            "url":"\/paper\/unmasked-teacher-towards-training-efficient",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":111075,
        "rank":16,
        "Model":"TESTA (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"TESTA ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-29",
        "metrics":{
            "Accuracy":"0.45",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.45,
            "Confidence score":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1310474,
            "title":"TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding",
            "url":"\/paper\/testa-temporal-spatial-token-aggregation-for",
            "published":"2023-10-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/testa-temporal-spatial-token-aggregation-for\/review\/?hl=111075"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":108837,
        "rank":17,
        "Model":"FrozenBiLM+",
        "mlmodel":{

        },
        "method_short":"FrozenBiLM+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Accuracy":"0.448",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.448,
            "Confidence score":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265311,
            "title":"Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models",
            "url":"\/paper\/open-vocabulary-video-question-answering-a",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/open-vocabulary-video-question-answering-a\/review\/?hl=108837"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":56551,
        "rank":18,
        "Model":"Singularity-temporal",
        "mlmodel":{

        },
        "method_short":"Singularity-temporal",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-07",
        "metrics":{
            "Accuracy":"0.441",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.441,
            "Confidence score":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1023192,
            "title":"Revealing Single Frame Bias for Video-and-Language Learning",
            "url":"\/paper\/revealing-single-frame-bias-for-video-and",
            "published":"2022-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revealing-single-frame-bias-for-video-and\/review\/?hl=56551"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":70083,
        "rank":19,
        "Model":"FrozenBiLM",
        "mlmodel":{

        },
        "method_short":"FrozenBiLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-16",
        "metrics":{
            "Accuracy":"0.432",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.432,
            "Confidence score":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1028387,
            "title":"Zero-Shot Video Question Answering via Frozen Bidirectional Language Models",
            "url":"\/paper\/zero-shot-video-question-answering-via-frozen",
            "published":"2022-06-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/zero-shot-video-question-answering-via-frozen\/review\/?hl=70083"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":56546,
        "rank":20,
        "Model":"Singularity",
        "mlmodel":{

        },
        "method_short":"Singularity",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-07",
        "metrics":{
            "Accuracy":"0.431",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.431,
            "Confidence score":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1023192,
            "title":"Revealing Single Frame Bias for Video-and-Language Learning",
            "url":"\/paper\/revealing-single-frame-bias-for-video-and",
            "published":"2022-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revealing-single-frame-bias-for-video-and\/review\/?hl=56546"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":97772,
        "rank":21,
        "Model":"Text + Text (no Multimodal Pretext Training)",
        "mlmodel":{

        },
        "method_short":"Text + Text ",
        "method_details":"no Multimodal Pretext Training",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-05",
        "metrics":{
            "Accuracy":"0.414",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.414,
            "Confidence score":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1022322,
            "title":"Towards Fast Adaptation of Pretrained Contrastive Models for Multi-channel Video-Language Retrieval",
            "url":"\/paper\/towards-fast-adaptation-of-pretrained",
            "published":"2022-06-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":108834,
        "rank":22,
        "Model":"All-in-one+",
        "mlmodel":{

        },
        "method_short":"All-in-one+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Accuracy":"0.400",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.4,
            "Confidence score":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265311,
            "title":"Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models",
            "url":"\/paper\/open-vocabulary-video-question-answering-a",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/open-vocabulary-video-question-answering-a\/review\/?hl=108834"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":108836,
        "rank":23,
        "Model":"VIOLET+",
        "mlmodel":{

        },
        "method_short":"VIOLET+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Accuracy":"0.397",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.397,
            "Confidence score":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265311,
            "title":"Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models",
            "url":"\/paper\/open-vocabulary-video-question-answering-a",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/open-vocabulary-video-question-answering-a\/review\/?hl=108836"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":37329,
        "rank":24,
        "Model":"Just Ask",
        "mlmodel":{

        },
        "method_short":"Just Ask",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Accuracy":"0.389",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.389,
            "Confidence score":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":238417,
            "title":"Just Ask: Learning to Answer Questions from Millions of Narrated Videos",
            "url":"\/paper\/just-ask-learning-to-answer-questions-from",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/just-ask-learning-to-answer-questions-from\/review\/?hl=37329"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":37328,
        "rank":25,
        "Model":"E-SA",
        "mlmodel":{

        },
        "method_short":"E-SA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-06",
        "metrics":{
            "Accuracy":"0.318",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.318,
            "Confidence score":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":119087,
            "title":"ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering",
            "url":"\/paper\/activitynet-qa-a-dataset-for-understanding",
            "published":"2019-06-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/activitynet-qa-a-dataset-for-understanding\/review\/?hl=37328"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":37333,
        "rank":26,
        "Model":"E-MN",
        "mlmodel":{

        },
        "method_short":"E-MN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-06",
        "metrics":{
            "Accuracy":"0.271",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.271,
            "Confidence score":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":119087,
            "title":"ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering",
            "url":"\/paper\/activitynet-qa-a-dataset-for-understanding",
            "published":"2019-06-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/activitynet-qa-a-dataset-for-understanding\/review\/?hl=37333"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":11042,
        "row_id":37334,
        "rank":27,
        "Model":"E-VQA",
        "mlmodel":{

        },
        "method_short":"E-VQA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-06",
        "metrics":{
            "Accuracy":"0.251",
            "Confidence score":null
        },
        "raw_metrics":{
            "Accuracy":0.251,
            "Confidence score":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":119087,
            "title":"ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering",
            "url":"\/paper\/activitynet-qa-a-dataset-for-understanding",
            "published":"2019-06-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/activitynet-qa-a-dataset-for-understanding\/review\/?hl=37334"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]