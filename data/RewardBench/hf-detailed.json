[
    {
        "Model":"Cohere March 2024",
        "Model Type":"Custom Classifier",
        "Average":86.6,
        "alpacaeval-easy":97,
        "alpacaeval-hard":97.9,
        "alpacaeval-length":87.4,
        "donotanswer":86.8,
        "hep-cpp":98.2,
        "hep-go":95.1,
        "hep-java":95.1,
        "hep-js":96.3,
        "hep-python":97.0,
        "hep-rust":96.3,
        "llmbar-adver-GPTInst":60.9,
        "llmbar-adver-GPTOut":68.1,
        "llmbar-adver-manual":43.5,
        "llmbar-adver-neighbor":52.2,
        "llmbar-natural":92,
        "math-prm":100.0,
        "mt-bench-easy":96.4,
        "mt-bench-hard":73.0,
        "mt-bench-med":97.5,
        "refusals-dangerous":85,
        "refusals-offensive":98,
        "xstest-should-refuse":98.7,
        "xstest-should-respond":78.4
    },
    {
        "Model":"Nexusflow\/Starling-RM-34B",
        "Model Type":"Seq. Classifier",
        "Average":83.9,
        "alpacaeval-easy":99,
        "alpacaeval-hard":100.0,
        "alpacaeval-length":92.6,
        "donotanswer":61.8,
        "hep-cpp":89.6,
        "hep-go":92.7,
        "hep-java":94.5,
        "hep-js":95.1,
        "hep-python":91.5,
        "hep-rust":86.6,
        "llmbar-adver-GPTInst":39.1,
        "llmbar-adver-GPTOut":76.6,
        "llmbar-adver-manual":47.8,
        "llmbar-adver-neighbor":31.3,
        "llmbar-natural":91,
        "math-prm":85.2,
        "mt-bench-easy":96.4,
        "mt-bench-hard":91.9,
        "mt-bench-med":95.0,
        "refusals-dangerous":84,
        "refusals-offensive":97,
        "xstest-should-refuse":97.4,
        "xstest-should-respond":93.6
    },
    {
        "Model":"weqweasdas\/RM-Mistral-7B",
        "Model Type":"Seq. Classifier",
        "Average":82.4,
        "alpacaeval-easy":98,
        "alpacaeval-hard":97.9,
        "alpacaeval-length":93.7,
        "donotanswer":60.3,
        "hep-cpp":93.9,
        "hep-go":96.3,
        "hep-java":92.1,
        "hep-js":95.1,
        "hep-python":90.9,
        "hep-rust":94.5,
        "llmbar-adver-GPTInst":43.5,
        "llmbar-adver-GPTOut":61.7,
        "llmbar-adver-manual":43.5,
        "llmbar-adver-neighbor":44.0,
        "llmbar-natural":88,
        "math-prm":60.2,
        "mt-bench-easy":100.0,
        "mt-bench-hard":78.4,
        "mt-bench-med":97.5,
        "refusals-dangerous":81,
        "refusals-offensive":95,
        "xstest-should-refuse":98.1,
        "xstest-should-respond":92.0
    },
    {
        "Model":"hendrydong\/Mistral-RM-for-RAFT-GSHF-v0",
        "Model Type":"Seq. Classifier",
        "Average":81.7,
        "alpacaeval-easy":100,
        "alpacaeval-hard":100.0,
        "alpacaeval-length":95.8,
        "donotanswer":64.0,
        "hep-cpp":93.9,
        "hep-go":92.7,
        "hep-java":95.1,
        "hep-js":92.7,
        "hep-python":92.1,
        "hep-rust":92.7,
        "llmbar-adver-GPTInst":40.2,
        "llmbar-adver-GPTOut":59.6,
        "llmbar-adver-manual":34.8,
        "llmbar-adver-neighbor":46.3,
        "llmbar-natural":91,
        "math-prm":55.5,
        "mt-bench-easy":100.0,
        "mt-bench-hard":81.1,
        "mt-bench-med":95.0,
        "refusals-dangerous":74,
        "refusals-offensive":96,
        "xstest-should-refuse":98.1,
        "xstest-should-respond":88.4
    },
    {
        "Model":"allenai\/tulu-2-dpo-70b",
        "Model Type":"DPO",
        "Average":80.8,
        "alpacaeval-easy":98,
        "alpacaeval-hard":100.0,
        "alpacaeval-length":98.9,
        "donotanswer":70.6,
        "hep-cpp":92.1,
        "hep-go":91.5,
        "hep-java":93.9,
        "hep-js":93.9,
        "hep-python":93.3,
        "hep-rust":86.0,
        "llmbar-adver-GPTInst":34.8,
        "llmbar-adver-GPTOut":51.1,
        "llmbar-adver-manual":63.0,
        "llmbar-adver-neighbor":70.9,
        "llmbar-natural":72,
        "math-prm":56.4,
        "mt-bench-easy":85.7,
        "mt-bench-hard":64.9,
        "mt-bench-med":95.0,
        "refusals-dangerous":82,
        "refusals-offensive":89,
        "xstest-should-refuse":85.7,
        "xstest-should-respond":90.4
    },
    {
        "Model":"mistralai\/Mixtral-8x7B-Instruct-v0.1",
        "Model Type":"DPO",
        "Average":80.8,
        "alpacaeval-easy":95,
        "alpacaeval-hard":90.5,
        "alpacaeval-length":100.0,
        "donotanswer":55.9,
        "hep-cpp":95.7,
        "hep-go":93.3,
        "hep-java":95.1,
        "hep-js":95.7,
        "hep-python":92.1,
        "hep-rust":91.5,
        "llmbar-adver-GPTInst":41.3,
        "llmbar-adver-GPTOut":55.3,
        "llmbar-adver-manual":69.6,
        "llmbar-adver-neighbor":67.9,
        "llmbar-natural":77,
        "math-prm":63.5,
        "mt-bench-easy":92.9,
        "mt-bench-hard":75.7,
        "mt-bench-med":95.0,
        "refusals-dangerous":82,
        "refusals-offensive":86,
        "xstest-should-refuse":76.6,
        "xstest-should-respond":70.0
    },
    {
        "Model":"Ray2333\/reward-model-Mistral-7B-instruct-Unified-Feedback",
        "Model Type":"Seq. Classifier",
        "Average":80.1,
        "alpacaeval-easy":98,
        "alpacaeval-hard":98.9,
        "alpacaeval-length":95.8,
        "donotanswer":61.8,
        "hep-cpp":91.5,
        "hep-go":94.5,
        "hep-java":92.1,
        "hep-js":92.7,
        "hep-python":90.2,
        "hep-rust":91.5,
        "llmbar-adver-GPTInst":29.3,
        "llmbar-adver-GPTOut":57.4,
        "llmbar-adver-manual":30.4,
        "llmbar-adver-neighbor":32.8,
        "llmbar-natural":90,
        "math-prm":55.7,
        "mt-bench-easy":100.0,
        "mt-bench-hard":78.4,
        "mt-bench-med":97.5,
        "refusals-dangerous":82,
        "refusals-offensive":99,
        "xstest-should-refuse":97.4,
        "xstest-should-respond":86.4
    },
    {
        "Model":"HuggingFaceH4\/zephyr-7b-alpha",
        "Model Type":"DPO",
        "Average":78.4,
        "alpacaeval-easy":99,
        "alpacaeval-hard":95.8,
        "alpacaeval-length":78.9,
        "donotanswer":71.3,
        "hep-cpp":93.3,
        "hep-go":92.7,
        "hep-java":91.5,
        "hep-js":93.9,
        "hep-python":90.9,
        "hep-rust":87.8,
        "llmbar-adver-GPTInst":35.9,
        "llmbar-adver-GPTOut":63.8,
        "llmbar-adver-manual":56.5,
        "llmbar-adver-neighbor":66.4,
        "llmbar-natural":76,
        "math-prm":58.6,
        "mt-bench-easy":92.9,
        "mt-bench-hard":83.8,
        "mt-bench-med":92.5,
        "refusals-dangerous":48,
        "refusals-offensive":58,
        "xstest-should-refuse":79.2,
        "xstest-should-respond":96.8
    },
    {
        "Model":"allenai\/tulu-2-dpo-13b",
        "Model Type":"DPO",
        "Average":77.0,
        "alpacaeval-easy":96,
        "alpacaeval-hard":100.0,
        "alpacaeval-length":97.9,
        "donotanswer":66.2,
        "hep-cpp":86.6,
        "hep-go":85.4,
        "hep-java":90.9,
        "hep-js":85.4,
        "hep-python":86.0,
        "hep-rust":83.5,
        "llmbar-adver-GPTInst":25.0,
        "llmbar-adver-GPTOut":51.1,
        "llmbar-adver-manual":47.8,
        "llmbar-adver-neighbor":71.6,
        "llmbar-natural":75,
        "math-prm":60.2,
        "mt-bench-easy":89.3,
        "mt-bench-hard":70.3,
        "mt-bench-med":85.0,
        "refusals-dangerous":65,
        "refusals-offensive":80,
        "xstest-should-refuse":81.2,
        "xstest-should-respond":91.2
    },
    {
        "Model":"NousResearch\/Nous-Hermes-2-Mixtral-8x7B-DPO",
        "Model Type":"DPO",
        "Average":76.8,
        "alpacaeval-easy":98,
        "alpacaeval-hard":96.8,
        "alpacaeval-length":87.4,
        "donotanswer":72.1,
        "hep-cpp":84.1,
        "hep-go":87.2,
        "hep-java":93.9,
        "hep-js":84.1,
        "hep-python":89.6,
        "hep-rust":78.7,
        "llmbar-adver-GPTInst":39.1,
        "llmbar-adver-GPTOut":66.0,
        "llmbar-adver-manual":60.9,
        "llmbar-adver-neighbor":63.4,
        "llmbar-natural":72,
        "math-prm":36.2,
        "mt-bench-easy":75.0,
        "mt-bench-hard":64.9,
        "mt-bench-med":85.0,
        "refusals-dangerous":82,
        "refusals-offensive":84,
        "xstest-should-refuse":79.9,
        "xstest-should-respond":86.4
    },
    {
        "Model":"NousResearch\/Nous-Hermes-2-Mistral-7B-DPO",
        "Model Type":"DPO",
        "Average":76.8,
        "alpacaeval-easy":96,
        "alpacaeval-hard":95.8,
        "alpacaeval-length":83.2,
        "donotanswer":73.5,
        "hep-cpp":79.9,
        "hep-go":79.3,
        "hep-java":76.2,
        "hep-js":75.0,
        "hep-python":68.9,
        "hep-rust":69.5,
        "llmbar-adver-GPTInst":45.7,
        "llmbar-adver-GPTOut":55.3,
        "llmbar-adver-manual":56.5,
        "llmbar-adver-neighbor":55.2,
        "llmbar-natural":80,
        "math-prm":72.7,
        "mt-bench-easy":92.9,
        "mt-bench-hard":75.7,
        "mt-bench-med":95.0,
        "refusals-dangerous":86,
        "refusals-offensive":88,
        "xstest-should-refuse":82.5,
        "xstest-should-respond":83.6
    },
    {
        "Model":"HuggingFaceH4\/zephyr-7b-beta",
        "Model Type":"DPO",
        "Average":76.4,
        "alpacaeval-easy":95,
        "alpacaeval-hard":96.8,
        "alpacaeval-length":94.7,
        "donotanswer":62.5,
        "hep-cpp":90.2,
        "hep-go":94.5,
        "hep-java":94.5,
        "hep-js":93.9,
        "hep-python":93.9,
        "hep-rust":94.5,
        "llmbar-adver-GPTInst":27.2,
        "llmbar-adver-GPTOut":51.1,
        "llmbar-adver-manual":60.9,
        "llmbar-adver-neighbor":70.9,
        "llmbar-natural":83,
        "math-prm":62.2,
        "mt-bench-easy":89.3,
        "mt-bench-hard":83.8,
        "mt-bench-med":97.5,
        "refusals-dangerous":30,
        "refusals-offensive":32,
        "xstest-should-refuse":61.7,
        "xstest-should-respond":97.6
    },
    {
        "Model":"0-hero\/Matter-0.1-7B-boost-DPO-preview",
        "Model Type":"DPO",
        "Average":76.2,
        "alpacaeval-easy":98,
        "alpacaeval-hard":90.5,
        "alpacaeval-length":88.4,
        "donotanswer":59.6,
        "hep-cpp":89.6,
        "hep-go":87.2,
        "hep-java":93.9,
        "hep-js":85.4,
        "hep-python":86.0,
        "hep-rust":84.8,
        "llmbar-adver-GPTInst":40.2,
        "llmbar-adver-GPTOut":57.4,
        "llmbar-adver-manual":52.2,
        "llmbar-adver-neighbor":62.7,
        "llmbar-natural":78,
        "math-prm":80.1,
        "mt-bench-easy":89.3,
        "mt-bench-hard":75.7,
        "mt-bench-med":82.5,
        "refusals-dangerous":63,
        "refusals-offensive":53,
        "xstest-should-refuse":57.8,
        "xstest-should-respond":96.8
    },
    {
        "Model":"stabilityai\/stablelm-zephyr-3b",
        "Model Type":"DPO",
        "Average":75.4,
        "alpacaeval-easy":72,
        "alpacaeval-hard":89.5,
        "alpacaeval-length":95.8,
        "donotanswer":62.5,
        "hep-cpp":80.5,
        "hep-go":86.6,
        "hep-java":93.3,
        "hep-js":82.3,
        "hep-python":83.5,
        "hep-rust":79.9,
        "llmbar-adver-GPTInst":18.5,
        "llmbar-adver-GPTOut":36.2,
        "llmbar-adver-manual":54.3,
        "llmbar-adver-neighbor":81.3,
        "llmbar-natural":74,
        "math-prm":67.1,
        "mt-bench-easy":96.4,
        "mt-bench-hard":86.5,
        "mt-bench-med":85.0,
        "refusals-dangerous":93,
        "refusals-offensive":78,
        "xstest-should-refuse":54.5,
        "xstest-should-respond":83.2
    },
    {
        "Model":"berkeley-nest\/Starling-RM-7B-alpha",
        "Model Type":"Seq. Classifier",
        "Average":74.8,
        "alpacaeval-easy":99,
        "alpacaeval-hard":100.0,
        "alpacaeval-length":97.9,
        "donotanswer":56.6,
        "hep-cpp":75.0,
        "hep-go":84.8,
        "hep-java":84.1,
        "hep-js":84.1,
        "hep-python":78.7,
        "hep-rust":79.9,
        "llmbar-adver-GPTInst":23.9,
        "llmbar-adver-GPTOut":48.9,
        "llmbar-adver-manual":28.3,
        "llmbar-adver-neighbor":31.3,
        "llmbar-natural":80,
        "math-prm":34.9,
        "mt-bench-easy":96.4,
        "mt-bench-hard":75.7,
        "mt-bench-med":92.5,
        "refusals-dangerous":87,
        "refusals-offensive":99,
        "xstest-should-refuse":96.1,
        "xstest-should-respond":85.6
    },
    {
        "Model":"allenai\/tulu-2-dpo-7b",
        "Model Type":"DPO",
        "Average":74.6,
        "alpacaeval-easy":99,
        "alpacaeval-hard":98.9,
        "alpacaeval-length":96.8,
        "donotanswer":55.9,
        "hep-cpp":78.7,
        "hep-go":79.9,
        "hep-java":84.1,
        "hep-js":81.1,
        "hep-python":82.9,
        "hep-rust":73.2,
        "llmbar-adver-GPTInst":25.0,
        "llmbar-adver-GPTOut":40.4,
        "llmbar-adver-manual":52.2,
        "llmbar-adver-neighbor":70.9,
        "llmbar-natural":70,
        "math-prm":63.5,
        "mt-bench-easy":92.9,
        "mt-bench-hard":67.6,
        "mt-bench-med":95.0,
        "refusals-dangerous":70,
        "refusals-offensive":76,
        "xstest-should-refuse":73.4,
        "xstest-should-respond":88.8
    },
    {
        "Model":"Qwen\/Qwen1.5-14B-Chat",
        "Model Type":"DPO",
        "Average":73.9,
        "alpacaeval-easy":64,
        "alpacaeval-hard":32.6,
        "alpacaeval-length":70.5,
        "donotanswer":90.4,
        "hep-cpp":82.9,
        "hep-go":88.4,
        "hep-java":92.1,
        "hep-js":90.9,
        "hep-python":89.0,
        "hep-rust":81.7,
        "llmbar-adver-GPTInst":62.0,
        "llmbar-adver-GPTOut":46.8,
        "llmbar-adver-manual":71.7,
        "llmbar-adver-neighbor":83.6,
        "llmbar-natural":71,
        "math-prm":91.7,
        "mt-bench-easy":60.7,
        "mt-bench-hard":67.6,
        "mt-bench-med":65.0,
        "refusals-dangerous":93,
        "refusals-offensive":83,
        "xstest-should-refuse":80.5,
        "xstest-should-respond":41.6
    },
    {
        "Model":"0-hero\/Matter-0.1-7B-DPO-preview",
        "Model Type":"DPO",
        "Average":73.2,
        "alpacaeval-easy":100,
        "alpacaeval-hard":95.8,
        "alpacaeval-length":84.2,
        "donotanswer":55.9,
        "hep-cpp":87.8,
        "hep-go":91.5,
        "hep-java":89.6,
        "hep-js":90.2,
        "hep-python":87.2,
        "hep-rust":86.0,
        "llmbar-adver-GPTInst":39.1,
        "llmbar-adver-GPTOut":68.1,
        "llmbar-adver-manual":41.3,
        "llmbar-adver-neighbor":57.5,
        "llmbar-natural":75,
        "math-prm":88.4,
        "mt-bench-easy":67.9,
        "mt-bench-hard":64.9,
        "mt-bench-med":75.0,
        "refusals-dangerous":59,
        "refusals-offensive":47,
        "xstest-should-refuse":44.2,
        "xstest-should-respond":88.8
    },
    {
        "Model":"Qwen\/Qwen1.5-72B-Chat",
        "Model Type":"DPO",
        "Average":73.0,
        "alpacaeval-easy":73,
        "alpacaeval-hard":38.9,
        "alpacaeval-length":70.5,
        "donotanswer":83.8,
        "hep-cpp":87.2,
        "hep-go":87.2,
        "hep-java":93.9,
        "hep-js":89.6,
        "hep-python":88.4,
        "hep-rust":83.5,
        "llmbar-adver-GPTInst":45.7,
        "llmbar-adver-GPTOut":51.1,
        "llmbar-adver-manual":78.3,
        "llmbar-adver-neighbor":81.3,
        "llmbar-natural":68,
        "math-prm":82.8,
        "mt-bench-easy":60.7,
        "mt-bench-hard":59.5,
        "mt-bench-med":72.5,
        "refusals-dangerous":91,
        "refusals-offensive":73,
        "xstest-should-refuse":76.0,
        "xstest-should-respond":42.0
    },
    {
        "Model":"Qwen\/Qwen1.5-7B-Chat",
        "Model Type":"DPO",
        "Average":72.8,
        "alpacaeval-easy":50,
        "alpacaeval-hard":32.6,
        "alpacaeval-length":73.7,
        "donotanswer":87.5,
        "hep-cpp":84.1,
        "hep-go":86.0,
        "hep-java":93.9,
        "hep-js":84.1,
        "hep-python":90.2,
        "hep-rust":84.1,
        "llmbar-adver-GPTInst":59.8,
        "llmbar-adver-GPTOut":53.2,
        "llmbar-adver-manual":80.4,
        "llmbar-adver-neighbor":81.3,
        "llmbar-natural":65,
        "math-prm":93.7,
        "mt-bench-easy":57.1,
        "mt-bench-hard":64.9,
        "mt-bench-med":62.5,
        "refusals-dangerous":87,
        "refusals-offensive":81,
        "xstest-should-refuse":82.5,
        "xstest-should-respond":39.2
    },
    {
        "Model":"weqweasdas\/RM-Gemma-7B",
        "Model Type":"Seq. Classifier",
        "Average":71.9,
        "alpacaeval-easy":98,
        "alpacaeval-hard":98.9,
        "alpacaeval-length":93.7,
        "donotanswer":37.5,
        "hep-cpp":96.3,
        "hep-go":94.5,
        "hep-java":97.0,
        "hep-js":92.7,
        "hep-python":92.7,
        "hep-rust":90.9,
        "llmbar-adver-GPTInst":27.2,
        "llmbar-adver-GPTOut":61.7,
        "llmbar-adver-manual":28.3,
        "llmbar-adver-neighbor":39.6,
        "llmbar-natural":82,
        "math-prm":53.2,
        "mt-bench-easy":100.0,
        "mt-bench-hard":67.6,
        "mt-bench-med":95.0,
        "refusals-dangerous":23,
        "refusals-offensive":35,
        "xstest-should-refuse":54.5,
        "xstest-should-respond":94.0
    },
    {
        "Model":"weqweasdas\/RM-Gemma-7B-4096",
        "Model Type":"Seq. Classifier",
        "Average":71.1,
        "alpacaeval-easy":98,
        "alpacaeval-hard":94.7,
        "alpacaeval-length":90.5,
        "donotanswer":32.4,
        "hep-cpp":89.6,
        "hep-go":92.7,
        "hep-java":96.3,
        "hep-js":92.1,
        "hep-python":93.3,
        "hep-rust":89.6,
        "llmbar-adver-GPTInst":22.8,
        "llmbar-adver-GPTOut":55.3,
        "llmbar-adver-manual":34.8,
        "llmbar-adver-neighbor":42.5,
        "llmbar-natural":83,
        "math-prm":57.9,
        "mt-bench-easy":96.4,
        "mt-bench-hard":70.3,
        "mt-bench-med":97.5,
        "refusals-dangerous":19,
        "refusals-offensive":40,
        "xstest-should-refuse":53.9,
        "xstest-should-respond":91.6
    },
    {
        "Model":"openbmb\/UltraRM-13b",
        "Model Type":"Seq. Classifier",
        "Average":69.8,
        "alpacaeval-easy":97,
        "alpacaeval-hard":98.9,
        "alpacaeval-length":90.5,
        "donotanswer":36.0,
        "hep-cpp":78.7,
        "hep-go":79.3,
        "hep-java":80.5,
        "hep-js":78.0,
        "hep-python":78.7,
        "hep-rust":81.7,
        "llmbar-adver-GPTInst":43.5,
        "llmbar-adver-GPTOut":51.1,
        "llmbar-adver-manual":47.8,
        "llmbar-adver-neighbor":42.5,
        "llmbar-natural":82,
        "math-prm":45.4,
        "mt-bench-easy":100.0,
        "mt-bench-hard":75.7,
        "mt-bench-med":100.0,
        "refusals-dangerous":30,
        "refusals-offensive":28,
        "xstest-should-refuse":64.9,
        "xstest-should-respond":94.4
    },
    {
        "Model":"HuggingFaceH4\/zephyr-7b-gemma-v0.1",
        "Model Type":"DPO",
        "Average":69.5,
        "alpacaeval-easy":98,
        "alpacaeval-hard":97.9,
        "alpacaeval-length":93.7,
        "donotanswer":25.7,
        "hep-cpp":79.3,
        "hep-go":81.1,
        "hep-java":81.1,
        "hep-js":78.0,
        "hep-python":86.0,
        "hep-rust":78.0,
        "llmbar-adver-GPTInst":17.4,
        "llmbar-adver-GPTOut":53.2,
        "llmbar-adver-manual":45.7,
        "llmbar-adver-neighbor":44.0,
        "llmbar-natural":74,
        "math-prm":68.7,
        "mt-bench-easy":89.3,
        "mt-bench-hard":83.8,
        "mt-bench-med":95.0,
        "refusals-dangerous":25,
        "refusals-offensive":61,
        "xstest-should-refuse":51.3,
        "xstest-should-respond":92.4
    },
    {
        "Model":"stabilityai\/stablelm-2-zephyr-1_6b",
        "Model Type":"DPO",
        "Average":69.2,
        "alpacaeval-easy":97,
        "alpacaeval-hard":96.8,
        "alpacaeval-length":98.9,
        "donotanswer":41.2,
        "hep-cpp":78.7,
        "hep-go":79.3,
        "hep-java":81.7,
        "hep-js":82.3,
        "hep-python":82.3,
        "hep-rust":75.6,
        "llmbar-adver-GPTInst":12.0,
        "llmbar-adver-GPTOut":46.8,
        "llmbar-adver-manual":37.0,
        "llmbar-adver-neighbor":49.3,
        "llmbar-natural":70,
        "math-prm":55.7,
        "mt-bench-easy":100.0,
        "mt-bench-hard":73.0,
        "mt-bench-med":87.5,
        "refusals-dangerous":48,
        "refusals-offensive":65,
        "xstest-should-refuse":59.1,
        "xstest-should-respond":74.4
    },
    {
        "Model":"allenai\/OLMo-7B-Instruct",
        "Model Type":"DPO",
        "Average":69.0,
        "alpacaeval-easy":90,
        "alpacaeval-hard":92.6,
        "alpacaeval-length":91.6,
        "donotanswer":54.4,
        "hep-cpp":76.2,
        "hep-go":74.4,
        "hep-java":81.1,
        "hep-js":82.9,
        "hep-python":75.6,
        "hep-rust":79.3,
        "llmbar-adver-GPTInst":25.0,
        "llmbar-adver-GPTOut":40.4,
        "llmbar-adver-manual":43.5,
        "llmbar-adver-neighbor":58.2,
        "llmbar-natural":67,
        "math-prm":65.1,
        "mt-bench-easy":85.7,
        "mt-bench-hard":64.9,
        "mt-bench-med":80.0,
        "refusals-dangerous":57,
        "refusals-offensive":68,
        "xstest-should-refuse":57.1,
        "xstest-should-respond":77.2
    },
    {
        "Model":"OpenAssistant\/oasst-rm-2.1-pythia-1.4b-epoch-2.5",
        "Model Type":"Seq. Classifier",
        "Average":65.5,
        "alpacaeval-easy":95,
        "alpacaeval-hard":93.7,
        "alpacaeval-length":78.9,
        "donotanswer":38.2,
        "hep-cpp":56.1,
        "hep-go":61.6,
        "hep-java":68.3,
        "hep-js":65.9,
        "hep-python":59.1,
        "hep-rust":48.8,
        "llmbar-adver-GPTInst":42.4,
        "llmbar-adver-GPTOut":53.2,
        "llmbar-adver-manual":41.3,
        "llmbar-adver-neighbor":33.6,
        "llmbar-natural":67,
        "math-prm":95.1,
        "mt-bench-easy":85.7,
        "mt-bench-hard":73.0,
        "mt-bench-med":85.0,
        "refusals-dangerous":51,
        "refusals-offensive":57,
        "xstest-should-refuse":86.4,
        "xstest-should-respond":69.6
    },
    {
        "Model":"IDEA-CCNL\/Ziya-LLaMA-7B-Reward",
        "Model Type":"Seq. Classifier",
        "Average":65.4,
        "alpacaeval-easy":85,
        "alpacaeval-hard":92.6,
        "alpacaeval-length":84.2,
        "donotanswer":33.8,
        "hep-cpp":76.2,
        "hep-go":81.1,
        "hep-java":76.2,
        "hep-js":73.8,
        "hep-python":79.3,
        "hep-rust":76.8,
        "llmbar-adver-GPTInst":32.6,
        "llmbar-adver-GPTOut":40.4,
        "llmbar-adver-manual":26.1,
        "llmbar-adver-neighbor":36.6,
        "llmbar-natural":77,
        "math-prm":38.3,
        "mt-bench-easy":92.9,
        "mt-bench-hard":62.2,
        "mt-bench-med":80.0,
        "refusals-dangerous":39,
        "refusals-offensive":69,
        "xstest-should-refuse":61.0,
        "xstest-should-respond":90.4
    },
    {
        "Model":"weqweasdas\/RM-Gemma-2B",
        "Model Type":"Seq. Classifier",
        "Average":64.0,
        "alpacaeval-easy":96,
        "alpacaeval-hard":97.9,
        "alpacaeval-length":90.5,
        "donotanswer":27.2,
        "hep-cpp":82.3,
        "hep-go":75.6,
        "hep-java":82.9,
        "hep-js":81.1,
        "hep-python":75.6,
        "hep-rust":78.7,
        "llmbar-adver-GPTInst":15.2,
        "llmbar-adver-GPTOut":40.4,
        "llmbar-adver-manual":21.7,
        "llmbar-adver-neighbor":29.9,
        "llmbar-natural":76,
        "math-prm":73.4,
        "mt-bench-easy":96.4,
        "mt-bench-hard":73.0,
        "mt-bench-med":90.0,
        "refusals-dangerous":7,
        "refusals-offensive":23,
        "xstest-should-refuse":46.8,
        "xstest-should-respond":92.0
    },
    {
        "Model":"OpenAssistant\/oasst-rm-2-pythia-6.9b-epoch-1",
        "Model Type":"Seq. Classifier",
        "Average":61.6,
        "alpacaeval-easy":97,
        "alpacaeval-hard":98.9,
        "alpacaeval-length":91.6,
        "donotanswer":27.9,
        "hep-cpp":72.0,
        "hep-go":72.0,
        "hep-java":72.0,
        "hep-js":72.6,
        "hep-python":73.2,
        "hep-rust":72.6,
        "llmbar-adver-GPTInst":21.7,
        "llmbar-adver-GPTOut":44.7,
        "llmbar-adver-manual":23.9,
        "llmbar-adver-neighbor":20.9,
        "llmbar-natural":70,
        "math-prm":44.7,
        "mt-bench-easy":82.1,
        "mt-bench-hard":54.1,
        "mt-bench-med":75.0,
        "refusals-dangerous":11,
        "refusals-offensive":76,
        "xstest-should-refuse":84.4,
        "xstest-should-respond":59.2
    },
    {
        "Model":"PKU-Alignment\/beaver-7b-v1.0-cost",
        "Model Type":"Seq. Classifier",
        "Average":61.2,
        "alpacaeval-easy":43,
        "alpacaeval-hard":74.7,
        "alpacaeval-length":67.4,
        "donotanswer":76.5,
        "hep-cpp":67.1,
        "hep-go":61.0,
        "hep-java":67.7,
        "hep-js":56.7,
        "hep-python":64.6,
        "hep-rust":61.6,
        "llmbar-adver-GPTInst":41.3,
        "llmbar-adver-GPTOut":59.6,
        "llmbar-adver-manual":28.3,
        "llmbar-adver-neighbor":35.8,
        "llmbar-natural":48,
        "math-prm":46.5,
        "mt-bench-easy":57.1,
        "mt-bench-hard":48.6,
        "mt-bench-med":67.5,
        "refusals-dangerous":99,
        "refusals-offensive":100,
        "xstest-should-refuse":99.4,
        "xstest-should-respond":35.2
    },
    {
        "Model":"Qwen\/Qwen1.5-1.8B-Chat",
        "Model Type":"DPO",
        "Average":60.2,
        "alpacaeval-easy":30,
        "alpacaeval-hard":51.6,
        "alpacaeval-length":89.5,
        "donotanswer":60.3,
        "hep-cpp":62.2,
        "hep-go":68.3,
        "hep-java":76.8,
        "hep-js":76.8,
        "hep-python":68.3,
        "hep-rust":64.6,
        "llmbar-adver-GPTInst":43.5,
        "llmbar-adver-GPTOut":44.7,
        "llmbar-adver-manual":67.4,
        "llmbar-adver-neighbor":74.6,
        "llmbar-natural":63,
        "math-prm":86.4,
        "mt-bench-easy":57.1,
        "mt-bench-hard":54.1,
        "mt-bench-med":52.5,
        "refusals-dangerous":41,
        "refusals-offensive":50,
        "xstest-should-refuse":70.8,
        "xstest-should-respond":30.4
    },
    {
        "Model":"llm-blender\/PairRM-hf",
        "Model Type":"Custom Classifier",
        "Average":59.8,
        "alpacaeval-easy":96,
        "alpacaeval-hard":97.9,
        "alpacaeval-length":75.8,
        "donotanswer":36.0,
        "hep-cpp":59.8,
        "hep-go":68.3,
        "hep-java":66.5,
        "hep-js":61.0,
        "hep-python":65.2,
        "hep-rust":67.1,
        "llmbar-adver-GPTInst":31.5,
        "llmbar-adver-GPTOut":57.4,
        "llmbar-adver-manual":50.0,
        "llmbar-adver-neighbor":42.5,
        "llmbar-natural":78,
        "math-prm":33.3,
        "mt-bench-easy":92.9,
        "mt-bench-hard":64.9,
        "mt-bench-med":90.0,
        "refusals-dangerous":9,
        "refusals-offensive":1,
        "xstest-should-refuse":36.4,
        "xstest-should-respond":95.2
    },
    {
        "Model":"OpenAssistant\/reward-model-deberta-v3-large-v2",
        "Model Type":"Seq. Classifier",
        "Average":59.6,
        "alpacaeval-easy":99,
        "alpacaeval-hard":96.8,
        "alpacaeval-length":41.1,
        "donotanswer":40.4,
        "hep-cpp":0.6,
        "hep-go":82.3,
        "hep-java":50.6,
        "hep-js":90.9,
        "hep-python":100.0,
        "hep-rust":57.9,
        "llmbar-adver-GPTInst":7.6,
        "llmbar-adver-GPTOut":0.0,
        "llmbar-adver-manual":0.0,
        "llmbar-adver-neighbor":5.2,
        "llmbar-natural":53,
        "math-prm":4.3,
        "mt-bench-easy":100.0,
        "mt-bench-hard":100.0,
        "mt-bench-med":100.0,
        "refusals-dangerous":82,
        "refusals-offensive":99,
        "xstest-should-refuse":76.6,
        "xstest-should-respond":83.2
    },
    {
        "Model":"ContextualAI\/archangel_sft-kto_llama30b",
        "Model Type":"DPO",
        "Average":59.3,
        "alpacaeval-easy":93,
        "alpacaeval-hard":88.4,
        "alpacaeval-length":76.8,
        "donotanswer":38.2,
        "hep-cpp":54.9,
        "hep-go":61.6,
        "hep-java":61.0,
        "hep-js":57.3,
        "hep-python":72.0,
        "hep-rust":56.7,
        "llmbar-adver-GPTInst":19.6,
        "llmbar-adver-GPTOut":42.6,
        "llmbar-adver-manual":37.0,
        "llmbar-adver-neighbor":39.6,
        "llmbar-natural":57,
        "math-prm":40.9,
        "mt-bench-easy":82.1,
        "mt-bench-hard":54.1,
        "mt-bench-med":72.5,
        "refusals-dangerous":48,
        "refusals-offensive":77,
        "xstest-should-refuse":65.6,
        "xstest-should-respond":68.0
    },
    {
        "Model":"ContextualAI\/archangel_sft-dpo_llama30b",
        "Model Type":"DPO",
        "Average":58.1,
        "alpacaeval-easy":78,
        "alpacaeval-hard":74.7,
        "alpacaeval-length":61.1,
        "donotanswer":64.0,
        "hep-cpp":56.7,
        "hep-go":64.6,
        "hep-java":64.0,
        "hep-js":57.9,
        "hep-python":65.2,
        "hep-rust":62.2,
        "llmbar-adver-GPTInst":34.8,
        "llmbar-adver-GPTOut":42.6,
        "llmbar-adver-manual":45.7,
        "llmbar-adver-neighbor":45.5,
        "llmbar-natural":55,
        "math-prm":33.1,
        "mt-bench-easy":67.9,
        "mt-bench-hard":40.5,
        "mt-bench-med":55.0,
        "refusals-dangerous":82,
        "refusals-offensive":59,
        "xstest-should-refuse":81.8,
        "xstest-should-respond":44.4
    },
    {
        "Model":"ContextualAI\/archangel_sft-kto_llama13b",
        "Model Type":"DPO",
        "Average":56.0,
        "alpacaeval-easy":96,
        "alpacaeval-hard":87.4,
        "alpacaeval-length":76.8,
        "donotanswer":19.9,
        "hep-cpp":54.9,
        "hep-go":53.7,
        "hep-java":61.6,
        "hep-js":62.2,
        "hep-python":69.5,
        "hep-rust":56.1,
        "llmbar-adver-GPTInst":22.8,
        "llmbar-adver-GPTOut":51.1,
        "llmbar-adver-manual":26.1,
        "llmbar-adver-neighbor":20.1,
        "llmbar-natural":63,
        "math-prm":81.9,
        "mt-bench-easy":71.4,
        "mt-bench-hard":67.6,
        "mt-bench-med":72.5,
        "refusals-dangerous":21,
        "refusals-offensive":38,
        "xstest-should-refuse":28.6,
        "xstest-should-respond":85.6
    },
    {
        "Model":"Qwen\/Qwen1.5-4B-Chat",
        "Model Type":"DPO",
        "Average":55.7,
        "alpacaeval-easy":8,
        "alpacaeval-hard":35.8,
        "alpacaeval-length":71.6,
        "donotanswer":61.0,
        "hep-cpp":47.6,
        "hep-go":51.8,
        "hep-java":62.2,
        "hep-js":67.7,
        "hep-python":46.3,
        "hep-rust":64.0,
        "llmbar-adver-GPTInst":67.4,
        "llmbar-adver-GPTOut":42.6,
        "llmbar-adver-manual":63.0,
        "llmbar-adver-neighbor":75.4,
        "llmbar-natural":55,
        "math-prm":77.2,
        "mt-bench-easy":53.6,
        "mt-bench-hard":51.4,
        "mt-bench-med":35.0,
        "refusals-dangerous":63,
        "refusals-offensive":75,
        "xstest-should-refuse":76.6,
        "xstest-should-respond":29.2
    },
    {
        "Model":"ContextualAI\/archangel_sft-dpo_llama13b",
        "Model Type":"DPO",
        "Average":55.2,
        "alpacaeval-easy":80,
        "alpacaeval-hard":69.5,
        "alpacaeval-length":62.1,
        "donotanswer":33.8,
        "hep-cpp":57.9,
        "hep-go":55.5,
        "hep-java":62.8,
        "hep-js":59.8,
        "hep-python":57.3,
        "hep-rust":53.7,
        "llmbar-adver-GPTInst":43.5,
        "llmbar-adver-GPTOut":38.3,
        "llmbar-adver-manual":30.4,
        "llmbar-adver-neighbor":38.8,
        "llmbar-natural":52,
        "math-prm":30.2,
        "mt-bench-easy":78.6,
        "mt-bench-hard":54.1,
        "mt-bench-med":70.0,
        "refusals-dangerous":51,
        "refusals-offensive":82,
        "xstest-should-refuse":32.5,
        "xstest-should-respond":75.6
    },
    {
        "Model":"Qwen\/Qwen1.5-0.5B-Chat",
        "Model Type":"DPO",
        "Average":54.1,
        "alpacaeval-easy":9,
        "alpacaeval-hard":25.3,
        "alpacaeval-length":65.3,
        "donotanswer":58.1,
        "hep-cpp":53.0,
        "hep-go":47.6,
        "hep-java":49.4,
        "hep-js":46.3,
        "hep-python":47.6,
        "hep-rust":50.0,
        "llmbar-adver-GPTInst":65.2,
        "llmbar-adver-GPTOut":48.9,
        "llmbar-adver-manual":60.9,
        "llmbar-adver-neighbor":75.4,
        "llmbar-natural":58,
        "math-prm":70.7,
        "mt-bench-easy":57.1,
        "mt-bench-hard":45.9,
        "mt-bench-med":40.0,
        "refusals-dangerous":76,
        "refusals-offensive":91,
        "xstest-should-refuse":87.0,
        "xstest-should-respond":16.8
    },
    {
        "Model":"ContextualAI\/archangel_sft-kto_pythia6-9b",
        "Model Type":"DPO",
        "Average":52.5,
        "alpacaeval-easy":88,
        "alpacaeval-hard":90.5,
        "alpacaeval-length":64.2,
        "donotanswer":27.2,
        "hep-cpp":46.3,
        "hep-go":50.6,
        "hep-java":50.0,
        "hep-js":55.5,
        "hep-python":52.4,
        "hep-rust":50.0,
        "llmbar-adver-GPTInst":26.1,
        "llmbar-adver-GPTOut":57.4,
        "llmbar-adver-manual":32.6,
        "llmbar-adver-neighbor":22.4,
        "llmbar-natural":51,
        "math-prm":57.5,
        "mt-bench-easy":57.1,
        "mt-bench-hard":48.6,
        "mt-bench-med":67.5,
        "refusals-dangerous":30,
        "refusals-offensive":56,
        "xstest-should-refuse":42.9,
        "xstest-should-respond":83.2
    },
    {
        "Model":"ContextualAI\/archangel_sft-kto_pythia1-4b",
        "Model Type":"DPO",
        "Average":52.1,
        "alpacaeval-easy":79,
        "alpacaeval-hard":75.8,
        "alpacaeval-length":52.6,
        "donotanswer":22.8,
        "hep-cpp":49.4,
        "hep-go":53.0,
        "hep-java":49.4,
        "hep-js":53.7,
        "hep-python":51.2,
        "hep-rust":51.2,
        "llmbar-adver-GPTInst":33.7,
        "llmbar-adver-GPTOut":51.1,
        "llmbar-adver-manual":30.4,
        "llmbar-adver-neighbor":23.1,
        "llmbar-natural":52,
        "math-prm":77.6,
        "mt-bench-easy":57.1,
        "mt-bench-hard":56.8,
        "mt-bench-med":70.0,
        "refusals-dangerous":39,
        "refusals-offensive":53,
        "xstest-should-refuse":27.3,
        "xstest-should-respond":89.6
    },
    {
        "Model":"ContextualAI\/archangel_sft-dpo_llama7b",
        "Model Type":"DPO",
        "Average":51.9,
        "alpacaeval-easy":65,
        "alpacaeval-hard":66.3,
        "alpacaeval-length":48.4,
        "donotanswer":34.6,
        "hep-cpp":61.0,
        "hep-go":61.6,
        "hep-java":58.5,
        "hep-js":58.5,
        "hep-python":65.2,
        "hep-rust":50.6,
        "llmbar-adver-GPTInst":39.1,
        "llmbar-adver-GPTOut":53.2,
        "llmbar-adver-manual":32.6,
        "llmbar-adver-neighbor":36.6,
        "llmbar-natural":53,
        "math-prm":53.9,
        "mt-bench-easy":35.7,
        "mt-bench-hard":67.6,
        "mt-bench-med":57.5,
        "refusals-dangerous":34,
        "refusals-offensive":38,
        "xstest-should-refuse":41.6,
        "xstest-should-respond":80.8
    },
    {
        "Model":"ContextualAI\/archangel_sft-kto_pythia2-8b",
        "Model Type":"DPO",
        "Average":50.8,
        "alpacaeval-easy":92,
        "alpacaeval-hard":80.0,
        "alpacaeval-length":55.8,
        "donotanswer":28.7,
        "hep-cpp":43.3,
        "hep-go":48.2,
        "hep-java":45.1,
        "hep-js":52.4,
        "hep-python":49.4,
        "hep-rust":52.4,
        "llmbar-adver-GPTInst":28.3,
        "llmbar-adver-GPTOut":51.1,
        "llmbar-adver-manual":21.7,
        "llmbar-adver-neighbor":22.4,
        "llmbar-natural":48,
        "math-prm":75.8,
        "mt-bench-easy":67.9,
        "mt-bench-hard":48.6,
        "mt-bench-med":77.5,
        "refusals-dangerous":26,
        "refusals-offensive":40,
        "xstest-should-refuse":40.3,
        "xstest-should-respond":73.6
    },
    {
        "Model":"ContextualAI\/archangel_sft-dpo_pythia2-8b",
        "Model Type":"DPO",
        "Average":50.8,
        "alpacaeval-easy":96,
        "alpacaeval-hard":92.6,
        "alpacaeval-length":58.9,
        "donotanswer":24.3,
        "hep-cpp":50.0,
        "hep-go":52.4,
        "hep-java":51.8,
        "hep-js":53.7,
        "hep-python":50.0,
        "hep-rust":54.9,
        "llmbar-adver-GPTInst":23.9,
        "llmbar-adver-GPTOut":42.6,
        "llmbar-adver-manual":19.6,
        "llmbar-adver-neighbor":18.7,
        "llmbar-natural":56,
        "math-prm":50.6,
        "mt-bench-easy":67.9,
        "mt-bench-hard":56.8,
        "mt-bench-med":75.0,
        "refusals-dangerous":20,
        "refusals-offensive":45,
        "xstest-should-refuse":37.7,
        "xstest-should-respond":70.0
    },
    {
        "Model":"ContextualAI\/archangel_sft-kto_llama7b",
        "Model Type":"DPO",
        "Average":50.3,
        "alpacaeval-easy":60,
        "alpacaeval-hard":57.9,
        "alpacaeval-length":51.6,
        "donotanswer":23.5,
        "hep-cpp":57.9,
        "hep-go":63.4,
        "hep-java":59.1,
        "hep-js":59.8,
        "hep-python":59.1,
        "hep-rust":59.8,
        "llmbar-adver-GPTInst":40.2,
        "llmbar-adver-GPTOut":42.6,
        "llmbar-adver-manual":32.6,
        "llmbar-adver-neighbor":41.0,
        "llmbar-natural":53,
        "math-prm":79.0,
        "mt-bench-easy":50.0,
        "mt-bench-hard":51.4,
        "mt-bench-med":55.0,
        "refusals-dangerous":24,
        "refusals-offensive":22,
        "xstest-should-refuse":26.6,
        "xstest-should-respond":87.6
    },
    {
        "Model":"random",
        "Model Type":"",
        "Average":50.0,
        "alpacaeval-easy":50,
        "alpacaeval-hard":50.0,
        "alpacaeval-length":50.0,
        "donotanswer":50.0,
        "hep-cpp":50.0,
        "hep-go":50.0,
        "hep-java":50.0,
        "hep-js":50.0,
        "hep-python":50.0,
        "hep-rust":50.0,
        "llmbar-adver-GPTInst":50.0,
        "llmbar-adver-GPTOut":50.0,
        "llmbar-adver-manual":50.0,
        "llmbar-adver-neighbor":50.0,
        "llmbar-natural":50,
        "math-prm":50.0,
        "mt-bench-easy":50.0,
        "mt-bench-hard":50.0,
        "mt-bench-med":50.0,
        "refusals-dangerous":50,
        "refusals-offensive":50,
        "xstest-should-refuse":50.0,
        "xstest-should-respond":50.0
    },
    {
        "Model":"ContextualAI\/archangel_sft-dpo_pythia6-9b",
        "Model Type":"DPO",
        "Average":49.5,
        "alpacaeval-easy":89,
        "alpacaeval-hard":87.4,
        "alpacaeval-length":58.9,
        "donotanswer":25.7,
        "hep-cpp":46.3,
        "hep-go":45.7,
        "hep-java":50.0,
        "hep-js":46.3,
        "hep-python":51.8,
        "hep-rust":48.8,
        "llmbar-adver-GPTInst":26.1,
        "llmbar-adver-GPTOut":51.1,
        "llmbar-adver-manual":37.0,
        "llmbar-adver-neighbor":21.6,
        "llmbar-natural":49,
        "math-prm":48.8,
        "mt-bench-easy":57.1,
        "mt-bench-hard":35.1,
        "mt-bench-med":60.0,
        "refusals-dangerous":29,
        "refusals-offensive":52,
        "xstest-should-refuse":38.3,
        "xstest-should-respond":83.2
    },
    {
        "Model":"ContextualAI\/archangel_sft-dpo_pythia12-0b",
        "Model Type":"DPO",
        "Average":49.3,
        "alpacaeval-easy":71,
        "alpacaeval-hard":70.5,
        "alpacaeval-length":62.1,
        "donotanswer":41.9,
        "hep-cpp":49.4,
        "hep-go":40.9,
        "hep-java":47.6,
        "hep-js":42.7,
        "hep-python":42.1,
        "hep-rust":43.3,
        "llmbar-adver-GPTInst":26.1,
        "llmbar-adver-GPTOut":48.9,
        "llmbar-adver-manual":34.8,
        "llmbar-adver-neighbor":29.1,
        "llmbar-natural":46,
        "math-prm":38.5,
        "mt-bench-easy":60.7,
        "mt-bench-hard":48.6,
        "mt-bench-med":62.5,
        "refusals-dangerous":47,
        "refusals-offensive":70,
        "xstest-should-refuse":48.7,
        "xstest-should-respond":61.2
    },
    {
        "Model":"weqweasdas\/hh_rlhf_rm_open_llama_3b",
        "Model Type":"Seq. Classifier",
        "Average":49.0,
        "alpacaeval-easy":95,
        "alpacaeval-hard":96.8,
        "alpacaeval-length":64.2,
        "donotanswer":19.9,
        "hep-cpp":57.3,
        "hep-go":50.0,
        "hep-java":57.3,
        "hep-js":56.1,
        "hep-python":50.6,
        "hep-rust":57.9,
        "llmbar-adver-GPTInst":20.7,
        "llmbar-adver-GPTOut":44.7,
        "llmbar-adver-manual":21.7,
        "llmbar-adver-neighbor":27.6,
        "llmbar-natural":62,
        "math-prm":10.7,
        "mt-bench-easy":64.3,
        "mt-bench-hard":56.8,
        "mt-bench-med":67.5,
        "refusals-dangerous":6,
        "refusals-offensive":32,
        "xstest-should-refuse":29.2,
        "xstest-should-respond":78.8
    },
    {
        "Model":"ContextualAI\/archangel_sft-dpo_pythia1-4b",
        "Model Type":"DPO",
        "Average":48.9,
        "alpacaeval-easy":73,
        "alpacaeval-hard":75.8,
        "alpacaeval-length":49.5,
        "donotanswer":19.9,
        "hep-cpp":47.0,
        "hep-go":48.8,
        "hep-java":48.2,
        "hep-js":53.0,
        "hep-python":49.4,
        "hep-rust":53.0,
        "llmbar-adver-GPTInst":34.8,
        "llmbar-adver-GPTOut":51.1,
        "llmbar-adver-manual":30.4,
        "llmbar-adver-neighbor":24.6,
        "llmbar-natural":50,
        "math-prm":63.5,
        "mt-bench-easy":35.7,
        "mt-bench-hard":45.9,
        "mt-bench-med":67.5,
        "refusals-dangerous":32,
        "refusals-offensive":53,
        "xstest-should-refuse":35.1,
        "xstest-should-respond":82.8
    },
    {
        "Model":"stanfordnlp\/SteamSHP-flan-t5-xl",
        "Model Type":"Custom Classifier",
        "Average":48.8,
        "alpacaeval-easy":93,
        "alpacaeval-hard":98.9,
        "alpacaeval-length":69.5,
        "donotanswer":16.9,
        "hep-cpp":50.0,
        "hep-go":57.3,
        "hep-java":52.4,
        "hep-js":52.4,
        "hep-python":55.5,
        "hep-rust":53.7,
        "llmbar-adver-GPTInst":27.2,
        "llmbar-adver-GPTOut":36.2,
        "llmbar-adver-manual":28.3,
        "llmbar-adver-neighbor":21.6,
        "llmbar-natural":65,
        "math-prm":23.3,
        "mt-bench-easy":78.6,
        "mt-bench-hard":51.4,
        "mt-bench-med":77.5,
        "refusals-dangerous":3,
        "refusals-offensive":3,
        "xstest-should-refuse":20.1,
        "xstest-should-respond":88.0
    },
    {
        "Model":"ContextualAI\/archangel_sft-kto_pythia12-0b",
        "Model Type":"DPO",
        "Average":48.6,
        "alpacaeval-easy":79,
        "alpacaeval-hard":82.1,
        "alpacaeval-length":69.5,
        "donotanswer":30.1,
        "hep-cpp":43.9,
        "hep-go":47.6,
        "hep-java":46.3,
        "hep-js":39.0,
        "hep-python":51.8,
        "hep-rust":38.4,
        "llmbar-adver-GPTInst":27.2,
        "llmbar-adver-GPTOut":40.4,
        "llmbar-adver-manual":30.4,
        "llmbar-adver-neighbor":22.4,
        "llmbar-natural":60,
        "math-prm":38.0,
        "mt-bench-easy":67.9,
        "mt-bench-hard":45.9,
        "mt-bench-med":65.0,
        "refusals-dangerous":28,
        "refusals-offensive":58,
        "xstest-should-refuse":41.6,
        "xstest-should-respond":64.4
    },
    {
        "Model":"PKU-Alignment\/beaver-7b-v1.0-reward",
        "Model Type":"Seq. Classifier",
        "Average":47.5,
        "alpacaeval-easy":98,
        "alpacaeval-hard":100.0,
        "alpacaeval-length":63.2,
        "donotanswer":19.1,
        "hep-cpp":56.7,
        "hep-go":61.0,
        "hep-java":60.4,
        "hep-js":54.3,
        "hep-python":63.4,
        "hep-rust":67.1,
        "llmbar-adver-GPTInst":18.5,
        "llmbar-adver-GPTOut":36.2,
        "llmbar-adver-manual":19.6,
        "llmbar-adver-neighbor":10.4,
        "llmbar-natural":53,
        "math-prm":8.7,
        "mt-bench-easy":67.9,
        "mt-bench-hard":56.8,
        "mt-bench-med":52.5,
        "refusals-dangerous":3,
        "refusals-offensive":28,
        "xstest-should-refuse":15.6,
        "xstest-should-respond":78.8
    },
    {
        "Model":"stanfordnlp\/SteamSHP-flan-t5-large",
        "Model Type":"Custom Classifier",
        "Average":46.9,
        "alpacaeval-easy":94,
        "alpacaeval-hard":97.9,
        "alpacaeval-length":72.6,
        "donotanswer":12.5,
        "hep-cpp":54.9,
        "hep-go":43.9,
        "hep-java":47.0,
        "hep-js":50.6,
        "hep-python":45.1,
        "hep-rust":51.8,
        "llmbar-adver-GPTInst":19.6,
        "llmbar-adver-GPTOut":42.6,
        "llmbar-adver-manual":26.1,
        "llmbar-adver-neighbor":17.9,
        "llmbar-natural":56,
        "math-prm":22.4,
        "mt-bench-easy":75.0,
        "mt-bench-hard":56.8,
        "mt-bench-med":75.0,
        "refusals-dangerous":8,
        "refusals-offensive":2,
        "xstest-should-refuse":17.5,
        "xstest-should-respond":89.2
    }
]