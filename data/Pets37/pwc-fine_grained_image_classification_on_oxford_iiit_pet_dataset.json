[
    {
        "table_id":350,
        "row_id":112957,
        "rank":1,
        "Model":"OmniVec",
        "mlmodel":{

        },
        "method_short":"OmniVec",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-07",
        "metrics":{
            "Accuracy":"99.2",
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "raw_metrics":{
            "Accuracy":99.2,
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1319233,
            "title":"OmniVec: Learning robust representations with cross modal sharing",
            "url":"\/paper\/omnivec-learning-robust-representations-with",
            "published":"2023-11-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivec-learning-robust-representations-with\/review\/?hl=112957"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":350,
        "row_id":101350,
        "rank":2,
        "Model":"DINOv2 (ViT-g\/14, frozen model, linear eval)",
        "mlmodel":{

        },
        "method_short":"DINOv2 ",
        "method_details":"ViT-g\/14, frozen model, linear eval",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-14",
        "metrics":{
            "Accuracy":"96.7",
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "raw_metrics":{
            "Accuracy":96.7,
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1191620,
            "title":"DINOv2: Learning Robust Visual Features without Supervision",
            "url":"\/paper\/dinov2-learning-robust-visual-features",
            "published":"2023-04-14T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":350,
        "row_id":38962,
        "rank":3,
        "Model":"ViT  R26 + S\/32 ( Augmented)",
        "mlmodel":{

        },
        "method_short":"ViT  R26 + S\/32 ",
        "method_details":" Augmented",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-08-28",
        "metrics":{
            "Accuracy":"96.28",
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "raw_metrics":{
            "Accuracy":96.28,
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":860798,
            "title":"Towards Fine-grained Image Classification with Generative Adversarial Networks and Facial Landmark Detection",
            "url":"\/paper\/towards-fine-grained-image-classification",
            "published":"2021-08-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/towards-fine-grained-image-classification\/review\/?hl=38962"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":350,
        "row_id":25442,
        "rank":4,
        "Model":"ALIGN",
        "mlmodel":{

        },
        "method_short":"ALIGN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Accuracy":"96.19%",
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "raw_metrics":{
            "Accuracy":96.19,
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":744362,
            "title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision",
            "url":"\/paper\/scaling-up-visual-and-vision-language",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-up-visual-and-vision-language\/review\/?hl=25442"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":350,
        "row_id":11958,
        "rank":5,
        "Model":"EfficientNet-B7",
        "mlmodel":{

        },
        "method_short":"EfficientNet-B7",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-28",
        "metrics":{
            "Accuracy":"95.4%",
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "raw_metrics":{
            "Accuracy":95.4,
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":117456,
            "title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "url":"\/paper\/efficientnet-rethinking-model-scaling-for",
            "published":"2019-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientnet-rethinking-model-scaling-for\/review\/?hl=11958"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":350,
        "row_id":70814,
        "rank":6,
        "Model":"\u00b52Net (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"\u00b52Net ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-25",
        "metrics":{
            "Accuracy":"95.3%",
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "raw_metrics":{
            "Accuracy":95.3,
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1015923,
            "title":"An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems",
            "url":"\/paper\/an-evolutionary-approach-to-dynamic",
            "published":"2022-05-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-evolutionary-approach-to-dynamic\/review\/?hl=70814"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":350,
        "row_id":97722,
        "rank":7,
        "Model":"IELT",
        "mlmodel":{

        },
        "method_short":"IELT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-13",
        "metrics":{
            "Accuracy":"95.28%",
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "raw_metrics":{
            "Accuracy":95.28,
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1159604,
            "title":"Fine-Grained Visual Classification via Internal Ensemble Learning Transformer",
            "url":"\/paper\/fine-grained-visual-classification-via-2",
            "published":"2023-02-13T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":350,
        "row_id":56710,
        "rank":8,
        "Model":"Bamboo (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"Bamboo ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-15",
        "metrics":{
            "Accuracy":"95.1%",
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "raw_metrics":{
            "Accuracy":95.1,
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":977452,
            "title":"Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy",
            "url":"\/paper\/bamboo-building-mega-scale-vision-dataset",
            "published":"2022-03-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bamboo-building-mega-scale-vision-dataset\/review\/?hl=56710"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":350,
        "row_id":27419,
        "rank":9,
        "Model":"TNT-B",
        "mlmodel":{

        },
        "method_short":"TNT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-27",
        "metrics":{
            "Accuracy":"95.0%",
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":"65.6M"
        },
        "raw_metrics":{
            "Accuracy":95.0,
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":65600000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":749686,
            "title":"Transformer in Transformer",
            "url":"\/paper\/transformer-in-transformer",
            "published":"2021-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transformer-in-transformer\/review\/?hl=27419"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":350,
        "row_id":37560,
        "rank":10,
        "Model":"AutoFormer-S | 384",
        "mlmodel":{

        },
        "method_short":"AutoFormer-S | 384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-01",
        "metrics":{
            "Accuracy":"94.9%",
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "raw_metrics":{
            "Accuracy":94.9,
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":828740,
            "title":"AutoFormer: Searching Transformers for Visual Recognition",
            "url":"\/paper\/autoformer-searching-transformers-for-visual",
            "published":"2021-07-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/autoformer-searching-transformers-for-visual\/review\/?hl=37560"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":350,
        "row_id":5983,
        "rank":11,
        "Model":"FixSENet-154",
        "mlmodel":{

        },
        "method_short":"FixSENet-154",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-14",
        "metrics":{
            "Accuracy":"94.8%",
            "Top-1 Error Rate":"5.2%",
            "FLOPS":null,
            "PARAMS":null
        },
        "raw_metrics":{
            "Accuracy":94.8,
            "Top-1 Error Rate":5.2,
            "FLOPS":null,
            "PARAMS":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":142997,
            "title":"Fixing the train-test resolution discrepancy",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy",
            "published":"2019-06-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy\/review\/?hl=5983"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":350,
        "row_id":16583,
        "rank":12,
        "Model":"NAT-M4",
        "mlmodel":{

        },
        "method_short":"NAT-M4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-12",
        "metrics":{
            "Accuracy":"94.3%",
            "Top-1 Error Rate":"5.7%",
            "FLOPS":"744M",
            "PARAMS":"8.5M"
        },
        "raw_metrics":{
            "Accuracy":94.3,
            "Top-1 Error Rate":5.7,
            "FLOPS":744000000.0,
            "PARAMS":8500000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":194717,
            "title":"Neural Architecture Transfer",
            "url":"\/paper\/neural-architecture-transfer",
            "published":"2020-05-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/neural-architecture-transfer\/review\/?hl=16583"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":350,
        "row_id":16584,
        "rank":13,
        "Model":"NAT-M3",
        "mlmodel":{

        },
        "method_short":"NAT-M3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-12",
        "metrics":{
            "Accuracy":"94.1%",
            "Top-1 Error Rate":"5.9%",
            "FLOPS":"471M",
            "PARAMS":"5.7M"
        },
        "raw_metrics":{
            "Accuracy":94.1,
            "Top-1 Error Rate":5.9,
            "FLOPS":471000000.0,
            "PARAMS":5700000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":194717,
            "title":"Neural Architecture Transfer",
            "url":"\/paper\/neural-architecture-transfer",
            "published":"2020-05-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/neural-architecture-transfer\/review\/?hl=16584"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":350,
        "row_id":16585,
        "rank":14,
        "Model":"NAT-M2",
        "mlmodel":{

        },
        "method_short":"NAT-M2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-12",
        "metrics":{
            "Accuracy":"93.5%",
            "Top-1 Error Rate":"6.5%",
            "FLOPS":"306M",
            "PARAMS":"5.5M"
        },
        "raw_metrics":{
            "Accuracy":93.5,
            "Top-1 Error Rate":6.5,
            "FLOPS":306000000.0,
            "PARAMS":5500000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":194717,
            "title":"Neural Architecture Transfer",
            "url":"\/paper\/neural-architecture-transfer",
            "published":"2020-05-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/neural-architecture-transfer\/review\/?hl=16585"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":350,
        "row_id":2507,
        "rank":15,
        "Model":"AutoAugment",
        "mlmodel":{

        },
        "method_short":"AutoAugment",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-05-24",
        "metrics":{
            "Accuracy":"88.98%",
            "Top-1 Error Rate":"11.02%",
            "FLOPS":null,
            "PARAMS":null
        },
        "raw_metrics":{
            "Accuracy":88.98,
            "Top-1 Error Rate":11.02,
            "FLOPS":null,
            "PARAMS":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":2738,
            "title":"AutoAugment: Learning Augmentation Policies from Data",
            "url":"\/paper\/autoaugment-learning-augmentation-policies",
            "published":"2018-05-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/autoaugment-learning-augmentation-policies\/review\/?hl=2507"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":350,
        "row_id":47985,
        "rank":16,
        "Model":"SEER (RegNet10B)",
        "mlmodel":{

        },
        "method_short":"SEER ",
        "method_details":"RegNet10B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-16",
        "metrics":{
            "Accuracy":"85.3%",
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "raw_metrics":{
            "Accuracy":85.3,
            "Top-1 Error Rate":null,
            "FLOPS":null,
            "PARAMS":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":963673,
            "title":"Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision",
            "url":"\/paper\/vision-models-are-more-robust-and-fair-when",
            "published":"2022-02-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-models-are-more-robust-and-fair-when\/review\/?hl=47985"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]