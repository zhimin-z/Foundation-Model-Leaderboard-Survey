[
    {
        "Model Name":"Otter-Image",
        "Elo Rating":1047.5,
        "Affiliation":"NTU",
        "Description":"a LVLM with in-context instruction tuning."
    },
    {
        "Model Name":"LLaMA-Adapter V2",
        "Elo Rating":1038.7,
        "Affiliation":"Shanghai AI Lab",
        "Description":"a LVLM with parameter-efficient instruction tuning."
    },
    {
        "Model Name":"MiniGPT-4",
        "Elo Rating":1009.9,
        "Affiliation":"KAUST",
        "Description":"a LVLM with only the FC layer tuned on 3.5K intruction data."
    },
    {
        "Model Name":"LLaVA",
        "Elo Rating":1002.8,
        "Affiliation":"Wisconsin-Madison",
        "Description":"a LVLM with the whole LLM trained on 158K visual instruction data."
    },
    {
        "Model Name":"VPGTrans",
        "Elo Rating":999.6,
        "Affiliation":"NUS",
        "Description":"a LVLM equips LLaMA with a visual model by transfer learning."
    },
    {
        "Model Name":"InstructBLIP",
        "Elo Rating":995.5,
        "Affiliation":"Salesforce",
        "Description":"a LVLM with the Q-Former trained on many instruction QA datasets."
    },
    {
        "Model Name":"mPLUG-Owl",
        "Elo Rating":985.6,
        "Affiliation":"DAMO Academy",
        "Description":"a LVLM with LLM instructionally tuned using LoRA technique."
    },
    {
        "Model Name":"Otter",
        "Elo Rating":972.1,
        "Affiliation":"NTU",
        "Description":"a LVLM with 1.3B adaption parameters tuned on 158K instruction data."
    },
    {
        "Model Name":"BLIP-2",
        "Elo Rating":948.8,
        "Affiliation":"Salesforce",
        "Description":"a LVLM without being tuned with instruction data."
    }
]