[
    {
        "table_id":22778,
        "row_id":96298,
        "rank":1,
        "Model":"BLIP-2 ViT-G (fine-tuned)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G ",
        "method_details":"fine-tuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "recall@1":"68.3",
            "recall@5":"87.7",
            "Recall@10":"92.6",
            "QPS":null
        },
        "raw_metrics":{
            "recall@1":68.3,
            "recall@5":87.7,
            "Recall@10":92.6,
            "QPS":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96298"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":22778,
        "row_id":96259,
        "rank":2,
        "Model":"VisualSparta",
        "mlmodel":{

        },
        "method_short":"VisualSparta",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-01",
        "metrics":{
            "recall@1":"68.2",
            "recall@5":"91.8",
            "Recall@10":"96.3",
            "QPS":"451.4"
        },
        "raw_metrics":{
            "recall@1":68.2,
            "recall@5":91.8,
            "Recall@10":96.3,
            "QPS":451.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":733201,
            "title":"VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words",
            "url":"\/paper\/visualsparta-sparse-transformer-fragment",
            "published":"2021-01-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visualsparta-sparse-transformer-fragment\/review\/?hl=96259"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":22778,
        "row_id":96297,
        "rank":3,
        "Model":"BLIP-2 ViT-L (fine-tuned)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-L ",
        "method_details":"fine-tuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "recall@1":"66.3",
            "recall@5":"86.5",
            "Recall@10":"91.8",
            "QPS":null
        },
        "raw_metrics":{
            "recall@1":66.3,
            "recall@5":86.5,
            "Recall@10":91.8,
            "QPS":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96297"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":22778,
        "row_id":96260,
        "rank":4,
        "Model":"FLAVA (zero-shot)",
        "mlmodel":{

        },
        "method_short":"FLAVA ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-08",
        "metrics":{
            "recall@1":"38.38",
            "recall@5":"67.47",
            "Recall@10":null,
            "QPS":null
        },
        "raw_metrics":{
            "recall@1":38.38,
            "recall@5":67.47,
            "Recall@10":null,
            "QPS":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":927969,
            "title":"FLAVA: A Foundational Language And Vision Alignment Model",
            "url":"\/paper\/flava-a-foundational-language-and-vision",
            "published":"2021-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/flava-a-foundational-language-and-vision\/review\/?hl=96260"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":22778,
        "row_id":96261,
        "rank":5,
        "Model":"CLIP (zero-shot)",
        "mlmodel":{

        },
        "method_short":"CLIP ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-08",
        "metrics":{
            "recall@1":"33.29",
            "recall@5":"62.47",
            "Recall@10":null,
            "QPS":null
        },
        "raw_metrics":{
            "recall@1":33.29,
            "recall@5":62.47,
            "Recall@10":null,
            "QPS":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":927969,
            "title":"FLAVA: A Foundational Language And Vision Alignment Model",
            "url":"\/paper\/flava-a-foundational-language-and-vision",
            "published":"2021-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/flava-a-foundational-language-and-vision\/review\/?hl=96261"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":22778,
        "row_id":96204,
        "rank":6,
        "Model":"Oscar",
        "mlmodel":{

        },
        "method_short":"Oscar",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-13",
        "metrics":{
            "recall@1":null,
            "recall@5":null,
            "Recall@10":"98.3",
            "QPS":null
        },
        "raw_metrics":{
            "recall@1":null,
            "recall@5":null,
            "Recall@10":98.3,
            "QPS":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":190875,
            "title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
            "url":"\/paper\/oscar-object-semantics-aligned-pre-training",
            "published":"2020-04-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/oscar-object-semantics-aligned-pre-training\/review\/?hl=96204"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]