[
    {
        "table_id":22777,
        "row_id":96296,
        "rank":1,
        "Model":"BLIP-2 ViT-G (fine-tuned)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G ",
        "method_details":"fine-tuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Recall@1":"85.4",
            "Recall@5":"97.0",
            "Recall@10":"98.5"
        },
        "raw_metrics":{
            "Recall@1":85.4,
            "Recall@5":97.0,
            "Recall@10":98.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96296"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":22777,
        "row_id":103015,
        "rank":2,
        "Model":"ONE-PEACE (w\/o ranking)",
        "mlmodel":{

        },
        "method_short":"ONE-PEACE ",
        "method_details":"w\/o ranking",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-18",
        "metrics":{
            "Recall@1":"84.1",
            "Recall@5":"96.3",
            "Recall@10":"98.3"
        },
        "raw_metrics":{
            "Recall@1":84.1,
            "Recall@5":96.3,
            "Recall@10":98.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1211430,
            "title":"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
            "url":"\/paper\/one-peace-exploring-one-general",
            "published":"2023-05-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-peace-exploring-one-general\/review\/?hl=103015"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":22777,
        "row_id":96295,
        "rank":3,
        "Model":"BLIP-2 ViT-L (fine-tuned)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-L ",
        "method_details":"fine-tuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Recall@1":"83.5",
            "Recall@5":"96.0",
            "Recall@10":"98.0"
        },
        "raw_metrics":{
            "Recall@1":83.5,
            "Recall@5":96.0,
            "Recall@10":98.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96295"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":22777,
        "row_id":96866,
        "rank":4,
        "Model":"IAIS",
        "mlmodel":{

        },
        "method_short":"IAIS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-28",
        "metrics":{
            "Recall@1":"67.78",
            "Recall@5":"89.7",
            "Recall@10":"94.48"
        },
        "raw_metrics":{
            "Recall@1":67.78,
            "Recall@5":89.7,
            "Recall@10":94.48
        },
        "uses_additional_data":false,
        "paper":{
            "id":807331,
            "title":"Learning Relation Alignment for Calibrated Cross-modal Retrieval",
            "url":"\/paper\/learning-relation-alignment-for-calibrated",
            "published":"2021-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-relation-alignment-for-calibrated\/review\/?hl=96866"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":22777,
        "row_id":96262,
        "rank":5,
        "Model":"FLAVA (zero-shot)",
        "mlmodel":{

        },
        "method_short":"FLAVA ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-08",
        "metrics":{
            "Recall@1":"42.74",
            "Recall@5":"76.76",
            "Recall@10":null
        },
        "raw_metrics":{
            "Recall@1":42.74,
            "Recall@5":76.76,
            "Recall@10":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":927969,
            "title":"FLAVA: A Foundational Language And Vision Alignment Model",
            "url":"\/paper\/flava-a-foundational-language-and-vision",
            "published":"2021-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/flava-a-foundational-language-and-vision\/review\/?hl=96262"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":22777,
        "row_id":96263,
        "rank":6,
        "Model":"CLIP (zero-shot)",
        "mlmodel":{

        },
        "method_short":"CLIP ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-08",
        "metrics":{
            "Recall@1":"37.12",
            "Recall@5":"69.48",
            "Recall@10":null
        },
        "raw_metrics":{
            "Recall@1":37.12,
            "Recall@5":69.48,
            "Recall@10":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":927969,
            "title":"FLAVA: A Foundational Language And Vision Alignment Model",
            "url":"\/paper\/flava-a-foundational-language-and-vision",
            "published":"2021-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/flava-a-foundational-language-and-vision\/review\/?hl=96263"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":22777,
        "row_id":96203,
        "rank":7,
        "Model":"Oscar",
        "mlmodel":{

        },
        "method_short":"Oscar",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-13",
        "metrics":{
            "Recall@1":null,
            "Recall@5":null,
            "Recall@10":"99.8"
        },
        "raw_metrics":{
            "Recall@1":null,
            "Recall@5":null,
            "Recall@10":99.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":190875,
            "title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
            "url":"\/paper\/oscar-object-semantics-aligned-pre-training",
            "published":"2020-04-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/oscar-object-semantics-aligned-pre-training\/review\/?hl=96203"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":22777,
        "row_id":96205,
        "rank":8,
        "Model":"Unicoder-VL",
        "mlmodel":{

        },
        "method_short":"Unicoder-VL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-16",
        "metrics":{
            "Recall@1":null,
            "Recall@5":null,
            "Recall@10":"97.2"
        },
        "raw_metrics":{
            "Recall@1":null,
            "Recall@5":null,
            "Recall@10":97.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":150148,
            "title":"Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training",
            "url":"\/paper\/unicoder-vl-a-universal-encoder-for-vision",
            "published":"2019-08-16T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/unicoder-vl-a-universal-encoder-for-vision\/review\/?hl=96205"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":22777,
        "row_id":96258,
        "rank":9,
        "Model":"DVSA",
        "mlmodel":{

        },
        "method_short":"DVSA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2014-12-07",
        "metrics":{
            "Recall@1":null,
            "Recall@5":null,
            "Recall@10":"74.8"
        },
        "raw_metrics":{
            "Recall@1":null,
            "Recall@5":null,
            "Recall@10":74.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":41251,
            "title":"Deep Visual-Semantic Alignments for Generating Image Descriptions",
            "url":"\/paper\/deep-visual-semantic-alignments-for",
            "published":"2014-12-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deep-visual-semantic-alignments-for\/review\/?hl=96258"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]