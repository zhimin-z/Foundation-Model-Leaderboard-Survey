[
    {
        "table_id":10071,
        "row_id":114692,
        "rank":1,
        "Model":"InternVL-G",
        "mlmodel":{

        },
        "method_short":"InternVL-G",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-12-21",
        "metrics":{
            "Image-to-text R@1":"74.9",
            "Image-to-text R@5":"91.3",
            "Image-to-text R@10":"95.2",
            "Text-to-image R@1":"58.6",
            "Text-to-image R@5":"81.3",
            "Text-to-image R@10":"88.0"
        },
        "raw_metrics":{
            "Image-to-text R@1":74.9,
            "Image-to-text R@5":91.3,
            "Image-to-text R@10":95.2,
            "Text-to-image R@1":58.6,
            "Text-to-image R@5":81.3,
            "Text-to-image R@10":88.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1349430,
            "title":"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks",
            "url":"\/paper\/internvl-scaling-up-vision-foundation-models",
            "published":"2023-12-21T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10071,
        "row_id":48604,
        "rank":2,
        "Model":"TCL",
        "mlmodel":{

        },
        "method_short":"TCL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-21",
        "metrics":{
            "Image-to-text R@1":"71.4",
            "Image-to-text R@5":"90.8",
            "Image-to-text R@10":"95.4",
            "Text-to-image R@1":"53.5",
            "Text-to-image R@5":"79.0",
            "Text-to-image R@10":"87.1"
        },
        "raw_metrics":{
            "Image-to-text R@1":71.4,
            "Image-to-text R@5":90.8,
            "Image-to-text R@10":95.4,
            "Text-to-image R@1":53.5,
            "Text-to-image R@5":79.0,
            "Text-to-image R@10":87.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":964746,
            "title":"Vision-Language Pre-Training with Triple Contrastive Learning",
            "url":"\/paper\/vision-language-pre-training-with-triple",
            "published":"2022-02-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-language-pre-training-with-triple\/review\/?hl=48604"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10071,
        "row_id":114691,
        "rank":3,
        "Model":"InternVL-C",
        "mlmodel":{

        },
        "method_short":"InternVL-C",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-12-21",
        "metrics":{
            "Image-to-text R@1":"70.6",
            "Image-to-text R@5":"89.0",
            "Image-to-text R@10":"93.5",
            "Text-to-image R@1":"54.1",
            "Text-to-image R@5":"77.3",
            "Text-to-image R@10":"84.6"
        },
        "raw_metrics":{
            "Image-to-text R@1":70.6,
            "Image-to-text R@5":89.0,
            "Image-to-text R@10":93.5,
            "Text-to-image R@1":54.1,
            "Text-to-image R@5":77.3,
            "Text-to-image R@10":84.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1349430,
            "title":"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks",
            "url":"\/paper\/internvl-scaling-up-vision-foundation-models",
            "published":"2023-12-21T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10071,
        "row_id":87861,
        "rank":4,
        "Model":"PTP-BLIP",
        "mlmodel":{

        },
        "method_short":"PTP-BLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-19",
        "metrics":{
            "Image-to-text R@1":"69.7",
            "Image-to-text R@5":"90.0",
            "Image-to-text R@10":"94.7",
            "Text-to-image R@1":"49.5",
            "Text-to-image R@5":"75.9",
            "Text-to-image R@10":"84.2"
        },
        "raw_metrics":{
            "Image-to-text R@1":69.7,
            "Image-to-text R@5":90.0,
            "Image-to-text R@10":94.7,
            "Text-to-image R@1":49.5,
            "Text-to-image R@5":75.9,
            "Text-to-image R@10":84.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1130454,
            "title":"Position-guided Text Prompt for Vision-Language Pre-training",
            "url":"\/paper\/position-guided-text-prompt-for-vision",
            "published":"2022-12-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/position-guided-text-prompt-for-vision\/review\/?hl=87861"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10071,
        "row_id":105534,
        "rank":5,
        "Model":"RO-ViT",
        "mlmodel":{

        },
        "method_short":"RO-ViT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-11",
        "metrics":{
            "Image-to-text R@1":"68.9",
            "Image-to-text R@5":"87.8",
            "Image-to-text R@10":"92.2",
            "Text-to-image R@1":"51.8",
            "Text-to-image R@5":"75.0",
            "Text-to-image R@10":"83.0"
        },
        "raw_metrics":{
            "Image-to-text R@1":68.9,
            "Image-to-text R@5":87.8,
            "Image-to-text R@10":92.2,
            "Text-to-image R@1":51.8,
            "Text-to-image R@5":75.0,
            "Text-to-image R@10":83.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1206407,
            "title":"Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers",
            "url":"\/paper\/region-aware-pretraining-for-open-vocabulary",
            "published":"2023-05-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/region-aware-pretraining-for-open-vocabulary\/review\/?hl=105534"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10071,
        "row_id":48605,
        "rank":6,
        "Model":"ALBEF",
        "mlmodel":{

        },
        "method_short":"ALBEF",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-16",
        "metrics":{
            "Image-to-text R@1":"68.7",
            "Image-to-text R@5":"89.5",
            "Image-to-text R@10":"94.7",
            "Text-to-image R@1":"50.1",
            "Text-to-image R@5":"76.4",
            "Text-to-image R@10":"84.5"
        },
        "raw_metrics":{
            "Image-to-text R@1":68.7,
            "Image-to-text R@5":89.5,
            "Image-to-text R@10":94.7,
            "Text-to-image R@1":50.1,
            "Text-to-image R@5":76.4,
            "Text-to-image R@10":84.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":836928,
            "title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
            "url":"\/paper\/align-before-fuse-vision-and-language",
            "published":"2021-07-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/align-before-fuse-vision-and-language\/review\/?hl=48605"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10071,
        "row_id":54162,
        "rank":7,
        "Model":"CoCa",
        "mlmodel":{

        },
        "method_short":"CoCa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Image-to-text R@1":"66.3",
            "Image-to-text R@5":"86.2",
            "Image-to-text R@10":"91.8",
            "Text-to-image R@1":"51.2",
            "Text-to-image R@5":"74.2",
            "Text-to-image R@10":"82.0"
        },
        "raw_metrics":{
            "Image-to-text R@1":66.3,
            "Image-to-text R@5":86.2,
            "Image-to-text R@10":91.8,
            "Text-to-image R@1":51.2,
            "Text-to-image R@5":74.2,
            "Text-to-image R@10":82.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=54162"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10071,
        "row_id":53743,
        "rank":8,
        "Model":"Flamingo",
        "mlmodel":{

        },
        "method_short":"Flamingo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-29",
        "metrics":{
            "Image-to-text R@1":"65.9",
            "Image-to-text R@5":"87.3",
            "Image-to-text R@10":"92.9",
            "Text-to-image R@1":"48.0",
            "Text-to-image R@5":"73.3",
            "Text-to-image R@10":"82.1"
        },
        "raw_metrics":{
            "Image-to-text R@1":65.9,
            "Image-to-text R@5":87.3,
            "Image-to-text R@10":92.9,
            "Text-to-image R@1":48.0,
            "Text-to-image R@5":73.3,
            "Text-to-image R@10":82.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1001838,
            "title":"Flamingo: a Visual Language Model for Few-Shot Learning",
            "url":"\/paper\/flamingo-a-visual-language-model-for-few-shot-1",
            "published":"2022-04-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10071,
        "row_id":48603,
        "rank":9,
        "Model":"Florence",
        "mlmodel":{

        },
        "method_short":"Florence",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-22",
        "metrics":{
            "Image-to-text R@1":"64.7",
            "Image-to-text R@5":"85.9",
            "Image-to-text R@10":null,
            "Text-to-image R@1":"47.2",
            "Text-to-image R@5":"71.4",
            "Text-to-image R@10":null
        },
        "raw_metrics":{
            "Image-to-text R@1":64.7,
            "Image-to-text R@5":85.9,
            "Image-to-text R@10":null,
            "Text-to-image R@1":47.2,
            "Text-to-image R@5":71.4,
            "Text-to-image R@10":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":914419,
            "title":"Florence: A New Foundation Model for Computer Vision",
            "url":"\/paper\/florence-a-new-foundation-model-for-computer",
            "published":"2021-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/florence-a-new-foundation-model-for-computer\/review\/?hl=48603"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10071,
        "row_id":74113,
        "rank":10,
        "Model":"ERNIE-ViL 2.0",
        "mlmodel":{

        },
        "method_short":"ERNIE-ViL 2.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-30",
        "metrics":{
            "Image-to-text R@1":"63.1",
            "Image-to-text R@5":"85.7",
            "Image-to-text R@10":"91.4",
            "Text-to-image R@1":"46.0",
            "Text-to-image R@5":"71.4",
            "Text-to-image R@10":"80.4"
        },
        "raw_metrics":{
            "Image-to-text R@1":63.1,
            "Image-to-text R@5":85.7,
            "Image-to-text R@10":91.4,
            "Text-to-image R@1":46.0,
            "Text-to-image R@5":71.4,
            "Text-to-image R@10":80.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1084534,
            "title":"ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text Pre-training",
            "url":"\/paper\/ernie-vil-2-0-multi-view-contrastive-learning",
            "published":"2022-09-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ernie-vil-2-0-multi-view-contrastive-learning\/review\/?hl=74113"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10071,
        "row_id":48608,
        "rank":11,
        "Model":"ALIGN",
        "mlmodel":{

        },
        "method_short":"ALIGN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Image-to-text R@1":"58.6",
            "Image-to-text R@5":"83.0",
            "Image-to-text R@10":"89.7",
            "Text-to-image R@1":"45.6",
            "Text-to-image R@5":"69.8",
            "Text-to-image R@10":"78.6"
        },
        "raw_metrics":{
            "Image-to-text R@1":58.6,
            "Image-to-text R@5":83.0,
            "Image-to-text R@10":89.7,
            "Text-to-image R@1":45.6,
            "Text-to-image R@5":69.8,
            "Text-to-image R@10":78.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":744362,
            "title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision",
            "url":"\/paper\/scaling-up-visual-and-vision-language",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-up-visual-and-vision-language\/review\/?hl=48608"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10071,
        "row_id":48606,
        "rank":12,
        "Model":"CLIP",
        "mlmodel":{

        },
        "method_short":"CLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-26",
        "metrics":{
            "Image-to-text R@1":"58.4",
            "Image-to-text R@5":"81.5",
            "Image-to-text R@10":"88.1",
            "Text-to-image R@1":"37.8",
            "Text-to-image R@5":"62.4",
            "Text-to-image R@10":"72.2"
        },
        "raw_metrics":{
            "Image-to-text R@1":58.4,
            "Image-to-text R@5":81.5,
            "Image-to-text R@10":88.1,
            "Text-to-image R@1":37.8,
            "Text-to-image R@5":62.4,
            "Text-to-image R@10":72.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":749733,
            "title":"Learning Transferable Visual Models From Natural Language Supervision",
            "url":"\/paper\/learning-transferable-visual-models-from",
            "published":"2021-02-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-transferable-visual-models-from\/review\/?hl=48606"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10071,
        "row_id":35253,
        "rank":13,
        "Model":"ViLT-B\/32",
        "mlmodel":{

        },
        "method_short":"ViLT-B\/32",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-05",
        "metrics":{
            "Image-to-text R@1":"56.5",
            "Image-to-text R@5":"82.6",
            "Image-to-text R@10":"89.6",
            "Text-to-image R@1":"40.4",
            "Text-to-image R@5":"70",
            "Text-to-image R@10":"81.1"
        },
        "raw_metrics":{
            "Image-to-text R@1":56.5,
            "Image-to-text R@5":82.6,
            "Image-to-text R@10":89.6,
            "Text-to-image R@1":40.4,
            "Text-to-image R@5":70.0,
            "Text-to-image R@10":81.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":742872,
            "title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
            "url":"\/paper\/vilt-vision-and-language-transformer-without",
            "published":"2021-02-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vilt-vision-and-language-transformer-without\/review\/?hl=35253"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10071,
        "row_id":48607,
        "rank":14,
        "Model":"ImageBERT",
        "mlmodel":{

        },
        "method_short":"ImageBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-01-22",
        "metrics":{
            "Image-to-text R@1":"44.0",
            "Image-to-text R@5":"71.2",
            "Image-to-text R@10":"80.4",
            "Text-to-image R@1":"32.3",
            "Text-to-image R@5":"59.0",
            "Text-to-image R@10":"70.2"
        },
        "raw_metrics":{
            "Image-to-text R@1":44.0,
            "Image-to-text R@5":71.2,
            "Image-to-text R@10":80.4,
            "Text-to-image R@1":32.3,
            "Text-to-image R@5":59.0,
            "Text-to-image R@10":70.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":180489,
            "title":"ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data",
            "url":"\/paper\/imagebert-cross-modal-pre-training-with-large",
            "published":"2020-01-22T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/imagebert-cross-modal-pre-training-with-large\/review\/?hl=48607"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]