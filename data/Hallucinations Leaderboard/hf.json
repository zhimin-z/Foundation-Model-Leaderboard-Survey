[
    {
        "T":"?",
        "Model":"DiscoResearch\/mixtral-7b-8expert",
        "NQ Open\/EM":0.0,
        "TriviaQA\/EM":0.0,
        "TruthQA MC1\/Acc":24.72,
        "TruthQA MC2\/Acc":49.48,
        "TruthQA Gen\/ROUGE":15.18,
        "XSum\/ROUGE":0.13,
        "XSum\/factKB":1.28,
        "XSum\/BERT-P":16.97,
        "CNN-DM\/ROUGE":0.19,
        "CNN-DM\/factKB":7.07,
        "CNN-DM\/BERT-P":22.31,
        "RACE\/Acc":22.2,
        "SQUaDv2\/EM":0.0,
        "MemoTrap\/Acc":56.2,
        "IFEval\/Acc":14.23,
        "FaithDial\/Acc":50.89,
        "HaluQA\/Acc":7.08,
        "HaluSumm\/Acc":7.78,
        "HaluDial\/Acc":7.18,
        "FEVER\/Acc":50.0,
        "TrueFalse\/Acc":50.65,
        "Type":22.2,
        "Architecture":"MixtralForCausalLM",
        "Precision":"float32",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/gpt-j-6b",
        "NQ Open\/EM":9.17,
        "TriviaQA\/EM":39.32,
        "TruthQA MC1\/Acc":20.2,
        "TruthQA MC2\/Acc":35.96,
        "TruthQA Gen\/ROUGE":24.36,
        "XSum\/ROUGE":5.0,
        "XSum\/factKB":35.71,
        "XSum\/BERT-P":21.19,
        "CNN-DM\/ROUGE":6.79,
        "CNN-DM\/factKB":95.63,
        "CNN-DM\/BERT-P":16.03,
        "RACE\/Acc":37.61,
        "SQUaDv2\/EM":23.4,
        "MemoTrap\/Acc":42.52,
        "IFEval\/Acc":17.93,
        "FaithDial\/Acc":62.22,
        "HaluQA\/Acc":51.36,
        "HaluSumm\/Acc":46.58,
        "HaluDial\/Acc":49.34,
        "FEVER\/Acc":52.1,
        "TrueFalse\/Acc":54.13,
        "Type":37.61,
        "Architecture":"GPTJForCausalLM",
        "Precision":"float32",
        "Hub License":"apache-2.0",
        "#Params (B)":6.0,
        "Hub":1316,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/gpt-neo-1.3B",
        "NQ Open\/EM":3.99,
        "TriviaQA\/EM":14.16,
        "TruthQA MC1\/Acc":23.13,
        "TruthQA MC2\/Acc":39.61,
        "TruthQA Gen\/ROUGE":28.4,
        "XSum\/ROUGE":9.52,
        "XSum\/factKB":64.97,
        "XSum\/BERT-P":44.63,
        "CNN-DM\/ROUGE":21.92,
        "CNN-DM\/factKB":98.66,
        "CNN-DM\/BERT-P":49.95,
        "RACE\/Acc":34.07,
        "SQUaDv2\/EM":15.11,
        "MemoTrap\/Acc":53.74,
        "IFEval\/Acc":14.23,
        "FaithDial\/Acc":75.61,
        "HaluQA\/Acc":46.25,
        "HaluSumm\/Acc":44.57,
        "HaluDial\/Acc":48.36,
        "FEVER\/Acc":51.61,
        "TrueFalse\/Acc":50.76,
        "Type":34.07,
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float32",
        "Hub License":"mit",
        "#Params (B)":1.37,
        "Hub":206,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/gpt-neo-125m",
        "NQ Open\/EM":0.53,
        "TriviaQA\/EM":1.55,
        "TruthQA MC1\/Acc":25.83,
        "TruthQA MC2\/Acc":45.58,
        "TruthQA Gen\/ROUGE":36.6,
        "XSum\/ROUGE":11.91,
        "XSum\/factKB":47.57,
        "XSum\/BERT-P":52.08,
        "CNN-DM\/ROUGE":15.45,
        "CNN-DM\/factKB":95.92,
        "CNN-DM\/BERT-P":40.07,
        "RACE\/Acc":27.56,
        "SQUaDv2\/EM":4.77,
        "MemoTrap\/Acc":75.85,
        "IFEval\/Acc":12.94,
        "FaithDial\/Acc":79.46,
        "HaluQA\/Acc":46.53,
        "HaluSumm\/Acc":46.51,
        "HaluDial\/Acc":47.2,
        "FEVER\/Acc":51.26,
        "TrueFalse\/Acc":49.4,
        "Type":27.56,
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float32",
        "Hub License":"mit",
        "#Params (B)":0.15,
        "Hub":132,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/gpt-neo-2.7B",
        "NQ Open\/EM":5.71,
        "TriviaQA\/EM":20.11,
        "TruthQA MC1\/Acc":23.87,
        "TruthQA MC2\/Acc":39.86,
        "TruthQA Gen\/ROUGE":34.27,
        "XSum\/ROUGE":2.01,
        "XSum\/factKB":24.16,
        "XSum\/BERT-P":8.54,
        "CNN-DM\/ROUGE":23.0,
        "CNN-DM\/factKB":97.87,
        "CNN-DM\/BERT-P":52.76,
        "RACE\/Acc":35.22,
        "SQUaDv2\/EM":15.84,
        "MemoTrap\/Acc":51.71,
        "IFEval\/Acc":18.85,
        "FaithDial\/Acc":77.0,
        "HaluQA\/Acc":43.18,
        "HaluSumm\/Acc":46.68,
        "HaluDial\/Acc":47.72,
        "FEVER\/Acc":52.07,
        "TrueFalse\/Acc":50.6,
        "Type":35.22,
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float32",
        "Hub License":"mit",
        "#Params (B)":2.72,
        "Hub":361,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"?",
        "Model":"Gryphe\/MythoMax-L2-13b",
        "NQ Open\/EM":29.09,
        "TriviaQA\/EM":9.74,
        "TruthQA MC1\/Acc":36.47,
        "TruthQA MC2\/Acc":51.54,
        "TruthQA Gen\/ROUGE":42.84,
        "XSum\/ROUGE":19.53,
        "XSum\/factKB":49.08,
        "XSum\/BERT-P":52.88,
        "CNN-DM\/ROUGE":25.98,
        "CNN-DM\/factKB":89.67,
        "CNN-DM\/BERT-P":52.44,
        "RACE\/Acc":43.35,
        "SQUaDv2\/EM":15.14,
        "MemoTrap\/Acc":39.21,
        "IFEval\/Acc":24.03,
        "FaithDial\/Acc":81.46,
        "HaluQA\/Acc":39.92,
        "HaluSumm\/Acc":47.6,
        "HaluDial\/Acc":71.73,
        "FEVER\/Acc":78.73,
        "TrueFalse\/Acc":86.98,
        "Type":43.35,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/zephyr-7b-alpha",
        "NQ Open\/EM":26.23,
        "TriviaQA\/EM":65.92,
        "TruthQA MC1\/Acc":40.64,
        "TruthQA MC2\/Acc":56.02,
        "TruthQA Gen\/ROUGE":48.35,
        "XSum\/ROUGE":15.47,
        "XSum\/factKB":69.74,
        "XSum\/BERT-P":47.96,
        "CNN-DM\/ROUGE":28.97,
        "CNN-DM\/factKB":99.12,
        "CNN-DM\/BERT-P":59.75,
        "RACE\/Acc":46.41,
        "SQUaDv2\/EM":30.35,
        "MemoTrap\/Acc":41.35,
        "IFEval\/Acc":6.1,
        "FaithDial\/Acc":74.8,
        "HaluQA\/Acc":62.35,
        "HaluSumm\/Acc":52.68,
        "HaluDial\/Acc":79.17,
        "FEVER\/Acc":82.41,
        "TrueFalse\/Acc":88.55,
        "Type":46.41,
        "Architecture":"MistralForCausalLM",
        "Precision":"float32",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":986,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/zephyr-7b-beta",
        "NQ Open\/EM":25.21,
        "TriviaQA\/EM":64.43,
        "TruthQA MC1\/Acc":38.68,
        "TruthQA MC2\/Acc":55.12,
        "TruthQA Gen\/ROUGE":44.68,
        "XSum\/ROUGE":16.66,
        "XSum\/factKB":59.2,
        "XSum\/BERT-P":56.8,
        "CNN-DM\/ROUGE":27.1,
        "CNN-DM\/factKB":98.61,
        "CNN-DM\/BERT-P":58.71,
        "RACE\/Acc":45.84,
        "SQUaDv2\/EM":31.12,
        "MemoTrap\/Acc":44.66,
        "IFEval\/Acc":9.24,
        "FaithDial\/Acc":58.75,
        "HaluQA\/Acc":51.85,
        "HaluSumm\/Acc":52.38,
        "HaluDial\/Acc":76.34,
        "FEVER\/Acc":78.47,
        "TrueFalse\/Acc":83.24,
        "Type":45.84,
        "Architecture":"MistralForCausalLM",
        "Precision":"float32",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":973,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Nexusflow\/NexusRaven-V2-13B",
        "NQ Open\/EM":14.21,
        "TriviaQA\/EM":42.29,
        "TruthQA MC1\/Acc":28.4,
        "TruthQA MC2\/Acc":44.39,
        "TruthQA Gen\/ROUGE":42.96,
        "XSum\/ROUGE":17.93,
        "XSum\/factKB":74.7,
        "XSum\/BERT-P":60.0,
        "CNN-DM\/ROUGE":25.04,
        "CNN-DM\/factKB":98.41,
        "CNN-DM\/BERT-P":54.93,
        "RACE\/Acc":42.39,
        "SQUaDv2\/EM":31.63,
        "MemoTrap\/Acc":61.97,
        "IFEval\/Acc":14.6,
        "FaithDial\/Acc":66.4,
        "HaluQA\/Acc":23.73,
        "HaluSumm\/Acc":49.89,
        "HaluDial\/Acc":49.82,
        "FEVER\/Acc":68.27,
        "TrueFalse\/Acc":70.76,
        "Type":42.39,
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":13.0,
        "Hub":344,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"NousResearch\/Nous-Hermes-Llama2-13b",
        "NQ Open\/EM":28.14,
        "TriviaQA\/EM":9.85,
        "TruthQA MC1\/Acc":34.76,
        "TruthQA MC2\/Acc":50.27,
        "TruthQA Gen\/ROUGE":41.62,
        "XSum\/ROUGE":19.17,
        "XSum\/factKB":52.28,
        "XSum\/BERT-P":52.5,
        "CNN-DM\/ROUGE":25.22,
        "CNN-DM\/factKB":90.94,
        "CNN-DM\/BERT-P":50.87,
        "RACE\/Acc":45.93,
        "SQUaDv2\/EM":30.64,
        "MemoTrap\/Acc":31.41,
        "IFEval\/Acc":14.23,
        "FaithDial\/Acc":82.9,
        "HaluQA\/Acc":57.15,
        "HaluSumm\/Acc":49.93,
        "HaluDial\/Acc":73.26,
        "FEVER\/Acc":78.33,
        "TrueFalse\/Acc":86.71,
        "Type":45.93,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":13.0,
        "Hub":251,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"NousResearch\/Yarn-Mistral-7b-128k",
        "NQ Open\/EM":29.34,
        "TriviaQA\/EM":70.03,
        "TruthQA MC1\/Acc":27.42,
        "TruthQA MC2\/Acc":42.25,
        "TruthQA Gen\/ROUGE":34.03,
        "XSum\/ROUGE":10.74,
        "XSum\/factKB":41.21,
        "XSum\/BERT-P":33.01,
        "CNN-DM\/ROUGE":21.77,
        "CNN-DM\/factKB":96.75,
        "CNN-DM\/BERT-P":44.88,
        "RACE\/Acc":40.77,
        "SQUaDv2\/EM":33.73,
        "MemoTrap\/Acc":39.96,
        "IFEval\/Acc":14.6,
        "FaithDial\/Acc":78.78,
        "HaluQA\/Acc":56.99,
        "HaluSumm\/Acc":45.88,
        "HaluDial\/Acc":55.8,
        "FEVER\/Acc":75.89,
        "TrueFalse\/Acc":83.76,
        "Type":40.77,
        "Architecture":"MistralForCausalLM",
        "Precision":"float32",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":470,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Open-Orca\/Mistral-7B-OpenOrca",
        "NQ Open\/EM":25.62,
        "TriviaQA\/EM":65.84,
        "TruthQA MC1\/Acc":35.25,
        "TruthQA MC2\/Acc":52.26,
        "TruthQA Gen\/ROUGE":44.8,
        "XSum\/ROUGE":21.24,
        "XSum\/factKB":39.63,
        "XSum\/BERT-P":53.62,
        "CNN-DM\/ROUGE":31.96,
        "CNN-DM\/factKB":95.29,
        "CNN-DM\/BERT-P":61.51,
        "RACE\/Acc":47.18,
        "SQUaDv2\/EM":30.5,
        "MemoTrap\/Acc":50.53,
        "IFEval\/Acc":3.33,
        "FaithDial\/Acc":82.85,
        "HaluQA\/Acc":44.64,
        "HaluSumm\/Acc":51.47,
        "HaluDial\/Acc":76.99,
        "FEVER\/Acc":81.05,
        "TrueFalse\/Acc":87.94,
        "Type":47.18,
        "Architecture":"MistralForCausalLM",
        "Precision":"float32",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":485,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Open-Orca\/OpenOrca-Platypus2-13B",
        "NQ Open\/EM":28.31,
        "TriviaQA\/EM":9.88,
        "TruthQA MC1\/Acc":38.07,
        "TruthQA MC2\/Acc":52.68,
        "TruthQA Gen\/ROUGE":40.39,
        "XSum\/ROUGE":25.78,
        "XSum\/factKB":52.59,
        "XSum\/BERT-P":69.83,
        "CNN-DM\/ROUGE":33.28,
        "CNN-DM\/factKB":99.08,
        "CNN-DM\/BERT-P":64.25,
        "RACE\/Acc":49.0,
        "SQUaDv2\/EM":30.85,
        "MemoTrap\/Acc":48.29,
        "IFEval\/Acc":26.06,
        "FaithDial\/Acc":80.05,
        "HaluQA\/Acc":20.11,
        "HaluSumm\/Acc":43.5,
        "HaluDial\/Acc":75.47,
        "FEVER\/Acc":80.92,
        "TrueFalse\/Acc":88.22,
        "Type":49.0,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.0,
        "Hub":219,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Wizard-Vicuna-13B-Uncensored-GPTQ",
        "NQ Open\/EM":24.71,
        "TriviaQA\/EM":9.61,
        "TruthQA MC1\/Acc":35.86,
        "TruthQA MC2\/Acc":51.64,
        "TruthQA Gen\/ROUGE":42.11,
        "XSum\/ROUGE":6.19,
        "XSum\/factKB":63.31,
        "XSum\/BERT-P":26.18,
        "CNN-DM\/ROUGE":17.83,
        "CNN-DM\/factKB":89.88,
        "CNN-DM\/BERT-P":37.7,
        "RACE\/Acc":43.25,
        "SQUaDv2\/EM":12.82,
        "MemoTrap\/Acc":39.21,
        "IFEval\/Acc":16.27,
        "FaithDial\/Acc":81.24,
        "HaluQA\/Acc":52.06,
        "HaluSumm\/Acc":44.36,
        "HaluDial\/Acc":64.71,
        "FEVER\/Acc":73.1,
        "TrueFalse\/Acc":80.87,
        "Type":43.25,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub":267,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Wizard-Vicuna-7B-Uncensored-GPTQ",
        "NQ Open\/EM":20.08,
        "TriviaQA\/EM":8.87,
        "TruthQA MC1\/Acc":29.62,
        "TruthQA MC2\/Acc":43.75,
        "TruthQA Gen\/ROUGE":35.13,
        "XSum\/ROUGE":6.84,
        "XSum\/factKB":76.93,
        "XSum\/BERT-P":33.41,
        "CNN-DM\/ROUGE":17.24,
        "CNN-DM\/factKB":83.6,
        "CNN-DM\/BERT-P":38.91,
        "RACE\/Acc":42.39,
        "SQUaDv2\/EM":14.41,
        "MemoTrap\/Acc":43.59,
        "IFEval\/Acc":10.91,
        "FaithDial\/Acc":71.72,
        "HaluQA\/Acc":27.95,
        "HaluSumm\/Acc":41.92,
        "HaluDial\/Acc":49.34,
        "FEVER\/Acc":70.05,
        "TrueFalse\/Acc":69.81,
        "Type":42.39,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":"other",
        "#Params (B)":9.04,
        "Hub":120,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"ai-forever\/mGPT",
        "NQ Open\/EM":2.74,
        "TriviaQA\/EM":10.61,
        "TruthQA MC1\/Acc":23.26,
        "TruthQA MC2\/Acc":39.62,
        "TruthQA Gen\/ROUGE":30.97,
        "XSum\/ROUGE":9.78,
        "XSum\/factKB":54.94,
        "XSum\/BERT-P":50.42,
        "CNN-DM\/ROUGE":7.53,
        "CNN-DM\/factKB":70.06,
        "CNN-DM\/BERT-P":47.52,
        "RACE\/Acc":29.38,
        "SQUaDv2\/EM":16.63,
        "MemoTrap\/Acc":74.36,
        "IFEval\/Acc":19.41,
        "FaithDial\/Acc":72.37,
        "HaluQA\/Acc":5.49,
        "HaluSumm\/Acc":0.0,
        "HaluDial\/Acc":0.01,
        "FEVER\/Acc":50.87,
        "TrueFalse\/Acc":49.27,
        "Type":29.38,
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float32",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":190,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"berkeley-nest\/Starling-LM-7B-alpha",
        "NQ Open\/EM":28.25,
        "TriviaQA\/EM":65.95,
        "TruthQA MC1\/Acc":31.58,
        "TruthQA MC2\/Acc":47.31,
        "TruthQA Gen\/ROUGE":40.76,
        "XSum\/ROUGE":30.66,
        "XSum\/factKB":30.55,
        "XSum\/BERT-P":73.44,
        "CNN-DM\/ROUGE":27.55,
        "CNN-DM\/factKB":98.44,
        "CNN-DM\/BERT-P":57.09,
        "RACE\/Acc":46.89,
        "SQUaDv2\/EM":39.83,
        "MemoTrap\/Acc":52.78,
        "IFEval\/Acc":23.11,
        "FaithDial\/Acc":62.42,
        "HaluQA\/Acc":59.69,
        "HaluSumm\/Acc":54.59,
        "HaluDial\/Acc":67.5,
        "FEVER\/Acc":81.8,
        "TrueFalse\/Acc":89.17,
        "Type":46.89,
        "Architecture":"MistralForCausalLM",
        "Precision":"float32",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":335,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigscience\/bloom-1b7",
        "NQ Open\/EM":3.52,
        "TriviaQA\/EM":12.66,
        "TruthQA MC1\/Acc":24.48,
        "TruthQA MC2\/Acc":41.32,
        "TruthQA Gen\/ROUGE":32.8,
        "XSum\/ROUGE":12.12,
        "XSum\/factKB":56.04,
        "XSum\/BERT-P":52.78,
        "CNN-DM\/ROUGE":19.62,
        "CNN-DM\/factKB":92.88,
        "CNN-DM\/BERT-P":45.86,
        "RACE\/Acc":33.3,
        "SQUaDv2\/EM":14.94,
        "MemoTrap\/Acc":63.89,
        "IFEval\/Acc":7.58,
        "FaithDial\/Acc":79.71,
        "HaluQA\/Acc":49.53,
        "HaluSumm\/Acc":46.27,
        "HaluDial\/Acc":49.98,
        "FEVER\/Acc":54.96,
        "TrueFalse\/Acc":50.83,
        "Type":33.3,
        "Architecture":"BloomForCausalLM",
        "Precision":"float32",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":1.72,
        "Hub":103,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigscience\/bloom-560m",
        "NQ Open\/EM":1.61,
        "TriviaQA\/EM":5.71,
        "TruthQA MC1\/Acc":24.48,
        "TruthQA MC2\/Acc":42.43,
        "TruthQA Gen\/ROUGE":21.42,
        "XSum\/ROUGE":11.14,
        "XSum\/factKB":55.57,
        "XSum\/BERT-P":49.49,
        "CNN-DM\/ROUGE":21.71,
        "CNN-DM\/factKB":97.65,
        "CNN-DM\/BERT-P":47.72,
        "RACE\/Acc":30.24,
        "SQUaDv2\/EM":7.99,
        "MemoTrap\/Acc":74.25,
        "IFEval\/Acc":8.5,
        "FaithDial\/Acc":79.77,
        "HaluQA\/Acc":49.95,
        "HaluSumm\/Acc":46.53,
        "HaluDial\/Acc":49.98,
        "FEVER\/Acc":53.14,
        "TrueFalse\/Acc":50.42,
        "Type":30.24,
        "Architecture":"BloomForCausalLM",
        "Precision":"float32",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.56,
        "Hub":285,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigscience\/bloom-7b1",
        "NQ Open\/EM":8.14,
        "TriviaQA\/EM":26.34,
        "TruthQA MC1\/Acc":22.4,
        "TruthQA MC2\/Acc":38.89,
        "TruthQA Gen\/ROUGE":20.07,
        "XSum\/ROUGE":13.22,
        "XSum\/factKB":58.27,
        "XSum\/BERT-P":53.29,
        "CNN-DM\/ROUGE":20.79,
        "CNN-DM\/factKB":95.76,
        "CNN-DM\/BERT-P":47.3,
        "RACE\/Acc":36.56,
        "SQUaDv2\/EM":22.29,
        "MemoTrap\/Acc":50.64,
        "IFEval\/Acc":8.5,
        "FaithDial\/Acc":84.15,
        "HaluQA\/Acc":58.06,
        "HaluSumm\/Acc":46.52,
        "HaluDial\/Acc":46.25,
        "FEVER\/Acc":53.76,
        "TrueFalse\/Acc":53.15,
        "Type":36.56,
        "Architecture":"BloomForCausalLM",
        "Precision":"float32",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":7.07,
        "Hub":148,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigscience\/bloomz-7b1",
        "NQ Open\/EM":0.06,
        "TriviaQA\/EM":4.88,
        "TruthQA MC1\/Acc":25.95,
        "TruthQA MC2\/Acc":45.22,
        "TruthQA Gen\/ROUGE":62.18,
        "XSum\/ROUGE":31.59,
        "XSum\/factKB":26.94,
        "XSum\/BERT-P":69.95,
        "CNN-DM\/ROUGE":19.41,
        "CNN-DM\/factKB":71.42,
        "CNN-DM\/BERT-P":49.39,
        "RACE\/Acc":45.65,
        "SQUaDv2\/EM":0.19,
        "MemoTrap\/Acc":55.34,
        "IFEval\/Acc":4.99,
        "FaithDial\/Acc":77.0,
        "HaluQA\/Acc":32.44,
        "HaluSumm\/Acc":41.62,
        "HaluDial\/Acc":43.28,
        "FEVER\/Acc":60.34,
        "TrueFalse\/Acc":63.16,
        "Type":45.65,
        "Architecture":"BloomForCausalLM",
        "Precision":"float32",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":7.0,
        "Hub":115,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"ehartford\/dolphin-2.1-mistral-7b",
        "NQ Open\/EM":28.28,
        "TriviaQA\/EM":66.75,
        "TruthQA MC1\/Acc":39.05,
        "TruthQA MC2\/Acc":55.9,
        "TruthQA Gen\/ROUGE":47.98,
        "XSum\/ROUGE":24.21,
        "XSum\/factKB":39.87,
        "XSum\/BERT-P":62.3,
        "CNN-DM\/ROUGE":32.0,
        "CNN-DM\/factKB":95.63,
        "CNN-DM\/BERT-P":61.04,
        "RACE\/Acc":45.55,
        "SQUaDv2\/EM":35.37,
        "MemoTrap\/Acc":43.8,
        "IFEval\/Acc":27.36,
        "FaithDial\/Acc":77.85,
        "HaluQA\/Acc":42.0,
        "HaluSumm\/Acc":44.8,
        "HaluDial\/Acc":79.63,
        "FEVER\/Acc":79.55,
        "TrueFalse\/Acc":88.35,
        "Type":45.55,
        "Architecture":"MistralForCausalLM",
        "Precision":"float32",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":233,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/dolphin-2.2.1-mistral-7b",
        "NQ Open\/EM":28.61,
        "TriviaQA\/EM":68.17,
        "TruthQA MC1\/Acc":37.21,
        "TruthQA MC2\/Acc":54.07,
        "TruthQA Gen\/ROUGE":47.12,
        "XSum\/ROUGE":19.89,
        "XSum\/factKB":50.26,
        "XSum\/BERT-P":52.84,
        "CNN-DM\/ROUGE":31.96,
        "CNN-DM\/factKB":96.15,
        "CNN-DM\/BERT-P":61.57,
        "RACE\/Acc":44.31,
        "SQUaDv2\/EM":33.9,
        "MemoTrap\/Acc":43.7,
        "IFEval\/Acc":27.17,
        "FaithDial\/Acc":76.63,
        "HaluQA\/Acc":53.63,
        "HaluSumm\/Acc":36.98,
        "HaluDial\/Acc":79.39,
        "FEVER\/Acc":78.16,
        "TrueFalse\/Acc":88.36,
        "Type":44.31,
        "Architecture":"MistralForCausalLM",
        "Precision":"float32",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":118,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-7b-v1.5",
        "NQ Open\/EM":20.64,
        "TriviaQA\/EM":56.36,
        "TruthQA MC1\/Acc":33.05,
        "TruthQA MC2\/Acc":50.39,
        "TruthQA Gen\/ROUGE":50.67,
        "XSum\/ROUGE":8.43,
        "XSum\/factKB":70.86,
        "XSum\/BERT-P":35.74,
        "CNN-DM\/ROUGE":21.3,
        "CNN-DM\/factKB":84.83,
        "CNN-DM\/BERT-P":43.15,
        "RACE\/Acc":41.72,
        "SQUaDv2\/EM":22.81,
        "MemoTrap\/Acc":42.63,
        "IFEval\/Acc":19.22,
        "FaithDial\/Acc":77.48,
        "HaluQA\/Acc":38.09,
        "HaluSumm\/Acc":50.52,
        "HaluDial\/Acc":57.74,
        "FEVER\/Acc":74.21,
        "TrueFalse\/Acc":82.74,
        "Type":41.72,
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":158,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udcac",
        "Model":"meta-llama\/Llama-2-13b-chat-hf",
        "NQ Open\/EM":27.29,
        "TriviaQA\/EM":9.79,
        "TruthQA MC1\/Acc":28.03,
        "TruthQA MC2\/Acc":43.95,
        "TruthQA Gen\/ROUGE":41.86,
        "XSum\/ROUGE":16.65,
        "XSum\/factKB":49.17,
        "XSum\/BERT-P":47.16,
        "CNN-DM\/ROUGE":13.67,
        "CNN-DM\/factKB":91.38,
        "CNN-DM\/BERT-P":27.59,
        "RACE\/Acc":46.12,
        "SQUaDv2\/EM":34.36,
        "MemoTrap\/Acc":41.35,
        "IFEval\/Acc":31.42,
        "FaithDial\/Acc":77.88,
        "HaluQA\/Acc":57.28,
        "HaluSumm\/Acc":47.72,
        "HaluDial\/Acc":65.48,
        "FEVER\/Acc":75.11,
        "TrueFalse\/Acc":85.14,
        "Type":46.12,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":617,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"meta-llama\/Llama-2-13b-hf",
        "NQ Open\/EM":32.22,
        "TriviaQA\/EM":9.49,
        "TruthQA MC1\/Acc":25.95,
        "TruthQA MC2\/Acc":36.89,
        "TruthQA Gen\/ROUGE":29.5,
        "XSum\/ROUGE":12.77,
        "XSum\/factKB":77.28,
        "XSum\/BERT-P":55.36,
        "CNN-DM\/ROUGE":18.22,
        "CNN-DM\/factKB":96.32,
        "CNN-DM\/BERT-P":37.81,
        "RACE\/Acc":40.48,
        "SQUaDv2\/EM":15.91,
        "MemoTrap\/Acc":35.58,
        "IFEval\/Acc":18.11,
        "FaithDial\/Acc":82.79,
        "HaluQA\/Acc":70.05,
        "HaluSumm\/Acc":44.53,
        "HaluDial\/Acc":73.93,
        "FEVER\/Acc":73.81,
        "TrueFalse\/Acc":83.16,
        "Type":40.48,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":389,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udcac",
        "Model":"meta-llama\/Llama-2-7b-chat-hf",
        "NQ Open\/EM":23.68,
        "TriviaQA\/EM":8.83,
        "TruthQA MC1\/Acc":30.23,
        "TruthQA MC2\/Acc":45.31,
        "TruthQA Gen\/ROUGE":44.92,
        "XSum\/ROUGE":6.13,
        "XSum\/factKB":47.64,
        "XSum\/BERT-P":23.97,
        "CNN-DM\/ROUGE":20.53,
        "CNN-DM\/factKB":84.93,
        "CNN-DM\/BERT-P":41.66,
        "RACE\/Acc":43.73,
        "SQUaDv2\/EM":11.99,
        "MemoTrap\/Acc":50.32,
        "IFEval\/Acc":28.47,
        "FaithDial\/Acc":69.74,
        "HaluQA\/Acc":52.31,
        "HaluSumm\/Acc":49.06,
        "HaluDial\/Acc":64.25,
        "FEVER\/Acc":73.55,
        "TrueFalse\/Acc":80.23,
        "Type":43.73,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":1415,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"meta-llama\/Llama-2-7b-hf",
        "NQ Open\/EM":27.12,
        "TriviaQA\/EM":9.22,
        "TruthQA MC1\/Acc":25.21,
        "TruthQA MC2\/Acc":38.97,
        "TruthQA Gen\/ROUGE":32.44,
        "XSum\/ROUGE":14.13,
        "XSum\/factKB":80.75,
        "XSum\/BERT-P":61.25,
        "CNN-DM\/ROUGE":24.73,
        "CNN-DM\/factKB":89.77,
        "CNN-DM\/BERT-P":52.77,
        "RACE\/Acc":39.52,
        "SQUaDv2\/EM":29.09,
        "MemoTrap\/Acc":39.32,
        "IFEval\/Acc":18.67,
        "FaithDial\/Acc":70.36,
        "HaluQA\/Acc":45.66,
        "HaluSumm\/Acc":42.79,
        "HaluDial\/Acc":49.97,
        "FEVER\/Acc":66.5,
        "TrueFalse\/Acc":72.69,
        "Type":39.52,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":679,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"microsoft\/Orca-2-13b",
        "NQ Open\/EM":17.78,
        "TriviaQA\/EM":9.79,
        "TruthQA MC1\/Acc":39.17,
        "TruthQA MC2\/Acc":55.76,
        "TruthQA Gen\/ROUGE":44.06,
        "XSum\/ROUGE":21.37,
        "XSum\/factKB":50.26,
        "XSum\/BERT-P":54.79,
        "CNN-DM\/ROUGE":0.74,
        "CNN-DM\/factKB":95.21,
        "CNN-DM\/BERT-P":1.44,
        "RACE\/Acc":46.22,
        "SQUaDv2\/EM":0.15,
        "MemoTrap\/Acc":43.06,
        "IFEval\/Acc":25.88,
        "FaithDial\/Acc":63.41,
        "HaluQA\/Acc":66.2,
        "HaluSumm\/Acc":56.38,
        "HaluDial\/Acc":78.98,
        "FEVER\/Acc":77.89,
        "TrueFalse\/Acc":88.59,
        "Type":46.22,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":"other",
        "#Params (B)":13.0,
        "Hub":552,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udcac",
        "Model":"mistralai\/Mistral-7B-Instruct-v0.1",
        "NQ Open\/EM":17.76,
        "TriviaQA\/EM":52.24,
        "TruthQA MC1\/Acc":39.17,
        "TruthQA MC2\/Acc":55.92,
        "TruthQA Gen\/ROUGE":54.83,
        "XSum\/ROUGE":20.76,
        "XSum\/factKB":49.26,
        "XSum\/BERT-P":54.96,
        "CNN-DM\/ROUGE":28.03,
        "CNN-DM\/factKB":98.62,
        "CNN-DM\/BERT-P":58.81,
        "RACE\/Acc":42.3,
        "SQUaDv2\/EM":33.46,
        "MemoTrap\/Acc":60.26,
        "IFEval\/Acc":30.5,
        "FaithDial\/Acc":69.96,
        "HaluQA\/Acc":49.17,
        "HaluSumm\/Acc":45.04,
        "HaluDial\/Acc":66.49,
        "FEVER\/Acc":75.26,
        "TrueFalse\/Acc":83.48,
        "Type":42.3,
        "Architecture":"MistralForCausalLM",
        "Precision":"float32",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1233,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udcac",
        "Model":"mistralai\/Mistral-7B-Instruct-v0.2",
        "NQ Open\/EM":24.71,
        "TriviaQA\/EM":63.89,
        "TruthQA MC1\/Acc":52.51,
        "TruthQA MC2\/Acc":66.82,
        "TruthQA Gen\/ROUGE":56.18,
        "XSum\/ROUGE":23.21,
        "XSum\/factKB":55.28,
        "XSum\/BERT-P":64.75,
        "CNN-DM\/ROUGE":24.43,
        "CNN-DM\/factKB":98.28,
        "CNN-DM\/BERT-P":52.51,
        "RACE\/Acc":45.93,
        "SQUaDv2\/EM":30.19,
        "MemoTrap\/Acc":43.27,
        "IFEval\/Acc":38.45,
        "FaithDial\/Acc":59.08,
        "HaluQA\/Acc":23.0,
        "HaluSumm\/Acc":44.65,
        "HaluDial\/Acc":71.53,
        "FEVER\/Acc":81.33,
        "TrueFalse\/Acc":89.06,
        "Type":45.93,
        "Architecture":"MistralForCausalLM",
        "Precision":"float32",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":455,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"mistralai\/Mistral-7B-v0.1",
        "NQ Open\/EM":30.11,
        "TriviaQA\/EM":70.86,
        "TruthQA MC1\/Acc":28.15,
        "TruthQA MC2\/Acc":42.63,
        "TruthQA Gen\/ROUGE":39.9,
        "XSum\/ROUGE":5.47,
        "XSum\/factKB":31.56,
        "XSum\/BERT-P":17.43,
        "CNN-DM\/ROUGE":8.76,
        "CNN-DM\/factKB":95.55,
        "CNN-DM\/BERT-P":17.78,
        "RACE\/Acc":40.96,
        "SQUaDv2\/EM":33.86,
        "MemoTrap\/Acc":39.42,
        "IFEval\/Acc":16.45,
        "FaithDial\/Acc":79.17,
        "HaluQA\/Acc":54.48,
        "HaluSumm\/Acc":43.13,
        "HaluDial\/Acc":59.74,
        "FEVER\/Acc":74.64,
        "TrueFalse\/Acc":85.41,
        "Type":40.96,
        "Architecture":"MistralForCausalLM",
        "Precision":"float32",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2087,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"?",
        "Model":"pankajmathur\/orca_mini_3b",
        "NQ Open\/EM":6.54,
        "TriviaQA\/EM":36.96,
        "TruthQA MC1\/Acc":28.03,
        "TruthQA MC2\/Acc":42.3,
        "TruthQA Gen\/ROUGE":40.15,
        "XSum\/ROUGE":6.43,
        "XSum\/factKB":76.45,
        "XSum\/BERT-P":30.07,
        "CNN-DM\/ROUGE":18.13,
        "CNN-DM\/factKB":96.5,
        "CNN-DM\/BERT-P":38.9,
        "RACE\/Acc":36.75,
        "SQUaDv2\/EM":19.01,
        "MemoTrap\/Acc":42.52,
        "IFEval\/Acc":6.28,
        "FaithDial\/Acc":68.21,
        "HaluQA\/Acc":44.46,
        "HaluSumm\/Acc":47.01,
        "HaluDial\/Acc":46.94,
        "FEVER\/Acc":64.73,
        "TrueFalse\/Acc":58.6,
        "Type":36.75,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"stabilityai\/StableBeluga-13B",
        "NQ Open\/EM":30.91,
        "TriviaQA\/EM":70.56,
        "TruthQA MC1\/Acc":34.52,
        "TruthQA MC2\/Acc":49.21,
        "TruthQA Gen\/ROUGE":40.88,
        "XSum\/ROUGE":14.57,
        "XSum\/factKB":79.95,
        "XSum\/BERT-P":53.85,
        "CNN-DM\/ROUGE":30.54,
        "CNN-DM\/factKB":95.29,
        "CNN-DM\/BERT-P":58.05,
        "RACE\/Acc":45.74,
        "SQUaDv2\/EM":14.16,
        "MemoTrap\/Acc":42.52,
        "IFEval\/Acc":23.48,
        "FaithDial\/Acc":82.9,
        "HaluQA\/Acc":27.56,
        "HaluSumm\/Acc":44.03,
        "HaluDial\/Acc":75.39,
        "FEVER\/Acc":80.24,
        "TrueFalse\/Acc":87.2,
        "Type":45.74,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":null,
        "#Params (B)":13.02,
        "Hub":112,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"stabilityai\/StableBeluga-7B",
        "NQ Open\/EM":26.18,
        "TriviaQA\/EM":9.27,
        "TruthQA MC1\/Acc":34.76,
        "TruthQA MC2\/Acc":50.08,
        "TruthQA Gen\/ROUGE":41.86,
        "XSum\/ROUGE":13.49,
        "XSum\/factKB":79.05,
        "XSum\/BERT-P":50.31,
        "CNN-DM\/ROUGE":25.2,
        "CNN-DM\/factKB":95.44,
        "CNN-DM\/BERT-P":50.24,
        "RACE\/Acc":43.64,
        "SQUaDv2\/EM":13.2,
        "MemoTrap\/Acc":47.44,
        "IFEval\/Acc":21.63,
        "FaithDial\/Acc":82.03,
        "HaluQA\/Acc":29.81,
        "HaluSumm\/Acc":40.52,
        "HaluDial\/Acc":57.0,
        "FEVER\/Acc":75.17,
        "TrueFalse\/Acc":84.11,
        "Type":43.64,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":null,
        "#Params (B)":6.74,
        "Hub":126,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"teknium\/OpenHermes-2-Mistral-7B",
        "NQ Open\/EM":29.72,
        "TriviaQA\/EM":68.05,
        "TruthQA MC1\/Acc":33.9,
        "TruthQA MC2\/Acc":50.93,
        "TruthQA Gen\/ROUGE":45.53,
        "XSum\/ROUGE":24.77,
        "XSum\/factKB":41.85,
        "XSum\/BERT-P":63.55,
        "CNN-DM\/ROUGE":31.48,
        "CNN-DM\/factKB":96.32,
        "CNN-DM\/BERT-P":60.33,
        "RACE\/Acc":43.92,
        "SQUaDv2\/EM":34.4,
        "MemoTrap\/Acc":47.01,
        "IFEval\/Acc":22.55,
        "FaithDial\/Acc":83.64,
        "HaluQA\/Acc":66.05,
        "HaluSumm\/Acc":54.64,
        "HaluDial\/Acc":78.17,
        "FEVER\/Acc":82.2,
        "TrueFalse\/Acc":88.96,
        "Type":43.92,
        "Architecture":"MistralForCausalLM",
        "Precision":"float32",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":216,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"teknium\/OpenHermes-2.5-Mistral-7B",
        "NQ Open\/EM":26.57,
        "TriviaQA\/EM":65.88,
        "TruthQA MC1\/Acc":35.99,
        "TruthQA MC2\/Acc":53.05,
        "TruthQA Gen\/ROUGE":49.94,
        "XSum\/ROUGE":24.82,
        "XSum\/factKB":35.45,
        "XSum\/BERT-P":61.73,
        "CNN-DM\/ROUGE":31.01,
        "CNN-DM\/factKB":95.32,
        "CNN-DM\/BERT-P":59.88,
        "RACE\/Acc":44.78,
        "SQUaDv2\/EM":6.81,
        "MemoTrap\/Acc":51.5,
        "IFEval\/Acc":30.5,
        "FaithDial\/Acc":83.89,
        "HaluQA\/Acc":51.35,
        "HaluSumm\/Acc":59.09,
        "HaluDial\/Acc":78.26,
        "FEVER\/Acc":82.47,
        "TrueFalse\/Acc":88.79,
        "Type":44.78,
        "Architecture":"MistralForCausalLM",
        "Precision":"float32",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":394,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"teknium\/OpenHermes-2.5-Mistral-7B",
        "NQ Open\/EM":26.62,
        "TriviaQA\/EM":65.98,
        "TruthQA MC1\/Acc":35.99,
        "TruthQA MC2\/Acc":52.99,
        "TruthQA Gen\/ROUGE":49.69,
        "XSum\/ROUGE":21.86,
        "XSum\/factKB":35.61,
        "XSum\/BERT-P":52.96,
        "CNN-DM\/ROUGE":30.98,
        "CNN-DM\/factKB":95.34,
        "CNN-DM\/BERT-P":59.84,
        "RACE\/Acc":44.59,
        "SQUaDv2\/EM":7.44,
        "MemoTrap\/Acc":51.5,
        "IFEval\/Acc":29.02,
        "FaithDial\/Acc":84.01,
        "HaluQA\/Acc":50.65,
        "HaluSumm\/Acc":59.02,
        "HaluDial\/Acc":78.62,
        "FEVER\/Acc":82.57,
        "TrueFalse\/Acc":88.69,
        "Type":44.59,
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":578,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udcac",
        "Model":"tiiuae\/falcon-7b-instruct",
        "NQ Open\/EM":13.57,
        "TriviaQA\/EM":39.16,
        "TruthQA MC1\/Acc":28.76,
        "TruthQA MC2\/Acc":44.08,
        "TruthQA Gen\/ROUGE":38.43,
        "XSum\/ROUGE":11.86,
        "XSum\/factKB":67.47,
        "XSum\/BERT-P":39.37,
        "CNN-DM\/ROUGE":23.77,
        "CNN-DM\/factKB":94.91,
        "CNN-DM\/BERT-P":45.65,
        "RACE\/Acc":37.22,
        "SQUaDv2\/EM":22.63,
        "MemoTrap\/Acc":61.22,
        "IFEval\/Acc":15.34,
        "FaithDial\/Acc":73.58,
        "HaluQA\/Acc":29.72,
        "HaluSumm\/Acc":44.02,
        "HaluDial\/Acc":38.55,
        "FEVER\/Acc":57.54,
        "TrueFalse\/Acc":63.65,
        "Type":37.22,
        "Architecture":"FalconForCausalLM",
        "Precision":"float32",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":757,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"tiiuae\/falcon-7b",
        "NQ Open\/EM":22.08,
        "TriviaQA\/EM":58.67,
        "TruthQA MC1\/Acc":22.4,
        "TruthQA MC2\/Acc":34.27,
        "TruthQA Gen\/ROUGE":25.34,
        "XSum\/ROUGE":4.36,
        "XSum\/factKB":39.55,
        "XSum\/BERT-P":17.89,
        "CNN-DM\/ROUGE":7.36,
        "CNN-DM\/factKB":95.59,
        "CNN-DM\/BERT-P":17.06,
        "RACE\/Acc":37.42,
        "SQUaDv2\/EM":26.42,
        "MemoTrap\/Acc":38.89,
        "IFEval\/Acc":11.65,
        "FaithDial\/Acc":78.98,
        "HaluQA\/Acc":46.72,
        "HaluSumm\/Acc":44.8,
        "HaluDial\/Acc":42.03,
        "FEVER\/Acc":54.8,
        "TrueFalse\/Acc":58.14,
        "Type":37.42,
        "Architecture":"FalconForCausalLM",
        "Precision":"float32",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":941,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"togethercomputer\/GPT-JT-6B-v1",
        "NQ Open\/EM":10.91,
        "TriviaQA\/EM":41.73,
        "TruthQA MC1\/Acc":22.64,
        "TruthQA MC2\/Acc":37.07,
        "TruthQA Gen\/ROUGE":34.76,
        "XSum\/ROUGE":25.59,
        "XSum\/factKB":39.13,
        "XSum\/BERT-P":71.85,
        "CNN-DM\/ROUGE":24.34,
        "CNN-DM\/factKB":97.0,
        "CNN-DM\/BERT-P":67.6,
        "RACE\/Acc":36.46,
        "SQUaDv2\/EM":31.77,
        "MemoTrap\/Acc":41.88,
        "IFEval\/Acc":14.6,
        "FaithDial\/Acc":50.58,
        "HaluQA\/Acc":50.93,
        "HaluSumm\/Acc":52.24,
        "HaluDial\/Acc":49.79,
        "FEVER\/Acc":58.54,
        "TrueFalse\/Acc":62.71,
        "Type":36.46,
        "Architecture":"GPTJForCausalLM",
        "Precision":"float32",
        "Hub License":"apache-2.0",
        "#Params (B)":6.0,
        "Hub":301,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"togethercomputer\/LLaMA-2-7B-32K",
        "NQ Open\/EM":24.35,
        "TriviaQA\/EM":8.16,
        "TruthQA MC1\/Acc":25.58,
        "TruthQA MC2\/Acc":38.41,
        "TruthQA Gen\/ROUGE":28.64,
        "XSum\/ROUGE":16.36,
        "XSum\/factKB":71.58,
        "XSum\/BERT-P":58.38,
        "CNN-DM\/ROUGE":21.77,
        "CNN-DM\/factKB":95.23,
        "CNN-DM\/BERT-P":51.87,
        "RACE\/Acc":37.7,
        "SQUaDv2\/EM":12.35,
        "MemoTrap\/Acc":35.47,
        "IFEval\/Acc":14.97,
        "FaithDial\/Acc":75.28,
        "HaluQA\/Acc":48.14,
        "HaluSumm\/Acc":49.04,
        "HaluDial\/Acc":54.78,
        "FEVER\/Acc":71.17,
        "TrueFalse\/Acc":80.59,
        "Type":37.7,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":483,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udcac",
        "Model":"togethercomputer\/Llama-2-7B-32K-Instruct",
        "NQ Open\/EM":23.46,
        "TriviaQA\/EM":9.27,
        "TruthQA MC1\/Acc":30.97,
        "TruthQA MC2\/Acc":45.73,
        "TruthQA Gen\/ROUGE":42.96,
        "XSum\/ROUGE":20.4,
        "XSum\/factKB":62.45,
        "XSum\/BERT-P":64.45,
        "CNN-DM\/ROUGE":27.71,
        "CNN-DM\/factKB":98.53,
        "CNN-DM\/BERT-P":57.88,
        "RACE\/Acc":40.0,
        "SQUaDv2\/EM":13.96,
        "MemoTrap\/Acc":33.76,
        "IFEval\/Acc":17.01,
        "FaithDial\/Acc":74.06,
        "HaluQA\/Acc":32.33,
        "HaluSumm\/Acc":46.96,
        "HaluDial\/Acc":60.43,
        "FEVER\/Acc":72.61,
        "TrueFalse\/Acc":81.96,
        "Type":40.0,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":141,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udcac",
        "Model":"upstage\/SOLAR-10.7B-Instruct-v1.0",
        "NQ Open\/EM":19.92,
        "TriviaQA\/EM":41.98,
        "TruthQA MC1\/Acc":57.77,
        "TruthQA MC2\/Acc":71.71,
        "TruthQA Gen\/ROUGE":50.31,
        "XSum\/ROUGE":6.24,
        "XSum\/factKB":54.34,
        "XSum\/BERT-P":20.64,
        "CNN-DM\/ROUGE":4.2,
        "CNN-DM\/factKB":95.79,
        "CNN-DM\/BERT-P":8.73,
        "RACE\/Acc":41.91,
        "SQUaDv2\/EM":0.87,
        "MemoTrap\/Acc":43.7,
        "IFEval\/Acc":31.42,
        "FaithDial\/Acc":82.57,
        "HaluQA\/Acc":47.38,
        "HaluSumm\/Acc":55.74,
        "HaluDial\/Acc":78.46,
        "FEVER\/Acc":82.78,
        "TrueFalse\/Acc":89.33,
        "Type":41.91,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float32",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":378,
        "Available on the hub":true,
        "Model sha":"main"
    },
    {
        "T":"\ud83d\udcac",
        "Model":"upstage\/SOLAR-10.7B-Instruct-v1.0",
        "NQ Open\/EM":18.45,
        "TriviaQA\/EM":38.3,
        "TruthQA MC1\/Acc":57.77,
        "TruthQA MC2\/Acc":71.72,
        "TruthQA Gen\/ROUGE":50.43,
        "XSum\/ROUGE":0.02,
        "XSum\/factKB":18.13,
        "XSum\/BERT-P":0.04,
        "CNN-DM\/ROUGE":0.0,
        "CNN-DM\/factKB":95.08,
        "CNN-DM\/BERT-P":0.01,
        "RACE\/Acc":41.82,
        "SQUaDv2\/EM":0.88,
        "MemoTrap\/Acc":43.7,
        "IFEval\/Acc":31.98,
        "FaithDial\/Acc":82.57,
        "HaluQA\/Acc":47.37,
        "HaluSumm\/Acc":55.78,
        "HaluDial\/Acc":78.46,
        "FEVER\/Acc":82.79,
        "TrueFalse\/Acc":89.4,
        "Type":41.82,
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":482,
        "Available on the hub":true,
        "Model sha":"main"
    }
]