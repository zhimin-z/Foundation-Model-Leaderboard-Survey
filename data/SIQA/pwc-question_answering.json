[
    {
        "table_id":20295,
        "row_id":108558,
        "rank":1,
        "Model":"phi-1.5-web 1.3B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"phi-1.5-web 1.3B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-11",
        "metrics":{
            "Accuracy":"53.0"
        },
        "raw_metrics":{
            "Accuracy":53.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1275377,
            "title":"Textbooks Are All You Need II: phi-1.5 technical report",
            "url":"\/paper\/textbooks-are-all-you-need-ii-phi-1-5",
            "published":"2023-09-11T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/textbooks-are-all-you-need-ii-phi-1-5\/review\/?hl=108558"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20295,
        "row_id":108559,
        "rank":2,
        "Model":"phi-1.5 1.3B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"phi-1.5 1.3B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-11",
        "metrics":{
            "Accuracy":"52.6"
        },
        "raw_metrics":{
            "Accuracy":52.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1275377,
            "title":"Textbooks Are All You Need II: phi-1.5 technical report",
            "url":"\/paper\/textbooks-are-all-you-need-ii-phi-1-5",
            "published":"2023-09-11T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/textbooks-are-all-you-need-ii-phi-1-5\/review\/?hl=108559"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20295,
        "row_id":97610,
        "rank":3,
        "Model":"LLaMA 65B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 65B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"52.3"
        },
        "raw_metrics":{
            "Accuracy":52.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97610"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20295,
        "row_id":66314,
        "rank":4,
        "Model":"Chinchilla (zero-shot)",
        "mlmodel":{

        },
        "method_short":"Chinchilla ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-29",
        "metrics":{
            "Accuracy":"51.3"
        },
        "raw_metrics":{
            "Accuracy":51.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":985465,
            "title":"Training Compute-Optimal Large Language Models",
            "url":"\/paper\/training-compute-optimal-large-language",
            "published":"2022-03-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":20295,
        "row_id":66315,
        "rank":5,
        "Model":"Gopher (zero-shot)",
        "mlmodel":{

        },
        "method_short":"Gopher ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-08",
        "metrics":{
            "Accuracy":"50.6"
        },
        "raw_metrics":{
            "Accuracy":50.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":942590,
            "title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
            "url":"\/paper\/scaling-language-models-methods-analysis-1",
            "published":"2021-12-08T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":20295,
        "row_id":97608,
        "rank":6,
        "Model":"LLaMA 13B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 13B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"50.4"
        },
        "raw_metrics":{
            "Accuracy":50.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97608"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20295,
        "row_id":97609,
        "rank":7,
        "Model":"LLaMA 33B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 33B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"50.4"
        },
        "raw_metrics":{
            "Accuracy":50.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97609"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20295,
        "row_id":97607,
        "rank":8,
        "Model":"LLaMA 7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"48.9"
        },
        "raw_metrics":{
            "Accuracy":48.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97607"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]