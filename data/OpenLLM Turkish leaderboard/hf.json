[
    {
        "T":"\ud83d\udd36",
        "Model":"TURKCELL\/Turkcell-LLM-7b-v1",
        "Average":38.18,
        "ARC":35.16,
        "HellaSwag":33.32,
        "MMLU":31.94,
        "TruthfulQA":40.89,
        "Winogrande":30.35,
        "GSM8K":57.42,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.38,
        "Hub":6,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NovusResearch\/Novus-7b-tr_v1",
        "Average":37.87,
        "ARC":41.89,
        "HellaSwag":41.36,
        "MMLU":28.35,
        "TruthfulQA":33.55,
        "Winogrande":30.5,
        "GSM8K":51.58,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Trendyol\/Trendyol-LLM-7b-chat-dpo-v1.0",
        "Average":35.57,
        "ARC":46.2,
        "HellaSwag":39.33,
        "MMLU":28.61,
        "TruthfulQA":37.12,
        "Winogrande":5.69,
        "GSM8K":56.48,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.34,
        "Hub":9,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"?",
        "Model":"NousResearch\/Hermes-2-Pro-Mistral-7B",
        "Average":34.89,
        "ARC":38.36,
        "HellaSwag":37.52,
        "MMLU":23.83,
        "TruthfulQA":31.31,
        "Winogrande":25.72,
        "GSM8K":52.61,
        "Type":"",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NovusResearch\/Thestral-0.1-tr-chat-7B",
        "Average":34.62,
        "ARC":40.76,
        "HellaSwag":37.91,
        "MMLU":27.5,
        "TruthfulQA":33.92,
        "Winogrande":16.84,
        "GSM8K":50.79,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sambanovasystems\/SambaLingo-Turkish-Chat",
        "Average":34.38,
        "ARC":34.35,
        "HellaSwag":36.23,
        "MMLU":32.71,
        "TruthfulQA":43.16,
        "Winogrande":5.46,
        "GSM8K":54.34,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.95,
        "Hub":30,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"malhajar\/Mistral-7B-Instruct-v0.2-turkish",
        "Average":34.19,
        "ARC":38.21,
        "HellaSwag":38.62,
        "MMLU":26.39,
        "TruthfulQA":33.4,
        "Winogrande":17.91,
        "GSM8K":50.63,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\u2b55",
        "Model":"umarigan\/Hermes-7B-TR",
        "Average":34.14,
        "ARC":38.81,
        "HellaSwag":36.33,
        "MMLU":29.89,
        "TruthfulQA":37.46,
        "Winogrande":5.39,
        "GSM8K":56.95,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"?",
        "Model":"Eurdem\/Pinokio_v1.0",
        "Average":33.95,
        "ARC":40.18,
        "HellaSwag":39.37,
        "MMLU":28.52,
        "TruthfulQA":34.61,
        "Winogrande":8.35,
        "GSM8K":52.69,
        "Type":"",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"?",
        "Model":"malhajar\/Qwen1.5-7B-turkish",
        "Average":33.74,
        "ARC":36.16,
        "HellaSwag":41.17,
        "MMLU":22.97,
        "TruthfulQA":31.52,
        "Winogrande":19.65,
        "GSM8K":50.95,
        "Type":"",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the Hub":false,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"umarigan\/Trendyol-LLM-7b-chat-v0.1-DPO",
        "Average":33.43,
        "ARC":38.71,
        "HellaSwag":35.54,
        "MMLU":30.23,
        "TruthfulQA":35.58,
        "Winogrande":3.79,
        "GSM8K":56.71,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.84,
        "Hub":2,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\u2b55",
        "Model":"Trendyol\/Trendyol-LLM-7b-chat-v1.0",
        "Average":33.23,
        "ARC":38.86,
        "HellaSwag":35.21,
        "MMLU":28.61,
        "TruthfulQA":35.46,
        "Winogrande":5.08,
        "GSM8K":56.16,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.34,
        "Hub":2,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"malhajar\/Mistral-7B-v0.2-meditron-turkish",
        "Average":33.05,
        "ARC":37.15,
        "HellaSwag":38.89,
        "MMLU":24.77,
        "TruthfulQA":32.91,
        "Winogrande":12.29,
        "GSM8K":52.29,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"?",
        "Model":"mistralai\/Mistral-7B-v0.1",
        "Average":32.64,
        "ARC":37.73,
        "HellaSwag":36.85,
        "MMLU":23.57,
        "TruthfulQA":31.96,
        "Winogrande":15.1,
        "GSM8K":50.63,
        "Type":"",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\u2b55",
        "Model":"Commencis\/Commencis-LLM",
        "Average":31.06,
        "ARC":36.43,
        "HellaSwag":39.34,
        "MMLU":26.22,
        "TruthfulQA":33.19,
        "Winogrande":0.15,
        "GSM8K":51.03,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Trendyol\/Trendyol-LLM-7b-chat-v0.1",
        "Average":30.82,
        "ARC":31.88,
        "HellaSwag":35.68,
        "MMLU":27.24,
        "TruthfulQA":34.64,
        "Winogrande":1.29,
        "GSM8K":54.19,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.84,
        "Hub":80,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"notbdq\/mistral-turkish-v2",
        "Average":30.29,
        "ARC":32.68,
        "HellaSwag":37.33,
        "MMLU":26.99,
        "TruthfulQA":32.67,
        "Winogrande":0.3,
        "GSM8K":51.74,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"asafaya\/kanarya-2b",
        "Average":28.76,
        "ARC":23.3,
        "HellaSwag":35.89,
        "MMLU":24.17,
        "TruthfulQA":35.14,
        "Winogrande":1.21,
        "GSM8K":52.84,
        "Type":"pretrained",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.06,
        "Hub":28,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"myzens\/XGLM_TR_FineTune_alpha-original",
        "Average":28.1,
        "ARC":24.83,
        "HellaSwag":34.49,
        "MMLU":22.54,
        "TruthfulQA":33.09,
        "Winogrande":1.9,
        "GSM8K":51.74,
        "Type":"fine-tuned",
        "Architecture":"XGLMForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.49,
        "Hub":2,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"asafaya\/kanarya-750m",
        "Average":27.86,
        "ARC":22.97,
        "HellaSwag":36.16,
        "MMLU":23.65,
        "TruthfulQA":32.85,
        "Winogrande":1.21,
        "GSM8K":50.32,
        "Type":"pretrained",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.74,
        "Hub":5,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"ytu-ce-cosmos\/turkish-gpt2-large",
        "Average":27.53,
        "ARC":24.31,
        "HellaSwag":37.92,
        "MMLU":20.75,
        "TruthfulQA":32.06,
        "Winogrande":0.3,
        "GSM8K":49.84,
        "Type":"pretrained",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.77,
        "Hub":16,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ytu-ce-cosmos\/turkish-gpt2-large-750m-instruct-v0.1",
        "Average":27.11,
        "ARC":23.44,
        "HellaSwag":37.0,
        "MMLU":20.83,
        "TruthfulQA":32.35,
        "Winogrande":0.0,
        "GSM8K":49.05,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.77,
        "Hub":4,
        "Available on the Hub":true,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"boun-tabi-LMG\/TURNA",
        "Average":26.7,
        "ARC":23.07,
        "HellaSwag":37.07,
        "MMLU":21.86,
        "TruthfulQA":29.45,
        "Winogrande":0.0,
        "GSM8K":48.74,
        "Type":"pretrained",
        "Architecture":"T5ForConditionalGeneration",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":1.14,
        "Hub":48,
        "Available on the Hub":true,
        "Model Sha":""
    }
]