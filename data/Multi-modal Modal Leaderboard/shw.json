[
    {
        "Model":"GPT-4v\nVerified\n\n\nOpenAI",
        "Time":"2023\/12\/23",
        "Params":null,
        "Language Model":null,
        "Vision Model":null,
        "Avg. Rank":3.5,
        "Avg. Score":63.1,
        "MMBench Test":77.0,
        "MMBench-CN Test":74.4,
        "CCBench":46.5,
        "MME (normalized)":63.3,
        "SEEDBench_IMG":71.6,
        "MMVet":56.8,
        "MMMU Val":53.8,
        "MathVista MiniTest":47.8,
        "HallusionBench Avg.":46.5,
        "LLaVABench (in-the-wild)":93.1
    },
    {
        "Model":"InternLM-XComposer2-VL\nVerified\n\n\nShanghai AI Lab",
        "Time":"2024\/01\/26",
        "Params":"7B",
        "Language Model":"InternLM2",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":2.5,
        "Avg. Score":62.8,
        "MMBench Test":79.6,
        "MMBench-CN Test":77.6,
        "CCBench":47.8,
        "MME (normalized)":80.1,
        "SEEDBench_IMG":75.9,
        "MMVet":51.2,
        "MMMU Val":42.0,
        "MathVista MiniTest":57.6,
        "HallusionBench Avg.":41.1,
        "LLaVABench (in-the-wild)":75.5
    },
    {
        "Model":"GeminiProVision\nVerified\n\n\nGoogle",
        "Time":"2023\/12\/23",
        "Params":null,
        "Language Model":null,
        "Vision Model":null,
        "Avg. Rank":2.8,
        "Avg. Score":62.7,
        "MMBench Test":73.6,
        "MMBench-CN Test":74.3,
        "CCBench":52.5,
        "MME (normalized)":76.7,
        "SEEDBench_IMG":70.7,
        "MMVet":59.2,
        "MMMU Val":48.9,
        "MathVista MiniTest":45.8,
        "HallusionBench Avg.":45.2,
        "LLaVABench (in-the-wild)":79.9
    },
    {
        "Model":"Qwen-VL-Plus\nVerified\n\n\nAlibaba Group",
        "Time":"2024\/01\/10",
        "Params":null,
        "Language Model":"QwenLM",
        "Vision Model":null,
        "Avg. Rank":6.3,
        "Avg. Score":58.5,
        "MMBench Test":67.0,
        "MMBench-CN Test":70.7,
        "CCBench":55.1,
        "MME (normalized)":79.6,
        "SEEDBench_IMG":65.7,
        "MMVet":55.7,
        "MMMU Val":40.9,
        "MathVista MiniTest":36.0,
        "HallusionBench Avg.":40.6,
        "LLaVABench (in-the-wild)":73.7
    },
    {
        "Model":"Monkey-Chat\nVerified\n\n\nHuazhong University of Science and Technology",
        "Time":"2024\/01\/16",
        "Params":"9.8B",
        "Language Model":"Qwen-7B",
        "Vision Model":"ViT-BigHuge",
        "Avg. Rank":7.2,
        "Avg. Score":54.0,
        "MMBench Test":72.4,
        "MMBench-CN Test":67.5,
        "CCBench":48.0,
        "MME (normalized)":67.4,
        "SEEDBench_IMG":68.9,
        "MMVet":41.0,
        "MMMU Val":40.7,
        "MathVista MiniTest":34.8,
        "HallusionBench Avg.":39.3,
        "LLaVABench (in-the-wild)":60.5
    },
    {
        "Model":"LLaVA-InternLM2-20B (QLoRA)\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2024\/01\/17",
        "Params":"20.2B",
        "Language Model":"InternLM2-20B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":11.2,
        "Avg. Score":52.3,
        "MMBench Test":75.1,
        "MMBench-CN Test":73.7,
        "CCBench":46.3,
        "MME (normalized)":66.7,
        "SEEDBench_IMG":70.2,
        "MMVet":37.2,
        "MMMU Val":39.4,
        "MathVista MiniTest":24.6,
        "HallusionBench Avg.":26.4,
        "LLaVABench (in-the-wild)":63.7
    },
    {
        "Model":"InternLM-XComposer-VL\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2023\/9\/26",
        "Params":"8B",
        "Language Model":"InternLM-7B",
        "Vision Model":"EVA-G",
        "Avg. Rank":11.2,
        "Avg. Score":52.0,
        "MMBench Test":74.4,
        "MMBench-CN Test":72.4,
        "CCBench":49.6,
        "MME (normalized)":66.9,
        "SEEDBench_IMG":66.1,
        "MMVet":35.2,
        "MMMU Val":35.6,
        "MathVista MiniTest":29.5,
        "HallusionBench Avg.":36.0,
        "LLaVABench (in-the-wild)":53.8
    },
    {
        "Model":"Qwen-VL-Chat\nVerified\n\n\nAlibaba Group",
        "Time":"2023\/8\/24",
        "Params":"9.6B",
        "Language Model":"Qwen-7B",
        "Vision Model":"ViT-G\/16",
        "Avg. Rank":12.0,
        "Avg. Score":51.3,
        "MMBench Test":61.8,
        "MMBench-CN Test":56.3,
        "CCBench":41.2,
        "MME (normalized)":66.4,
        "SEEDBench_IMG":64.8,
        "MMVet":47.3,
        "MMMU Val":37.0,
        "MathVista MiniTest":33.8,
        "HallusionBench Avg.":36.8,
        "LLaVABench (in-the-wild)":67.7
    },
    {
        "Model":"CogVLM-17B-Chat\nVerified\n\n\nZhipu AI",
        "Time":"2024\/01\/03",
        "Params":"17B",
        "Language Model":"Vicuna-v1.5-7B",
        "Vision Model":"EVA2-CLIP-E",
        "Avg. Rank":12.8,
        "Avg. Score":50.7,
        "MMBench Test":65.8,
        "MMBench-CN Test":55.9,
        "CCBench":19.0,
        "MME (normalized)":62.0,
        "SEEDBench_IMG":68.8,
        "MMVet":54.5,
        "MMMU Val":37.3,
        "MathVista MiniTest":34.7,
        "HallusionBench Avg.":35.4,
        "LLaVABench (in-the-wild)":73.9
    },
    {
        "Model":"TransCore-M\nVerified\n\n\nPCI Research",
        "Time":"2023\/11\/30",
        "Params":"13.4B",
        "Language Model":"PCITransGPT-13B",
        "Vision Model":"CLIP ViT\/L-14",
        "Avg. Rank":11.2,
        "Avg. Score":50.5,
        "MMBench Test":71.4,
        "MMBench-CN Test":67.8,
        "CCBench":32.7,
        "MME (normalized)":67.8,
        "SEEDBench_IMG":71.2,
        "MMVet":38.8,
        "MMMU Val":36.4,
        "MathVista MiniTest":27.8,
        "HallusionBench Avg.":29.0,
        "LLaVABench (in-the-wild)":61.7
    },
    {
        "Model":"LLaVA-InternLM2-7B (QLoRA)\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2024\/01\/17",
        "Params":"8.1B",
        "Language Model":"InternLM2-7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":13.4,
        "Avg. Score":50.4,
        "MMBench Test":73.3,
        "MMBench-CN Test":71.7,
        "CCBench":42.5,
        "MME (normalized)":60.7,
        "SEEDBench_IMG":71.2,
        "MMVet":35.9,
        "MMMU Val":40.1,
        "MathVista MiniTest":25.5,
        "HallusionBench Avg.":26.7,
        "LLaVABench (in-the-wild)":56.2
    },
    {
        "Model":"ShareGPT4V-13B\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2023\/12\/08",
        "Params":"13.4B",
        "Language Model":"Vicuna-v1.5-13B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":12.5,
        "Avg. Score":49.8,
        "MMBench Test":68.9,
        "MMBench-CN Test":64.7,
        "CCBench":30.6,
        "MME (normalized)":65.3,
        "SEEDBench_IMG":70.7,
        "MMVet":39.2,
        "MMMU Val":34.8,
        "MathVista MiniTest":27.8,
        "HallusionBench Avg.":29.4,
        "LLaVABench (in-the-wild)":66.6
    },
    {
        "Model":"Yi-VL-6B\nVerified\n\n\n01-AI",
        "Time":"2024\/01\/25",
        "Params":"6.6B",
        "Language Model":"Yi-6B",
        "Vision Model":"CLIP ViT-H\/14",
        "Avg. Rank":12.5,
        "Avg. Score":49.8,
        "MMBench Test":67.9,
        "MMBench-CN Test":66.1,
        "CCBench":40.8,
        "MME (normalized)":68.6,
        "SEEDBench_IMG":67.6,
        "MMVet":31.1,
        "MMMU Val":39.9,
        "MathVista MiniTest":28.0,
        "HallusionBench Avg.":36.7,
        "LLaVABench (in-the-wild)":51.5
    },
    {
        "Model":"LLaVA-v1.5-13B\nVerified\n\n\nUniversity of Wisconsin\u2013Madison",
        "Time":"2023\/10\/05",
        "Params":"13.4B",
        "Language Model":"Vicuna-v1.5-13B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":14.8,
        "Avg. Score":48.5,
        "MMBench Test":67.8,
        "MMBench-CN Test":63.3,
        "CCBench":29.8,
        "MME (normalized)":64.5,
        "SEEDBench_IMG":68.1,
        "MMVet":38.3,
        "MMMU Val":36.9,
        "MathVista MiniTest":26.4,
        "HallusionBench Avg.":25.7,
        "LLaVABench (in-the-wild)":64.6
    },
    {
        "Model":"LLaVA-v1.5-13B (QLoRA)\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2023\/12\/28",
        "Params":"13.4B",
        "Language Model":"Vicuna-v1.5-13B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":15.8,
        "Avg. Score":48.4,
        "MMBench Test":68.8,
        "MMBench-CN Test":64.7,
        "CCBench":32.9,
        "MME (normalized)":63.1,
        "SEEDBench_IMG":67.9,
        "MMVet":35.9,
        "MMMU Val":35.2,
        "MathVista MiniTest":26.2,
        "HallusionBench Avg.":26.2,
        "LLaVABench (in-the-wild)":63.6
    },
    {
        "Model":"ShareGPT4V-7B\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2023\/12\/08",
        "Params":"7.2B",
        "Language Model":"Vicuna-v1.5-7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":15.2,
        "Avg. Score":48.3,
        "MMBench Test":66.4,
        "MMBench-CN Test":59.4,
        "CCBench":30.6,
        "MME (normalized)":66.9,
        "SEEDBench_IMG":69.3,
        "MMVet":34.7,
        "MMMU Val":36.6,
        "MathVista MiniTest":25.8,
        "HallusionBench Avg.":28.5,
        "LLaVABench (in-the-wild)":64.9
    },
    {
        "Model":"LLaVA-InternLM-7B (QLoRA)\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2023\/12\/28",
        "Params":"7.6B",
        "Language Model":"InternLM-7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":16.0,
        "Avg. Score":48.1,
        "MMBench Test":69.0,
        "MMBench-CN Test":66.7,
        "CCBench":37.3,
        "MME (normalized)":58.5,
        "SEEDBench_IMG":65.7,
        "MMVet":32.4,
        "MMMU Val":36.9,
        "MathVista MiniTest":26.3,
        "HallusionBench Avg.":28.9,
        "LLaVABench (in-the-wild)":59.7
    },
    {
        "Model":"Yi-VL-34B\nVerified\n\n\n01-AI",
        "Time":"2024\/01\/25",
        "Params":"34.6B",
        "Language Model":"Yi-34B",
        "Vision Model":"CLIP ViT-H\/14",
        "Avg. Rank":16.9,
        "Avg. Score":47.5,
        "MMBench Test":62.8,
        "MMBench-CN Test":60.5,
        "CCBench":36.3,
        "MME (normalized)":64.1,
        "SEEDBench_IMG":60.5,
        "MMVet":30.8,
        "MMMU Val":44.7,
        "MathVista MiniTest":25.6,
        "HallusionBench Avg.":32.5,
        "LLaVABench (in-the-wild)":56.9
    },
    {
        "Model":"Monkey\nVerified\n\n\nHuazhong University of Science and Technology",
        "Time":"2024\/01\/11",
        "Params":"9.8B",
        "Language Model":"Qwen-7B",
        "Vision Model":"ViT-BigHuge",
        "Avg. Rank":16.8,
        "Avg. Score":46.6,
        "MMBench Test":59.6,
        "MMBench-CN Test":54.7,
        "CCBench":37.1,
        "MME (normalized)":62.9,
        "SEEDBench_IMG":64.3,
        "MMVet":38.1,
        "MMMU Val":38.9,
        "MathVista MiniTest":32.5,
        "HallusionBench Avg.":34.9,
        "LLaVABench (in-the-wild)":43.3
    },
    {
        "Model":"SharedCaptioner\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2024\/01\/03",
        "Params":"8B",
        "Language Model":"InternLM-7B",
        "Vision Model":"EVA-G",
        "Avg. Rank":17.9,
        "Avg. Score":46.5,
        "MMBench Test":66.5,
        "MMBench-CN Test":60.9,
        "CCBench":40.4,
        "MME (normalized)":58.7,
        "SEEDBench_IMG":61.2,
        "MMVet":30.1,
        "MMMU Val":36.3,
        "MathVista MiniTest":29.0,
        "HallusionBench Avg.":34.2,
        "LLaVABench (in-the-wild)":47.4
    },
    {
        "Model":"LLaVA-v1.5-7B\nVerified\n\n\nUniversity of Wisconsin\u2013Madison",
        "Time":"2023\/10\/05",
        "Params":"7.2B",
        "Language Model":"Vicuna-v1.5-7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":19.9,
        "Avg. Score":45.9,
        "MMBench Test":65.2,
        "MMBench-CN Test":57.3,
        "CCBench":26.7,
        "MME (normalized)":63.4,
        "SEEDBench_IMG":65.6,
        "MMVet":32.7,
        "MMMU Val":36.2,
        "MathVista MiniTest":23.6,
        "HallusionBench Avg.":27.4,
        "LLaVABench (in-the-wild)":60.7
    },
    {
        "Model":"LLaVA-v1.5-7B (QLoRA)\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2023\/12\/28",
        "Params":"7.2B",
        "Language Model":"Vicuna-v1.5-7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":20.4,
        "Avg. Score":45.7,
        "MMBench Test":67.7,
        "MMBench-CN Test":61.0,
        "CCBench":28.4,
        "MME (normalized)":61.3,
        "SEEDBench_IMG":66.4,
        "MMVet":32.2,
        "MMMU Val":33.7,
        "MathVista MiniTest":24.2,
        "HallusionBench Avg.":25.2,
        "LLaVABench (in-the-wild)":57.2
    },
    {
        "Model":"Emu2_chat\nVerified\n\n\nBeijing Academy of Artificial Intelligence",
        "Time":"2024\/01\/17",
        "Params":"37B",
        "Language Model":"LLaMA-33B",
        "Vision Model":"EVA-02-CLIP-E-plus",
        "Avg. Rank":19.2,
        "Avg. Score":43.9,
        "MMBench Test":63.6,
        "MMBench-CN Test":45.9,
        "CCBench":18.8,
        "MME (normalized)":59.9,
        "SEEDBench_IMG":68.9,
        "MMVet":31.0,
        "MMMU Val":35.0,
        "MathVista MiniTest":30.0,
        "HallusionBench Avg.":29.5,
        "LLaVABench (in-the-wild)":56.4
    },
    {
        "Model":"mPLUG-Owl2\nVerified\n\n\nDAMO Academy",
        "Time":"2023\/11\/10",
        "Params":"8.2B",
        "Language Model":"LLaMA2 7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":20.0,
        "Avg. Score":43.6,
        "MMBench Test":66.0,
        "MMBench-CN Test":60.3,
        "CCBench":31.0,
        "MME (normalized)":63.8,
        "SEEDBench_IMG":64.5,
        "MMVet":35.7,
        "MMMU Val":34.7,
        "MathVista MiniTest":25.3,
        "HallusionBench Avg.":29.4,
        "LLaVABench (in-the-wild)":25.0
    },
    {
        "Model":"IDEFICS-80B-Instruct\nVerified\n\n\nHugging Face",
        "Time":"2023\/7\/25",
        "Params":"80B",
        "Language Model":"LLaMA65B",
        "Vision Model":"CLIP ViT-H\/14",
        "Avg. Rank":23.6,
        "Avg. Score":38.0,
        "MMBench Test":54.6,
        "MMBench-CN Test":38.1,
        "CCBench":10.6,
        "MME (normalized)":54.2,
        "SEEDBench_IMG":52.0,
        "MMVet":39.7,
        "MMMU Val":24.0,
        "MathVista MiniTest":26.2,
        "HallusionBench Avg.":23.4,
        "LLaVABench (in-the-wild)":56.9
    },
    {
        "Model":"InstructBLIP-7B\nVerified\n\n\nSalesforce Research",
        "Time":"2023\/5\/11",
        "Params":"8B",
        "Language Model":"Vicuna 7B",
        "Vision Model":"EVA-G",
        "Avg. Rank":24.4,
        "Avg. Score":34.3,
        "MMBench Test":33.9,
        "MMBench-CN Test":23.9,
        "CCBench":12.7,
        "MME (normalized)":49.7,
        "SEEDBench_IMG":44.5,
        "MMVet":33.1,
        "MMMU Val":30.6,
        "MathVista MiniTest":23.7,
        "HallusionBench Avg.":31.2,
        "LLaVABench (in-the-wild)":59.8
    },
    {
        "Model":"LLaVA-v1-7B\nVerified\n\n\nUniversity of Wisconsin-Madison",
        "Time":"2023\/4\/17",
        "Params":"7.2B",
        "Language Model":"LLaMA 7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":27.0,
        "Avg. Score":33.8,
        "MMBench Test":41.8,
        "MMBench-CN Test":35.1,
        "CCBench":9.2,
        "MME (normalized)":37.3,
        "SEEDBench_IMG":49.5,
        "MMVet":27.4,
        "MMMU Val":32.3,
        "MathVista MiniTest":23.7,
        "HallusionBench Avg.":22.4,
        "LLaVABench (in-the-wild)":58.9
    },
    {
        "Model":"IDEFICS-9B-Instruct\nVerified\n\n\nHugging Face",
        "Time":"2023\/7\/25",
        "Params":"9B",
        "Language Model":"LLaMA 7B",
        "Vision Model":"CLIP ViT-H\/14",
        "Avg. Rank":28.5,
        "Avg. Score":30.6,
        "MMBench Test":45.3,
        "MMBench-CN Test":25.2,
        "CCBench":7.8,
        "MME (normalized)":42.0,
        "SEEDBench_IMG":45.0,
        "MMVet":30.0,
        "MMMU Val":18.4,
        "MathVista MiniTest":20.4,
        "HallusionBench Avg.":27.3,
        "LLaVABench (in-the-wild)":45.0
    },
    {
        "Model":"PandaGPT-13B\nVerified\n\n\nTencent AI Lab",
        "Time":"2023\/5\/25",
        "Params":"14B",
        "Language Model":"Vicuna 13B",
        "Vision Model":"ImageBind ViT-H\/14",
        "Avg. Rank":28.3,
        "Avg. Score":30.1,
        "MMBench Test":42.5,
        "MMBench-CN Test":32.0,
        "CCBench":6.7,
        "MME (normalized)":38.3,
        "SEEDBench_IMG":47.6,
        "MMVet":19.6,
        "MMMU Val":32.9,
        "MathVista MiniTest":24.6,
        "HallusionBench Avg.":20.0,
        "LLaVABench (in-the-wild)":37.1
    },
    {
        "Model":"VisualGLM\nVerified\n\n\nTsinghua University",
        "Time":"2023\/5\/17",
        "Params":"8B",
        "Language Model":"ChatGLM 6B",
        "Vision Model":"EVA-CLIP",
        "Avg. Rank":29.1,
        "Avg. Score":29.2,
        "MMBench Test":37.6,
        "MMBench-CN Test":35.5,
        "CCBench":17.3,
        "MME (normalized)":26.4,
        "SEEDBench_IMG":47.0,
        "MMVet":14.8,
        "MMMU Val":29.9,
        "MathVista MiniTest":21.5,
        "HallusionBench Avg.":25.0,
        "LLaVABench (in-the-wild)":37.3
    },
    {
        "Model":"MiniGPT-4-v1-7B\nVerified\n\n\nKing Abdullah University of Science and Technology",
        "Time":"2023\/4\/20",
        "Params":"8B",
        "Language Model":"Vicuna 7B",
        "Vision Model":"EVA-G",
        "Avg. Rank":29.3,
        "Avg. Score":24.2,
        "MMBench Test":23.0,
        "MMBench-CN Test":11.9,
        "CCBench":1.8,
        "MME (normalized)":37.4,
        "SEEDBench_IMG":31.6,
        "MMVet":15.6,
        "MMMU Val":23.6,
        "MathVista MiniTest":20.2,
        "HallusionBench Avg.":31.9,
        "LLaVABench (in-the-wild)":45.1
    },
    {
        "Model":"Qwen-VL\nVerified\n\n\nAlibaba Group",
        "Time":"2023\/8\/24",
        "Params":"9.6B",
        "Language Model":"Qwen-7B",
        "Vision Model":"ViT-G\/16",
        "Avg. Rank":30.1,
        "Avg. Score":21.7,
        "MMBench Test":32.2,
        "MMBench-CN Test":7.8,
        "CCBench":6.1,
        "MME (normalized)":17.2,
        "SEEDBench_IMG":52.5,
        "MMVet":13.0,
        "MMMU Val":29.6,
        "MathVista MiniTest":15.5,
        "HallusionBench Avg.":29.9,
        "LLaVABench (in-the-wild)":12.9
    },
    {
        "Model":"OpenFlamingo v2\nVerified\n\n\nThe University of Washington",
        "Time":"2023\/6\/28",
        "Params":"9B",
        "Language Model":"MPT 7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":30.4,
        "Avg. Score":21.1,
        "MMBench Test":5.7,
        "MMBench-CN Test":14.4,
        "CCBench":6.3,
        "MME (normalized)":21.7,
        "SEEDBench_IMG":28.8,
        "MMVet":23.3,
        "MMMU Val":28.8,
        "MathVista MiniTest":18.6,
        "HallusionBench Avg.":29.4,
        "LLaVABench (in-the-wild)":34.2
    },
    {
        "Model":"MiniGPT-4-v2\nVerified\n\n\nKing Abdullah University of Science and Technology",
        "Time":"2023\/10\/27",
        "Params":"8B",
        "Language Model":"LLaMA2 13B",
        "Vision Model":"EVA-G",
        "Avg. Rank":30.6,
        "Avg. Score":19.7,
        "MMBench Test":9.4,
        "MMBench-CN Test":4.7,
        "CCBench":1.4,
        "MME (normalized)":34.6,
        "SEEDBench_IMG":29.4,
        "MMVet":10.5,
        "MMMU Val":25.0,
        "MathVista MiniTest":22.9,
        "HallusionBench Avg.":30.0,
        "LLaVABench (in-the-wild)":28.8
    }
]