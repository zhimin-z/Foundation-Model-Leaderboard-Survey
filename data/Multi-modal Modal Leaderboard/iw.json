[
    {
        "Model":"Qwen-VL\nVerified\n\n\nAlibaba Group",
        "Time":"2023\/8\/24",
        "Params":"9.6B",
        "Language Model":"Qwen-7B",
        "Vision Model":"ViT-G\/16",
        "Avg. Rank":32.91,
        "Avg. Score":24.9,
        "MMBench Test":32.2,
        "MMBench-CN Test":7.8,
        "CCBench":6.1,
        "MME (normalized)":482.7,
        "SEEDBench_IMG":52.5,
        "MMVet":13.0,
        "MMMU Val":29.6,
        "MathVista MiniTest":15.5,
        "HallusionBench Avg.":29.9,
        "LLaVABench (in-the-wild)":12.9,
        "AI2D Test":57.7
    },
    {
        "Model":"MiniGPT-4-v2\nVerified\n\n\nKing Abdullah University of Science and Technology",
        "Time":"2023\/10\/27",
        "Params":"8B",
        "Language Model":"LLaMA2 13B",
        "Vision Model":"EVA-G",
        "Avg. Rank":34.82,
        "Avg. Score":20.7,
        "MMBench Test":9.4,
        "MMBench-CN Test":4.7,
        "CCBench":1.4,
        "MME (normalized)":968.4,
        "SEEDBench_IMG":29.4,
        "MMVet":10.5,
        "MMMU Val":25.0,
        "MathVista MiniTest":22.9,
        "HallusionBench Avg.":30.0,
        "LLaVABench (in-the-wild)":28.8,
        "AI2D Test":30.5
    },
    {
        "Model":"OpenFlamingo v2\nVerified\n\n\nThe University of Washington",
        "Time":"2023\/6\/28",
        "Params":"9B",
        "Language Model":"MPT 7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":34.55,
        "Avg. Score":22.1,
        "MMBench Test":5.7,
        "MMBench-CN Test":14.4,
        "CCBench":6.3,
        "MME (normalized)":607.2,
        "SEEDBench_IMG":28.8,
        "MMVet":23.3,
        "MMMU Val":28.8,
        "MathVista MiniTest":18.6,
        "HallusionBench Avg.":29.4,
        "LLaVABench (in-the-wild)":34.2,
        "AI2D Test":31.7
    },
    {
        "Model":"PandaGPT-13B\nVerified\n\n\nTencent AI Lab",
        "Time":"2023\/5\/25",
        "Params":"14B",
        "Language Model":"Vicuna 13B",
        "Vision Model":"ImageBind ViT-H\/14",
        "Avg. Rank":31.91,
        "Avg. Score":32.0,
        "MMBench Test":42.5,
        "MMBench-CN Test":32.0,
        "CCBench":6.7,
        "MME (normalized)":1072.2,
        "SEEDBench_IMG":47.6,
        "MMVet":19.6,
        "MMMU Val":32.9,
        "MathVista MiniTest":24.6,
        "HallusionBench Avg.":20.0,
        "LLaVABench (in-the-wild)":37.1,
        "AI2D Test":50.3
    },
    {
        "Model":"VisualGLM\nVerified\n\n\nTsinghua University",
        "Time":"2023\/5\/17",
        "Params":"8B",
        "Language Model":"ChatGLM 6B",
        "Vision Model":"EVA-CLIP",
        "Avg. Rank":33.27,
        "Avg. Score":30.3,
        "MMBench Test":37.6,
        "MMBench-CN Test":35.5,
        "CCBench":17.3,
        "MME (normalized)":738.1,
        "SEEDBench_IMG":47.0,
        "MMVet":14.8,
        "MMMU Val":29.9,
        "MathVista MiniTest":21.5,
        "HallusionBench Avg.":25.0,
        "LLaVABench (in-the-wild)":37.3,
        "AI2D Test":41.2
    },
    {
        "Model":"Monkey\nVerified\n\n\nHuazhong University of Science and Technology",
        "Time":"2024\/01\/11",
        "Params":"9.8B",
        "Language Model":"Qwen-7B",
        "Vision Model":"ViT-BigHuge",
        "Avg. Rank":19.0,
        "Avg. Score":48.1,
        "MMBench Test":59.6,
        "MMBench-CN Test":54.7,
        "CCBench":37.1,
        "MME (normalized)":1759.9,
        "SEEDBench_IMG":64.3,
        "MMVet":38.1,
        "MMMU Val":38.9,
        "MathVista MiniTest":32.5,
        "HallusionBench Avg.":34.9,
        "LLaVABench (in-the-wild)":43.3,
        "AI2D Test":62.5
    },
    {
        "Model":"IDEFICS-9B-Instruct\nVerified\n\n\nHugging Face",
        "Time":"2023\/7\/25",
        "Params":"9B",
        "Language Model":"LLaMA 7B",
        "Vision Model":"CLIP ViT-H\/14",
        "Avg. Rank":32.55,
        "Avg. Score":31.7,
        "MMBench Test":45.3,
        "MMBench-CN Test":25.2,
        "CCBench":7.8,
        "MME (normalized)":1177.3,
        "SEEDBench_IMG":45.0,
        "MMVet":30.0,
        "MMMU Val":18.4,
        "MathVista MiniTest":20.4,
        "HallusionBench Avg.":27.3,
        "LLaVABench (in-the-wild)":45.0,
        "AI2D Test":42.2
    },
    {
        "Model":"MiniGPT-4-v1-7B\nVerified\n\n\nKing Abdullah University of Science and Technology",
        "Time":"2023\/4\/20",
        "Params":"8B",
        "Language Model":"Vicuna 7B",
        "Vision Model":"EVA-G",
        "Avg. Rank":33.82,
        "Avg. Score":24.6,
        "MMBench Test":23.0,
        "MMBench-CN Test":11.9,
        "CCBench":1.8,
        "MME (normalized)":1047.4,
        "SEEDBench_IMG":31.6,
        "MMVet":15.6,
        "MMMU Val":23.6,
        "MathVista MiniTest":20.2,
        "HallusionBench Avg.":31.9,
        "LLaVABench (in-the-wild)":45.1,
        "AI2D Test":28.4
    },
    {
        "Model":"SharedCaptioner\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2024\/01\/03",
        "Params":"8B",
        "Language Model":"InternLM-7B",
        "Vision Model":"EVA-G",
        "Avg. Rank":21.0,
        "Avg. Score":47.4,
        "MMBench Test":66.5,
        "MMBench-CN Test":60.9,
        "CCBench":40.4,
        "MME (normalized)":1642.5,
        "SEEDBench_IMG":61.2,
        "MMVet":30.1,
        "MMMU Val":36.3,
        "MathVista MiniTest":29.0,
        "HallusionBench Avg.":34.2,
        "LLaVABench (in-the-wild)":47.4,
        "AI2D Test":56.7
    },
    {
        "Model":"MMAlaya\nVerified\n\n\nDataCanvas",
        "Time":"2024\/01\/29",
        "Params":"7.8B",
        "Language Model":"Alaya-7B-Chat",
        "Vision Model":"EVA-G",
        "Avg. Rank":27.73,
        "Avg. Score":41.2,
        "MMBench Test":58.7,
        "MMBench-CN Test":58.6,
        "CCBench":28.6,
        "MME (normalized)":1356.0,
        "SEEDBench_IMG":57.5,
        "MMVet":22.2,
        "MMMU Val":32.0,
        "MathVista MiniTest":22.1,
        "HallusionBench Avg.":32.9,
        "LLaVABench (in-the-wild)":50.2,
        "AI2D Test":42.3
    },
    {
        "Model":"MiniCPM-V\nVerified\n\n\nOpenBMB",
        "Time":"2024\/02\/07",
        "Params":"3B",
        "Language Model":"MiniCPM-2.4B",
        "Vision Model":"SigLip-400M",
        "Avg. Rank":18.91,
        "Avg. Score":48.6,
        "MMBench Test":64.1,
        "MMBench-CN Test":62.6,
        "CCBench":41.4,
        "MME (normalized)":1650.2,
        "SEEDBench_IMG":65.6,
        "MMVet":31.1,
        "MMMU Val":38.3,
        "MathVista MiniTest":28.9,
        "HallusionBench Avg.":36.2,
        "LLaVABench (in-the-wild)":51.3,
        "AI2D Test":56.3
    },
    {
        "Model":"Yi-VL-6B\nVerified\n\n\n01-AI",
        "Time":"2024\/01\/25",
        "Params":"6.6B",
        "Language Model":"Yi-6B",
        "Vision Model":"CLIP ViT-H\/14",
        "Avg. Rank":14.73,
        "Avg. Score":50.7,
        "MMBench Test":67.9,
        "MMBench-CN Test":66.1,
        "CCBench":40.8,
        "MME (normalized)":1920.0,
        "SEEDBench_IMG":67.6,
        "MMVet":31.1,
        "MMMU Val":39.9,
        "MathVista MiniTest":28.0,
        "HallusionBench Avg.":36.7,
        "LLaVABench (in-the-wild)":51.5,
        "AI2D Test":59.7
    },
    {
        "Model":"InternLM-XComposer-VL\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2023\/9\/26",
        "Params":"8B",
        "Language Model":"InternLM-7B",
        "Vision Model":"EVA-G",
        "Avg. Rank":14.0,
        "Avg. Score":52.4,
        "MMBench Test":74.4,
        "MMBench-CN Test":72.4,
        "CCBench":49.6,
        "MME (normalized)":1874.2,
        "SEEDBench_IMG":66.1,
        "MMVet":35.2,
        "MMMU Val":35.6,
        "MathVista MiniTest":29.5,
        "HallusionBench Avg.":36.0,
        "LLaVABench (in-the-wild)":53.8,
        "AI2D Test":56.9
    },
    {
        "Model":"LLaVA-InternLM2-7B (QLoRA)\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2024\/01\/17",
        "Params":"8.1B",
        "Language Model":"InternLM2-7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":14.82,
        "Avg. Score":51.6,
        "MMBench Test":73.3,
        "MMBench-CN Test":71.7,
        "CCBench":42.5,
        "MME (normalized)":1699.5,
        "SEEDBench_IMG":71.2,
        "MMVet":35.9,
        "MMMU Val":40.1,
        "MathVista MiniTest":25.5,
        "HallusionBench Avg.":26.7,
        "LLaVABench (in-the-wild)":56.2,
        "AI2D Test":63.6
    },
    {
        "Model":"Emu2_chat\nVerified\n\n\nBeijing Academy of Artificial Intelligence",
        "Time":"2024\/01\/17",
        "Params":"37B",
        "Language Model":"LLaMA-33B",
        "Vision Model":"EVA-02-CLIP-E-plus",
        "Avg. Rank":22.91,
        "Avg. Score":44.4,
        "MMBench Test":63.6,
        "MMBench-CN Test":45.9,
        "CCBench":18.8,
        "MME (normalized)":1678.0,
        "SEEDBench_IMG":68.9,
        "MMVet":31.0,
        "MMMU Val":35.0,
        "MathVista MiniTest":30.0,
        "HallusionBench Avg.":29.5,
        "LLaVABench (in-the-wild)":56.4,
        "AI2D Test":49.7
    },
    {
        "Model":"Yi-VL-34B\nVerified\n\n\n01-AI",
        "Time":"2024\/01\/25",
        "Params":"34.6B",
        "Language Model":"Yi-34B",
        "Vision Model":"CLIP ViT-H\/14",
        "Avg. Rank":18.45,
        "Avg. Score":49.1,
        "MMBench Test":62.8,
        "MMBench-CN Test":60.5,
        "CCBench":36.3,
        "MME (normalized)":1796.1,
        "SEEDBench_IMG":60.5,
        "MMVet":30.8,
        "MMMU Val":44.7,
        "MathVista MiniTest":25.6,
        "HallusionBench Avg.":32.5,
        "LLaVABench (in-the-wild)":56.9,
        "AI2D Test":65.8
    },
    {
        "Model":"IDEFICS-80B-Instruct\nVerified\n\n\nHugging Face",
        "Time":"2023\/7\/25",
        "Params":"80B",
        "Language Model":"LLaMA65B",
        "Vision Model":"CLIP ViT-H\/14",
        "Avg. Rank":27.18,
        "Avg. Score":39.5,
        "MMBench Test":54.6,
        "MMBench-CN Test":38.1,
        "CCBench":10.6,
        "MME (normalized)":1518.2,
        "SEEDBench_IMG":52.0,
        "MMVet":39.7,
        "MMMU Val":24.0,
        "MathVista MiniTest":26.2,
        "HallusionBench Avg.":23.4,
        "LLaVABench (in-the-wild)":56.9,
        "AI2D Test":54.8
    },
    {
        "Model":"LLaVA-v1.5-7B (QLoRA)\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2023\/12\/28",
        "Params":"7.2B",
        "Language Model":"Vicuna-v1.5-7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":23.36,
        "Avg. Score":46.7,
        "MMBench Test":67.7,
        "MMBench-CN Test":61.0,
        "CCBench":28.4,
        "MME (normalized)":1716.6,
        "SEEDBench_IMG":66.4,
        "MMVet":32.2,
        "MMMU Val":33.7,
        "MathVista MiniTest":24.2,
        "HallusionBench Avg.":25.2,
        "LLaVABench (in-the-wild)":57.2,
        "AI2D Test":55.9
    },
    {
        "Model":"LLaVA-v1-7B\nVerified\n\n\nUniversity of Wisconsin-Madison",
        "Time":"2023\/4\/17",
        "Params":"7.2B",
        "Language Model":"LLaMA 7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":30.64,
        "Avg. Score":35.1,
        "MMBench Test":41.8,
        "MMBench-CN Test":35.1,
        "CCBench":9.2,
        "MME (normalized)":1044.2,
        "SEEDBench_IMG":49.5,
        "MMVet":27.4,
        "MMMU Val":32.3,
        "MathVista MiniTest":23.7,
        "HallusionBench Avg.":22.4,
        "LLaVABench (in-the-wild)":58.9,
        "AI2D Test":48.2
    },
    {
        "Model":"LLaVA-InternLM-7B (QLoRA)\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2023\/12\/28",
        "Params":"7.6B",
        "Language Model":"InternLM-7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":18.64,
        "Avg. Score":49.0,
        "MMBench Test":69.0,
        "MMBench-CN Test":66.7,
        "CCBench":37.3,
        "MME (normalized)":1637.1,
        "SEEDBench_IMG":65.7,
        "MMVet":32.4,
        "MMMU Val":36.9,
        "MathVista MiniTest":26.3,
        "HallusionBench Avg.":28.9,
        "LLaVABench (in-the-wild)":59.7,
        "AI2D Test":58.0
    },
    {
        "Model":"InstructBLIP-7B\nVerified\n\n\nSalesforce Research",
        "Time":"2023\/5\/11",
        "Params":"8B",
        "Language Model":"Vicuna 7B",
        "Vision Model":"EVA-G",
        "Avg. Rank":28.55,
        "Avg. Score":34.9,
        "MMBench Test":33.9,
        "MMBench-CN Test":23.9,
        "CCBench":12.7,
        "MME (normalized)":1391.4,
        "SEEDBench_IMG":44.5,
        "MMVet":33.1,
        "MMMU Val":30.6,
        "MathVista MiniTest":23.7,
        "HallusionBench Avg.":31.2,
        "LLaVABench (in-the-wild)":59.8,
        "AI2D Test":40.6
    },
    {
        "Model":"mPLUG-Owl2\nVerified\n\n\nDAMO Academy",
        "Time":"2023\/11\/10",
        "Params":"8.2B",
        "Language Model":"LLaMA2 7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":21.36,
        "Avg. Score":47.8,
        "MMBench Test":66.0,
        "MMBench-CN Test":60.3,
        "CCBench":31.0,
        "MME (normalized)":1786.4,
        "SEEDBench_IMG":64.5,
        "MMVet":35.7,
        "MMMU Val":34.7,
        "MathVista MiniTest":25.3,
        "HallusionBench Avg.":29.4,
        "LLaVABench (in-the-wild)":59.9,
        "AI2D Test":55.7
    },
    {
        "Model":"Monkey-Chat\nVerified\n\n\nHuazhong University of Science and Technology",
        "Time":"2024\/01\/16",
        "Params":"9.8B",
        "Language Model":"Qwen-7B",
        "Vision Model":"ViT-BigHuge",
        "Avg. Rank":8.36,
        "Avg. Score":55.4,
        "MMBench Test":72.4,
        "MMBench-CN Test":67.5,
        "CCBench":48.0,
        "MME (normalized)":1887.4,
        "SEEDBench_IMG":68.9,
        "MMVet":41.0,
        "MMMU Val":40.7,
        "MathVista MiniTest":34.8,
        "HallusionBench Avg.":39.3,
        "LLaVABench (in-the-wild)":60.5,
        "AI2D Test":68.5
    },
    {
        "Model":"LLaVA-v1.5-7B\nVerified\n\n\nUniversity of Wisconsin\u2013Madison",
        "Time":"2023\/10\/05",
        "Params":"7.2B",
        "Language Model":"Vicuna-v1.5-7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":23.09,
        "Avg. Score":46.8,
        "MMBench Test":65.2,
        "MMBench-CN Test":57.3,
        "CCBench":26.7,
        "MME (normalized)":1775.6,
        "SEEDBench_IMG":65.6,
        "MMVet":32.7,
        "MMMU Val":36.2,
        "MathVista MiniTest":23.6,
        "HallusionBench Avg.":27.4,
        "LLaVABench (in-the-wild)":60.7,
        "AI2D Test":55.6
    },
    {
        "Model":"TransCore-M\nVerified\n\n\nPCI Research",
        "Time":"2023\/11\/30",
        "Params":"13.4B",
        "Language Model":"PCITransGPT-13B",
        "Vision Model":"CLIP ViT\/L-14",
        "Avg. Rank":13.64,
        "Avg. Score":51.5,
        "MMBench Test":71.4,
        "MMBench-CN Test":67.8,
        "CCBench":32.7,
        "MME (normalized)":1897.9,
        "SEEDBench_IMG":71.2,
        "MMVet":38.8,
        "MMMU Val":36.4,
        "MathVista MiniTest":27.8,
        "HallusionBench Avg.":29.0,
        "LLaVABench (in-the-wild)":61.7,
        "AI2D Test":62.2
    },
    {
        "Model":"LLaVA-v1.5-13B (QLoRA)\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2023\/12\/28",
        "Params":"13.4B",
        "Language Model":"Vicuna-v1.5-13B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":17.91,
        "Avg. Score":49.6,
        "MMBench Test":68.8,
        "MMBench-CN Test":64.7,
        "CCBench":32.9,
        "MME (normalized)":1766.0,
        "SEEDBench_IMG":67.9,
        "MMVet":35.9,
        "MMMU Val":35.2,
        "MathVista MiniTest":26.2,
        "HallusionBench Avg.":26.2,
        "LLaVABench (in-the-wild)":63.6,
        "AI2D Test":61.3
    },
    {
        "Model":"LLaVA-InternLM2-20B (QLoRA)\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2024\/01\/17",
        "Params":"20.2B",
        "Language Model":"InternLM2-20B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":12.73,
        "Avg. Score":53.5,
        "MMBench Test":75.1,
        "MMBench-CN Test":73.7,
        "CCBench":46.3,
        "MME (normalized)":1867.1,
        "SEEDBench_IMG":70.2,
        "MMVet":37.2,
        "MMMU Val":39.4,
        "MathVista MiniTest":24.6,
        "HallusionBench Avg.":26.4,
        "LLaVABench (in-the-wild)":63.7,
        "AI2D Test":65.4
    },
    {
        "Model":"LLaVA-v1.5-13B\nVerified\n\n\nUniversity of Wisconsin\u2013Madison",
        "Time":"2023\/10\/05",
        "Params":"13.4B",
        "Language Model":"Vicuna-v1.5-13B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":17.09,
        "Avg. Score":49.7,
        "MMBench Test":67.8,
        "MMBench-CN Test":63.3,
        "CCBench":29.8,
        "MME (normalized)":1804.6,
        "SEEDBench_IMG":68.1,
        "MMVet":38.3,
        "MMMU Val":36.9,
        "MathVista MiniTest":26.4,
        "HallusionBench Avg.":25.7,
        "LLaVABench (in-the-wild)":64.6,
        "AI2D Test":61.1
    },
    {
        "Model":"ShareGPT4V-7B\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2023\/12\/08",
        "Params":"7.2B",
        "Language Model":"Vicuna-v1.5-7B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":17.91,
        "Avg. Score":49.2,
        "MMBench Test":66.4,
        "MMBench-CN Test":59.4,
        "CCBench":30.6,
        "MME (normalized)":1874.4,
        "SEEDBench_IMG":69.3,
        "MMVet":34.7,
        "MMMU Val":36.6,
        "MathVista MiniTest":25.8,
        "HallusionBench Avg.":28.5,
        "LLaVABench (in-the-wild)":64.9,
        "AI2D Test":58.3
    },
    {
        "Model":"ShareGPT4V-13B\nVerified\n\n\nShanghai AI Laboratory",
        "Time":"2023\/12\/08",
        "Params":"13.4B",
        "Language Model":"Vicuna-v1.5-13B",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":15.0,
        "Avg. Score":50.8,
        "MMBench Test":68.9,
        "MMBench-CN Test":64.7,
        "CCBench":30.6,
        "MME (normalized)":1827.8,
        "SEEDBench_IMG":70.7,
        "MMVet":39.2,
        "MMMU Val":34.8,
        "MathVista MiniTest":27.8,
        "HallusionBench Avg.":29.4,
        "LLaVABench (in-the-wild)":66.6,
        "AI2D Test":61.1
    },
    {
        "Model":"Qwen-VL-Chat\nVerified\n\n\nAlibaba Group",
        "Time":"2023\/8\/24",
        "Params":"9.6B",
        "Language Model":"Qwen-7B",
        "Vision Model":"ViT-G\/16",
        "Avg. Rank":14.0,
        "Avg. Score":52.4,
        "MMBench Test":61.8,
        "MMBench-CN Test":56.3,
        "CCBench":41.2,
        "MME (normalized)":1860.0,
        "SEEDBench_IMG":64.8,
        "MMVet":47.3,
        "MMMU Val":37.0,
        "MathVista MiniTest":33.8,
        "HallusionBench Avg.":36.8,
        "LLaVABench (in-the-wild)":67.7,
        "AI2D Test":63.0
    },
    {
        "Model":"InternLM-XComposer2-VL\nVerified\n\n\nShanghai AI Lab",
        "Time":"2024\/01\/26",
        "Params":"7B",
        "Language Model":"InternLM2",
        "Vision Model":"CLIP ViT-L\/14",
        "Avg. Rank":3.45,
        "Avg. Score":63.9,
        "MMBench Test":80.7,
        "MMBench-CN Test":79.4,
        "CCBench":48.2,
        "MME (normalized)":2220.4,
        "SEEDBench_IMG":74.9,
        "MMVet":46.7,
        "MMMU Val":41.4,
        "MathVista MiniTest":57.9,
        "HallusionBench Avg.":41.0,
        "LLaVABench (in-the-wild)":72.6,
        "AI2D Test":81.2
    },
    {
        "Model":"Qwen-VL-Plus\nVerified\n\n\nAlibaba Group",
        "Time":"2024\/01\/10",
        "Params":null,
        "Language Model":"QwenLM",
        "Vision Model":null,
        "Avg. Rank":7.73,
        "Avg. Score":59.1,
        "MMBench Test":67.0,
        "MMBench-CN Test":70.7,
        "CCBench":55.1,
        "MME (normalized)":2229.8,
        "SEEDBench_IMG":65.7,
        "MMVet":55.7,
        "MMMU Val":39.8,
        "MathVista MiniTest":36.0,
        "HallusionBench Avg.":40.6,
        "LLaVABench (in-the-wild)":73.7,
        "AI2D Test":65.7
    },
    {
        "Model":"CogVLM-17B-Chat\nVerified\n\n\nZhipu AI",
        "Time":"2024\/01\/03",
        "Params":"17B",
        "Language Model":"Vicuna-v1.5-7B",
        "Vision Model":"EVA2-CLIP-E",
        "Avg. Rank":14.64,
        "Avg. Score":51.9,
        "MMBench Test":65.8,
        "MMBench-CN Test":55.9,
        "CCBench":19.0,
        "MME (normalized)":1736.6,
        "SEEDBench_IMG":68.8,
        "MMVet":54.5,
        "MMMU Val":37.3,
        "MathVista MiniTest":34.7,
        "HallusionBench Avg.":35.4,
        "LLaVABench (in-the-wild)":73.9,
        "AI2D Test":63.3
    },
    {
        "Model":"OmniLMM-12B\nVerified\n\n\nOpenBMB",
        "Time":"2024\/02\/07",
        "Params":"12B",
        "Language Model":"Zephyr-7B-\u03b2",
        "Vision Model":"EVA-02-5B",
        "Avg. Rank":8.64,
        "Avg. Score":55.3,
        "MMBench Test":71.7,
        "MMBench-CN Test":62.0,
        "CCBench":37.1,
        "MME (normalized)":1935.8,
        "SEEDBench_IMG":71.5,
        "MMVet":47.4,
        "MMMU Val":41.8,
        "MathVista MiniTest":33.3,
        "HallusionBench Avg.":35.8,
        "LLaVABench (in-the-wild)":75.8,
        "AI2D Test":63.3
    },
    {
        "Model":"GeminiProVision\nVerified\n\n\nGoogle",
        "Time":"2023\/12\/23",
        "Params":null,
        "Language Model":null,
        "Vision Model":null,
        "Avg. Rank":3.73,
        "Avg. Score":63.4,
        "MMBench Test":73.6,
        "MMBench-CN Test":74.3,
        "CCBench":52.5,
        "MME (normalized)":2148.9,
        "SEEDBench_IMG":70.7,
        "MMVet":59.2,
        "MMMU Val":48.9,
        "MathVista MiniTest":45.8,
        "HallusionBench Avg.":45.2,
        "LLaVABench (in-the-wild)":79.9,
        "AI2D Test":70.2
    },
    {
        "Model":"Qwen-VL-Max\nVerified\n\n\nAlibaba Group",
        "Time":"2024\/02\/02",
        "Params":null,
        "Language Model":"QwenLM",
        "Vision Model":null,
        "Avg. Rank":2.0,
        "Avg. Score":65.9,
        "MMBench Test":77.6,
        "MMBench-CN Test":75.7,
        "CCBench":63.5,
        "MME (normalized)":2281.7,
        "SEEDBench_IMG":72.7,
        "MMVet":61.8,
        "MMMU Val":52.0,
        "MathVista MiniTest":41.4,
        "HallusionBench Avg.":41.2,
        "LLaVABench (in-the-wild)":82.3,
        "AI2D Test":75.7
    },
    {
        "Model":"GPT-4v\nVerified\n\n\nOpenAI",
        "Time":"2023\/12\/23",
        "Params":null,
        "Language Model":null,
        "Vision Model":null,
        "Avg. Rank":4.09,
        "Avg. Score":64.2,
        "MMBench Test":77.0,
        "MMBench-CN Test":74.4,
        "CCBench":46.5,
        "MME (normalized)":1771.5,
        "SEEDBench_IMG":71.6,
        "MMVet":56.8,
        "MMMU Val":53.8,
        "MathVista MiniTest":47.8,
        "HallusionBench Avg.":46.5,
        "LLaVABench (in-the-wild)":93.1,
        "AI2D Test":75.5
    }
]