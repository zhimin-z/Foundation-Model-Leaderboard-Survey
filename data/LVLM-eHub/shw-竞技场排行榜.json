[
    {
        "Model":"Otter-Image",
        "Elo\u5f97\u5206":1047.5,
        "\u6240\u5c5e\u673a\u6784":"NTU",
        "\u6a21\u578b\u63cf\u8ff0":"a LVLM with in-context instruction tuning."
    },
    {
        "Model":"LLaMA-Adapter V2",
        "Elo\u5f97\u5206":1038.7,
        "\u6240\u5c5e\u673a\u6784":"Shanghai AI Lab",
        "\u6a21\u578b\u63cf\u8ff0":"a LVLM with parameter-efficient instruction tuning."
    },
    {
        "Model":"MiniGPT-4",
        "Elo\u5f97\u5206":1009.9,
        "\u6240\u5c5e\u673a\u6784":"KAUST",
        "\u6a21\u578b\u63cf\u8ff0":"a LVLM with only the FC layer tuned on 3.5K intruction data."
    },
    {
        "Model":"LLaVA",
        "Elo\u5f97\u5206":1002.8,
        "\u6240\u5c5e\u673a\u6784":"Wisconsin-Madison",
        "\u6a21\u578b\u63cf\u8ff0":"a LVLM with the whole LLM trained on 158K visual instruction data."
    },
    {
        "Model":"VPGTrans",
        "Elo\u5f97\u5206":999.6,
        "\u6240\u5c5e\u673a\u6784":"NUS",
        "\u6a21\u578b\u63cf\u8ff0":"a LVLM equips LLaMA with a visual model by transfer learning."
    },
    {
        "Model":"InstructBLIP",
        "Elo\u5f97\u5206":995.5,
        "\u6240\u5c5e\u673a\u6784":"Salesforce",
        "\u6a21\u578b\u63cf\u8ff0":"a LVLM with the Q-Former trained on many instruction QA datasets."
    },
    {
        "Model":"mPLUG-Owl",
        "Elo\u5f97\u5206":985.6,
        "\u6240\u5c5e\u673a\u6784":"DAMO Academy",
        "\u6a21\u578b\u63cf\u8ff0":"a LVLM with LLM instructionally tuned using LoRA technique."
    },
    {
        "Model":"Otter",
        "Elo\u5f97\u5206":972.1,
        "\u6240\u5c5e\u673a\u6784":"NTU",
        "\u6a21\u578b\u63cf\u8ff0":"a LVLM with 1.3B adaption parameters tuned on 158K instruction data."
    },
    {
        "Model":"BLIP-2",
        "Elo\u5f97\u5206":948.8,
        "\u6240\u5c5e\u673a\u6784":"Salesforce",
        "\u6a21\u578b\u63cf\u8ff0":"a LVLM without being tuned with instruction data."
    }
]