[
    {
        "table_id":8454,
        "row_id":102109,
        "rank":1,
        "Model":"LART (Hiera-H, K700 PT+FT)",
        "mlmodel":{

        },
        "method_short":"LART ",
        "method_details":"Hiera-H, K700 PT+FT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-19",
        "metrics":{
            "mAP":"45.1"
        },
        "raw_metrics":{
            "mAP":45.1
        },
        "uses_additional_data":true,
        "paper":{
            "id":1184373,
            "title":"On the Benefits of 3D Pose and Tracking for Human Action Recognition",
            "url":"\/paper\/on-the-benefits-of-3d-pose-and-tracking-for",
            "published":"2023-04-03T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":104382,
        "rank":2,
        "Model":"Hiera-H (K700 PT+FT)",
        "mlmodel":{

        },
        "method_short":"Hiera-H ",
        "method_details":"K700 PT+FT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-01",
        "metrics":{
            "mAP":"43.3"
        },
        "raw_metrics":{
            "mAP":43.3
        },
        "uses_additional_data":true,
        "paper":{
            "id":1221231,
            "title":"Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
            "url":"\/paper\/hiera-a-hierarchical-vision-transformer",
            "published":"2023-06-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hiera-a-hierarchical-vision-transformer\/review\/?hl=104382"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":98811,
        "rank":3,
        "Model":"VideoMAE V2-g",
        "mlmodel":{

        },
        "method_short":"VideoMAE V2-g",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-29",
        "metrics":{
            "mAP":"42.6"
        },
        "raw_metrics":{
            "mAP":42.6
        },
        "uses_additional_data":true,
        "paper":{
            "id":1182705,
            "title":"VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
            "url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders",
            "published":"2023-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders\/review\/?hl=98811"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":87,
                "name":"Vision Transformer",
                "color":"#2771D3"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":108312,
        "rank":4,
        "Model":"STAR\/L",
        "mlmodel":{

        },
        "method_short":"STAR\/L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-24",
        "metrics":{
            "mAP":"41.7"
        },
        "raw_metrics":{
            "mAP":41.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":1195788,
            "title":"End-to-End Spatio-Temporal Action Localisation with Video Transformers",
            "url":"\/paper\/end-to-end-spatio-temporal-action",
            "published":"2023-04-24T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/end-to-end-spatio-temporal-action\/review\/?hl=108312"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":100621,
        "rank":5,
        "Model":"MVD (Kinetics400 pretrain+finetune, ViT-H, 16x4)",
        "mlmodel":{

        },
        "method_short":"MVD ",
        "method_details":"Kinetics400 pretrain+finetune, ViT-H, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "mAP":"41.1"
        },
        "raw_metrics":{
            "mAP":41.1
        },
        "uses_additional_data":true,
        "paper":{
            "id":1125592,
            "title":"Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning",
            "url":"\/paper\/masked-video-distillation-rethinking-masked",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-video-distillation-rethinking-masked\/review\/?hl=100621"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":87,
                "name":"Vision Transformer",
                "color":"#2771D3"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":86719,
        "rank":6,
        "Model":"InternVideo",
        "mlmodel":{

        },
        "method_short":"InternVideo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "mAP":"41.01"
        },
        "raw_metrics":{
            "mAP":41.01
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124231,
            "title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
            "url":"\/paper\/internvideo-general-video-foundation-models",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internvideo-general-video-foundation-models\/review\/?hl=86719"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":98812,
        "rank":7,
        "Model":"MVD (Kinetics400 pretrain, ViT-H, 16x4)",
        "mlmodel":{

        },
        "method_short":"MVD ",
        "method_details":"Kinetics400 pretrain, ViT-H, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "mAP":"40.1"
        },
        "raw_metrics":{
            "mAP":40.1
        },
        "uses_additional_data":true,
        "paper":{
            "id":1125592,
            "title":"Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning",
            "url":"\/paper\/masked-video-distillation-rethinking-masked",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-video-distillation-rethinking-masked\/review\/?hl=98812"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":87,
                "name":"Vision Transformer",
                "color":"#2771D3"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":44631,
        "rank":8,
        "Model":"MaskFeat (Kinetics-600 pretrain, MViT-L)",
        "mlmodel":{

        },
        "method_short":"MaskFeat ",
        "method_details":"Kinetics-600 pretrain, MViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-16",
        "metrics":{
            "mAP":"39.8"
        },
        "raw_metrics":{
            "mAP":39.8
        },
        "uses_additional_data":true,
        "paper":{
            "id":932132,
            "title":"Masked Feature Prediction for Self-Supervised Visual Pre-Training",
            "url":"\/paper\/masked-feature-prediction-for-self-supervised",
            "published":"2021-12-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-feature-prediction-for-self-supervised\/review\/?hl=44631"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":154,
                "name":"MViT",
                "color":"#d327c5"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":100383,
        "rank":9,
        "Model":"UMT-L (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"UMT-L ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "mAP":"39.8"
        },
        "raw_metrics":{
            "mAP":39.8
        },
        "uses_additional_data":true,
        "paper":{
            "id":1181934,
            "title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
            "url":"\/paper\/unmasked-teacher-towards-training-efficient",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":72354,
        "rank":10,
        "Model":"VideoMAE (K400 pretrain+finetune, ViT-H, 16x4)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"K400 pretrain+finetune, ViT-H, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "mAP":"39.5"
        },
        "raw_metrics":{
            "mAP":39.5
        },
        "uses_additional_data":true,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=72354"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":87,
                "name":"Vision Transformer",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":58958,
        "rank":11,
        "Model":"VideoMAE (K700 pretrain+finetune, ViT-L, 16x4)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"K700 pretrain+finetune, ViT-L, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "mAP":"39.3"
        },
        "raw_metrics":{
            "mAP":39.3
        },
        "uses_additional_data":true,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=58958"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":87,
                "name":"Vision Transformer",
                "color":"#2771D3"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":98813,
        "rank":12,
        "Model":"MVD (Kinetics400 pretrain+finetune, ViT-L, 16x4)",
        "mlmodel":{

        },
        "method_short":"MVD ",
        "method_details":"Kinetics400 pretrain+finetune, ViT-L, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "mAP":"38.7"
        },
        "raw_metrics":{
            "mAP":38.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":1125592,
            "title":"Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning",
            "url":"\/paper\/masked-video-distillation-rethinking-masked",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-video-distillation-rethinking-masked\/review\/?hl=98813"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":87,
                "name":"Vision Transformer",
                "color":"#2771D3"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":58956,
        "rank":13,
        "Model":"VideoMAE (K400 pretrain+finetune, ViT-L, 16x4)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"K400 pretrain+finetune, ViT-L, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "mAP":"37.8"
        },
        "raw_metrics":{
            "mAP":37.8
        },
        "uses_additional_data":true,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=58956"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":87,
                "name":"Vision Transformer",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":98814,
        "rank":14,
        "Model":"MVD (Kinetics400 pretrain, ViT-L, 16x4)",
        "mlmodel":{

        },
        "method_short":"MVD ",
        "method_details":"Kinetics400 pretrain, ViT-L, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "mAP":"37.7"
        },
        "raw_metrics":{
            "mAP":37.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":1125592,
            "title":"Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning",
            "url":"\/paper\/masked-video-distillation-rethinking-masked",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-video-distillation-rethinking-masked\/review\/?hl=98814"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":87,
                "name":"Vision Transformer",
                "color":"#2771D3"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":72355,
        "rank":15,
        "Model":"VideoMAE (K400 pretrain, ViT-H, 16x4)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"K400 pretrain, ViT-H, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "mAP":"36.5"
        },
        "raw_metrics":{
            "mAP":36.5
        },
        "uses_additional_data":true,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=72355"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":58957,
        "rank":16,
        "Model":"VideoMAE (K700 pretrain, ViT-L, 16x4)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"K700 pretrain, ViT-L, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "mAP":"36.1"
        },
        "raw_metrics":{
            "mAP":36.1
        },
        "uses_additional_data":true,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=58957"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":87,
                "name":"Vision Transformer",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":45656,
        "rank":17,
        "Model":"MeMViT-24",
        "mlmodel":{

        },
        "method_short":"MeMViT-24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-20",
        "metrics":{
            "mAP":"35.4"
        },
        "raw_metrics":{
            "mAP":35.4
        },
        "uses_additional_data":true,
        "paper":{
            "id":948294,
            "title":"MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition",
            "url":"\/paper\/memvit-memory-augmented-multiscale-vision",
            "published":"2022-01-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/memvit-memory-augmented-multiscale-vision\/review\/?hl=45656"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":53608,
        "rank":18,
        "Model":"MViTv2-L (IN21k, K700)",
        "mlmodel":{

        },
        "method_short":"MViTv2-L ",
        "method_details":"IN21k, K700",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "mAP":"34.4"
        },
        "raw_metrics":{
            "mAP":34.4
        },
        "uses_additional_data":true,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=53608"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":154,
                "name":"MViT",
                "color":"#d327c5"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":58955,
        "rank":19,
        "Model":"VideoMAE (K400 pretrain, ViT-L, 16x4)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"K400 pretrain, ViT-L, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "mAP":"34.3"
        },
        "raw_metrics":{
            "mAP":34.3
        },
        "uses_additional_data":true,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=58955"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":87,
                "name":"Vision Transformer",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":98815,
        "rank":20,
        "Model":"MVD (Kinetics400 pretrain+finetune, ViT-B, 16x4)",
        "mlmodel":{

        },
        "method_short":"MVD ",
        "method_details":"Kinetics400 pretrain+finetune, ViT-B, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "mAP":"34.2"
        },
        "raw_metrics":{
            "mAP":34.2
        },
        "uses_additional_data":true,
        "paper":{
            "id":1125592,
            "title":"Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning",
            "url":"\/paper\/masked-video-distillation-rethinking-masked",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-video-distillation-rethinking-masked\/review\/?hl=98815"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":87,
                "name":"Vision Transformer",
                "color":"#2771D3"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":73339,
        "rank":21,
        "Model":"HIT",
        "mlmodel":{

        },
        "method_short":"HIT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-23",
        "metrics":{
            "mAP":"32.6"
        },
        "raw_metrics":{
            "mAP":32.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098496,
            "title":"Holistic Interaction Transformer Network for Action Detection",
            "url":"\/paper\/holistic-interaction-transformer-network-for",
            "published":"2022-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/holistic-interaction-transformer-network-for\/review\/?hl=73339"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":58953,
        "rank":22,
        "Model":"VideoMAE (K400 pretrain+finetune, ViT-B, 16x4)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"K400 pretrain+finetune, ViT-B, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "mAP":"31.8"
        },
        "raw_metrics":{
            "mAP":31.8
        },
        "uses_additional_data":true,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=58953"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":87,
                "name":"Vision Transformer",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":41128,
        "rank":23,
        "Model":"ACAR-Net, SlowFast R-101 (Kinetics-700 pretraining)",
        "mlmodel":{

        },
        "method_short":"ACAR-Net, SlowFast R-101 ",
        "method_details":"Kinetics-700 pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-14",
        "metrics":{
            "mAP":"31.72"
        },
        "raw_metrics":{
            "mAP":31.72
        },
        "uses_additional_data":true,
        "paper":{
            "id":202344,
            "title":"Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization",
            "url":"\/paper\/actor-context-actor-relation-network-for",
            "published":"2020-06-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/actor-context-actor-relation-network-for\/review\/?hl=41128"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":98816,
        "rank":24,
        "Model":"MVD (Kinetics400 pretrain, ViT-B, 16x4)",
        "mlmodel":{

        },
        "method_short":"MVD ",
        "method_details":"Kinetics400 pretrain, ViT-B, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "mAP":"31.1"
        },
        "raw_metrics":{
            "mAP":31.1
        },
        "uses_additional_data":true,
        "paper":{
            "id":1125592,
            "title":"Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning",
            "url":"\/paper\/masked-video-distillation-rethinking-masked",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-video-distillation-rethinking-masked\/review\/?hl=98816"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":87,
                "name":"Vision Transformer",
                "color":"#2771D3"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":35319,
        "rank":25,
        "Model":"Object Transformer",
        "mlmodel":{

        },
        "method_short":"Object Transformer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-21",
        "metrics":{
            "mAP":"31.0"
        },
        "raw_metrics":{
            "mAP":31.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":821697,
            "title":"Towards Long-Form Video Understanding",
            "url":"\/paper\/towards-long-form-video-understanding-1",
            "published":"2021-06-21T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":30560,
        "rank":26,
        "Model":"MViT-B-24, 32x3 (Kinetics-600 pretraining)",
        "mlmodel":{

        },
        "method_short":"MViT-B-24, 32x3 ",
        "method_details":"Kinetics-600 pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "mAP":"28.7"
        },
        "raw_metrics":{
            "mAP":28.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30560"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":30559,
        "rank":27,
        "Model":"MViT-B, 32x3 (Kinetics-500 pretraining)",
        "mlmodel":{

        },
        "method_short":"MViT-B, 32x3 ",
        "method_details":"Kinetics-500 pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "mAP":"27.5"
        },
        "raw_metrics":{
            "mAP":27.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30559"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":30564,
        "rank":28,
        "Model":"SlowFast, 16x8 R101+NL (Kinetics-600 pretraining)",
        "mlmodel":{

        },
        "method_short":"SlowFast, 16x8 R101+NL ",
        "method_details":"Kinetics-600 pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "mAP":"27.5"
        },
        "raw_metrics":{
            "mAP":27.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=30564"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":30557,
        "rank":29,
        "Model":"MViT-B, 64x3 (Kinetics-400 pretraining)",
        "mlmodel":{

        },
        "method_short":"MViT-B, 64x3 ",
        "method_details":"Kinetics-400 pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "mAP":"27.3"
        },
        "raw_metrics":{
            "mAP":27.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30557"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":30563,
        "rank":30,
        "Model":"SlowFast, 8x8 R101+NL (Kinetics-600 pretraining)",
        "mlmodel":{

        },
        "method_short":"SlowFast, 8x8 R101+NL ",
        "method_details":"Kinetics-600 pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "mAP":"27.1"
        },
        "raw_metrics":{
            "mAP":27.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=30563"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":30556,
        "rank":31,
        "Model":"MViT-B, 32x3 (Kinetics-400 pretraining)",
        "mlmodel":{

        },
        "method_short":"MViT-B, 32x3 ",
        "method_details":"Kinetics-400 pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "mAP":"26.8"
        },
        "raw_metrics":{
            "mAP":26.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30556"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":58954,
        "rank":32,
        "Model":"VideoMAE (K400 pretrain, ViT-B, 16x4)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"K400 pretrain, ViT-B, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "mAP":"26.7"
        },
        "raw_metrics":{
            "mAP":26.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=58954"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":87,
                "name":"Vision Transformer",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":40970,
        "rank":33,
        "Model":"ORViT MViT-B, 16x4 (K400 pretraining)",
        "mlmodel":{

        },
        "method_short":"ORViT MViT-B, 16x4 ",
        "method_details":"K400 pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-13",
        "metrics":{
            "mAP":"26.6"
        },
        "raw_metrics":{
            "mAP":26.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":887844,
            "title":"Object-Region Video Transformers",
            "url":"\/paper\/object-region-video-transformers-1",
            "published":"2021-10-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/object-region-video-transformers-1\/review\/?hl=40970"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":30558,
        "rank":34,
        "Model":"MViT-B, 16x4 (Kinetics-600 pretraining)",
        "mlmodel":{

        },
        "method_short":"MViT-B, 16x4 ",
        "method_details":"Kinetics-600 pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "mAP":"26.1"
        },
        "raw_metrics":{
            "mAP":26.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30558"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":30555,
        "rank":35,
        "Model":"MViT-B, 16x4 (Kinetics-400 pretraining)",
        "mlmodel":{

        },
        "method_short":"MViT-B, 16x4 ",
        "method_details":"Kinetics-400 pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "mAP":"24.5"
        },
        "raw_metrics":{
            "mAP":24.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30555"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":30562,
        "rank":36,
        "Model":"SlowFast, 8x8, R101 (Kinetics-400 pretraining)",
        "mlmodel":{

        },
        "method_short":"SlowFast, 8x8, R101 ",
        "method_details":"Kinetics-400 pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "mAP":"23.8"
        },
        "raw_metrics":{
            "mAP":23.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=30562"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8454,
        "row_id":30561,
        "rank":37,
        "Model":"SlowFast, 4x16, R50 (Kinetics-400 pretraining)",
        "mlmodel":{

        },
        "method_short":"SlowFast, 4x16, R50 ",
        "method_details":"Kinetics-400 pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "mAP":"21.9"
        },
        "raw_metrics":{
            "mAP":21.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=30561"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]