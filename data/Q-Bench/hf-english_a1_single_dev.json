[
    {
        "Model (variant)":"InfiMM (Zephyr-7B)",
        "Yes-or-No":57.45,
        "What":57.96,
        "How":44.62,
        "Distortion":47.27,
        "Other":57.17,
        "In-context Distortion":49.67,
        "In-context Other":64.08,
        "Overall":53.37
    },
    {
        "Model (variant)":"Emu2-Chat (LLaMA-33B)",
        "Yes-or-No":71.81,
        "What":67.25,
        "How":56.18,
        "Distortion":64.78,
        "Other":63.19,
        "In-context Distortion":63.48,
        "In-context Other":72.24,
        "Overall":65.28
    },
    {
        "Model (variant)":"Fuyu-8B (Persimmon-8B)",
        "Yes-or-No":53.33,
        "What":43.7,
        "How":38.0,
        "Distortion":40.81,
        "Other":47.4,
        "In-context Distortion":45.45,
        "In-context Other":49.23,
        "Overall":45.05
    },
    {
        "Model (variant)":"BakLLava (Mistral-7B)",
        "Yes-or-No":66.0,
        "What":56.16,
        "How":51.12,
        "Distortion":51.15,
        "Other":61.57,
        "In-context Distortion":53.72,
        "In-context Other":72.0,
        "Overall":57.48
    },
    {
        "Model (variant)":"SPHINX",
        "Yes-or-No":74.18,
        "What":68.81,
        "How":62.07,
        "Distortion":63.62,
        "Other":71.76,
        "In-context Distortion":66.12,
        "In-context Other":76.33,
        "Overall":68.56
    },
    {
        "Model (variant)":"mPLUG-Owl2 (LLaMA-7B)",
        "Yes-or-No":72.18,
        "What":57.96,
        "How":56.19,
        "Distortion":56.68,
        "Other":69.21,
        "In-context Distortion":53.29,
        "In-context Other":72.65,
        "Overall":61.61
    },
    {
        "Model (variant)":"LLaVA-v1.5 (Vicuna-v1.5-7B)",
        "Yes-or-No":66.36,
        "What":58.19,
        "How":50.51,
        "Distortion":49.42,
        "Other":65.74,
        "In-context Distortion":54.61,
        "In-context Other":70.61,
        "Overall":58.66
    },
    {
        "Model (variant)":"LLaVA-v1.5 (Vicuna-v1.5-13B)",
        "Yes-or-No":65.27,
        "What":64.38,
        "How":56.59,
        "Distortion":56.03,
        "Other":67.13,
        "In-context Distortion":61.18,
        "In-context Other":67.35,
        "Overall":62.14
    },
    {
        "Model (variant)":"InternLM-XComposer-VL (InternLM)",
        "Yes-or-No":69.45,
        "What":65.27,
        "How":60.85,
        "Distortion":61.67,
        "Other":70.14,
        "In-context Distortion":56.91,
        "In-context Other":75.1,
        "Overall":65.35
    },
    {
        "Model (variant)":"IDEFICS-Instruct (LLaMA-7B)",
        "Yes-or-No":56.18,
        "What":44.69,
        "How":44.02,
        "Distortion":42.8,
        "Other":54.17,
        "In-context Distortion":44.74,
        "In-context Other":56.33,
        "Overall":48.7
    },
    {
        "Model (variant)":"Qwen-VL (QwenLM)",
        "Yes-or-No":63.09,
        "What":58.19,
        "How":56.39,
        "Distortion":50.58,
        "Other":62.73,
        "In-context Distortion":57.89,
        "In-context Other":73.88,
        "Overall":59.4
    },
    {
        "Model (variant)":"Shikra (Vicuna-7B)",
        "Yes-or-No":65.64,
        "What":47.35,
        "How":49.09,
        "Distortion":48.83,
        "Other":59.49,
        "In-context Distortion":50.0,
        "In-context Other":64.08,
        "Overall":54.65
    },
    {
        "Model (variant)":"Otter-v1 (MPT-7B)",
        "Yes-or-No":57.09,
        "What":40.71,
        "How":39.55,
        "Distortion":42.22,
        "Other":49.31,
        "In-context Distortion":44.08,
        "In-context Other":52.65,
        "Overall":46.35
    },
    {
        "Model (variant)":"InstructBLIP (Flan-T5-XL)",
        "Yes-or-No":67.64,
        "What":59.96,
        "How":55.98,
        "Distortion":56.23,
        "Other":65.51,
        "In-context Distortion":58.22,
        "In-context Other":69.39,
        "Overall":61.47
    },
    {
        "Model (variant)":"InstructBLIP (Vicuna-7B)",
        "Yes-or-No":71.64,
        "What":52.65,
        "How":43.81,
        "Distortion":48.64,
        "Other":62.5,
        "In-context Distortion":55.59,
        "In-context Other":64.9,
        "Overall":56.72
    },
    {
        "Model (variant)":"VisualGLM-6B (GLM-6B)",
        "Yes-or-No":60.18,
        "What":54.2,
        "How":46.25,
        "Distortion":51.75,
        "Other":54.4,
        "In-context Distortion":53.62,
        "In-context Other":57.14,
        "Overall":53.78
    },
    {
        "Model (variant)":"mPLUG-Owl (LLaMA-7B)",
        "Yes-or-No":66.0,
        "What":54.87,
        "How":44.02,
        "Distortion":51.36,
        "Other":55.09,
        "In-context Distortion":54.28,
        "In-context Other":65.71,
        "Overall":55.38
    },
    {
        "Model (variant)":"LLaMA-Adapter-V2",
        "Yes-or-No":66.18,
        "What":59.29,
        "How":52.13,
        "Distortion":57.39,
        "Other":56.25,
        "In-context Distortion":63.16,
        "In-context Other":64.9,
        "Overall":59.46
    },
    {
        "Model (variant)":"LLaVA-v1 (Vicuna-13B)",
        "Yes-or-No":54.0,
        "What":53.1,
        "How":55.38,
        "Distortion":48.64,
        "Other":54.63,
        "In-context Distortion":55.59,
        "In-context Other":63.27,
        "Overall":54.18
    },
    {
        "Model (variant)":"MiniGPT-4 (Vicuna-13B)",
        "Yes-or-No":55.82,
        "What":50.22,
        "How":40.37,
        "Distortion":42.02,
        "Other":48.38,
        "In-context Distortion":51.97,
        "In-context Other":61.22,
        "Overall":49.03
    },
    {
        "Model (variant)":"Qwen-VL-Plus (Close-Source)",
        "Yes-or-No":73.77,
        "What":69.47,
        "How":53.88,
        "Distortion":66.21,
        "Other":65.72,
        "In-context Distortion":63.81,
        "In-context Other":68.75,
        "Overall":66.04
    },
    {
        "Model (variant)":"Qwen-VL-Max (Close-Source)",
        "Yes-or-No":75.6,
        "What":79.43,
        "How":66.09,
        "Distortion":73.39,
        "Other":74.08,
        "In-context Distortion":71.0,
        "In-context Other":76.92,
        "Overall":73.63
    },
    {
        "Model (variant)":"Gemini-Pro (Close-Source)",
        "Yes-or-No":68.8,
        "What":73.74,
        "How":62.34,
        "Distortion":66.3,
        "Other":71.34,
        "In-context Distortion":63.91,
        "In-context Other":73.09,
        "Overall":68.16
    },
    {
        "Model (variant)":"GPT-4V (Close-Source)",
        "Yes-or-No":76.85,
        "What":79.17,
        "How":67.52,
        "Distortion":73.53,
        "Other":76.18,
        "In-context Distortion":72.83,
        "In-context Other":76.47,
        "Overall":74.51
    }
]