[
    {
        "table_id":25716,
        "row_id":113963,
        "rank":1,
        "method":"SPHINX v2",
        "mlmodel":{

        },
        "method_short":"SPHINX v2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-13",
        "metrics":{
            "Overall score":"39.48",
            "Deductive":"42.17",
            "Abductive":"49.85",
            "Analogical":"20.69",
            "Params":"16B"
        },
        "raw_metrics":{
            "Overall score":39.48,
            "Deductive":42.17,
            "Abductive":49.85,
            "Analogical":20.69,
            "Params":16.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1319249,
            "title":"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models",
            "url":"\/paper\/sphinx-the-joint-mixing-of-weights-tasks-and",
            "published":"2023-11-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sphinx-the-joint-mixing-of-weights-tasks-and\/review\/?hl=113963"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":25716,
        "row_id":112623,
        "rank":2,
        "method":"Qwen-VL-Chat",
        "mlmodel":{

        },
        "method_short":"Qwen-VL-Chat",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-24",
        "metrics":{
            "Overall score":"37.39",
            "Deductive":"37.55",
            "Abductive":"44.39",
            "Analogical":"30.42",
            "Params":"16B"
        },
        "raw_metrics":{
            "Overall score":37.39,
            "Deductive":37.55,
            "Abductive":44.39,
            "Analogical":30.42,
            "Params":16.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1269007,
            "title":"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
            "url":"\/paper\/qwen-vl-a-frontier-large-vision-language",
            "published":"2023-08-24T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":25716,
        "row_id":112624,
        "rank":3,
        "method":"CogVLM-Chat",
        "mlmodel":{

        },
        "method_short":"CogVLM-Chat",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-06",
        "metrics":{
            "Overall score":"37.16",
            "Deductive":"36.75",
            "Abductive":"47.88",
            "Analogical":"28.75",
            "Params":"17B"
        },
        "raw_metrics":{
            "Overall score":37.16,
            "Deductive":36.75,
            "Abductive":47.88,
            "Analogical":28.75,
            "Params":17.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1315192,
            "title":"CogVLM: Visual Expert for Pretrained Language Models",
            "url":"\/paper\/cogvlm-visual-expert-for-pretrained-language",
            "published":"2023-11-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cogvlm-visual-expert-for-pretrained-language\/review\/?hl=112624"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":25716,
        "row_id":112674,
        "rank":4,
        "method":"LLaVA-1.5",
        "mlmodel":{

        },
        "method_short":"LLaVA-1.5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-05",
        "metrics":{
            "Overall score":"32.62",
            "Deductive":"30.94",
            "Abductive":"47.91",
            "Analogical":"24.31",
            "Params":"13B"
        },
        "raw_metrics":{
            "Overall score":32.62,
            "Deductive":30.94,
            "Abductive":47.91,
            "Analogical":24.31,
            "Params":13.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1293065,
            "title":"Improved Baselines with Visual Instruction Tuning",
            "url":"\/paper\/improved-baselines-with-visual-instruction",
            "published":"2023-10-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-baselines-with-visual-instruction\/review\/?hl=112674"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":25716,
        "row_id":112700,
        "rank":5,
        "method":"LLaMA-Adapter V2 ",
        "mlmodel":{

        },
        "method_short":"LLaMA-Adapter V2 ",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-28",
        "metrics":{
            "Overall score":"30.46",
            "Deductive":"28.7",
            "Abductive":"46.12",
            "Analogical":"22.08",
            "Params":"7B"
        },
        "raw_metrics":{
            "Overall score":30.46,
            "Deductive":28.7,
            "Abductive":46.12,
            "Analogical":22.08,
            "Params":0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1199360,
            "title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
            "url":"\/paper\/llama-adapter-v2-parameter-efficient-visual",
            "published":"2023-04-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-adapter-v2-parameter-efficient-visual\/review\/?hl=112700"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":25716,
        "row_id":112675,
        "rank":6,
        "method":"Emu",
        "mlmodel":{

        },
        "method_short":"Emu",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-11",
        "metrics":{
            "Overall score":"28.24",
            "Deductive":"28.9",
            "Abductive":"36.57",
            "Analogical":"18.19",
            "Params":"14B"
        },
        "raw_metrics":{
            "Overall score":28.24,
            "Deductive":28.9,
            "Abductive":36.57,
            "Analogical":18.19,
            "Params":14.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1244202,
            "title":"Generative Pretraining in Multimodality",
            "url":"\/paper\/generative-pretraining-in-multimodality",
            "published":"2023-07-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/generative-pretraining-in-multimodality\/review\/?hl=112675"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":25716,
        "row_id":112701,
        "rank":7,
        "method":"InstructBLIP",
        "mlmodel":{

        },
        "method_short":"InstructBLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-11",
        "metrics":{
            "Overall score":"28.02",
            "Deductive":"27.56",
            "Abductive":"37.76",
            "Analogical":"20.56",
            "Params":"8B"
        },
        "raw_metrics":{
            "Overall score":28.02,
            "Deductive":27.56,
            "Abductive":37.76,
            "Analogical":20.56,
            "Params":0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1333348,
            "title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
            "url":"\/paper\/instructblip-towards-general-purpose-vision",
            "published":"2023-05-11T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":25716,
        "row_id":112702,
        "rank":8,
        "method":"InternLM-XComposer-VL",
        "mlmodel":{

        },
        "method_short":"InternLM-XComposer-VL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-26",
        "metrics":{
            "Overall score":"26.84",
            "Deductive":"26.77",
            "Abductive":"35.97",
            "Analogical":"18.61",
            "Params":"9B"
        },
        "raw_metrics":{
            "Overall score":26.84,
            "Deductive":26.77,
            "Abductive":35.97,
            "Analogical":18.61,
            "Params":0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1288209,
            "title":"InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition",
            "url":"\/paper\/internlm-xcomposer-a-vision-language-large",
            "published":"2023-09-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internlm-xcomposer-a-vision-language-large\/review\/?hl=112702"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":25716,
        "row_id":112703,
        "rank":9,
        "method":"Otter",
        "mlmodel":{

        },
        "method_short":"Otter",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-05",
        "metrics":{
            "Overall score":"22.69",
            "Deductive":"22.49",
            "Abductive":"33.64",
            "Analogical":"13.33",
            "Params":"7B"
        },
        "raw_metrics":{
            "Overall score":22.69,
            "Deductive":22.49,
            "Abductive":33.64,
            "Analogical":13.33,
            "Params":0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1203116,
            "title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning",
            "url":"\/paper\/otter-a-multi-modal-model-with-in-context",
            "published":"2023-05-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":25716,
        "row_id":112704,
        "rank":10,
        "method":"mPLUG-Owl2",
        "mlmodel":{

        },
        "method_short":"mPLUG-Owl2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-07",
        "metrics":{
            "Overall score":"20.05",
            "Deductive":"23.43",
            "Abductive":"20.6",
            "Analogical":"7.64",
            "Params":"7B"
        },
        "raw_metrics":{
            "Overall score":20.05,
            "Deductive":23.43,
            "Abductive":20.6,
            "Analogical":7.64,
            "Params":0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1316925,
            "title":"mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration",
            "url":"\/paper\/mplug-owl2-revolutionizing-multi-modal-large",
            "published":"2023-11-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-owl2-revolutionizing-multi-modal-large\/review\/?hl=112704"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":25716,
        "row_id":112705,
        "rank":11,
        "method":"BLIP-2-OPT2.7B",
        "mlmodel":{

        },
        "method_short":"BLIP-2-OPT2.7B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Overall score":"19.31",
            "Deductive":"2.76",
            "Abductive":"18.96",
            "Analogical":"7.5",
            "Params":"3B"
        },
        "raw_metrics":{
            "Overall score":19.31,
            "Deductive":2.76,
            "Abductive":18.96,
            "Analogical":7.5,
            "Params":0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=112705"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":25716,
        "row_id":112706,
        "rank":12,
        "method":"Fuyu-8B",
        "mlmodel":{

        },
        "method_short":"Fuyu-8B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Overall score":"15.7",
            "Deductive":"16.42",
            "Abductive":"21.49",
            "Analogical":"7.78",
            "Params":"8B"
        },
        "raw_metrics":{
            "Overall score":15.7,
            "Deductive":16.42,
            "Abductive":21.49,
            "Analogical":7.78,
            "Params":0
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":25716,
        "row_id":112707,
        "rank":13,
        "method":"MiniGPT-v2",
        "mlmodel":{

        },
        "method_short":"MiniGPT-v2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-20",
        "metrics":{
            "Overall score":"10.43",
            "Deductive":"11.02",
            "Abductive":"13.28",
            "Analogical":"5.69",
            "Params":"8B"
        },
        "raw_metrics":{
            "Overall score":10.43,
            "Deductive":11.02,
            "Abductive":13.28,
            "Analogical":5.69,
            "Params":0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1195560,
            "title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
            "url":"\/paper\/minigpt-4-enhancing-vision-language",
            "published":"2023-04-20T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":25716,
        "row_id":112708,
        "rank":14,
        "method":"OpenFlamingo-v2",
        "mlmodel":{

        },
        "method_short":"OpenFlamingo-v2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-02",
        "metrics":{
            "Overall score":"6.82",
            "Deductive":"8.88",
            "Abductive":"5.3",
            "Analogical":"1.11",
            "Params":"9B"
        },
        "raw_metrics":{
            "Overall score":6.82,
            "Deductive":8.88,
            "Abductive":5.3,
            "Analogical":1.11,
            "Params":0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1257786,
            "title":"OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models",
            "url":"\/paper\/openflamingo-an-open-source-framework-for",
            "published":"2023-08-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/openflamingo-an-open-source-framework-for\/review\/?hl=112708"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]