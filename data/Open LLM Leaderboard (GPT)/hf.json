[
    {
        "T":"?",
        "Model":"GPT-4",
        "Average":84.3,
        "ARC":96.3,
        "HellaSwag":95.3,
        "MMLU":86.4,
        "TruthfulQA":59.0,
        "Type":"Unknown",
        "Precision":null,
        "Hub License":"llama2",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":null,
        "Model Sha":"tech report"
    },
    {
        "T":"?",
        "Model":"Contamination\/contaminated_proof_7b_v1.0_safetensor",
        "Average":82.38,
        "ARC":78.07,
        "HellaSwag":90.22,
        "MMLU":78.92,
        "TruthfulQA":82.29,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"unknown",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5d7fcb3724d6b08cf82e1b0c1faa1695b9fd6932"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"davidkim205\/Rhea-72b-v0.5",
        "Average":80.84,
        "ARC":79.78,
        "HellaSwag":91.15,
        "MMLU":77.95,
        "TruthfulQA":74.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":72.29,
        "Hub":31,
        "Available on the hub":true,
        "Model Sha":"fda5cf998a0f2d89b53b5fa490793e3e50bb8239"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SF-Foundation\/Ein-72B-v0.11",
        "Average":80.51,
        "ARC":76.79,
        "HellaSwag":89.02,
        "MMLU":77.2,
        "TruthfulQA":79.02,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":72.0,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"40d451f32b1a6c9ad694b32ba8ed4822c27f3022"
    },
    {
        "T":"?",
        "Model":"MTSAIR\/MultiVerse_70B",
        "Average":80.46,
        "ARC":78.67,
        "HellaSwag":89.77,
        "MMLU":78.22,
        "TruthfulQA":75.18,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":72.29,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"ea2b4ff8e5acd7a48993f56b2d7b99e049eb6939"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MTSAIR\/MultiVerse_70B",
        "Average":80.42,
        "ARC":78.58,
        "HellaSwag":89.74,
        "MMLU":78.27,
        "TruthfulQA":75.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":72.29,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"ea2b4ff8e5acd7a48993f56b2d7b99e049eb6939"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"binbi\/Ein-72B-v0.1",
        "Average":80.33,
        "ARC":76.54,
        "HellaSwag":89.2,
        "MMLU":77.11,
        "TruthfulQA":78.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":72.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"84ec4c0fcefc5af86f649a70c9d3ff493334e868"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"davidkim205\/Rhea-72b-v0.4",
        "Average":80.29,
        "ARC":78.5,
        "HellaSwag":90.75,
        "MMLU":78.01,
        "TruthfulQA":73.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":72.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5502123c46485914a580d6794eeb5fb3554b46aa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"binbi\/Ein-72B-v0.1",
        "Average":80.28,
        "ARC":76.45,
        "HellaSwag":89.43,
        "MMLU":77.14,
        "TruthfulQA":78.09,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":72.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"84ec4c0fcefc5af86f649a70c9d3ff493334e868"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"davidkim205\/Rhea-72b-v0.2",
        "Average":80.22,
        "ARC":77.56,
        "HellaSwag":90.84,
        "MMLU":77.98,
        "TruthfulQA":74.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":72.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c51bcf1a3dc3c5e512e805f52d5e15384d798ba7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SF-Foundation\/Ein-72B-v0.12",
        "Average":80.15,
        "ARC":76.19,
        "HellaSwag":89.46,
        "MMLU":77.17,
        "TruthfulQA":77.78,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":72.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"84d38e29fec0dc9c274237968fdafe9396702f9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SF-Foundation\/Ein-72B-v0.13",
        "Average":80.13,
        "ARC":76.19,
        "HellaSwag":89.44,
        "MMLU":77.07,
        "TruthfulQA":77.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":72.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"1f302e0e15f3d3711778cd61686eb9b28b0c72ae"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"davidkim205\/Rhea-72b-v0.3",
        "Average":80.04,
        "ARC":76.79,
        "HellaSwag":89.98,
        "MMLU":77.47,
        "TruthfulQA":75.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":72.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7db39c93177958d94ebc3b719f8bfc75826b345e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Smaug-72B-v0.1",
        "Average":79.78,
        "ARC":76.02,
        "HellaSwag":89.27,
        "MMLU":77.15,
        "TruthfulQA":76.67,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":72.29,
        "Hub":430,
        "Available on the hub":true,
        "Model Sha":"54a8c35600ec5cb30ca2129247854ece23e57f57"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Smaug-70B-v0.1",
        "Average":79.78,
        "ARC":76.02,
        "HellaSwag":89.27,
        "MMLU":77.15,
        "TruthfulQA":76.67,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":72.29,
        "Hub":430,
        "Available on the hub":true,
        "Model Sha":"54a8c35600ec5cb30ca2129247854ece23e57f57"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"saltlux\/luxia-21.4b-alignment-v1.1",
        "Average":79.27,
        "ARC":78.24,
        "HellaSwag":89.69,
        "MMLU":68.22,
        "TruthfulQA":80.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":21.42,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"d1dda8b111024dc06eb3a7072100e74d5039a782"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"4season\/alignment_model_test",
        "Average":79.22,
        "ARC":78.24,
        "HellaSwag":89.68,
        "MMLU":68.08,
        "TruthfulQA":80.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":21.42,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"791a326ee0f6d5246962039803fd79b28608e54c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"4season\/alignment-model-test3",
        "Average":79.22,
        "ARC":78.24,
        "HellaSwag":89.68,
        "MMLU":68.08,
        "TruthfulQA":80.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":21.42,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e29bc3a6d611c728fd1952cd73d1b8da50375c19"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"saltlux\/luxia-21.4b-alignment-v1.1",
        "Average":79.22,
        "ARC":78.24,
        "HellaSwag":89.68,
        "MMLU":68.08,
        "TruthfulQA":80.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":21.42,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"d1dda8b111024dc06eb3a7072100e74d5039a782"
    },
    {
        "T":"?",
        "Model":"saltlux\/luxia-21.4b-alignment-v1.0",
        "Average":79.2,
        "ARC":77.73,
        "HellaSwag":91.82,
        "MMLU":68.05,
        "TruthfulQA":79.2,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":21.42,
        "Hub":20,
        "Available on the hub":false,
        "Model Sha":"910c73192c30fb51dc94f69777b2ec7cc3a4465b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"saltlux\/luxia-21.4b-alignment-v1.0",
        "Average":79.15,
        "ARC":77.47,
        "HellaSwag":91.88,
        "MMLU":68.1,
        "TruthfulQA":79.17,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":21.42,
        "Hub":20,
        "Available on the hub":false,
        "Model Sha":"ba3403eaafc6d1f6e3a73245314ee96025c08d96"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"saltlux\/luxia-21.4b-alignment-v0.2",
        "Average":79.09,
        "ARC":76.71,
        "HellaSwag":91.61,
        "MMLU":68.27,
        "TruthfulQA":79.8,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.4,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"59243de958296a4516f72ebfb1b597188dd59229"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"saltlux\/luxia-21.4b-alignment-v0.4",
        "Average":78.37,
        "ARC":76.88,
        "HellaSwag":91.83,
        "MMLU":68.06,
        "TruthfulQA":76.72,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.4,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4c4342a9c3e8e793a0969b74222d887d53cb294e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"saltlux\/luxia-21.4b-alignment-v0.1",
        "Average":78.37,
        "ARC":76.79,
        "HellaSwag":91.79,
        "MMLU":68.18,
        "TruthfulQA":76.7,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.4,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"88a47c498102132f5262581803fe1ed9252a16bc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/alpaca-dragon-72b-v1",
        "Average":78.04,
        "ARC":73.89,
        "HellaSwag":88.16,
        "MMLU":77.4,
        "TruthfulQA":72.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":72.29,
        "Hub":19,
        "Available on the hub":true,
        "Model Sha":"4df251a558c53b6b6a4c459045b161951cfc3c4e"
    },
    {
        "T":"\u2b55",
        "Model":"DopeorNope\/COKAL-v1-70B",
        "Average":77.92,
        "ARC":87.46,
        "HellaSwag":83.29,
        "MMLU":68.13,
        "TruthfulQA":72.79,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":69.44,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"6898ebe887fd7debab6b26aa650f2876c1e2f4cf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/UNA-34BeagleSimpleMath-32K-v1",
        "Average":77.59,
        "ARC":74.15,
        "HellaSwag":85.98,
        "MMLU":76.52,
        "TruthfulQA":73.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":34.39,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"ead4b4aedf94b98916f30388b85620a3583375e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JaeyeonKang\/CCK_Asura_v1",
        "Average":77.54,
        "ARC":73.89,
        "HellaSwag":89.07,
        "MMLU":75.44,
        "TruthfulQA":71.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"7dd3ddea090bd63f3143e70d7d6237cc40c046e4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/TomGrc_FusionNet_34Bx2_MoE_v0.1_DPO_f16",
        "Average":77.42,
        "ARC":74.06,
        "HellaSwag":86.74,
        "MMLU":76.65,
        "TruthfulQA":72.24,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":60.81,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"cd29cfa124072c96ba8601230bead65d76e04dcb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/Truthful_DPO_TomGrc_FusionNet_34Bx2_MoE",
        "Average":77.41,
        "ARC":72.87,
        "HellaSwag":86.52,
        "MMLU":76.96,
        "TruthfulQA":73.28,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":60.81,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"097b951c2524e6113252fcd98ba5830c85dc450f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/UNA-34Beagles-32K-v1",
        "Average":77.37,
        "ARC":73.55,
        "HellaSwag":85.93,
        "MMLU":76.45,
        "TruthfulQA":73.55,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":34.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"e02a631564990af3d9c8b0232f979af11cd8b6f6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"one-man-army\/UNA-34Beagles-32K-bf16-v1",
        "Average":77.37,
        "ARC":73.55,
        "HellaSwag":85.93,
        "MMLU":76.45,
        "TruthfulQA":73.55,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"d6024b97f624e9169a63f5faccb8c5ab121eb13a"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"sumo43\/Yi-32b-x2-v2.0",
        "Average":77.25,
        "ARC":73.04,
        "HellaSwag":85.95,
        "MMLU":76.79,
        "TruthfulQA":73.22,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":60.81,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"1e61f28b326fe0080ad476ce2b1dd041ec9f147f"
    },
    {
        "T":"?",
        "Model":"HanNayeoniee\/LHK_DPO_v1",
        "Average":77.22,
        "ARC":74.74,
        "HellaSwag":89.37,
        "MMLU":64.87,
        "TruthfulQA":79.88,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4e2c0a8fb1a1654312a573e85fec79832bfa489c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/orthorus-125b-v2",
        "Average":77.21,
        "ARC":73.63,
        "HellaSwag":89.04,
        "MMLU":75.99,
        "TruthfulQA":70.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":125.35,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"95b3b4e432d98b804d64cfe42dd9fa6b67198e5b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HanNayeoniee\/LHK_DPO_v1",
        "Average":77.21,
        "ARC":74.74,
        "HellaSwag":89.3,
        "MMLU":64.9,
        "TruthfulQA":79.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4e2c0a8fb1a1654312a573e85fec79832bfa489c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/TomGrc_FusionNet_34Bx2_MoE_v0.1_full_linear_DPO",
        "Average":77.18,
        "ARC":74.06,
        "HellaSwag":86.67,
        "MMLU":76.69,
        "TruthfulQA":71.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":60.81,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"e8e558b5fd4ac9da839577b1295d10ca75fc2663"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"moreh\/MoMo-72B-lora-1.8.7-DPO",
        "Average":77.15,
        "ARC":70.82,
        "HellaSwag":85.96,
        "MMLU":77.13,
        "TruthfulQA":74.71,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":72.29,
        "Hub":67,
        "Available on the hub":true,
        "Model Sha":"c64edea08b27be1e7e2ae6a95bcdd74849cb887e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/Mixtral_7Bx2_MoE_DPO",
        "Average":77.06,
        "ARC":73.04,
        "HellaSwag":88.76,
        "MMLU":64.94,
        "TruthfulQA":81.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.88,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"bf4cb27f17bfc58aaf6011a8ba8393a1177ebbe7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/UNA-SimpleSmaug-34b-v1beta",
        "Average":77.04,
        "ARC":74.57,
        "HellaSwag":86.74,
        "MMLU":76.68,
        "TruthfulQA":70.17,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":34.39,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"e1cdc5b02c662c5f29a50d0b22c64a8902ca856b"
    },
    {
        "T":"?",
        "Model":"ConvexAI\/Luminex-32B-v0.2",
        "Average":77.0,
        "ARC":74.49,
        "HellaSwag":86.76,
        "MMLU":76.55,
        "TruthfulQA":70.21,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"3880710724abcaffbdf8fa4031e1d02066fbfe9d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ConvexAI\/Luminex-34B-v0.2",
        "Average":77.0,
        "ARC":74.49,
        "HellaSwag":86.76,
        "MMLU":76.55,
        "TruthfulQA":70.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"3880710724abcaffbdf8fa4031e1d02066fbfe9d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TomGrc\/FusionNet_34Bx2_MoE_v0.1",
        "Average":76.98,
        "ARC":73.72,
        "HellaSwag":86.46,
        "MMLU":76.72,
        "TruthfulQA":71.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":60.81,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"6c7ec6d2ca1c0d126a26963fedc9bbdf5210b0d1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Smaugv0.1",
        "Average":76.97,
        "ARC":74.23,
        "HellaSwag":86.76,
        "MMLU":76.66,
        "TruthfulQA":70.22,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":50,
        "Available on the hub":true,
        "Model Sha":"036927bc2b54d408bb9e9357c3df8353f5853ea8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Smaug-34B-v0.1",
        "Average":76.97,
        "ARC":74.23,
        "HellaSwag":86.76,
        "MMLU":76.66,
        "TruthfulQA":70.22,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":50,
        "Available on the hub":true,
        "Model Sha":"7b74a95019f01b59630cbd6469814c752d0e59e5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TomGrc\/FusionNet_34Bx2_MoE",
        "Average":76.88,
        "ARC":72.95,
        "HellaSwag":86.22,
        "MMLU":77.05,
        "TruthfulQA":71.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":60.81,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"c5575550053c84a401baf56174cb2e5d5bd9e79a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/4bit_quant_TomGrc_FusionNet_34Bx2_MoE_v0.1_DPO",
        "Average":76.88,
        "ARC":73.21,
        "HellaSwag":86.11,
        "MMLU":75.44,
        "TruthfulQA":72.78,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"other",
        "#Params (B)":31.8,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"331bb6bdba4140bbf0031bd37076f2c8a76d7dbb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"senseable\/Wilbur-30B",
        "Average":76.85,
        "ARC":74.06,
        "HellaSwag":86.68,
        "MMLU":76.7,
        "TruthfulQA":69.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":30.0,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"eab679f95e078efb71fbaa7b1aa0be05bb4e46ca"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yunconglong\/13B_MATH_DPO",
        "Average":76.83,
        "ARC":74.66,
        "HellaSwag":89.51,
        "MMLU":64.53,
        "TruthfulQA":78.63,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"96c62ad90f2b82016a1cdbfe96cfa5c4bb278e21"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sumo43\/Yi-34b-x2",
        "Average":76.83,
        "ARC":72.87,
        "HellaSwag":85.7,
        "MMLU":76.64,
        "TruthfulQA":72.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"09876944a5d29e7f8e4da1347cd1d8f6f2151444"
    },
    {
        "T":"?",
        "Model":"alchemonaut\/QuartetAnemoi-70B-t0.0001",
        "Average":76.81,
        "ARC":73.38,
        "HellaSwag":88.9,
        "MMLU":75.42,
        "TruthfulQA":69.53,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.98,
        "Hub":25,
        "Available on the hub":true,
        "Model Sha":"392d963e63267650f2aea7dc26c60ee6fd2b26d4"
    },
    {
        "T":"?",
        "Model":"Infinimol\/miiqu-f16",
        "Average":76.8,
        "ARC":72.87,
        "HellaSwag":88.97,
        "MMLU":75.99,
        "TruthfulQA":69.37,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":90.37,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"395d6398cb2ab71621a43f5f5df8994de9c46175"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/nontoxic-bagel-34b-v0.2",
        "Average":76.8,
        "ARC":72.44,
        "HellaSwag":85.64,
        "MMLU":76.41,
        "TruthfulQA":72.7,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":35,
        "Available on the hub":true,
        "Model Sha":"08903c93d929829aabbde2681c7ad2465d7d4189"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"alchemonaut\/BoreanGale-70B",
        "Average":76.76,
        "ARC":73.89,
        "HellaSwag":89.37,
        "MMLU":75.19,
        "TruthfulQA":68.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.98,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"f7768207c1f37d3f4374dccc182d7a86c6539ead"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yunconglong\/DARE_TIES_13B",
        "Average":76.74,
        "ARC":74.32,
        "HellaSwag":89.5,
        "MMLU":64.47,
        "TruthfulQA":78.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":[
            "other"
        ],
        "#Params (B)":12.88,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"74c6e4fbd272c9d897e8c93ee7de8a234f61900f"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"yunconglong\/Truthful_DPO_TomGrc_FusionNet_7Bx2_MoE_13B",
        "Average":76.73,
        "ARC":74.91,
        "HellaSwag":89.3,
        "MMLU":64.67,
        "TruthfulQA":78.02,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":12.88,
        "Hub":46,
        "Available on the hub":false,
        "Model Sha":"915651208ea9f40c65a60d1f971a09f9461ee691"
    },
    {
        "T":"?",
        "Model":"vicgalleorg\/test1",
        "Average":76.7,
        "ARC":72.27,
        "HellaSwag":89.52,
        "MMLU":66.67,
        "TruthfulQA":78.32,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"50b34c24addd4efd365e9d3f44f370c3b6b56c2e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JaeyeonKang\/CCK_Asura_v1.1.0",
        "Average":76.69,
        "ARC":73.21,
        "HellaSwag":88.55,
        "MMLU":75.43,
        "TruthfulQA":69.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"baf3e2cc3a8d18098199b3cee4bdf79f00935be1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jefferylovely\/MoeLovely-13B",
        "Average":76.68,
        "ARC":73.72,
        "HellaSwag":89.49,
        "MMLU":64.78,
        "TruthfulQA":78.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ac4f0ad8a665eb6b54c286810a9b4551b0bcdc25"
    },
    {
        "T":"?",
        "Model":"vicgalle\/CarbonBeagle-11B-truthy",
        "Average":76.67,
        "ARC":72.27,
        "HellaSwag":89.31,
        "MMLU":66.55,
        "TruthfulQA":78.55,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"5c649b6bbb8aa16d52dda26c5ce8574d1c7a3274"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yunconglong\/MoE_13B_DPO",
        "Average":76.67,
        "ARC":74.32,
        "HellaSwag":89.39,
        "MMLU":64.48,
        "TruthfulQA":78.47,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":12.88,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"d8d6a47f877fee3e638a158c2bd637c0013ed4e4"
    },
    {
        "T":"?",
        "Model":"Undi95\/Miqu-70B-Alpaca-DPO",
        "Average":76.66,
        "ARC":73.21,
        "HellaSwag":88.6,
        "MMLU":75.41,
        "TruthfulQA":69.44,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"f7ee9b9099cd518060e9e61ff7ae11a39428bd93"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"krevas\/SOLAR-10.7B",
        "Average":76.66,
        "ARC":74.32,
        "HellaSwag":89.05,
        "MMLU":62.94,
        "TruthfulQA":80.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"9c85e654ecc292f4491e332f7f25e6870f166f1e"
    },
    {
        "T":"?",
        "Model":"152334H\/miqu-1-70b-sf",
        "Average":76.63,
        "ARC":73.04,
        "HellaSwag":88.61,
        "MMLU":75.49,
        "TruthfulQA":69.38,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub":205,
        "Available on the hub":true,
        "Model Sha":"97c24b15a7e26985fb18540800516aa2ac03ad03"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zhengr\/MixTAO-7Bx2-MoE-v8.1",
        "Average":76.63,
        "ARC":73.81,
        "HellaSwag":89.22,
        "MMLU":64.92,
        "TruthfulQA":78.57,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":27,
        "Available on the hub":false,
        "Model Sha":"2d8cff968dbfb31e0c1ccc42053ccc4d2698a390"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ConvexAI\/Luminex-34B-v0.1",
        "Average":76.61,
        "ARC":73.63,
        "HellaSwag":86.59,
        "MMLU":76.55,
        "TruthfulQA":69.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"d3efc551679d7ec00da14722d44151c948a48d25"
    },
    {
        "T":"?",
        "Model":"vicgalle\/RoleBeagle-11B",
        "Average":76.6,
        "ARC":72.35,
        "HellaSwag":89.77,
        "MMLU":66.35,
        "TruthfulQA":77.92,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"7637cbf40c746030910154e0b344c5358f35a878"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JaeyeonKang\/CCK_Asura_v3.0",
        "Average":76.58,
        "ARC":72.95,
        "HellaSwag":88.86,
        "MMLU":75.41,
        "TruthfulQA":69.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"06fd0e293aeb3b2722e3910daefcd185fad4558c"
    },
    {
        "T":"?",
        "Model":"NLPinas\/yi-bagel-2x34b",
        "Average":76.54,
        "ARC":72.7,
        "HellaSwag":85.44,
        "MMLU":76.6,
        "TruthfulQA":71.42,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"ce6765e4a2a1f5914969c9fe48e2d1e8f3e3a247"
    },
    {
        "T":"\u2b55",
        "Model":"NLPinas\/yi-bagel-2x34b-moe",
        "Average":76.54,
        "ARC":72.7,
        "HellaSwag":85.44,
        "MMLU":76.6,
        "TruthfulQA":71.42,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"a6de6f3ccb21eeef12a354c720a9a85e5e53433d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ycros\/BagelMIsteryTour-8x7B",
        "Average":76.54,
        "ARC":72.44,
        "HellaSwag":87.5,
        "MMLU":71.25,
        "TruthfulQA":74.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"e210ea8194895c3429657556b41daaf722fd44a4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"daxiongshu\/Pluto_24B_DPO_63",
        "Average":76.5,
        "ARC":73.98,
        "HellaSwag":88.17,
        "MMLU":64.49,
        "TruthfulQA":79.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"72f6e05eddabe6f3fa8891c99c4ba02aa60158c1"
    },
    {
        "T":"?",
        "Model":"louisgrc\/Montebello_7B_SLERP",
        "Average":76.48,
        "ARC":72.95,
        "HellaSwag":89.07,
        "MMLU":64.56,
        "TruthfulQA":79.33,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1097b6038dc48f86382cacb1a27c76faacf8f607"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment27-7B",
        "Average":76.46,
        "ARC":73.55,
        "HellaSwag":89.13,
        "MMLU":64.45,
        "TruthfulQA":78.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2b81b03b242a548e54e9e10af6a4c24f24a4c5fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/NeuralTrix-bf16",
        "Average":76.45,
        "ARC":72.87,
        "HellaSwag":89.12,
        "MMLU":64.27,
        "TruthfulQA":79.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"9bc11a59594b3a4c601f3e86d88ef363fb5000f5"
    },
    {
        "T":"?",
        "Model":"ycros\/BagelMIsteryTour-v2-8x7B",
        "Average":76.44,
        "ARC":72.7,
        "HellaSwag":87.36,
        "MMLU":71.16,
        "TruthfulQA":74.54,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":15,
        "Available on the hub":false,
        "Model Sha":"98a8b319707be3dab1659594da69a37ed8f8c148"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment24-7B",
        "Average":76.43,
        "ARC":73.81,
        "HellaSwag":89.06,
        "MMLU":64.34,
        "TruthfulQA":78.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b7f5aa8d4c899c175a1dad40a03b4071df90bd8e"
    },
    {
        "T":"?",
        "Model":"jsfs11\/MixtureofMerges-MoE-2x7b-v6",
        "Average":76.41,
        "ARC":73.38,
        "HellaSwag":89.16,
        "MMLU":64.53,
        "TruthfulQA":78.58,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"3dff5c8580d594e1da355a2745106c82f4b6e3d7"
    },
    {
        "T":"?",
        "Model":"automerger\/OgnoExperiment27-7B",
        "Average":76.41,
        "ARC":73.38,
        "HellaSwag":89.4,
        "MMLU":64.43,
        "TruthfulQA":78.41,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"4c81b65d91016bbec6479f6ff839ebe6985abac8"
    },
    {
        "T":"?",
        "Model":"automerger\/Experiment27Pastiche-7B",
        "Average":76.41,
        "ARC":73.04,
        "HellaSwag":89.08,
        "MMLU":64.2,
        "TruthfulQA":79.31,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f69af11ca954a3441cca023a9e1cb6bb8bf4eb66"
    },
    {
        "T":"?",
        "Model":"arcee-ai\/Clown-DPO-Extended",
        "Average":76.38,
        "ARC":73.12,
        "HellaSwag":89.09,
        "MMLU":64.52,
        "TruthfulQA":78.78,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a6c74a9d47c1c311d8387877f85c4ae0f70eacca"
    },
    {
        "T":"?",
        "Model":"automerger\/YamshadowExperiment28-7B",
        "Average":76.36,
        "ARC":73.29,
        "HellaSwag":89.25,
        "MMLU":64.38,
        "TruthfulQA":78.53,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"b8f628c51f138538afc4c3d0d7dbcbab523c3b7a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment31-7B",
        "Average":76.35,
        "ARC":73.55,
        "HellaSwag":89.19,
        "MMLU":64.36,
        "TruthfulQA":78.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a27e0dfaf79af8da32fc4ff6c5eb8be46c9f5a13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment31-7B",
        "Average":76.35,
        "ARC":73.55,
        "HellaSwag":89.14,
        "MMLU":64.29,
        "TruthfulQA":78.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a27e0dfaf79af8da32fc4ff6c5eb8be46c9f5a13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-72B-v1.5b",
        "Average":76.35,
        "ARC":71.25,
        "HellaSwag":85.53,
        "MMLU":76.63,
        "TruthfulQA":71.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":72.29,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"dc092ecc5d5a424678eac445a9f4443069776691"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment29-7B",
        "Average":76.35,
        "ARC":73.12,
        "HellaSwag":89.06,
        "MMLU":64.49,
        "TruthfulQA":78.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"11a51df04f85047e166d63eb64cedc1ec02732a1"
    },
    {
        "T":"?",
        "Model":"automerger\/PasticheInex12-7B",
        "Average":76.34,
        "ARC":73.81,
        "HellaSwag":89.25,
        "MMLU":64.76,
        "TruthfulQA":77.56,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8916b8fab824480388c1aa6e72f81c1fc752cc5d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/NeuralCeptrix-7B-SLERP",
        "Average":76.34,
        "ARC":72.44,
        "HellaSwag":89.3,
        "MMLU":64.5,
        "TruthfulQA":79.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e16e6c9f76d8521f1c535cb9b1e940c63449d0ff"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"saltlux\/luxia-21.4b-alignment-v0.3",
        "Average":76.33,
        "ARC":76.28,
        "HellaSwag":91.53,
        "MMLU":68.1,
        "TruthfulQA":69.44,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.4,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"89d77a1219490fc423615f3ca28c1888bb4845a5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/MergeCeption-7B-v3",
        "Average":76.33,
        "ARC":72.95,
        "HellaSwag":89.18,
        "MMLU":64.59,
        "TruthfulQA":78.62,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"c7df5c504a5e057be540470a27a02579338884fb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bardsai\/jaskier-7b-dpo-v7.1",
        "Average":76.33,
        "ARC":73.38,
        "HellaSwag":89.28,
        "MMLU":64.37,
        "TruthfulQA":78.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"305544e9edd98253540141e91653d308e9b135cc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/pastiche-crown-clown-7b-dare-dpo",
        "Average":76.31,
        "ARC":72.78,
        "HellaSwag":89.15,
        "MMLU":64.51,
        "TruthfulQA":78.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1e1cd6e84d02a9c1d70c2a2037f485bc2b646391"
    },
    {
        "T":"?",
        "Model":"jsfs11\/MixtureofMerges-MoE-2x7b-v7",
        "Average":76.31,
        "ARC":73.21,
        "HellaSwag":89.05,
        "MMLU":64.63,
        "TruthfulQA":78.34,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"abf8d9dc6522658ab943bf69c475f899d66bcc20"
    },
    {
        "T":"?",
        "Model":"vicgalle\/ConfigurableBeagle-11B",
        "Average":76.3,
        "ARC":72.53,
        "HellaSwag":88.85,
        "MMLU":66.71,
        "TruthfulQA":77.13,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"fbf1c9958c47062e2db30276c723867c0d019652"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chihoonlee10\/T3Q-EN-DPO-Mistral-7B",
        "Average":76.3,
        "ARC":73.04,
        "HellaSwag":89.3,
        "MMLU":64.13,
        "TruthfulQA":78.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b1ec306bf85762b28ce29ac71924bb9a8fa01e5a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AiMavenAi\/Prometheus-1.3",
        "Average":76.29,
        "ARC":72.61,
        "HellaSwag":89.02,
        "MMLU":64.26,
        "TruthfulQA":79.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c241960b69943b3d32b8af110bbed20508265334"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment25-7B",
        "Average":76.29,
        "ARC":73.21,
        "HellaSwag":89.01,
        "MMLU":64.45,
        "TruthfulQA":78.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"aa6e42036cea01cb99426a9333481b353fd36e61"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LeroyDyer\/Mixtral_AI_Cyber_3.m1",
        "Average":76.28,
        "ARC":74.06,
        "HellaSwag":88.96,
        "MMLU":64.45,
        "TruthfulQA":77.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0da1865ae1ce682d4002dd9935d20520e79ed520"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment26-7B",
        "Average":76.27,
        "ARC":73.38,
        "HellaSwag":89.15,
        "MMLU":64.32,
        "TruthfulQA":78.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":71,
        "Available on the hub":false,
        "Model Sha":"bbaef291e93a7f6c9f8cb76a4dbd8c3c054d3f3c"
    },
    {
        "T":"?",
        "Model":"mayacinka\/yam-jom-7B",
        "Average":76.27,
        "ARC":73.38,
        "HellaSwag":89.15,
        "MMLU":64.51,
        "TruthfulQA":78.04,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fdd98b8000db4e2a9112184fa384de812069b5cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jefferylovely\/AiMaven-Merkaba-7b",
        "Average":76.27,
        "ARC":73.21,
        "HellaSwag":89.03,
        "MMLU":64.53,
        "TruthfulQA":78.3,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"af1576f357ce8c5c3ee2e8bda45f8ffd7e0535f0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chihoonlee10\/T3Q-Mistral-Orca-Math-DPO",
        "Average":76.25,
        "ARC":72.95,
        "HellaSwag":89.23,
        "MMLU":64.42,
        "TruthfulQA":78.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"f136ec75c9fb7c86c071291ddf418089c8f43da0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment28-7B",
        "Average":76.25,
        "ARC":73.04,
        "HellaSwag":89.04,
        "MMLU":64.44,
        "TruthfulQA":78.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5efde29924cf7158e4cbd642311a92a14e85597c"
    },
    {
        "T":"?",
        "Model":"abideen\/AlphaMonarch-dora",
        "Average":76.24,
        "ARC":73.21,
        "HellaSwag":89.26,
        "MMLU":64.47,
        "TruthfulQA":78.02,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"34e3f31067be2bcbf86c8af9d137db227b2ece20"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mayacinka\/yam-jom-7B-dare",
        "Average":76.23,
        "ARC":73.38,
        "HellaSwag":89.14,
        "MMLU":64.38,
        "TruthfulQA":78.04,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5d79006083e269006e4cfdf8ebe2e902a258e6f3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chihoonlee10\/T3Q-DPO-Mistral-7B",
        "Average":76.22,
        "ARC":72.78,
        "HellaSwag":89.29,
        "MMLU":64.25,
        "TruthfulQA":78.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"87382cefca257137b983fd01d0e6a8839704d75e"
    },
    {
        "T":"?",
        "Model":"mlabonne\/UltraMerge-7B",
        "Average":76.21,
        "ARC":73.04,
        "HellaSwag":89.25,
        "MMLU":64.4,
        "TruthfulQA":78.17,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"cd343f0846ceb4180297920b2da50d6b28dcb242"
    },
    {
        "T":"?",
        "Model":"LewisDeBenoisIV\/Jason1903_SLERP",
        "Average":76.2,
        "ARC":73.12,
        "HellaSwag":89.13,
        "MMLU":64.43,
        "TruthfulQA":78.13,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ea187cf89f44197d9007798316a087bc63286227"
    },
    {
        "T":"?",
        "Model":"automerger\/ShadowYam-7B",
        "Average":76.2,
        "ARC":73.21,
        "HellaSwag":89.07,
        "MMLU":64.49,
        "TruthfulQA":78.05,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0525f9aa8d500470fcf80f7b20390c1533c73a1c"
    },
    {
        "T":"?",
        "Model":"MSL7\/INEX8-7B",
        "Average":76.2,
        "ARC":73.29,
        "HellaSwag":89.19,
        "MMLU":64.47,
        "TruthfulQA":77.83,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"4b63f8040ad51f7d265722fa65758a4d7e6acec3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/NeuralMaxime-7B-slerp",
        "Average":76.19,
        "ARC":73.38,
        "HellaSwag":89.18,
        "MMLU":64.44,
        "TruthfulQA":77.79,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"055dc83f36a3a6c6d477dba1547f60a9592b3978"
    },
    {
        "T":"?",
        "Model":"louisgrc\/Marengoli_7B_SLERP",
        "Average":76.19,
        "ARC":73.63,
        "HellaSwag":89.24,
        "MMLU":64.68,
        "TruthfulQA":77.23,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"86b0adb1715855794161ba18db1c115f7ffa6ad7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment30-7B",
        "Average":76.19,
        "ARC":73.38,
        "HellaSwag":89.13,
        "MMLU":64.28,
        "TruthfulQA":77.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ff261dadc107d0ce67b836a052d7131f9d9e4260"
    },
    {
        "T":"?",
        "Model":"AurelPx\/Percival_01-7b-slerp",
        "Average":76.19,
        "ARC":73.21,
        "HellaSwag":89.16,
        "MMLU":64.42,
        "TruthfulQA":77.97,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6d415ca49b7717b8e851ae3271f569e83d4de589"
    },
    {
        "T":"?",
        "Model":"louisbrulenaudet\/Maxine-7B-0401-stock",
        "Average":76.18,
        "ARC":73.12,
        "HellaSwag":89.13,
        "MMLU":64.42,
        "TruthfulQA":78.07,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a23c75b9b6d9c47bdd106af999f6a33c981e2bd6"
    },
    {
        "T":"?",
        "Model":"nlpguy\/T3QM7X",
        "Average":76.18,
        "ARC":73.12,
        "HellaSwag":89.14,
        "MMLU":64.45,
        "TruthfulQA":78.02,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"299f7b97eb2791b3ef492e1addfe5706bff92e29"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment30-7B",
        "Average":76.18,
        "ARC":73.46,
        "HellaSwag":89.09,
        "MMLU":64.4,
        "TruthfulQA":77.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ff261dadc107d0ce67b836a052d7131f9d9e4260"
    },
    {
        "T":"?",
        "Model":"nlpguy\/T3QM7",
        "Average":76.18,
        "ARC":73.12,
        "HellaSwag":89.14,
        "MMLU":64.48,
        "TruthfulQA":77.96,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fa6bd0d1019345cddabd90127c6a8f524a0d7a67"
    },
    {
        "T":"?",
        "Model":"abideen\/AlphaMonarch-daser",
        "Average":76.17,
        "ARC":73.04,
        "HellaSwag":89.23,
        "MMLU":64.43,
        "TruthfulQA":78.01,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"5b806671b663295f5212704dfb7373ddfefe804f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/shadow-clown-7B-slerp",
        "Average":76.17,
        "ARC":73.38,
        "HellaSwag":89.05,
        "MMLU":64.32,
        "TruthfulQA":77.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"432ca89337ce47e2dd4703fffe1703f57d2b67d3"
    },
    {
        "T":"?",
        "Model":"rizla\/rizla-17",
        "Average":76.17,
        "ARC":73.63,
        "HellaSwag":89.72,
        "MMLU":64.4,
        "TruthfulQA":76.93,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":15.64,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"06aa2af4648aef092e914c9ae518a4ae2ec55f04"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/Calme-7B-Instruct-v0.2",
        "Average":76.17,
        "ARC":73.12,
        "HellaSwag":89.19,
        "MMLU":64.36,
        "TruthfulQA":78.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"a283f4e8169009d683b329ae1a96c9a77ce5936a"
    },
    {
        "T":"?",
        "Model":"abideen\/AlphaMonarch-laser",
        "Average":76.17,
        "ARC":73.12,
        "HellaSwag":89.21,
        "MMLU":64.43,
        "TruthfulQA":77.9,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"bff2cd7ba1f8a742cd22cd9df22485636c3b6410"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nlpguy\/T3QM7XP",
        "Average":76.16,
        "ARC":73.04,
        "HellaSwag":89.12,
        "MMLU":64.45,
        "TruthfulQA":78.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1da031f9fdf04ea93b04e0bba7672560ea9d6255"
    },
    {
        "T":"?",
        "Model":"Eurdem\/megatron_2.1_MoE_2x7B",
        "Average":76.16,
        "ARC":72.95,
        "HellaSwag":88.94,
        "MMLU":64.56,
        "TruthfulQA":78.2,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"3104c1f36336085fdf0ad44e62695b0215c5e5b9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/Calme-7B-Instruct-v0.1.1",
        "Average":76.16,
        "ARC":72.95,
        "HellaSwag":89.26,
        "MMLU":64.32,
        "TruthfulQA":78.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"22a9da7289d20a1d5452f77aa5bc49e97344af52"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rwitz\/experiment26-truthy-iter-0",
        "Average":76.15,
        "ARC":73.29,
        "HellaSwag":89.11,
        "MMLU":64.35,
        "TruthfulQA":77.86,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cd8bfad664fb7f9b017388d974dd3265f8c40396"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"automerger\/NeuralsirkrishnaExperiment26-7B",
        "Average":76.15,
        "ARC":73.89,
        "HellaSwag":89.14,
        "MMLU":64.32,
        "TruthfulQA":77.25,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d9d92b2ae2ce1dd459170896bb5eff9325660916"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/MonaTrix-v4",
        "Average":76.15,
        "ARC":73.38,
        "HellaSwag":89.11,
        "MMLU":64.08,
        "TruthfulQA":78.02,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"323db6a9bd5ce5e56e663a954838f446b3aeb385"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment26-7B",
        "Average":76.15,
        "ARC":73.12,
        "HellaSwag":89.12,
        "MMLU":64.3,
        "TruthfulQA":78.04,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":71,
        "Available on the hub":false,
        "Model Sha":"bbaef291e93a7f6c9f8cb76a4dbd8c3c054d3f3c"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/strange_3236-7B",
        "Average":76.14,
        "ARC":73.21,
        "HellaSwag":88.96,
        "MMLU":64.78,
        "TruthfulQA":77.6,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4f72a5270fadf90343354174bd37b796540b822e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eren23\/ogno-monarch-jaskier-merge-7b-v2",
        "Average":76.13,
        "ARC":72.87,
        "HellaSwag":89.15,
        "MMLU":64.77,
        "TruthfulQA":77.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0468ff62a3f4b152a80c9acf34a8419b01bb9569"
    },
    {
        "T":"?",
        "Model":"mayacinka\/yam-jom-7B-ties",
        "Average":76.13,
        "ARC":73.21,
        "HellaSwag":89.05,
        "MMLU":64.77,
        "TruthfulQA":77.51,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f57717445a39fdaf5cae2eafb2c46576e4481e6d"
    },
    {
        "T":"?",
        "Model":"mlabonne\/Zebrafish-7B",
        "Average":76.13,
        "ARC":73.12,
        "HellaSwag":89.13,
        "MMLU":64.36,
        "TruthfulQA":77.92,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"b88aacba35524dd4cb11f931bb502ecba2849482"
    },
    {
        "T":"?",
        "Model":"MSL7\/INEX12-7b",
        "Average":76.13,
        "ARC":72.95,
        "HellaSwag":89.14,
        "MMLU":64.4,
        "TruthfulQA":78.04,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7c78dddda4c5b72bf5c6e0efb64f52772ff1ae84"
    },
    {
        "T":"?",
        "Model":"mlabonne\/AlphaMonarch-7B",
        "Average":76.13,
        "ARC":73.04,
        "HellaSwag":89.18,
        "MMLU":64.4,
        "TruthfulQA":77.91,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":131,
        "Available on the hub":false,
        "Model Sha":"9a1c8000e25d27264c66c58603590f0acb8ef168"
    },
    {
        "T":"?",
        "Model":"eren23\/ogno-monarch-jaskier-merge-7b-OH-PREF-DPO-v4-test",
        "Average":76.13,
        "ARC":73.12,
        "HellaSwag":89.09,
        "MMLU":64.79,
        "TruthfulQA":77.52,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ff89febead2585b2a1efae12b53887b18c283a8a"
    },
    {
        "T":"?",
        "Model":"BuckeyBarnes\/TriFusionNexus-7b",
        "Average":76.13,
        "ARC":72.78,
        "HellaSwag":89.17,
        "MMLU":64.44,
        "TruthfulQA":78.13,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"eb1fccccc6f51f19590fb24f320219915f1e9b87"
    },
    {
        "T":"?",
        "Model":"bardsai\/jaskier-7b-dpo-v3.3",
        "Average":76.13,
        "ARC":72.27,
        "HellaSwag":88.89,
        "MMLU":64.34,
        "TruthfulQA":79.0,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e1460ba3fe5adcad670796528d9a163e13099c6d"
    },
    {
        "T":"?",
        "Model":"mlabonne\/NeuralMonarch-7B",
        "Average":76.12,
        "ARC":73.21,
        "HellaSwag":89.09,
        "MMLU":64.41,
        "TruthfulQA":77.79,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"d98f13e5e25a34bfa67d310e5922c5a2ffb6c4eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chlee10\/T3Q-Merge-Mistral7B",
        "Average":76.12,
        "ARC":72.95,
        "HellaSwag":89.15,
        "MMLU":64.44,
        "TruthfulQA":77.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"03405145ca06170f1b2e0acc838f573f0e090df8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/NeuralMona_MoE-4x7B",
        "Average":76.12,
        "ARC":73.89,
        "HellaSwag":89.02,
        "MMLU":64.31,
        "TruthfulQA":77.27,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"869c5cafb3f5002a0d273621519e3f352418eded"
    },
    {
        "T":"?",
        "Model":"eren23\/ogno-monarch-jaskier-merge-7b-OH-PREF-DPO",
        "Average":76.11,
        "ARC":73.12,
        "HellaSwag":89.09,
        "MMLU":64.8,
        "TruthfulQA":77.45,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"f92057866ff68bf215487d34ca1080707bb4e98c"
    },
    {
        "T":"?",
        "Model":"eren23\/ogno-monarch-jaskier-merge-7b-OH-PREF-DPO-v2",
        "Average":76.11,
        "ARC":73.12,
        "HellaSwag":89.07,
        "MMLU":64.8,
        "TruthfulQA":77.46,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c00b0fa78ab41aec778209fdf7640ebbe6d83065"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JaeyeonKang\/CCK_Gony_v3",
        "Average":76.11,
        "ARC":71.33,
        "HellaSwag":88.71,
        "MMLU":71.07,
        "TruthfulQA":73.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"73b5302f1efc7ba87e123cfed0c9c998e098c16a"
    },
    {
        "T":"?",
        "Model":"liminerity\/Multiverse-Experiment-slerp-7b",
        "Average":76.11,
        "ARC":72.87,
        "HellaSwag":89.15,
        "MMLU":64.5,
        "TruthfulQA":77.93,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"2103c07a06ff4d6e7f4c031b98d4c1a455690436"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"liminerity\/M7-7b",
        "Average":76.11,
        "ARC":72.87,
        "HellaSwag":89.15,
        "MMLU":64.5,
        "TruthfulQA":77.93,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"23497a39fe5d290494fad49e5b8077f76440ad11"
    },
    {
        "T":"?",
        "Model":"automerger\/ShadowYamshadow-7B",
        "Average":76.11,
        "ARC":72.7,
        "HellaSwag":88.99,
        "MMLU":64.65,
        "TruthfulQA":78.1,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"882a7d1f6d25f7f1363074f608ad8f78c26812a0"
    },
    {
        "T":"?",
        "Model":"AurelPx\/Meliodas-7b-dare",
        "Average":76.11,
        "ARC":72.87,
        "HellaSwag":89.11,
        "MMLU":64.43,
        "TruthfulQA":78.02,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"2bae3c13529794307f17d9bfca4e6881736909a4"
    },
    {
        "T":"?",
        "Model":"rizla\/raccoon-small",
        "Average":76.1,
        "ARC":74.4,
        "HellaSwag":88.73,
        "MMLU":64.55,
        "TruthfulQA":76.74,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.19,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"79d748d0646f11bd18a8d785000c63279a9a5cde"
    },
    {
        "T":"?",
        "Model":"automerger\/YamShadow-7B",
        "Average":76.1,
        "ARC":72.53,
        "HellaSwag":88.9,
        "MMLU":64.64,
        "TruthfulQA":78.35,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"53746d4d0be4a0b0c2ec5decaeff28f692e06216"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eren23\/ogno-monarch-jaskier-merge-7b-OH-PREF-DPO-v3",
        "Average":76.1,
        "ARC":73.04,
        "HellaSwag":89.11,
        "MMLU":64.79,
        "TruthfulQA":77.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dd1a314a04b8b4faf33e7d5037a71246d3e65bad"
    },
    {
        "T":"?",
        "Model":"automerger\/Strangemerges_32Yamshadow-7B",
        "Average":76.1,
        "ARC":72.95,
        "HellaSwag":88.88,
        "MMLU":64.52,
        "TruthfulQA":78.06,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4efa1c02b2a14eda16906102992ae2fb7c6c06a5"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_32-7B-slerp",
        "Average":76.1,
        "ARC":72.95,
        "HellaSwag":89.0,
        "MMLU":64.52,
        "TruthfulQA":77.94,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"47f8a90a2c35e0affbba8bdaaf458e1aca3e3599"
    },
    {
        "T":"?",
        "Model":"eren23\/dpo-binarized-NeuralTrix-7B",
        "Average":76.1,
        "ARC":72.35,
        "HellaSwag":88.89,
        "MMLU":64.09,
        "TruthfulQA":79.07,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"853370b5907d272f93870b47e67a5622da643801"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MTSAIR\/multi_verse_model",
        "Average":76.1,
        "ARC":72.87,
        "HellaSwag":89.2,
        "MMLU":64.4,
        "TruthfulQA":77.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":19,
        "Available on the hub":false,
        "Model Sha":"a4ca706d1bbc263b95e223a80ad68b0f125840b3"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"ammarali32\/multi_verse_model",
        "Average":76.1,
        "ARC":72.87,
        "HellaSwag":89.2,
        "MMLU":64.4,
        "TruthfulQA":77.92,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":19,
        "Available on the hub":false,
        "Model Sha":"e2aa6fdad0b28a6019b0fc7c178a3579c3d671e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/crown-clown-7b-slerp",
        "Average":76.09,
        "ARC":73.46,
        "HellaSwag":89.26,
        "MMLU":64.13,
        "TruthfulQA":77.52,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bfe7413f586fe57ce629e04b2cb08b67a8775bc7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eren23\/ogno-monarch-jaskier-merge-7b",
        "Average":76.09,
        "ARC":73.04,
        "HellaSwag":89.09,
        "MMLU":64.78,
        "TruthfulQA":77.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"a1179e6c346ba93db60c45d6d219ca86f2260102"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"NExtNewChattingAI\/Mutliverse_model_official",
        "Average":76.09,
        "ARC":72.87,
        "HellaSwag":89.13,
        "MMLU":64.42,
        "TruthfulQA":77.93,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"4b8f0409e2844641fe9bd6d45385f4e71e0ee940"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rwitz\/experiment26-truthy-iter-1",
        "Average":76.08,
        "ARC":73.21,
        "HellaSwag":89.13,
        "MMLU":64.34,
        "TruthfulQA":77.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cb04e33c4ff559b31767765100cd50c24ec2531c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/NeuralTrix-7B-dpo",
        "Average":76.08,
        "ARC":72.27,
        "HellaSwag":88.91,
        "MMLU":64.06,
        "TruthfulQA":79.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"f820ce616f062fa76c13845fb198490418676223"
    },
    {
        "T":"?",
        "Model":"AtAndDev\/Ogno-Monarch-Neurotic-7B-Dare-Ties",
        "Average":76.08,
        "ARC":73.21,
        "HellaSwag":88.99,
        "MMLU":64.58,
        "TruthfulQA":77.53,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4f49f24bfdbbcab0ec9195d62ff3cece08f93b26"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/NeuralMona_MoE-4x7B",
        "Average":76.07,
        "ARC":73.72,
        "HellaSwag":89.03,
        "MMLU":64.3,
        "TruthfulQA":77.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"869c5cafb3f5002a0d273621519e3f352418eded"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"222limin\/Nexim-7b",
        "Average":76.07,
        "ARC":73.04,
        "HellaSwag":89.1,
        "MMLU":64.48,
        "TruthfulQA":77.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a585b520352fc0d9fc68f811d3e2c903ef4230ec"
    },
    {
        "T":"?",
        "Model":"automerger\/Experiment27Neuralsirkrishna-7B",
        "Average":76.07,
        "ARC":73.21,
        "HellaSwag":89.04,
        "MMLU":64.62,
        "TruthfulQA":77.4,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6be791ab6dff0e9b0e222743d2973717ef5250c1"
    },
    {
        "T":"?",
        "Model":"bardsai\/jaskier-7b-dpo-v4.3",
        "Average":76.06,
        "ARC":72.61,
        "HellaSwag":89.09,
        "MMLU":64.29,
        "TruthfulQA":78.27,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dc22b7692b8d54575545f1614029ebc898c9a6e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/shadow-clown-7B-dare",
        "Average":76.06,
        "ARC":72.61,
        "HellaSwag":88.86,
        "MMLU":64.44,
        "TruthfulQA":78.33,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"f292aba5b64521ba8e0d5e8469d1380394ff22f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bardsai\/jaskier-7b-dpo-v5.6",
        "Average":76.06,
        "ARC":73.04,
        "HellaSwag":89.0,
        "MMLU":64.38,
        "TruthfulQA":77.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":26,
        "Available on the hub":false,
        "Model Sha":"25c0f5c1edad0ed1ab02347adf02fe03e0a3b62a"
    },
    {
        "T":"?",
        "Model":"nlpguy\/AlloyIngotNeoY",
        "Average":76.05,
        "ARC":72.78,
        "HellaSwag":89.12,
        "MMLU":64.32,
        "TruthfulQA":77.99,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e90d4a4a13d5ff4d3bee099212ffc1e1985d8236"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment23-7B",
        "Average":76.04,
        "ARC":72.35,
        "HellaSwag":88.77,
        "MMLU":64.17,
        "TruthfulQA":78.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1442ca4e728892f18ef101c4987bdf11ef5bbae5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment21-7B",
        "Average":76.04,
        "ARC":71.42,
        "HellaSwag":89.03,
        "MMLU":63.92,
        "TruthfulQA":79.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"36a9851b8c9213c4e1bcfd2c46b3f799c36caa69"
    },
    {
        "T":"?",
        "Model":"macadeliccc\/SmaugDolphin-60B",
        "Average":76.04,
        "ARC":73.38,
        "HellaSwag":86.55,
        "MMLU":76.78,
        "TruthfulQA":67.44,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":60.81,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"cf8bc1d4103c4fde2a3da0cf86bdfcec95f5fe35"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rwitz\/experiment26-truthy-iter-2",
        "Average":76.04,
        "ARC":73.38,
        "HellaSwag":89.11,
        "MMLU":64.36,
        "TruthfulQA":77.3,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1dc4edde961960f7263dc3bdd37ca9e9f7e451ea"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/NeuralTrixlaser-bf16",
        "Average":76.03,
        "ARC":72.18,
        "HellaSwag":89.06,
        "MMLU":64.21,
        "TruthfulQA":78.69,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"942b35ccb48ded2f0fd462c21b6e1df35b4ea910"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MSL7\/INEX16-7b",
        "Average":76.03,
        "ARC":73.12,
        "HellaSwag":89.1,
        "MMLU":64.56,
        "TruthfulQA":77.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"def8a359e2fb6ee05ecda3577f158d9838b029c4"
    },
    {
        "T":"?",
        "Model":"mayacinka\/yam-jom-7B-slerp",
        "Average":76.03,
        "ARC":72.7,
        "HellaSwag":89.02,
        "MMLU":64.64,
        "TruthfulQA":77.77,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"24f3ae950139f9962e34003d567ba2825ec39e64"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/pastiche-crown-clown-7b-dare",
        "Average":76.02,
        "ARC":73.81,
        "HellaSwag":89.09,
        "MMLU":64.65,
        "TruthfulQA":76.55,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6e4720f8466ceb624e2a60986fe1fc00cf9a75cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/neurotic-crown-clown-7b-tak-stack-dpo",
        "Average":76.02,
        "ARC":72.44,
        "HellaSwag":88.73,
        "MMLU":64.56,
        "TruthfulQA":78.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ec27a21a66dc4411f24f36d585787853ba2e6354"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_30-7B-slerp",
        "Average":76.02,
        "ARC":74.15,
        "HellaSwag":89.15,
        "MMLU":64.65,
        "TruthfulQA":76.12,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"82906a18499932d2a6f029a2782839390e5ef811"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bardsai\/jaskier-7b-dpo-v6.1",
        "Average":76.01,
        "ARC":73.29,
        "HellaSwag":88.89,
        "MMLU":64.39,
        "TruthfulQA":77.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"aa3528c04c38fa49b5b65e1d064c46db3e9774f1"
    },
    {
        "T":"?",
        "Model":"liminerity\/Neurotic-Jomainotrik-7b-slerp",
        "Average":76.01,
        "ARC":72.95,
        "HellaSwag":89.15,
        "MMLU":64.28,
        "TruthfulQA":77.64,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c9925fc46ab14f2b2d3200802555d6bd3cc8c61b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment22-7B",
        "Average":76.0,
        "ARC":71.5,
        "HellaSwag":88.89,
        "MMLU":64.13,
        "TruthfulQA":79.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"46afad714b0528863bcf67b2bf5fcd4318235ccf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/bagel-dpo-34b-v0.2",
        "Average":76.0,
        "ARC":72.01,
        "HellaSwag":85.24,
        "MMLU":76.58,
        "TruthfulQA":70.16,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":87,
        "Available on the hub":true,
        "Model Sha":"fcc6ada5ea6dbf2f644d26b545ac402d2202cc74"
    },
    {
        "T":"?",
        "Model":"yleo\/EmertonMonarch-7B",
        "Average":76.0,
        "ARC":72.7,
        "HellaSwag":89.16,
        "MMLU":64.05,
        "TruthfulQA":78.09,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"1a8a1ce0ceea0e298d9c8d5cce0b869a4a8c0514"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/Neural-Krishna-Multiverse-7b-v2",
        "Average":75.99,
        "ARC":72.95,
        "HellaSwag":89.06,
        "MMLU":64.62,
        "TruthfulQA":77.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8bacb25bdd4a81b7725c6d72845f5e1519b0a1b0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jefferylovely\/Merkaba-Maven-0.1",
        "Average":75.99,
        "ARC":72.87,
        "HellaSwag":89.2,
        "MMLU":64.45,
        "TruthfulQA":77.45,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"13377de9ab6ccde2b8bf4fec28f271d4e07a93bc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/Neural-Krishna-Multiverse-7b-v3",
        "Average":75.97,
        "ARC":72.87,
        "HellaSwag":89.07,
        "MMLU":64.55,
        "TruthfulQA":77.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"26b8fbb836dbc8a72412db625ab2de858c60bad4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/Ramakrishna-7b-v3",
        "Average":75.97,
        "ARC":73.63,
        "HellaSwag":89.0,
        "MMLU":64.57,
        "TruthfulQA":76.67,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ea88953423fb5a18feef2f27954c8af1a5b5a489"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SF-Foundation\/TextSum-v0.1",
        "Average":75.96,
        "ARC":72.78,
        "HellaSwag":89.44,
        "MMLU":64.37,
        "TruthfulQA":77.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6cad111a218f29f42eb8887a359ec648423db7f4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SF-Foundation\/TextBase-v0.1",
        "Average":75.96,
        "ARC":72.78,
        "HellaSwag":89.44,
        "MMLU":64.37,
        "TruthfulQA":77.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6cad111a218f29f42eb8887a359ec648423db7f4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/NeuralContamination-7B-ties",
        "Average":75.96,
        "ARC":73.46,
        "HellaSwag":88.9,
        "MMLU":64.76,
        "TruthfulQA":76.71,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8e01722995275d4b8a6943fb207977a5bde0829e"
    },
    {
        "T":"?",
        "Model":"mlabonne\/Monarch-7B",
        "Average":75.96,
        "ARC":73.04,
        "HellaSwag":89.03,
        "MMLU":64.41,
        "TruthfulQA":77.35,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"1ccf1b7b37818c3b11d14dc0ef6fe4344a3cb4d5"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/bagel-dpo-34b-v0.2",
        "Average":75.95,
        "ARC":71.93,
        "HellaSwag":85.25,
        "MMLU":76.58,
        "TruthfulQA":70.05,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":87,
        "Available on the hub":true,
        "Model Sha":"fcc6ada5ea6dbf2f644d26b545ac402d2202cc74"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lodrick-the-lafted\/Grafted-Wind-Elementals-2x70B",
        "Average":75.95,
        "ARC":73.38,
        "HellaSwag":89.08,
        "MMLU":75.79,
        "TruthfulQA":65.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":125.35,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"46f056338f51bcc7c80745b95e9198aec4c198d4"
    },
    {
        "T":"?",
        "Model":"eldogbbhed\/NeuralBeagleJaskier",
        "Average":75.95,
        "ARC":73.21,
        "HellaSwag":89.05,
        "MMLU":64.47,
        "TruthfulQA":77.08,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b2102db3ca9307b5a84c5b89727e1341cdcd6ae7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/MoNeuTrix-7B-v1",
        "Average":75.92,
        "ARC":72.87,
        "HellaSwag":88.98,
        "MMLU":64.65,
        "TruthfulQA":77.18,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5b1baedc23f57ba43c07c257fb665c171cc78cfa"
    },
    {
        "T":"?",
        "Model":"Kukedlc\/NeuralKrishna-7B-V2-DPO",
        "Average":75.91,
        "ARC":74.06,
        "HellaSwag":88.97,
        "MMLU":64.41,
        "TruthfulQA":76.19,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c9beb3cba8030cb4fe7d96dd513c9e7ab40da126"
    },
    {
        "T":"?",
        "Model":"bobofrut\/ladybird-base-7B-v8",
        "Average":75.9,
        "ARC":73.21,
        "HellaSwag":89.19,
        "MMLU":64.39,
        "TruthfulQA":76.82,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"4774173a54be9a648e1cf03248af3ae3d51a0434"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_27-7B-dare_ties",
        "Average":75.9,
        "ARC":73.72,
        "HellaSwag":89.0,
        "MMLU":64.5,
        "TruthfulQA":76.36,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"828713b163db29b7836c20ce72c50f269c3086f3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/MonaTrix-v6",
        "Average":75.89,
        "ARC":72.78,
        "HellaSwag":88.9,
        "MMLU":64.45,
        "TruthfulQA":77.45,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c1f07a85b276483239956c4aa7d8e062c7ce8da1"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_24-7B-slerp",
        "Average":75.89,
        "ARC":73.98,
        "HellaSwag":89.09,
        "MMLU":64.99,
        "TruthfulQA":75.52,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c0b444df4fbeb1106fc6e2a3ceb9ff0521de32bd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JaeyeonKang\/CCK_Asura_v2.1",
        "Average":75.89,
        "ARC":72.53,
        "HellaSwag":88.75,
        "MMLU":74.96,
        "TruthfulQA":67.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"546cdd443abc56b48aaadb4ebb5fb9249015f0bb"
    },
    {
        "T":"?",
        "Model":"ammarali32\/MultiVerse_LASER",
        "Average":75.89,
        "ARC":72.53,
        "HellaSwag":88.81,
        "MMLU":64.52,
        "TruthfulQA":77.7,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":417,
        "Available on the hub":false,
        "Model Sha":"7385d3aa94cebfb10f983bc905fea3e83c4a4e3c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/NeuTrixOmniBe-DPO",
        "Average":75.89,
        "ARC":72.95,
        "HellaSwag":89.04,
        "MMLU":64.34,
        "TruthfulQA":77.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"40d3c8030b014b0c6fc7de07a4ee300e850a4566"
    },
    {
        "T":"?",
        "Model":"yleo\/OgnoMonarch-7B",
        "Average":75.88,
        "ARC":72.61,
        "HellaSwag":88.92,
        "MMLU":64.94,
        "TruthfulQA":77.06,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1aaa8b8fd3f7a455be518c4d70b4a434b9977c87"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_28-7B-dare_ties",
        "Average":75.87,
        "ARC":72.18,
        "HellaSwag":89.08,
        "MMLU":64.68,
        "TruthfulQA":77.55,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6bcfb1abbd779df524a45e73c68e17b8255ec99f"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_40-7B-dare_ties",
        "Average":75.86,
        "ARC":73.04,
        "HellaSwag":88.62,
        "MMLU":64.59,
        "TruthfulQA":77.21,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1d5b6444180fe3c92b2ecb7647ca2fe15f30756f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mayacinka\/Buttercup-7b-dpo-ties",
        "Average":75.86,
        "ARC":72.7,
        "HellaSwag":89.09,
        "MMLU":64.5,
        "TruthfulQA":77.17,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"608d7998c1b8f4707e065642a7cfa3d0ddb80100"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mayacinka\/Buttercup-7b-dpo-slerp",
        "Average":75.86,
        "ARC":72.7,
        "HellaSwag":89.09,
        "MMLU":64.5,
        "TruthfulQA":77.17,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"a9f4d04b59d764a45fabac9dd3d7f72b795967f0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/NeuralArjuna-7B-DT",
        "Average":75.85,
        "ARC":73.12,
        "HellaSwag":88.97,
        "MMLU":64.63,
        "TruthfulQA":76.68,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ac404e8a016bc77dce533c8746daedd5cefa8cb3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/Neural-Krishna-Multiverse-7b",
        "Average":75.85,
        "ARC":72.87,
        "HellaSwag":89.06,
        "MMLU":64.72,
        "TruthfulQA":76.75,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f87fafa1e5df8a3b9dfb46c6dd0813b0c9e56e6b"
    },
    {
        "T":"?",
        "Model":"u66u\/NeuralJaskier-7b-dpo",
        "Average":75.84,
        "ARC":71.59,
        "HellaSwag":88.87,
        "MMLU":64.49,
        "TruthfulQA":78.42,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dc363869e472a853dc3d298e44f2098ab1e9b788"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_29-7B-dare_ties",
        "Average":75.84,
        "ARC":73.04,
        "HellaSwag":89.04,
        "MMLU":64.29,
        "TruthfulQA":76.98,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9a628dcfb95cd17d0343467c29bc11053ad6851a"
    },
    {
        "T":"?",
        "Model":"liminerity\/Blur-7b-slerp-v1.46",
        "Average":75.84,
        "ARC":73.29,
        "HellaSwag":89.07,
        "MMLU":64.37,
        "TruthfulQA":76.61,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"744c0e181c49b43857828fe1d14bdf9976d6c6a8"
    },
    {
        "T":"?",
        "Model":"paulml\/DPOB-INMTOB-7B",
        "Average":75.84,
        "ARC":73.21,
        "HellaSwag":89.0,
        "MMLU":64.54,
        "TruthfulQA":76.6,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"a8871af9db183f2e7fe7c30bb2242b3b7827e53f"
    },
    {
        "T":"?",
        "Model":"AurelPx\/Pegasus-7b-slerp",
        "Average":75.84,
        "ARC":72.7,
        "HellaSwag":89.05,
        "MMLU":64.47,
        "TruthfulQA":77.13,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"2c42b3c5aa99602ffe02ff1a0702d6a40e6cb426"
    },
    {
        "T":"?",
        "Model":"eren23\/dpo-binarized-NeutrixOmnibe-7B",
        "Average":75.83,
        "ARC":72.78,
        "HellaSwag":89.05,
        "MMLU":64.6,
        "TruthfulQA":76.9,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"5c485c124bf1af920ebfba6c0de615db5dcb5ae4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/NeuTrixOmniBe-DPO",
        "Average":75.82,
        "ARC":72.78,
        "HellaSwag":89.03,
        "MMLU":64.28,
        "TruthfulQA":77.21,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"1b1cf19bc1d574906c7d100a8dbb85ec4cad5bb5"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_25-7B-dare_ties",
        "Average":75.82,
        "ARC":73.46,
        "HellaSwag":88.89,
        "MMLU":64.37,
        "TruthfulQA":76.54,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6918e7ea07572b932ba43e7e339cc79406b75e30"
    },
    {
        "T":"?",
        "Model":"liminerity\/Omningotex-7b-slerp",
        "Average":75.82,
        "ARC":73.29,
        "HellaSwag":88.96,
        "MMLU":64.69,
        "TruthfulQA":76.32,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"8d31526e43bbc2aa7324a4e5182d25aedcd24f1e"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_36-7B-slerp",
        "Average":75.81,
        "ARC":72.61,
        "HellaSwag":88.83,
        "MMLU":64.77,
        "TruthfulQA":77.05,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f9fcbd3895d6c63aed550134353473de0bd9b662"
    },
    {
        "T":"?",
        "Model":"Eric111\/UltraCatunaMayo-DPO",
        "Average":75.81,
        "ARC":72.87,
        "HellaSwag":88.75,
        "MMLU":65.18,
        "TruthfulQA":76.44,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"67f6f5ea337547b3f5e287e0ed1392ef0462e65a"
    },
    {
        "T":"?",
        "Model":"paulml\/OGNO-7B",
        "Average":75.81,
        "ARC":73.12,
        "HellaSwag":89.0,
        "MMLU":64.59,
        "TruthfulQA":76.52,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"a5d97f2e6962dc2c539a5bbca6a1160f87ccce84"
    },
    {
        "T":"?",
        "Model":"Kukedlc\/NeuTrixOmniBe-7B-model-remix",
        "Average":75.81,
        "ARC":72.61,
        "HellaSwag":89.07,
        "MMLU":64.63,
        "TruthfulQA":76.91,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a9516ed719359e08e5b716bcf9d80d91f81fa471"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eren23\/merged-dpo-binarized-NeutrixOmnibe-7B",
        "Average":75.8,
        "ARC":72.7,
        "HellaSwag":89.03,
        "MMLU":64.59,
        "TruthfulQA":76.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"616d72f1c130f9ea0118c45d5a6f12f3848a97db"
    },
    {
        "T":"?",
        "Model":"Kukedlc\/NeuTrixOmniBe-7B-model-remix",
        "Average":75.8,
        "ARC":72.7,
        "HellaSwag":89.03,
        "MMLU":64.57,
        "TruthfulQA":76.9,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a9516ed719359e08e5b716bcf9d80d91f81fa471"
    },
    {
        "T":"?",
        "Model":"eren23\/OGNO-7b-dpo-truthful",
        "Average":75.8,
        "ARC":72.95,
        "HellaSwag":89.02,
        "MMLU":64.61,
        "TruthfulQA":76.61,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b30fc2edf47ab3d2c472d91611f7f3bae99174bb"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_43-7B-dare_ties",
        "Average":75.78,
        "ARC":73.55,
        "HellaSwag":89.05,
        "MMLU":64.8,
        "TruthfulQA":75.72,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"be1fb5b492bb23ea2b58cbe3e598268013ce52db"
    },
    {
        "T":"?",
        "Model":"yleo\/ParrotOgno-7B",
        "Average":75.78,
        "ARC":73.04,
        "HellaSwag":89.03,
        "MMLU":64.51,
        "TruthfulQA":76.53,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"95d7acea20309a1eeb0be3c4db77ecba5fdf6df9"
    },
    {
        "T":"?",
        "Model":"Kukedlc\/NeuralSirKrishna-7b",
        "Average":75.75,
        "ARC":73.72,
        "HellaSwag":89.05,
        "MMLU":64.63,
        "TruthfulQA":75.6,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"38905a9bf8b4c000a99daaea69e63c15efbbc152"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nlpguy\/AlloyIngot",
        "Average":75.74,
        "ARC":73.98,
        "HellaSwag":89.05,
        "MMLU":64.83,
        "TruthfulQA":75.12,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e9bda1899505cae6cbdde05dc763c2fad5e2183e"
    },
    {
        "T":"?",
        "Model":"yleo\/EmertonMonarch-7B-slerp",
        "Average":75.74,
        "ARC":73.04,
        "HellaSwag":88.94,
        "MMLU":64.44,
        "TruthfulQA":76.55,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b146ef2ff4b397d626a5945bf2caa4c2832bf9c3"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_50-7B-slerp",
        "Average":75.74,
        "ARC":73.04,
        "HellaSwag":88.73,
        "MMLU":64.67,
        "TruthfulQA":76.51,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0e60ac5af2c35afb2c0c400332341e15395ff745"
    },
    {
        "T":"?",
        "Model":"nlpguy\/AlloyIngotNeoX",
        "Average":75.73,
        "ARC":74.32,
        "HellaSwag":89.07,
        "MMLU":64.97,
        "TruthfulQA":74.57,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0d4277f842643d3c23fad0c86cfb1edd658adab7"
    },
    {
        "T":"?",
        "Model":"RubielLabarta\/LogoS-7Bx2-MoE-13B-v0.2",
        "Average":75.73,
        "ARC":74.4,
        "HellaSwag":89.09,
        "MMLU":64.9,
        "TruthfulQA":74.53,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"354f0eb0a1299473c861c0505c2ede04ced90972"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/Capricorn-7B-DPO",
        "Average":75.72,
        "ARC":72.87,
        "HellaSwag":88.47,
        "MMLU":64.29,
        "TruthfulQA":77.23,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bfba5a5114005c849a49662b4c7e53debac98105"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RubielLabarta\/LogoS-7Bx2-MoE-13B-v0.1",
        "Average":75.72,
        "ARC":74.49,
        "HellaSwag":89.07,
        "MMLU":64.74,
        "TruthfulQA":74.57,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"1e4670ddb878fa696f2e6293a4db9d8657993fd8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"macadeliccc\/MonarchLake-7B",
        "Average":75.71,
        "ARC":74.15,
        "HellaSwag":89.29,
        "MMLU":64.44,
        "TruthfulQA":74.97,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"f1dc346e4c117d73dd706971a50d6b393390984b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/NeuralTrix-7B-v1",
        "Average":75.71,
        "ARC":74.15,
        "HellaSwag":89.27,
        "MMLU":64.55,
        "TruthfulQA":74.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c39ed28b498cdd86294ed3102cb7bdd5de5ec4fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/NeuralMergeTest-001",
        "Average":75.7,
        "ARC":73.38,
        "HellaSwag":88.95,
        "MMLU":64.64,
        "TruthfulQA":75.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"54bd62136c520275549b5feecd2e24d168551b24"
    },
    {
        "T":"?",
        "Model":"louisbrulenaudet\/Pearl-34B-ties",
        "Average":75.69,
        "ARC":70.99,
        "HellaSwag":84.83,
        "MMLU":76.63,
        "TruthfulQA":70.32,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"be28f8663c6f49e1df04ddd59f4475cb93575272"
    },
    {
        "T":"?",
        "Model":"touqir\/Cyrax-7B",
        "Average":75.69,
        "ARC":72.95,
        "HellaSwag":88.19,
        "MMLU":64.6,
        "TruthfulQA":77.01,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"783a2f1231542b9fe8bc728dc676745c62f35b9f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"macadeliccc\/SOLAR-10.7b-Instruct-truthy-dpo",
        "Average":75.68,
        "ARC":72.1,
        "HellaSwag":88.44,
        "MMLU":65.45,
        "TruthfulQA":76.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc",
        "#Params (B)":10.73,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"31bbd3c348400c942a33c1f952dca8e7125996b7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeverSleep\/MiquMaid-v2-2x70B-DPO",
        "Average":75.67,
        "ARC":72.53,
        "HellaSwag":88.36,
        "MMLU":75.31,
        "TruthfulQA":66.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":125.35,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"30e44c452e38ff3d879d7ba92a130fa2cc072754"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_26-7B-dare_ties",
        "Average":75.67,
        "ARC":72.95,
        "HellaSwag":89.0,
        "MMLU":64.35,
        "TruthfulQA":76.39,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"38dc5cdc607d7171ef9a21a820d4fc58d6b9811b"
    },
    {
        "T":"?",
        "Model":"bardsai\/jaskier-7b-dpo-v4.1",
        "Average":75.67,
        "ARC":72.95,
        "HellaSwag":89.07,
        "MMLU":64.75,
        "TruthfulQA":75.92,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ee2a4761bb0030ef340120b30f6f52ae78f74e71"
    },
    {
        "T":"?",
        "Model":"Kquant03\/Kaltsit-16x7B-bf16",
        "Average":75.66,
        "ARC":73.46,
        "HellaSwag":88.92,
        "MMLU":64.62,
        "TruthfulQA":75.63,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":91.8,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fd3605c2b349939acfbd8a63bf08b8247b005485"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment20-7B",
        "Average":75.65,
        "ARC":73.04,
        "HellaSwag":88.62,
        "MMLU":63.23,
        "TruthfulQA":77.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2491f12e51d7b74fb47ef5480d4b5f547d4d19ea"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"moreh\/MoMo-72B-lora-1.8.6-DPO",
        "Average":75.64,
        "ARC":70.14,
        "HellaSwag":86.03,
        "MMLU":77.4,
        "TruthfulQA":69.0,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":72.29,
        "Hub":32,
        "Available on the hub":true,
        "Model Sha":"76389d5d825c3743cc70bc75b902bbfdad11beba"
    },
    {
        "T":"?",
        "Model":"moreh\/MoMo-70B-lora-1.8.6-DPO",
        "Average":75.64,
        "ARC":70.14,
        "HellaSwag":86.03,
        "MMLU":77.4,
        "TruthfulQA":69.0,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":72.29,
        "Hub":32,
        "Available on the hub":true,
        "Model Sha":"76389d5d825c3743cc70bc75b902bbfdad11beba"
    },
    {
        "T":"?",
        "Model":"paulml\/NeuralOmniWestBeaglake-7B",
        "Average":75.62,
        "ARC":73.72,
        "HellaSwag":89.69,
        "MMLU":63.96,
        "TruthfulQA":75.1,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b02cba26616d558094f7dca72419367c56937a47"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_23-7B-slerp",
        "Average":75.61,
        "ARC":73.55,
        "HellaSwag":88.9,
        "MMLU":64.87,
        "TruthfulQA":75.13,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b6369c9e0b592d8be55d5f00076159c7d3fa9f64"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_22-7B-slerp",
        "Average":75.61,
        "ARC":73.72,
        "HellaSwag":89.03,
        "MMLU":64.8,
        "TruthfulQA":74.9,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7fad5c740489e631fd94d4e1b54fb959ec953c2e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/NeuralMarioMonarch-7B-slerp",
        "Average":75.61,
        "ARC":73.81,
        "HellaSwag":89.04,
        "MMLU":64.61,
        "TruthfulQA":74.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"8fca4987ba90d95544840e1921ebc819b880cc8d"
    },
    {
        "T":"?",
        "Model":"zhengr\/MixTAO-7Bx2-MoE-Instruct-v7.0",
        "Average":75.6,
        "ARC":74.23,
        "HellaSwag":89.37,
        "MMLU":64.54,
        "TruthfulQA":74.26,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"69b9280ee4d2a20ef5645798621e62dd9777c139"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nlpguy\/AlloyIngotNeo",
        "Average":75.6,
        "ARC":72.87,
        "HellaSwag":88.99,
        "MMLU":64.61,
        "TruthfulQA":75.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a05c142502808099af9d1daec3002a3ccaad5b31"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/NeuralKrishnaMath-7B-slerp",
        "Average":75.6,
        "ARC":73.29,
        "HellaSwag":88.92,
        "MMLU":64.67,
        "TruthfulQA":75.52,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b2619e24f8b91f9e8328fc74f8259d4f87965351"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_42-7B-dare_ties",
        "Average":75.6,
        "ARC":73.38,
        "HellaSwag":88.96,
        "MMLU":64.62,
        "TruthfulQA":75.44,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"01eb01b50a5036f094e95cc20a0275ab60939ce3"
    },
    {
        "T":"?",
        "Model":"yleo\/EmertonBeagle-7B-dpo",
        "Average":75.58,
        "ARC":72.78,
        "HellaSwag":89.12,
        "MMLU":64.47,
        "TruthfulQA":75.96,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2a1ecaab96bfb99cb01cb3217d34002b059eba3f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YKM11\/Mistral-7B-adaptv1",
        "Average":75.58,
        "ARC":73.98,
        "HellaSwag":89.37,
        "MMLU":64.42,
        "TruthfulQA":74.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"105cb07b032ae67bcb204f873e96bab953cc3294"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Cedaros\/BetaMonarch-10.7B",
        "Average":75.57,
        "ARC":72.7,
        "HellaSwag":88.37,
        "MMLU":64.37,
        "TruthfulQA":76.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ca5d801cd3b457771f6b17342c323197e90dee91"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/testtest",
        "Average":75.57,
        "ARC":70.82,
        "HellaSwag":84.88,
        "MMLU":76.66,
        "TruthfulQA":69.9,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e9be60931d3abdf3b08a55f13e4c7586918b2be8"
    },
    {
        "T":"?",
        "Model":"liminerity\/binarized-ingotrix-slerp-7b",
        "Average":75.57,
        "ARC":73.21,
        "HellaSwag":88.64,
        "MMLU":64.85,
        "TruthfulQA":75.57,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a8b29283e9a14ebbe162639339fa7f6cb37a3388"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/neurotic-crown-clown-7b-ties",
        "Average":75.56,
        "ARC":72.35,
        "HellaSwag":88.61,
        "MMLU":64.77,
        "TruthfulQA":76.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c990d5a67bb589e73c355a81e99940c8d7155c34"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment19-7B",
        "Average":75.55,
        "ARC":72.35,
        "HellaSwag":88.61,
        "MMLU":63.08,
        "TruthfulQA":78.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ebc7cba80494385e29bce8b1b86a75d14666c19e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/Neural-4-ARC-7b",
        "Average":75.54,
        "ARC":74.06,
        "HellaSwag":89.05,
        "MMLU":64.93,
        "TruthfulQA":74.12,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"36a55a14cd7b3e4143b113b9aab4fe4085a78838"
    },
    {
        "T":"?",
        "Model":"abhishekchohan\/SOLAR-10.7B-Instruct-Forest-DPO-v1",
        "Average":75.53,
        "ARC":71.93,
        "HellaSwag":88.44,
        "MMLU":65.63,
        "TruthfulQA":76.13,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.6,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"543b52f9b6c96a4922dc8ed1251625b1bd919e19"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_21-7B-slerp",
        "Average":75.51,
        "ARC":74.23,
        "HellaSwag":88.95,
        "MMLU":65.05,
        "TruthfulQA":73.81,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"92cd3ea976fe78cfdbf3f45a9d81ca30dc5fdc38"
    },
    {
        "T":"?",
        "Model":"Kukedlc\/NeuralExperiment-7b-dare-ties",
        "Average":75.5,
        "ARC":73.63,
        "HellaSwag":88.87,
        "MMLU":64.66,
        "TruthfulQA":74.86,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"3fc29ef22b0e7fc4d17250422804ed1b03eb9732"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/Jupiter-k-7B-slerp",
        "Average":75.5,
        "ARC":74.23,
        "HellaSwag":88.82,
        "MMLU":65.01,
        "TruthfulQA":73.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d696e99a2a4eeb13994c277f2fb113e9ddd1e632"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kquant03\/NeuralTrix-7B-dpo-laser",
        "Average":75.49,
        "ARC":71.33,
        "HellaSwag":88.51,
        "MMLU":63.99,
        "TruthfulQA":78.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"ac42a1ab3cb135dd8ff9ab600d5562251e8c6986"
    },
    {
        "T":"?",
        "Model":"paulml\/DPOB-NMTOB-7B",
        "Average":75.46,
        "ARC":73.12,
        "HellaSwag":88.95,
        "MMLU":64.7,
        "TruthfulQA":75.08,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"547fe9adccf3ab12b91bb77f6ee5daa033757a15"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051612\/A0126",
        "Average":75.46,
        "ARC":70.39,
        "HellaSwag":85.87,
        "MMLU":84.03,
        "TruthfulQA":61.53,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c0c062b6fe4202b7aa5510bfda390da2e8b393ef"
    },
    {
        "T":"?",
        "Model":"Kquant03\/Samlagast-7B-bf16",
        "Average":75.45,
        "ARC":73.98,
        "HellaSwag":89.34,
        "MMLU":64.58,
        "TruthfulQA":73.9,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0b134ed1f5abb3a0f10fb64166f1b6b33eb99a68"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kquant03\/NeuralTrix-7B-dpo-relaser",
        "Average":75.43,
        "ARC":71.33,
        "HellaSwag":88.41,
        "MMLU":64.01,
        "TruthfulQA":77.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"e9e9d8a90abb9716c93ec93ec7d977527794201f"
    },
    {
        "T":"?",
        "Model":"paulml\/NMTOB-7B",
        "Average":75.42,
        "ARC":73.04,
        "HellaSwag":88.94,
        "MMLU":64.63,
        "TruthfulQA":75.06,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"27380e38769851edfc8d720ec88a066b40d8a85e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051612\/A0125",
        "Average":75.4,
        "ARC":69.71,
        "HellaSwag":85.0,
        "MMLU":86.64,
        "TruthfulQA":60.27,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"194a9c07a60f6064fe77adb0daf378d0ded7ac43"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/bagel-dpo-8x7b-v0.2",
        "Average":75.4,
        "ARC":72.1,
        "HellaSwag":86.41,
        "MMLU":70.27,
        "TruthfulQA":72.83,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":20,
        "Available on the hub":false,
        "Model Sha":"61822ea65b8a4c56d2b5622e2adf69e430fac29a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"macadeliccc\/MBX-7B-v3-DPO",
        "Average":75.39,
        "ARC":73.55,
        "HellaSwag":89.11,
        "MMLU":64.91,
        "TruthfulQA":74.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"099b9c3e105fbb579d561fe93174ae3bd75dac8d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ResplendentAI\/Paradigm_7B",
        "Average":75.37,
        "ARC":73.63,
        "HellaSwag":88.66,
        "MMLU":64.02,
        "TruthfulQA":75.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"33122df31c3aa5ae69d591ed51c9415342541225"
    },
    {
        "T":"?",
        "Model":"Kukedlc\/Brocae-Area-7B-slerp",
        "Average":75.37,
        "ARC":73.81,
        "HellaSwag":88.98,
        "MMLU":64.55,
        "TruthfulQA":74.13,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"09f40713a4cbe65f4843b13d9ae8cf840b75da86"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/NeuralGanesha-7b",
        "Average":75.35,
        "ARC":73.98,
        "HellaSwag":88.85,
        "MMLU":64.41,
        "TruthfulQA":74.14,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"16e76c8a0fcf0556441f4cf4cd1549613f8d2084"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/NeuralKrishna-7B-slerp",
        "Average":75.33,
        "ARC":73.46,
        "HellaSwag":88.96,
        "MMLU":64.62,
        "TruthfulQA":74.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5d25e45fbb1eb8301eee7e16884689e37cf02792"
    },
    {
        "T":"?",
        "Model":"jsfs11\/MixtureofMerges-MoE-4x7b-v5",
        "Average":75.33,
        "ARC":73.89,
        "HellaSwag":89.0,
        "MMLU":64.69,
        "TruthfulQA":73.73,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"86fff26d9a2ad942ae1ca1ad31d659fbf8429db5"
    },
    {
        "T":"?",
        "Model":"yleo\/EmertonOmniBeagle-7B-dpo",
        "Average":75.3,
        "ARC":72.7,
        "HellaSwag":88.44,
        "MMLU":64.44,
        "TruthfulQA":75.62,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f484e7af54735acaa1b60d9ec95825818e74c46a"
    },
    {
        "T":"?",
        "Model":"jsfs11\/MixtureofMerges-MoE-4x7b-v4",
        "Average":75.3,
        "ARC":72.53,
        "HellaSwag":88.85,
        "MMLU":64.53,
        "TruthfulQA":75.3,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"4d233a36010353fde821dc65434194a797e4f7bd"
    },
    {
        "T":"?",
        "Model":"Kukedlc\/NeuralKrishna-7B-v3",
        "Average":75.28,
        "ARC":73.63,
        "HellaSwag":88.91,
        "MMLU":64.45,
        "TruthfulQA":74.11,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9e743423df10fe2b4dcc37ac0da13ad56e49a16c"
    },
    {
        "T":"?",
        "Model":"Test157t\/Prima-LelantaclesV6.69-7b",
        "Average":75.26,
        "ARC":72.61,
        "HellaSwag":88.65,
        "MMLU":64.53,
        "TruthfulQA":75.26,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"185e114fd89bd0d91805e5e14541d0e28a20b821"
    },
    {
        "T":"?",
        "Model":"paulml\/OmniBeagleMBX-v3-7B",
        "Average":75.26,
        "ARC":73.81,
        "HellaSwag":89.07,
        "MMLU":64.66,
        "TruthfulQA":73.52,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"01bc122ec9d4a523fc012e792e2ba23f0f9bea68"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dfurman\/HermesBagel-34B-v0.1",
        "Average":75.25,
        "ARC":70.56,
        "HellaSwag":85.74,
        "MMLU":77.38,
        "TruthfulQA":67.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"7364cfc1f2c7fc56d460adc0dc90d7a6d13641fb"
    },
    {
        "T":"?",
        "Model":"paulml\/OmniBeagleSquaredMBX-v3-7B",
        "Average":75.25,
        "ARC":74.4,
        "HellaSwag":88.82,
        "MMLU":65.09,
        "TruthfulQA":72.7,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"88928f55d51c0819de3b64e6c37689b87a89aac4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/MyModelsMerge-7b",
        "Average":75.23,
        "ARC":73.46,
        "HellaSwag":88.59,
        "MMLU":64.39,
        "TruthfulQA":74.47,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cc04d33e0008214f7a2937c5236b14f98c2c8bda"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MSL7\/INEX4-7b",
        "Average":75.21,
        "ARC":72.95,
        "HellaSwag":88.79,
        "MMLU":64.7,
        "TruthfulQA":74.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"4f256a193876920b54ac8d6c0fa67accb2dd25b8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Bagel-Hermes-34B-Slerp",
        "Average":75.2,
        "ARC":70.73,
        "HellaSwag":85.68,
        "MMLU":77.29,
        "TruthfulQA":67.09,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ca42d74d2b7fa947e27305e41c61784f8fe9dafa"
    },
    {
        "T":"?",
        "Model":"mlabonne\/OmniBeagle-7B",
        "Average":75.2,
        "ARC":72.61,
        "HellaSwag":88.93,
        "MMLU":64.8,
        "TruthfulQA":74.45,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":19,
        "Available on the hub":false,
        "Model Sha":"035047bcb642b054307ae49052b2b8a6e58c7b2a"
    },
    {
        "T":"?",
        "Model":"rwitz\/experiment26-SPIN-iter-0",
        "Average":75.18,
        "ARC":72.44,
        "HellaSwag":88.74,
        "MMLU":64.64,
        "TruthfulQA":74.9,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b7174ccf5c91095737cdb29f50853512017a1ac4"
    },
    {
        "T":"?",
        "Model":"jan-hq\/stealth-v2",
        "Average":75.14,
        "ARC":73.89,
        "HellaSwag":89.26,
        "MMLU":64.94,
        "TruthfulQA":72.47,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"d718acb1b95c85009db8dd34af1318bcaf23ebcd"
    },
    {
        "T":"?",
        "Model":"vicgalle\/NeuralBeagle-11B-truthy",
        "Average":75.13,
        "ARC":73.63,
        "HellaSwag":87.86,
        "MMLU":63.11,
        "TruthfulQA":75.92,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e02f5cdd529677c97fb2c8e7a1ccaec378ba60df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ChaoticNeutrals\/Prima-LelantaclesV7-experimental-7b",
        "Average":75.13,
        "ARC":72.87,
        "HellaSwag":88.72,
        "MMLU":64.31,
        "TruthfulQA":74.62,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"61bb0467d7026d9ab5354e6c33d8e6361ba00677"
    },
    {
        "T":"?",
        "Model":"cloudyu\/Mixtral_34Bx2_MoE_60B",
        "Average":75.13,
        "ARC":71.33,
        "HellaSwag":85.25,
        "MMLU":77.34,
        "TruthfulQA":66.59,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":60.81,
        "Hub":105,
        "Available on the hub":false,
        "Model Sha":"f49d7cf0a7b99b15bc98b0ef4a681e7f0f4aa92c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/Truthful_DPO_cloudyu_Mixtral_34Bx2_MoE_60B",
        "Average":75.13,
        "ARC":71.25,
        "HellaSwag":85.24,
        "MMLU":77.28,
        "TruthfulQA":66.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":60.81,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6ba7b5acb65dd62c28585cba298e0d3671c14f3a"
    },
    {
        "T":"?",
        "Model":"liminerity\/Blur-7b-slerp-v1.41",
        "Average":75.13,
        "ARC":72.78,
        "HellaSwag":88.65,
        "MMLU":64.84,
        "TruthfulQA":74.23,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4e76e89ba74eff149bc5f3ffbe7bc35beaef9269"
    },
    {
        "T":"?",
        "Model":"cloudyu\/Mixtral_34Bx2_MoE_60B",
        "Average":75.12,
        "ARC":71.25,
        "HellaSwag":85.36,
        "MMLU":77.28,
        "TruthfulQA":66.61,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":60.81,
        "Hub":105,
        "Available on the hub":false,
        "Model Sha":"f49d7cf0a7b99b15bc98b0ef4a681e7f0f4aa92c"
    },
    {
        "T":"?",
        "Model":"FoxEngineAi\/Mega-Destroyer-8x7B",
        "Average":75.11,
        "ARC":71.76,
        "HellaSwag":86.47,
        "MMLU":70.11,
        "TruthfulQA":72.12,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":46.7,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"d257c7bb089eaf791f0c1fba83a4029ccead5544"
    },
    {
        "T":"?",
        "Model":"paulml\/OmniBeagleSquaredMBX-v3-7B-v2",
        "Average":75.11,
        "ARC":74.06,
        "HellaSwag":88.93,
        "MMLU":64.53,
        "TruthfulQA":72.93,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7eb4f63abc5c6891503008eb613287eff8c15e30"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_35-7B-slerp",
        "Average":75.11,
        "ARC":71.67,
        "HellaSwag":88.34,
        "MMLU":64.66,
        "TruthfulQA":75.76,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ff7f67d4f4a8053ac77969d96cffcd53c55f8b38"
    },
    {
        "T":"?",
        "Model":"paulml\/NeuralOmniBeagleMBX-v3-7B",
        "Average":75.1,
        "ARC":73.38,
        "HellaSwag":88.91,
        "MMLU":64.99,
        "TruthfulQA":73.1,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"37084955ee092548abfe356be4e6cfc46daa9cb4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sumo43\/SOLAR-10.7B-Instruct-DPO-v1.0",
        "Average":75.09,
        "ARC":73.12,
        "HellaSwag":89.77,
        "MMLU":64.21,
        "TruthfulQA":73.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.7,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"9e439597e3e788e3ff8a41df54e0dae0acda14a4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YKM11\/Mistral-7B-adaptv0.9",
        "Average":75.09,
        "ARC":73.55,
        "HellaSwag":88.96,
        "MMLU":64.73,
        "TruthfulQA":73.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3bdfb578bb9ee94cb8b0f8c10cfaf1db3e9dc684"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lodrick-the-lafted\/Grafted-Llama2-2x70B",
        "Average":75.09,
        "ARC":72.61,
        "HellaSwag":89.57,
        "MMLU":71.67,
        "TruthfulQA":66.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":125.35,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"68b4f64541479fb6f6691de1fb2f4db07e1634e2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051611\/O0201",
        "Average":75.08,
        "ARC":67.83,
        "HellaSwag":84.49,
        "MMLU":89.35,
        "TruthfulQA":58.63,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a198e7afd29087237d6346114e75482deaf99fe4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YouKnowMee\/Mistral-7b-instruct-v0.2-summ-sft-dpo-ed2",
        "Average":75.07,
        "ARC":74.06,
        "HellaSwag":89.25,
        "MMLU":64.25,
        "TruthfulQA":72.73,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f0774eebdb546c237cf312fe7041abe375c88ac0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YouKnowMee\/Mistral-7b-instruct-v0.2-summ-dpo-ed2",
        "Average":75.07,
        "ARC":74.4,
        "HellaSwag":89.29,
        "MMLU":64.23,
        "TruthfulQA":72.34,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b6c9416b0deeb567f59891db1a1dce7a59fc54e4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YouKnowMee\/Mistral-7b-instruct-v0.2-summ-sft-dpo-ed3",
        "Average":75.07,
        "ARC":73.98,
        "HellaSwag":89.26,
        "MMLU":64.28,
        "TruthfulQA":72.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9e501956cd6183dd13b80f5be6eaa5d37fff8848"
    },
    {
        "T":"?",
        "Model":"Kukedlc\/NeuralFusion-7b-Dare-Ties",
        "Average":75.06,
        "ARC":73.21,
        "HellaSwag":88.96,
        "MMLU":64.77,
        "TruthfulQA":73.32,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"58d3d87993899cb6fbdead56c7554ff897b9657c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/NeuralShiva-7B-DT",
        "Average":75.05,
        "ARC":72.7,
        "HellaSwag":88.68,
        "MMLU":64.66,
        "TruthfulQA":74.17,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"65bf4997806c3131ac8ff164503282e5bb6df795"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AiMavenAi\/AiMaven-Prometheus",
        "Average":75.05,
        "ARC":73.98,
        "HellaSwag":88.83,
        "MMLU":65.17,
        "TruthfulQA":72.22,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"ee2dba5b9877b3b6d8f5b28f2900e4bfd4152a1b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"silvercoder67\/Mistral-7b-instruct-v0.2-summ-sft-dpo-e1",
        "Average":75.05,
        "ARC":73.98,
        "HellaSwag":89.27,
        "MMLU":64.16,
        "TruthfulQA":72.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9eae12f70b601824919394f486dcad3fba26ca3e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YouKnowMee\/Mistral-7b-instruct-v0.2-summ-dpo-ed3",
        "Average":75.05,
        "ARC":74.23,
        "HellaSwag":89.28,
        "MMLU":64.37,
        "TruthfulQA":72.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"23f8fc9ed8d1d5ff71695307e3e1dd43ae670549"
    },
    {
        "T":"\u2b55",
        "Model":"nisten\/shqiponja-59b-v1",
        "Average":75.02,
        "ARC":70.05,
        "HellaSwag":84.06,
        "MMLU":75.54,
        "TruthfulQA":70.43,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":58.94,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"a2dd71db32b23412fcea0ad8a36ee32e0641b9fc"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"yunconglong\/10.7Bx2_DPO_200",
        "Average":75.02,
        "ARC":70.22,
        "HellaSwag":88.23,
        "MMLU":66.25,
        "TruthfulQA":75.38,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.7,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"e6e9534becf65017d359db8704e6bcc9caf3ff60"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_53-7B-model_stock",
        "Average":75.02,
        "ARC":72.78,
        "HellaSwag":88.46,
        "MMLU":64.97,
        "TruthfulQA":73.86,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"54d4ebc09c145e4b46732cd00d11a6aad336139e"
    },
    {
        "T":"?",
        "Model":"Eric111\/UltraCatunaMayo",
        "Average":75.02,
        "ARC":72.61,
        "HellaSwag":88.37,
        "MMLU":65.03,
        "TruthfulQA":74.06,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"17de371a605ac49c69ba0130037251ba5a034192"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/Calme-7B-Instruct-v0.5",
        "Average":75.0,
        "ARC":72.87,
        "HellaSwag":88.77,
        "MMLU":64.69,
        "TruthfulQA":73.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"7e26287665e6214be131f4e7ee20a312a07a4c1c"
    },
    {
        "T":"?",
        "Model":"flemmingmiguel\/MBX-7B-v3",
        "Average":74.99,
        "ARC":74.15,
        "HellaSwag":88.91,
        "MMLU":65.06,
        "TruthfulQA":71.87,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"ca8c55fbbb2a0f7dd0de41579d98bbf24946b712"
    },
    {
        "T":"?",
        "Model":"cloudyu\/Yi-34Bx2-MoE-60B",
        "Average":74.99,
        "ARC":71.08,
        "HellaSwag":85.23,
        "MMLU":77.47,
        "TruthfulQA":66.19,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":60.81,
        "Hub":59,
        "Available on the hub":false,
        "Model Sha":"483359d70b3fef480cdaeb6d722a18626d34f0ce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/Yi-34Bx3-MoE-90B",
        "Average":74.99,
        "ARC":70.9,
        "HellaSwag":85.33,
        "MMLU":77.41,
        "TruthfulQA":66.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.0,
        "Hub":59,
        "Available on the hub":false,
        "Model Sha":"b4b717be590394a4e70853cb444bd0964526c500"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/Yi-34Bx2-MoE-60B-DPO",
        "Average":74.99,
        "ARC":71.25,
        "HellaSwag":85.1,
        "MMLU":77.36,
        "TruthfulQA":66.24,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":60.81,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"3d0181b920304bca0bdfd41aff55188a574c85e3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/Yi-34Bx2-MOE-200K",
        "Average":74.99,
        "ARC":70.48,
        "HellaSwag":84.63,
        "MMLU":76.64,
        "TruthfulQA":68.19,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":60.81,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a5965f77bbb0fe23f16a5137918af27c753800af"
    },
    {
        "T":"?",
        "Model":"vicgalle\/Mixtral-7Bx2-truthy",
        "Average":74.99,
        "ARC":72.18,
        "HellaSwag":87.88,
        "MMLU":65.2,
        "TruthfulQA":74.68,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"4bfad083e96a4ab129cc202fc941994be2e3adc4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YKM12\/Mistral-7B-summ-privatev1",
        "Average":74.97,
        "ARC":74.15,
        "HellaSwag":88.85,
        "MMLU":64.99,
        "TruthfulQA":71.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7ba08a489f9dff577b853077466aae751615fdf0"
    },
    {
        "T":"?",
        "Model":"mlabonne\/Beyonder-4x7B-v3",
        "Average":74.96,
        "ARC":71.67,
        "HellaSwag":88.86,
        "MMLU":64.87,
        "TruthfulQA":74.44,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":24.15,
        "Hub":46,
        "Available on the hub":false,
        "Model Sha":"d8f0911773d8d881ffa04dafc5c3120dc4c18a3c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YouKnwMe\/Mistral-7b-instruct-v0.2-private-eds2",
        "Average":74.96,
        "ARC":72.7,
        "HellaSwag":89.05,
        "MMLU":64.21,
        "TruthfulQA":73.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d4cffeadbb02eaf6273e954fc5aea4f745747705"
    },
    {
        "T":"?",
        "Model":"alnrg2arg\/blockchainlabs_joe_bez_seminar",
        "Average":74.95,
        "ARC":73.98,
        "HellaSwag":88.75,
        "MMLU":65.17,
        "TruthfulQA":71.9,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"17ead25f366603e2c21d08485e01fefdb7f5f740"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"macadeliccc\/OmniCorso-7B",
        "Average":74.95,
        "ARC":72.44,
        "HellaSwag":88.78,
        "MMLU":65.08,
        "TruthfulQA":73.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"3f9de5b67315d1b35a377aa1a6ca8dad580a8370"
    },
    {
        "T":"?",
        "Model":"vanillaOVO\/supermario_v3",
        "Average":74.95,
        "ARC":73.81,
        "HellaSwag":88.92,
        "MMLU":65.07,
        "TruthfulQA":72.01,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5d20941300a647c563a385ca95c963bb5f26bc69"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/WinterGoddess-1.4x-70B-L2",
        "Average":74.94,
        "ARC":72.78,
        "HellaSwag":90.11,
        "MMLU":71.12,
        "TruthfulQA":65.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"5197257333076dd80821a5055abae7d21a7dc844"
    },
    {
        "T":"?",
        "Model":"macadeliccc\/OmniCorso-7B",
        "Average":74.93,
        "ARC":72.7,
        "HellaSwag":88.7,
        "MMLU":64.91,
        "TruthfulQA":73.43,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"bb0af56aafce88413fb8c823ee7831a9a0d09e96"
    },
    {
        "T":"?",
        "Model":"vanillaOVO\/supermario_v4",
        "Average":74.93,
        "ARC":73.46,
        "HellaSwag":88.77,
        "MMLU":65.41,
        "TruthfulQA":72.07,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7a5f87e239e9d24a4455bb81717d6ae0f3c2e7a5"
    },
    {
        "T":"?",
        "Model":"NeuralNovel\/Confinus-2x7B",
        "Average":74.93,
        "ARC":73.89,
        "HellaSwag":88.82,
        "MMLU":65.12,
        "TruthfulQA":71.88,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"5eeb9b70e39db055b51811931a04481ed58c8092"
    },
    {
        "T":"?",
        "Model":"Kukedlc\/Fasciculus-Arcuatus-7B-slerp",
        "Average":74.92,
        "ARC":73.55,
        "HellaSwag":88.95,
        "MMLU":64.65,
        "TruthfulQA":72.53,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5ceeb8b14477b15960c21443107befb46228b0b3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Smaug-Mixtral-v0.1",
        "Average":74.92,
        "ARC":74.91,
        "HellaSwag":87.79,
        "MMLU":70.08,
        "TruthfulQA":66.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"2f83b45077479bc3f663da50c4c40372894bf92e"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_49-7B-dare_ties",
        "Average":74.91,
        "ARC":72.35,
        "HellaSwag":88.3,
        "MMLU":64.31,
        "TruthfulQA":74.7,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d6a54a400d379b44fe330be9be94030c8d5e48a7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/Wernicke-7B-dpo",
        "Average":74.9,
        "ARC":71.84,
        "HellaSwag":88.63,
        "MMLU":65.22,
        "TruthfulQA":73.91,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"59ae4689046b197444514a0ec531fdb3341b33cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051615\/A0204",
        "Average":74.88,
        "ARC":70.31,
        "HellaSwag":84.42,
        "MMLU":86.86,
        "TruthfulQA":57.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5f2395801bf236bb5d6a0b5ca48122b261f7a2cf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/Capricorn-7B",
        "Average":74.88,
        "ARC":72.44,
        "HellaSwag":88.41,
        "MMLU":64.9,
        "TruthfulQA":73.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ff7d668721b961a73a95098cf7436db0170b1db6"
    },
    {
        "T":"?",
        "Model":"alnrg2arg\/blockchainlabs_joe_bez_seminar",
        "Average":74.88,
        "ARC":73.81,
        "HellaSwag":88.72,
        "MMLU":65.12,
        "TruthfulQA":71.86,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"17ead25f366603e2c21d08485e01fefdb7f5f740"
    },
    {
        "T":"?",
        "Model":"louisbrulenaudet\/Maxine-7B-0401-ties",
        "Average":74.86,
        "ARC":71.76,
        "HellaSwag":88.84,
        "MMLU":64.35,
        "TruthfulQA":74.51,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f317d8b1807e65236b62f4b98e4b460754a6c82e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC44\/Mistral-7B-private-oia",
        "Average":74.86,
        "ARC":72.78,
        "HellaSwag":89.24,
        "MMLU":64.26,
        "TruthfulQA":73.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"602744dea3ddc2c1b33a6a67718dc594be3e4694"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"silvercoder67\/Mistral-7b-instruct-v0.2-summ-sft-dpo-e2",
        "Average":74.85,
        "ARC":73.81,
        "HellaSwag":88.85,
        "MMLU":64.61,
        "TruthfulQA":72.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"803a3e7f24f61e8cd53ef4133ae22c3ce2568a78"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Smaug-Mixtral-v0.1",
        "Average":74.85,
        "ARC":74.66,
        "HellaSwag":87.72,
        "MMLU":70.06,
        "TruthfulQA":66.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"98fdc8315906b0a8b9e7f24bad89914869fcfc20"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rizla\/trrapi-16b",
        "Average":74.84,
        "ARC":72.1,
        "HellaSwag":88.88,
        "MMLU":64.26,
        "TruthfulQA":74.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":18.79,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"96b27c4205881920289b29ac3d83ba5edf5cf672"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kquant03\/Samlagast-7B-laser-bf16",
        "Average":74.83,
        "ARC":72.87,
        "HellaSwag":88.96,
        "MMLU":64.35,
        "TruthfulQA":73.16,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"3d2e5b5fd23cee9303ce2b8e068aa49973a3f61e"
    },
    {
        "T":"?",
        "Model":"Sao10K\/Typhon-Mixtral-v1",
        "Average":74.81,
        "ARC":71.84,
        "HellaSwag":87.47,
        "MMLU":71.11,
        "TruthfulQA":68.81,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"8483318133a7763eb2dedc59294559febbf657c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jefferylovely\/SuperThetaMaven",
        "Average":74.81,
        "ARC":73.63,
        "HellaSwag":89.0,
        "MMLU":64.82,
        "TruthfulQA":71.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1344e7cb96b5c037c7bf00ed67311b582b56327f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/Topxtral-4x7B-v0.1",
        "Average":74.8,
        "ARC":72.53,
        "HellaSwag":88.33,
        "MMLU":64.96,
        "TruthfulQA":73.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":18.52,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"1a219935a01db03820ddabb2e29c199222a772e5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TomGrc\/FusionNet_7Bx2_MoE_v0.1",
        "Average":74.79,
        "ARC":74.06,
        "HellaSwag":88.9,
        "MMLU":65.0,
        "TruthfulQA":71.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.88,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"a0d648c1bcc3f1615bb2f0a94c6d32e7abde355d"
    },
    {
        "T":"?",
        "Model":"Nitral-AI\/Eris_PrimeV3.05-Vision-7B",
        "Average":74.78,
        "ARC":72.78,
        "HellaSwag":88.48,
        "MMLU":65.09,
        "TruthfulQA":72.76,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"fe0e9d11143618c06eba0b2756dce22491dc263f"
    },
    {
        "T":"?",
        "Model":"dddsaty\/FusionNet_7Bx2_MoE_Ko_DPO_Adapter_Attach",
        "Average":74.77,
        "ARC":73.89,
        "HellaSwag":88.94,
        "MMLU":65.03,
        "TruthfulQA":71.24,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0c9f2823a900408cf3c70c532288f89e452067f7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/Faraday-7B",
        "Average":74.76,
        "ARC":72.44,
        "HellaSwag":88.91,
        "MMLU":64.68,
        "TruthfulQA":73.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"2d197f7a290d191183b86f35c3857dd15a16d9b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/UNAversal-8x7B-v1beta",
        "Average":74.76,
        "ARC":69.8,
        "HellaSwag":86.9,
        "MMLU":70.39,
        "TruthfulQA":71.97,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":46.7,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"db160d4bc5bd9f2e66a764aeb44dcd18fb8afa6d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/pikus-pikantny-7B-dare",
        "Average":74.76,
        "ARC":72.18,
        "HellaSwag":88.56,
        "MMLU":65.0,
        "TruthfulQA":73.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ae6673e66e35679c14f4cc09237568d852d43b3d"
    },
    {
        "T":"?",
        "Model":"Sao10K\/Franziska-Mixtral-v1",
        "Average":74.75,
        "ARC":71.76,
        "HellaSwag":87.37,
        "MMLU":69.78,
        "TruthfulQA":70.07,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0121c0f6d769e8c0ecafeae0e85092855a4e95c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC56\/Mistral-7B-orca-dpo-8h",
        "Average":74.74,
        "ARC":72.44,
        "HellaSwag":88.99,
        "MMLU":64.59,
        "TruthfulQA":72.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6ab6f2489b77ba23d0cc230ff4cbb826eb5bc6e5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/60B_MoE_Coder_v3",
        "Average":74.74,
        "ARC":71.16,
        "HellaSwag":85.44,
        "MMLU":75.37,
        "TruthfulQA":67.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":60.81,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"ccd128942c5a6bb1672ceed21730d0e172655d77"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/Faraday-7B",
        "Average":74.73,
        "ARC":72.27,
        "HellaSwag":88.9,
        "MMLU":64.69,
        "TruthfulQA":73.07,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"2d197f7a290d191183b86f35c3857dd15a16d9b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/LUNA-SOLARkrautLM-Instruct",
        "Average":74.73,
        "ARC":71.16,
        "HellaSwag":88.28,
        "MMLU":66.11,
        "TruthfulQA":73.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"3b6604be8133f311d0719acb95d1a3a1f62a7d67"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YouKnwMe\/Direct-sm-private-e1",
        "Average":74.71,
        "ARC":72.53,
        "HellaSwag":88.98,
        "MMLU":64.55,
        "TruthfulQA":72.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0465002ffc1849eee1223b57dcf180e4dbd09d34"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rombodawg\/Open_Gpt4_8x7B_v0.2",
        "Average":74.71,
        "ARC":68.69,
        "HellaSwag":86.16,
        "MMLU":72.07,
        "TruthfulQA":71.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":46.7,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"3aba335d2131a014494a9df7c8a3d0783f50bad8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC56\/Mistral-7B-sumz-dpo-5h",
        "Average":74.7,
        "ARC":72.7,
        "HellaSwag":88.99,
        "MMLU":64.78,
        "TruthfulQA":72.36,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6a311ececab8cb8de5a3f6fd6a9afd1a4d5f7a80"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CausalLM\/34b-beta",
        "Average":74.68,
        "ARC":70.56,
        "HellaSwag":84.2,
        "MMLU":85.6,
        "TruthfulQA":58.38,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":34.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1a68e2717bd4b42dc9860695c3a192845e388cf7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC56\/Mistral-7B-orca-dpo-4h",
        "Average":74.68,
        "ARC":73.38,
        "HellaSwag":88.73,
        "MMLU":64.97,
        "TruthfulQA":71.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b82e05bf781ad9be0febdb2d8aaa4efbc7ca43fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YouKnwMe\/Mistral-7b-instruct-v0.2-private-eds2",
        "Average":74.68,
        "ARC":73.12,
        "HellaSwag":89.23,
        "MMLU":64.11,
        "TruthfulQA":72.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d4cffeadbb02eaf6273e954fc5aea4f745747705"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC44\/Mistral-7B-private-spef",
        "Average":74.67,
        "ARC":73.21,
        "HellaSwag":88.68,
        "MMLU":64.9,
        "TruthfulQA":71.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bb29794e86ff6a39f77185f547c6bb335d2f5649"
    },
    {
        "T":"?",
        "Model":"jsfs11\/MixtureofMerges-MoE-4x7b-v3",
        "Average":74.66,
        "ARC":74.4,
        "HellaSwag":88.62,
        "MMLU":64.82,
        "TruthfulQA":70.78,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0f467f3f3c2182f5798885f4166c2445ea817129"
    },
    {
        "T":"?",
        "Model":"AbacusResearch\/haLLawa4-7b",
        "Average":74.66,
        "ARC":71.5,
        "HellaSwag":88.36,
        "MMLU":64.49,
        "TruthfulQA":74.27,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"246c8ccf36db945de3575f2896bfe82d17ee628f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Nitral-AI\/Lelanta-lake-7b",
        "Average":74.64,
        "ARC":72.27,
        "HellaSwag":88.95,
        "MMLU":64.31,
        "TruthfulQA":73.05,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"9a99e78b811a98cb9929530b261fd4cdaed4c558"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC44\/Mistral-7B-private-sia",
        "Average":74.62,
        "ARC":72.53,
        "HellaSwag":89.08,
        "MMLU":64.45,
        "TruthfulQA":72.44,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"213b8ab1da8a0de0183f2fd177e95a34bc157b1c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC44\/Mistral-7B-private-spnf",
        "Average":74.62,
        "ARC":73.04,
        "HellaSwag":88.67,
        "MMLU":64.91,
        "TruthfulQA":71.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b517b17cdec49dfa1dbb1927cfcac97eec020a59"
    },
    {
        "T":"?",
        "Model":"Kquant03\/Cognito-2x7B-bf16",
        "Average":74.62,
        "ARC":72.95,
        "HellaSwag":88.96,
        "MMLU":64.86,
        "TruthfulQA":71.7,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"b743668c7cf34e29d52adf22203850ee407b4e2e"
    },
    {
        "T":"?",
        "Model":"maywell\/kiqu-70b",
        "Average":74.61,
        "ARC":72.1,
        "HellaSwag":87.94,
        "MMLU":74.93,
        "TruthfulQA":63.48,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":68.98,
        "Hub":22,
        "Available on the hub":true,
        "Model Sha":"c8ad8ee000e4e042d80e4cf53fb6d0815d7743dd"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"moreh\/MoMo-70B-lora-1.8.5-DPO",
        "Average":74.61,
        "ARC":69.54,
        "HellaSwag":85.6,
        "MMLU":77.49,
        "TruthfulQA":65.79,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":70.0,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"7a0aadea285a82d50c96b0988b12cc3c6267249a"
    },
    {
        "T":"?",
        "Model":"moreh\/MoMo-72B-lora-1.8.5-DPO",
        "Average":74.61,
        "ARC":69.54,
        "HellaSwag":85.6,
        "MMLU":77.49,
        "TruthfulQA":65.79,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":72.0,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"7a0aadea285a82d50c96b0988b12cc3c6267249a"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_6-7B-dare_ties",
        "Average":74.6,
        "ARC":73.04,
        "HellaSwag":88.82,
        "MMLU":64.52,
        "TruthfulQA":72.0,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d4afbe8b1ac311c82f9f195b0bbb933d8c16cbb0"
    },
    {
        "T":"?",
        "Model":"shadowml\/BeagSake-7B",
        "Average":74.58,
        "ARC":72.44,
        "HellaSwag":88.39,
        "MMLU":65.23,
        "TruthfulQA":72.27,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e1ae2c1e9bea8b54f6b8bff41a4f50895625a6ed"
    },
    {
        "T":"?",
        "Model":"jsfs11\/RandomMergeNoNormWEIGHTED-7B-DARETIES",
        "Average":74.58,
        "ARC":73.38,
        "HellaSwag":88.5,
        "MMLU":64.94,
        "TruthfulQA":71.5,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"611f9e29fc041be6c915538c4883669b17d1e1e2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/19B_TRUTH_DPO",
        "Average":74.58,
        "ARC":71.67,
        "HellaSwag":88.63,
        "MMLU":65.78,
        "TruthfulQA":72.23,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.19,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a388bd7af444f632e5e9370bedaeb69572f861af"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC56\/Mistral-7B-sumz-dpo-4h",
        "Average":74.57,
        "ARC":72.95,
        "HellaSwag":88.81,
        "MMLU":64.8,
        "TruthfulQA":71.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"64895c1f9916b5299f2a94412d6cfddd4d0845a0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Eric111\/CatunaMayo-DPO",
        "Average":74.56,
        "ARC":72.87,
        "HellaSwag":88.3,
        "MMLU":65.24,
        "TruthfulQA":71.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"54959097f175441fcb25c9eec1f4169ee7d5232f"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/WestMonarchLasers-7B-slerp",
        "Average":74.56,
        "ARC":72.44,
        "HellaSwag":88.66,
        "MMLU":64.73,
        "TruthfulQA":72.4,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cce3df27caa1c26a96308e429c27d27465f6d6d2"
    },
    {
        "T":"?",
        "Model":"Kquant03\/Nanashi-2x7B-bf16",
        "Average":74.56,
        "ARC":73.12,
        "HellaSwag":88.76,
        "MMLU":65.04,
        "TruthfulQA":71.31,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a922f962cb95e4eb3d77357e82405486e64ab7b1"
    },
    {
        "T":"?",
        "Model":"alnrg2arg\/blockchainlabs_test3_seminar",
        "Average":74.55,
        "ARC":72.18,
        "HellaSwag":88.94,
        "MMLU":64.63,
        "TruthfulQA":72.47,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5c306a299896f682c0a392d893f258603d5d0706"
    },
    {
        "T":"?",
        "Model":"jsfs11\/WONMSeverusDevil-TIES-7B",
        "Average":74.54,
        "ARC":72.95,
        "HellaSwag":88.45,
        "MMLU":64.77,
        "TruthfulQA":72.0,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bbe4f09eb68a0f6f628b2cc54ba55d8ac5d34e5f"
    },
    {
        "T":"?",
        "Model":"kaitchup\/Mayonnaise-4in1-022",
        "Average":74.54,
        "ARC":72.87,
        "HellaSwag":88.63,
        "MMLU":64.93,
        "TruthfulQA":71.73,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"3b5f7ff495b82d71f4f542779435ce737811ea25"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"cloudyu\/Venus_DPO_50",
        "Average":74.53,
        "ARC":70.73,
        "HellaSwag":88.47,
        "MMLU":66.3,
        "TruthfulQA":72.63,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":19.19,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"750695fe8e57714551d261b8c101a594c634d5b9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/Phoenix_DPO_60B",
        "Average":74.53,
        "ARC":71.16,
        "HellaSwag":85.46,
        "MMLU":77.66,
        "TruthfulQA":63.84,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":60.81,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"bd9ac169a0d6acb8fb66d55a6471ef162271b248"
    },
    {
        "T":"?",
        "Model":"vanillaOVO\/merge_7B_state_2",
        "Average":74.53,
        "ARC":73.12,
        "HellaSwag":88.62,
        "MMLU":65.0,
        "TruthfulQA":71.37,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7b5b4c6ecd7e6698aac07f0d440ba90260ad202e"
    },
    {
        "T":"\u2b55",
        "Model":"jeonsworld\/CarbonVillain-en-13B-v1",
        "Average":74.53,
        "ARC":71.25,
        "HellaSwag":88.46,
        "MMLU":66.42,
        "TruthfulQA":71.98,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"a797e7e81f7929a31ca232858318d72b93b6abe0"
    },
    {
        "T":"\u2b55",
        "Model":"jeonsworld\/CarbonVillain-en-10.7B-v1",
        "Average":74.53,
        "ARC":71.25,
        "HellaSwag":88.46,
        "MMLU":66.42,
        "TruthfulQA":71.98,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"a797e7e81f7929a31ca232858318d72b93b6abe0"
    },
    {
        "T":"\u2b55",
        "Model":"jeonsworld\/CarbonVillain-en-10.7B-v5",
        "Average":74.52,
        "ARC":71.16,
        "HellaSwag":88.51,
        "MMLU":66.44,
        "TruthfulQA":71.97,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"941b5a690781dd412eb435446b65e92048992abe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC56\/Mistral-7B-orca-dpo-2h",
        "Average":74.52,
        "ARC":73.12,
        "HellaSwag":88.65,
        "MMLU":64.99,
        "TruthfulQA":71.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"5b96d5e0a38d25fe6bd72939a8eb193c34e59232"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"kyujinpy\/Sakura-SOLRCA-Math-Instruct-DPO-v1",
        "Average":74.51,
        "ARC":71.25,
        "HellaSwag":88.48,
        "MMLU":66.21,
        "TruthfulQA":72.12,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"60e68b717f30144757b2e51d1db879c0c628f128"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"kyujinpy\/Sakura-SOLRCA-Math-Instruct-DPO-v2",
        "Average":74.51,
        "ARC":71.25,
        "HellaSwag":88.52,
        "MMLU":66.13,
        "TruthfulQA":72.16,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"c994171eefa80df644e31ac01c1ee2d9e5546d99"
    },
    {
        "T":"?",
        "Model":"vishnukv\/WestSeverusJaskier",
        "Average":74.51,
        "ARC":71.76,
        "HellaSwag":88.16,
        "MMLU":64.94,
        "TruthfulQA":73.18,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d3c32e9c804c110986c4c526e50297d7a0545f03"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC56\/Mistral-7B-sumz-dpo-3h",
        "Average":74.5,
        "ARC":73.04,
        "HellaSwag":88.67,
        "MMLU":64.78,
        "TruthfulQA":71.53,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a704fa5a292b5f8667a23f2e818ae8cdcf51779b"
    },
    {
        "T":"?",
        "Model":"alnrg2arg\/test3_sft_16bit_dpo2",
        "Average":74.5,
        "ARC":73.63,
        "HellaSwag":89.03,
        "MMLU":64.63,
        "TruthfulQA":70.71,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8a741a32e8d1d426c408c3eeb208eccc172c655e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TomGrc\/FusionNet_linear",
        "Average":74.49,
        "ARC":71.25,
        "HellaSwag":88.44,
        "MMLU":66.35,
        "TruthfulQA":71.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":10.73,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"a6eba075d53fc4bdbcded071f9bdeb287d1ac260"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TomGrc\/FusionNet",
        "Average":74.49,
        "ARC":71.25,
        "HellaSwag":88.42,
        "MMLU":66.36,
        "TruthfulQA":71.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"34421f146e5eb3306a86dd8b67ec938e800ee52e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kodonho\/SolarM-SakuraSolar-SLERP",
        "Average":74.49,
        "ARC":71.16,
        "HellaSwag":88.47,
        "MMLU":66.24,
        "TruthfulQA":72.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"c54dbc0da9e028cfaf92114206c6b84c0198d2b0"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"yunconglong\/Truthful_DPO_MOE_19B",
        "Average":74.49,
        "ARC":71.08,
        "HellaSwag":88.46,
        "MMLU":66.13,
        "TruthfulQA":72.29,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":19.19,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"de574b57d45cfea00748c464af17f1c1ca53e548"
    },
    {
        "T":"\u2b55",
        "Model":"jeonsworld\/CarbonVillain-en-10.7B-v4",
        "Average":74.49,
        "ARC":71.25,
        "HellaSwag":88.48,
        "MMLU":66.27,
        "TruthfulQA":71.95,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"904ffe8106a3facbea0d0e61d9a53a525675871e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cloudyu\/Mixtral_11Bx2_MoE_19B",
        "Average":74.48,
        "ARC":71.16,
        "HellaSwag":88.47,
        "MMLU":66.31,
        "TruthfulQA":72.0,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.19,
        "Hub":36,
        "Available on the hub":false,
        "Model Sha":"092208b5bfab866b301545149a6b14fde48a0dd6"
    },
    {
        "T":"\u2b55",
        "Model":"kekmodel\/StopCarbon-10.7B-v6",
        "Average":74.48,
        "ARC":71.16,
        "HellaSwag":88.5,
        "MMLU":66.31,
        "TruthfulQA":71.96,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8953a705c909ef98fe3b0ea524c5816a57f1954c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment15-7B",
        "Average":74.48,
        "ARC":72.18,
        "HellaSwag":88.68,
        "MMLU":60.01,
        "TruthfulQA":77.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"45b93bfc4297b0bc1ef0b7316cbae11d2bb527d1"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"kyujinpy\/Sakura-SOLRCA-Instruct-DPO",
        "Average":74.48,
        "ARC":71.16,
        "HellaSwag":88.49,
        "MMLU":66.17,
        "TruthfulQA":72.1,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"48977e38731685ad9a45eef6ff94d5d6f60471f2"
    },
    {
        "T":"\u2b55",
        "Model":"kekmodel\/StopCarbon-10.7B-v2",
        "Average":74.48,
        "ARC":71.08,
        "HellaSwag":88.6,
        "MMLU":66.23,
        "TruthfulQA":72.01,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7b49c998e2a32006e27d3e826d19240ed6bdd697"
    },
    {
        "T":"?",
        "Model":"invalid-coder\/Sakura-SOLAR-Instruct-CarbonVillain-en-10.7B-v2-slerp",
        "Average":74.48,
        "ARC":71.25,
        "HellaSwag":88.42,
        "MMLU":66.31,
        "TruthfulQA":71.94,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"39a1c76ddb5fa3a82c5b4071121d2e4866a25300"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051612\/B0121",
        "Average":74.48,
        "ARC":68.34,
        "HellaSwag":85.3,
        "MMLU":85.63,
        "TruthfulQA":58.63,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7be68dd360bea0171316ccc646261532599f14dc"
    },
    {
        "T":"\u2b55",
        "Model":"logicker\/SkkuDataScienceGlobal-10.7b",
        "Average":74.47,
        "ARC":71.25,
        "HellaSwag":88.41,
        "MMLU":66.31,
        "TruthfulQA":71.92,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4f5e40b38099084b86fb18b294e4e61e7d20cc7c"
    },
    {
        "T":"\u2b55",
        "Model":"jeonsworld\/CarbonVillain-en-10.7B-v2",
        "Average":74.47,
        "ARC":71.25,
        "HellaSwag":88.4,
        "MMLU":66.31,
        "TruthfulQA":71.94,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"70b507c12dfe6ce8a7d050be5475fc9684a4929f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"macadeliccc\/SOLAR-10.7b-Instruct-dpo",
        "Average":74.47,
        "ARC":71.76,
        "HellaSwag":88.08,
        "MMLU":66.06,
        "TruthfulQA":71.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"0219ef0ce5c8aaa6abe5e6c30f287edb777c7e8c"
    },
    {
        "T":"\u2b55",
        "Model":"kekmodel\/StopCarbon-10.7B-v4",
        "Average":74.47,
        "ARC":71.25,
        "HellaSwag":88.5,
        "MMLU":66.24,
        "TruthfulQA":71.89,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c8d98bb8c6b23b3c3b7462df7eb02a3b05622612"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/19B_MATH_DPO",
        "Average":74.47,
        "ARC":71.08,
        "HellaSwag":88.43,
        "MMLU":66.25,
        "TruthfulQA":72.11,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":19.19,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0a25a243957b41c7ac8d59af50294547151ae621"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gagan3012\/MetaModelv2",
        "Average":74.47,
        "ARC":71.08,
        "HellaSwag":88.56,
        "MMLU":66.29,
        "TruthfulQA":71.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2cb9c69984ee3e5506f055238fd1aa5fe4ea91bd"
    },
    {
        "T":"?",
        "Model":"cloudyu\/Mixtral-8x7B-Instruct-v0.1-DPO",
        "Average":74.46,
        "ARC":69.8,
        "HellaSwag":87.83,
        "MMLU":71.05,
        "TruthfulQA":69.18,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"9311a4300f61f4cba381ba8347b73f0f2977a8f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"saishf\/Multi-Verse-RP-7B",
        "Average":74.46,
        "ARC":72.35,
        "HellaSwag":88.37,
        "MMLU":63.94,
        "TruthfulQA":73.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"ca05b22adfc6ef9a9af7d2a07d617ac8684b1b9a"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/bruphin-lambda",
        "Average":74.46,
        "ARC":72.35,
        "HellaSwag":88.22,
        "MMLU":64.9,
        "TruthfulQA":72.36,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"276d1c0358bdf051b0be4f3e8eb0146f8e863f36"
    },
    {
        "T":"?",
        "Model":"ChaoticNeutrals\/Eris_PrimeV4-Vision-7B",
        "Average":74.45,
        "ARC":72.78,
        "HellaSwag":88.47,
        "MMLU":65.13,
        "TruthfulQA":71.43,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"fec915be390bd7304e0cddcf0aff216edd4ac87e"
    },
    {
        "T":"?",
        "Model":"mlabonne\/FrankenMonarch-7B",
        "Average":74.45,
        "ARC":71.59,
        "HellaSwag":88.59,
        "MMLU":63.93,
        "TruthfulQA":73.69,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5f6c842da4acef82352939453fdd6e50d716e288"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"gagan3012\/MetaModel_moe",
        "Average":74.44,
        "ARC":71.25,
        "HellaSwag":88.4,
        "MMLU":66.26,
        "TruthfulQA":71.86,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":36.1,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"015dae67b68e6e5007b7b13a448886eb5f6bfea8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/Wernicke-7B-v9",
        "Average":74.43,
        "ARC":72.44,
        "HellaSwag":88.54,
        "MMLU":64.9,
        "TruthfulQA":71.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"2ad6dc3fc3551ef641c0ca6b7dbb157194d9a911"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Xenon1\/MetaModel_moex8",
        "Average":74.43,
        "ARC":71.16,
        "HellaSwag":88.38,
        "MMLU":66.29,
        "TruthfulQA":71.91,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":69.92,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"333524a8c6ed8415fd48f852e53c405cac82733d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DopeorNope\/SOLARC-M-10.7B",
        "Average":74.43,
        "ARC":71.16,
        "HellaSwag":88.41,
        "MMLU":66.31,
        "TruthfulQA":71.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"fa95c376fdad1670d4125e833322dbf6aeb8f410"
    },
    {
        "T":"\u2b55",
        "Model":"gagan3012\/MetaModelv3",
        "Average":74.43,
        "ARC":71.16,
        "HellaSwag":88.39,
        "MMLU":66.32,
        "TruthfulQA":71.86,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"862f5ca5e66a0b053c14e40c8f16f2c2807b6d92"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"naseerfaheem\/SOLAR-10.7B-Instruct-ties",
        "Average":74.42,
        "ARC":70.9,
        "HellaSwag":88.58,
        "MMLU":66.34,
        "TruthfulQA":71.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"333fbc56f7406a47435ad9afbde01c4f8116287e"
    },
    {
        "T":"?",
        "Model":"vanillaOVO\/supermario_v2",
        "Average":74.42,
        "ARC":72.95,
        "HellaSwag":88.53,
        "MMLU":64.99,
        "TruthfulQA":71.22,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ae95b6254eee9bd1de68fc6f4881a7bb98d57235"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"fblgit\/UNA-SOLAR-10.7B-Instruct-v1.0",
        "Average":74.42,
        "ARC":70.73,
        "HellaSwag":88.32,
        "MMLU":66.1,
        "TruthfulQA":72.52,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":10.73,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"c63d06344214886094d7ab6c7fd5692cc59fdf0d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DopeorNope\/SOLARC-MOE-10.7Bx4",
        "Average":74.42,
        "ARC":70.99,
        "HellaSwag":88.43,
        "MMLU":66.34,
        "TruthfulQA":71.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":36.1,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"07cee5a25fd8d85486f888893d5bee532e5f5cd8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC99\/Mistral-7B-privatemix-ia1",
        "Average":74.42,
        "ARC":72.78,
        "HellaSwag":88.59,
        "MMLU":64.5,
        "TruthfulQA":71.79,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dda79b349b53c2d4efd52a99a0ae31c28f3693cf"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"kyujinpy\/Sakura-SOLAR-Instruct-DPO-v2",
        "Average":74.41,
        "ARC":70.9,
        "HellaSwag":88.41,
        "MMLU":66.48,
        "TruthfulQA":71.86,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"7f45a1ed9ca0f88b9ec23aa9b6202e8783ab35ac"
    },
    {
        "T":"\u2b55",
        "Model":"jeonsworld\/CarbonVillain-en-10.7B-v3",
        "Average":74.41,
        "ARC":70.99,
        "HellaSwag":88.48,
        "MMLU":66.34,
        "TruthfulQA":71.84,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"285436a72c10e0f2b8eb897549350fe40c2e8bbe"
    },
    {
        "T":"\u2b55",
        "Model":"kekmodel\/StopCarbon-10.7B-v5",
        "Average":74.41,
        "ARC":70.99,
        "HellaSwag":88.48,
        "MMLU":66.34,
        "TruthfulQA":71.84,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"57966bc616a9db7756488661f4ed16b40ee23780"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rombodawg\/Open_Gpt4_8x7B",
        "Average":74.41,
        "ARC":69.28,
        "HellaSwag":86.77,
        "MMLU":71.2,
        "TruthfulQA":70.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":46.7,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"fa90ffb7fb57cb609d9d47719b3731693d23b312"
    },
    {
        "T":"\u2b55",
        "Model":"kekmodel\/StopCarbon-10.7B-v3",
        "Average":74.41,
        "ARC":70.99,
        "HellaSwag":88.57,
        "MMLU":66.13,
        "TruthfulQA":71.94,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4cf314aa78f585376918a1be8b5a246edf9f4e71"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gagan3012\/MetaModel",
        "Average":74.41,
        "ARC":71.08,
        "HellaSwag":88.45,
        "MMLU":66.26,
        "TruthfulQA":71.84,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"06308e54585a49a01a93c99caa2fb34daf4e7619"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shadowml\/Mixolar-4x7b",
        "Average":74.4,
        "ARC":71.08,
        "HellaSwag":88.44,
        "MMLU":66.29,
        "TruthfulQA":71.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":36.1,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"5a1b8a9c8df923c7c0e38fe9e534f73968603030"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/flammen5-mistral-7B",
        "Average":74.4,
        "ARC":72.01,
        "HellaSwag":88.47,
        "MMLU":64.95,
        "TruthfulQA":72.17,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"327c3aa9b5c4dfd66b59f9b86eece1f87459ccf7"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"gagan3012\/MetaModel_moe",
        "Average":74.4,
        "ARC":71.08,
        "HellaSwag":88.39,
        "MMLU":66.31,
        "TruthfulQA":71.82,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":36.1,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"015dae67b68e6e5007b7b13a448886eb5f6bfea8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/Wernicke-7B-v1",
        "Average":74.4,
        "ARC":73.21,
        "HellaSwag":88.48,
        "MMLU":64.95,
        "TruthfulQA":70.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"752573009b7d0518958d4b98b3154678af7c2bde"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"VAGOsolutions\/SauerkrautLM-SOLAR-Instruct",
        "Average":74.4,
        "ARC":70.82,
        "HellaSwag":88.63,
        "MMLU":66.2,
        "TruthfulQA":71.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":44,
        "Available on the hub":true,
        "Model Sha":"8b9615124a0bcadd7fa984eaadd066da0fb4fbae"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/Sectumsempra-7B-DPO",
        "Average":74.4,
        "ARC":71.5,
        "HellaSwag":88.7,
        "MMLU":64.9,
        "TruthfulQA":72.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"5ecc835f4137adac99198831c61c2afff4f340cf"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_20-7B-slerp",
        "Average":74.38,
        "ARC":73.12,
        "HellaSwag":88.45,
        "MMLU":65.06,
        "TruthfulQA":70.9,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"17dfc31820b675771f1409caf36e1837de1a6abe"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Neuronovo\/neuronovo-7B-v0.2",
        "Average":74.38,
        "ARC":73.04,
        "HellaSwag":88.32,
        "MMLU":65.15,
        "TruthfulQA":71.02,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"72b49b8390caf1413a4bc33a759c147525510482"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kyujinpy\/Sakura-SOLAR-Instruct",
        "Average":74.38,
        "ARC":70.99,
        "HellaSwag":88.42,
        "MMLU":66.33,
        "TruthfulQA":71.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":27,
        "Available on the hub":true,
        "Model Sha":"cc4531a25fff7cbb146c0e12f2cf4e19189c37a2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DopeorNope\/SOLARC-MOE-10.7Bx6",
        "Average":74.38,
        "ARC":70.9,
        "HellaSwag":88.4,
        "MMLU":66.36,
        "TruthfulQA":71.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":53.01,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"64c8ef9fa6d9b54b68261d839b656b0dc8717374"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/TheTop-5x7B-Instruct-T-v0.1",
        "Average":74.37,
        "ARC":73.63,
        "HellaSwag":88.85,
        "MMLU":64.22,
        "TruthfulQA":70.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"dd841bf2fc42cb4f872fab04a638465cb06a4b7a"
    },
    {
        "T":"?",
        "Model":"dddsaty\/Merge_Sakura_Solar",
        "Average":74.37,
        "ARC":70.73,
        "HellaSwag":88.51,
        "MMLU":66.03,
        "TruthfulQA":72.21,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"0cce8842b179e19e6faac936a8c44ea1ba05b6b9"
    },
    {
        "T":"?",
        "Model":"zhengr\/MixTAO-7Bx2-MoE-Instruct-v6.0",
        "Average":74.36,
        "ARC":73.38,
        "HellaSwag":89.02,
        "MMLU":64.61,
        "TruthfulQA":70.45,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"29ceaff6300241003171ae2219ae1bfa77128b54"
    },
    {
        "T":"?",
        "Model":"shadowml\/WestBeagle-7B",
        "Average":74.36,
        "ARC":72.27,
        "HellaSwag":88.29,
        "MMLU":65.17,
        "TruthfulQA":71.71,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"e84cd31eaed755a286dee0c9ca2fe4308e693f64"
    },
    {
        "T":"?",
        "Model":"invalid-coder\/SOLAR-10.7B-Instruct-SOLARC-M-10.7B-slerp",
        "Average":74.36,
        "ARC":71.08,
        "HellaSwag":88.34,
        "MMLU":66.29,
        "TruthfulQA":71.73,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"656f2be86e3f30067e62e61cedf78b0697ce0a97"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Neuronovo\/neuronovo-7B-v0.3",
        "Average":74.35,
        "ARC":72.7,
        "HellaSwag":88.26,
        "MMLU":65.1,
        "TruthfulQA":71.35,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"6f5c9f242610ade5940a6e04d367ef9398409b73"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"macadeliccc\/WestLake-7B-v2-laser-truthy-dpo",
        "Average":74.35,
        "ARC":73.89,
        "HellaSwag":88.85,
        "MMLU":64.84,
        "TruthfulQA":69.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":21,
        "Available on the hub":false,
        "Model Sha":"e01fb197b4303ba63ba2f4d68a897006ec7ec4fd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kodonho\/Solar-OrcaDPO-Solar-Instruct-SLERP",
        "Average":74.35,
        "ARC":70.99,
        "HellaSwag":88.22,
        "MMLU":66.22,
        "TruthfulQA":71.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ea7a7a1c14c4b67bad56dbd08245dbb79dc71ec3"
    },
    {
        "T":"\u2b55",
        "Model":"kekmodel\/StopCarbon-10.7B-v1",
        "Average":74.34,
        "ARC":70.9,
        "HellaSwag":88.41,
        "MMLU":66.32,
        "TruthfulQA":71.71,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a1681ef65f3d06b421969199ae07b8d32feecf9a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"s3nh\/Severusectum-7B-DPO",
        "Average":74.32,
        "ARC":71.5,
        "HellaSwag":88.55,
        "MMLU":64.79,
        "TruthfulQA":72.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"50f69f6cddaee727bb25f23a0eb525175a2c6491"
    },
    {
        "T":"?",
        "Model":"BarryFutureman\/WildMarcoroni-Variant3-7B",
        "Average":74.32,
        "ARC":72.27,
        "HellaSwag":88.96,
        "MMLU":64.38,
        "TruthfulQA":71.68,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"77ec3ea64cb134ae694dd72606235ef497cf46d3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SilverCoder66\/Mistral-7B-Instruct-adapt-v0.22",
        "Average":74.32,
        "ARC":72.53,
        "HellaSwag":88.5,
        "MMLU":65.0,
        "TruthfulQA":71.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5bcce7562c547b81d8bc41363e1fedb15fde429c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SilverCoder66\/Mistral-7B-Instruct-adapt-v0.23",
        "Average":74.32,
        "ARC":72.53,
        "HellaSwag":88.5,
        "MMLU":65.0,
        "TruthfulQA":71.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c11553f07f16813545fe8c8eab410b164eb7cea3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SilverCoder66\/Mistral-7B-Instruct-adapt-v0.21",
        "Average":74.32,
        "ARC":72.53,
        "HellaSwag":88.5,
        "MMLU":65.0,
        "TruthfulQA":71.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"15f7ace0735678feec75a36589ed4760bfa172bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YouKnwMe\/Mistral-7B-Instruct-exp-e2",
        "Average":74.32,
        "ARC":72.53,
        "HellaSwag":88.5,
        "MMLU":65.0,
        "TruthfulQA":71.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9e85b1810b7e4c549711b217cadde6e93c6b60d8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"s3nh\/SeverusWestLake-7B-DPO",
        "Average":74.31,
        "ARC":72.18,
        "HellaSwag":88.94,
        "MMLU":64.65,
        "TruthfulQA":71.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"5d6dd3a16f0469cff432410b29ec410167b71c49"
    },
    {
        "T":"?",
        "Model":"vicgalle\/ConfigurableSOLAR-10.7B",
        "Average":74.3,
        "ARC":70.39,
        "HellaSwag":88.03,
        "MMLU":66.44,
        "TruthfulQA":72.34,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"df83494a4366e081563659e1142464029a0dec82"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/multimaster-7b-v6",
        "Average":74.29,
        "ARC":72.78,
        "HellaSwag":88.77,
        "MMLU":64.74,
        "TruthfulQA":70.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":35.43,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"cc18e2b0b9764f255341d3e530d018545987544b"
    },
    {
        "T":"?",
        "Model":"LewisDeBenoisIV\/BillyTheKid1803",
        "Average":74.29,
        "ARC":71.84,
        "HellaSwag":88.09,
        "MMLU":65.07,
        "TruthfulQA":72.16,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"23dab1dc473bc9a2d345f5a19e5a564528665d48"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Weyaxi\/SauerkrautLM-UNA-SOLAR-Instruct",
        "Average":74.29,
        "ARC":70.9,
        "HellaSwag":88.3,
        "MMLU":66.15,
        "TruthfulQA":71.8,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":25,
        "Available on the hub":true,
        "Model Sha":"564c02554a8b1f91c0860096bdb830dc15ac7805"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/SauerkrautLM-UNA-SOLAR-Instruct-test",
        "Average":74.29,
        "ARC":70.9,
        "HellaSwag":88.3,
        "MMLU":66.15,
        "TruthfulQA":71.8,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ae0cab05b071dcde2e89e80ab511fa1bc0f53f1c"
    },
    {
        "T":"?",
        "Model":"BarryFutureman\/WildMarcoroni-Variant1-7B",
        "Average":74.29,
        "ARC":73.98,
        "HellaSwag":88.61,
        "MMLU":64.81,
        "TruthfulQA":69.76,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"288b2e327f81e6f97fe1d68e2f0f08a46cdbf6eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SilverCoder66\/Mistral-7B-Instruct-adapt-v0.21",
        "Average":74.29,
        "ARC":73.98,
        "HellaSwag":88.61,
        "MMLU":64.81,
        "TruthfulQA":69.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"15f7ace0735678feec75a36589ed4760bfa172bb"
    },
    {
        "T":"\u2b55",
        "Model":"bhavinjawade\/SOLAR-10B-OrcaDPO-Jawade",
        "Average":74.28,
        "ARC":71.16,
        "HellaSwag":88.27,
        "MMLU":66.12,
        "TruthfulQA":71.57,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":10.73,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"02a497125bbf85fe0355eb22424315c920d1aec4"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"macadeliccc\/SOLAR-math-2x10.7b-v0.2",
        "Average":74.28,
        "ARC":70.9,
        "HellaSwag":88.29,
        "MMLU":66.25,
        "TruthfulQA":71.68,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.19,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"1aa7540c34d4dad02ec2b9bcc991bdcd12d3134d"
    },
    {
        "T":"?",
        "Model":"Weyaxi\/Bagel-Hermes-2x34b",
        "Average":74.28,
        "ARC":69.8,
        "HellaSwag":85.26,
        "MMLU":77.24,
        "TruthfulQA":64.82,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":60.81,
        "Hub":15,
        "Available on the hub":false,
        "Model Sha":"d187b7bd6757d78bf89aaad8b0b5834ddbf29392"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Bagel-Hermes-2x34B",
        "Average":74.28,
        "ARC":69.8,
        "HellaSwag":85.26,
        "MMLU":77.24,
        "TruthfulQA":64.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":60.81,
        "Hub":15,
        "Available on the hub":false,
        "Model Sha":"d187b7bd6757d78bf89aaad8b0b5834ddbf29392"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JaeyeonKang\/CCK_Gony_v3.3",
        "Average":74.28,
        "ARC":70.39,
        "HellaSwag":87.88,
        "MMLU":71.43,
        "TruthfulQA":67.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cbf9c2350f24d9d10ebb1961965e7fbb4361cafb"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"bhavinjawade\/SOLAR-10B-Nector-DPO-Jawade",
        "Average":74.27,
        "ARC":71.33,
        "HellaSwag":88.62,
        "MMLU":66.22,
        "TruthfulQA":70.92,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"669f8f726fac4a588ced06a4da3959eb8ca20f9f"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Neuronovo\/neuronovo-9B-v0.4",
        "Average":74.27,
        "ARC":72.44,
        "HellaSwag":88.33,
        "MMLU":65.24,
        "TruthfulQA":71.07,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"f4bfa8b298cbd0acc236117231d5b00de5f43240"
    },
    {
        "T":"?",
        "Model":"tushar310\/Hippy-AAI-7B",
        "Average":74.27,
        "ARC":71.84,
        "HellaSwag":88.04,
        "MMLU":65.17,
        "TruthfulQA":72.02,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"798c990e93e7ec827037b806c52fbc30a7009894"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_44-7B-dare_ties",
        "Average":74.27,
        "ARC":71.76,
        "HellaSwag":87.84,
        "MMLU":65.61,
        "TruthfulQA":71.85,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7a55d23772bf5862603076df515f230970a53532"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/Wernicke-7B-v8",
        "Average":74.27,
        "ARC":72.44,
        "HellaSwag":88.7,
        "MMLU":64.62,
        "TruthfulQA":71.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f65b1538d77cc48cb646e0c53350b55fd03bc96c"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"ryandt\/MusingCaterpillar",
        "Average":74.26,
        "ARC":72.53,
        "HellaSwag":88.34,
        "MMLU":65.26,
        "TruthfulQA":70.93,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":8.99,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"83c266f92d51adb87ed2c259f2c151f05fb10cc2"
    },
    {
        "T":"?",
        "Model":"flemmingmiguel\/MBX-7B-v2",
        "Average":74.26,
        "ARC":73.55,
        "HellaSwag":88.5,
        "MMLU":64.78,
        "TruthfulQA":70.21,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1e8604ec6f544415814c68ef0b9666393567e7dd"
    },
    {
        "T":"?",
        "Model":"zhengr\/MixTAO-7Bx2-MoE-Instruct-v5.0",
        "Average":74.26,
        "ARC":73.63,
        "HellaSwag":88.93,
        "MMLU":64.65,
        "TruthfulQA":69.83,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"e2c043dadae748feedea411e1ce2548d1b91aa80"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/Scorpio-7B",
        "Average":74.26,
        "ARC":71.33,
        "HellaSwag":88.5,
        "MMLU":64.7,
        "TruthfulQA":72.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ea5855b529987fde6eca87492bccbd28eef8d052"
    },
    {
        "T":"?",
        "Model":"FelixChao\/WestSeverus-10.7B",
        "Average":74.25,
        "ARC":72.18,
        "HellaSwag":87.47,
        "MMLU":65.06,
        "TruthfulQA":72.3,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"22fbfded4e563a0b1e0a750ff24e742ee19831b1"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/flammen10-mistral-7B",
        "Average":74.25,
        "ARC":71.76,
        "HellaSwag":88.27,
        "MMLU":64.75,
        "TruthfulQA":72.23,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"2d6906ecf27b829b66db8bd9900aca0be220910d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SilverCoder66\/Mistral-7B-Instruct-adapt-v0.2",
        "Average":74.25,
        "ARC":73.81,
        "HellaSwag":88.65,
        "MMLU":64.76,
        "TruthfulQA":69.79,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c3fa3d4679f303bce6d56357abb9069fdf8b44c2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Test157t\/Eris-Floramix-7b",
        "Average":74.25,
        "ARC":73.12,
        "HellaSwag":88.28,
        "MMLU":64.63,
        "TruthfulQA":70.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"a20c4fa9027d5a622e2e6b6ea9a255d84f3a8228"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"dhanushreddy29\/BrokenKeyboard",
        "Average":74.24,
        "ARC":71.25,
        "HellaSwag":88.34,
        "MMLU":66.04,
        "TruthfulQA":71.36,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"c03dfcda5d45ea4c518bd14641d9604726e00477"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/O0128",
        "Average":74.24,
        "ARC":67.92,
        "HellaSwag":85.34,
        "MMLU":83.59,
        "TruthfulQA":60.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d5cc987f61f58763eb2d02a06ddd103992d9a3dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC56\/Mistral-7B-orca-dpo-12h",
        "Average":74.24,
        "ARC":71.59,
        "HellaSwag":89.01,
        "MMLU":64.23,
        "TruthfulQA":72.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7453b717ab9010c3c5cfa5d38af4b174529bc457"
    },
    {
        "T":"?",
        "Model":"ChaoticNeutrals\/Eris_Floramix_DPO_7B",
        "Average":74.24,
        "ARC":73.04,
        "HellaSwag":88.28,
        "MMLU":64.71,
        "TruthfulQA":70.94,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"16e9d47cb25c33d57328638e5c56e257c6021ce1"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/flammen13-mistral-7B",
        "Average":74.24,
        "ARC":71.5,
        "HellaSwag":88.33,
        "MMLU":64.79,
        "TruthfulQA":72.34,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"876d2ad8c5d29ccd9590b9c5df191b6e206a4b54"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"moreh\/MoMo-72B-lora-1.8.4-DPO",
        "Average":74.24,
        "ARC":69.62,
        "HellaSwag":85.35,
        "MMLU":77.33,
        "TruthfulQA":64.64,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":72.29,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"a2c3a87dd53a87dc9fc622ce4ddbb05d3e9cf6a9"
    },
    {
        "T":"?",
        "Model":"moreh\/MoMo-70B-lora-1.8.4-DPO",
        "Average":74.24,
        "ARC":69.62,
        "HellaSwag":85.35,
        "MMLU":77.33,
        "TruthfulQA":64.64,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":72.29,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"a2c3a87dd53a87dc9fc622ce4ddbb05d3e9cf6a9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-70B-v1.6",
        "Average":74.24,
        "ARC":71.33,
        "HellaSwag":87.06,
        "MMLU":74.76,
        "TruthfulQA":63.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":19,
        "Available on the hub":true,
        "Model Sha":"031e9404b7a1467fdcc96bc109e05b640d573209"
    },
    {
        "T":"?",
        "Model":"louisbrulenaudet\/Pearl-34B-dare",
        "Average":74.23,
        "ARC":68.43,
        "HellaSwag":83.61,
        "MMLU":76.4,
        "TruthfulQA":68.5,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":34.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8c37fc9bad0de353a597b133a1570b556211c01b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"leveldevai\/TurdusBeagle-7B",
        "Average":74.23,
        "ARC":73.63,
        "HellaSwag":88.89,
        "MMLU":64.7,
        "TruthfulQA":69.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"bdd2c0aa848a559c6f55c51c0abd1f3cde683909"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/flammen8-mistral-7B",
        "Average":74.23,
        "ARC":71.93,
        "HellaSwag":88.06,
        "MMLU":64.92,
        "TruthfulQA":72.02,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9655aca675b8dcf0062257cf818c71592aad65d0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Smaug-2-72B",
        "Average":74.23,
        "ARC":67.92,
        "HellaSwag":86.37,
        "MMLU":77.73,
        "TruthfulQA":64.9,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":72.29,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"1454e0e3e2dea0db430612b9558852e3e5793021"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"CultriX\/MistralTrixTest",
        "Average":74.23,
        "ARC":72.53,
        "HellaSwag":88.4,
        "MMLU":65.22,
        "TruthfulQA":70.77,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":106,
        "Available on the hub":false,
        "Model Sha":"4e6a6b8022ce4b3b71b332c3389067613bd7f850"
    },
    {
        "T":"?",
        "Model":"Kquant03\/Azathoth-16x7B-bf16",
        "Average":74.22,
        "ARC":73.81,
        "HellaSwag":88.87,
        "MMLU":64.6,
        "TruthfulQA":69.61,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":91.8,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"452d46da45e058c4dcaf2c14216e9832bfb994dd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/multimaster-7b-v4",
        "Average":74.22,
        "ARC":72.53,
        "HellaSwag":88.77,
        "MMLU":64.85,
        "TruthfulQA":70.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":35.43,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a89b5a4ce482c531b1cb3b8703e8eb2b9321994c"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"upstage\/SOLAR-10.7B-Instruct-v1.0",
        "Average":74.22,
        "ARC":71.08,
        "HellaSwag":88.16,
        "MMLU":66.21,
        "TruthfulQA":71.43,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":555,
        "Available on the hub":true,
        "Model Sha":"d3167df97a44b8632538b32ee8cd887893ea1435"
    },
    {
        "T":"?",
        "Model":"dddsaty\/SOLAR-Instruct-ko-Adapter-Attach",
        "Average":74.22,
        "ARC":71.08,
        "HellaSwag":88.2,
        "MMLU":66.09,
        "TruthfulQA":71.51,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c2519bf48d73f5751cfecfe2c4c796fbcb73c390"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/UNA-SOLAR-10.7B-Instruct-v1.0",
        "Average":74.22,
        "ARC":70.56,
        "HellaSwag":88.18,
        "MMLU":66.08,
        "TruthfulQA":72.05,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":10.73,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"08d3f07da7160e9657630ba98531850905619def"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/WestSeverus-7B-DPO-v2",
        "Average":74.21,
        "ARC":71.42,
        "HellaSwag":88.27,
        "MMLU":64.79,
        "TruthfulQA":72.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"50dd207ce4319397d862a91f8295d902549dbdf7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vicgalle\/CarbonBeagle-11B",
        "Average":74.2,
        "ARC":71.84,
        "HellaSwag":88.93,
        "MMLU":66.62,
        "TruthfulQA":69.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"d774c746ac8f9df026d106f2466dbeeae3a49337"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"alnrg2arg\/blockchainlabs_7B_merged_test2_4",
        "Average":74.2,
        "ARC":73.55,
        "HellaSwag":88.87,
        "MMLU":64.63,
        "TruthfulQA":69.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"eb671cc5e88553f568c17eaf0e60d1616665ed95"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"alnrg2arg\/test2_4",
        "Average":74.2,
        "ARC":73.55,
        "HellaSwag":88.87,
        "MMLU":64.63,
        "TruthfulQA":69.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"ed17cf5af87733ffd7836ab99f27991544ba2547"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"alnrg2arg\/test3_sft_16bit",
        "Average":74.2,
        "ARC":73.55,
        "HellaSwag":88.87,
        "MMLU":64.63,
        "TruthfulQA":69.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a892e9a26785d59d8bf4ccef48606664c6cbc48b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/una-xaberius-34b-v1beta",
        "Average":74.19,
        "ARC":70.39,
        "HellaSwag":86.77,
        "MMLU":78.15,
        "TruthfulQA":61.45,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":34.39,
        "Hub":84,
        "Available on the hub":true,
        "Model Sha":"233b63015f389d0023cfa21727632b340cadbdb5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ConvexAI\/Seraphim-8x10.7B-bf16",
        "Average":74.19,
        "ARC":71.16,
        "HellaSwag":88.68,
        "MMLU":66.26,
        "TruthfulQA":70.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":69.92,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"24a09bb2a8addae43f82106d405b6dc39072759c"
    },
    {
        "T":"?",
        "Model":"tushar310\/Hippy-AAI-7B",
        "Average":74.19,
        "ARC":71.59,
        "HellaSwag":88.07,
        "MMLU":65.15,
        "TruthfulQA":71.95,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"798c990e93e7ec827037b806c52fbc30a7009894"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/CombinaTrix-7B",
        "Average":74.19,
        "ARC":72.87,
        "HellaSwag":88.4,
        "MMLU":64.85,
        "TruthfulQA":70.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"1962b09249cb27870cef33edde88872b088f7dc7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/OmniTrixAI",
        "Average":74.18,
        "ARC":72.95,
        "HellaSwag":88.52,
        "MMLU":65.12,
        "TruthfulQA":70.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"53c0bd452b1b4535d5c97bcf6405b4c3d3b260a1"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"eren23\/slerp-test-turdus-beagle",
        "Average":74.18,
        "ARC":73.55,
        "HellaSwag":88.85,
        "MMLU":64.62,
        "TruthfulQA":69.69,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"f2aef36538bb0c7aab30ffe889e12b72f51a6816"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC99\/Mistral-7B-privatemix-ia2",
        "Average":74.18,
        "ARC":72.27,
        "HellaSwag":88.59,
        "MMLU":64.53,
        "TruthfulQA":71.33,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7c9dada69222c0bc0366422b848ca4f74f8a0fdb"
    },
    {
        "T":"?",
        "Model":"allenai\/tulu-2-dpo-70b",
        "Average":74.17,
        "ARC":72.1,
        "HellaSwag":88.99,
        "MMLU":69.84,
        "TruthfulQA":65.78,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":68.98,
        "Hub":142,
        "Available on the hub":true,
        "Model Sha":"0ab5c875f0070d5aee8d36bc55f41de440a13f02"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/UNA-POLAR-10.7B-InstructMath-v2",
        "Average":74.17,
        "ARC":70.73,
        "HellaSwag":88.2,
        "MMLU":66.03,
        "TruthfulQA":71.73,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":10.73,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"b47d17b0df02e38e97f565784bb3cf948b29a6ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/NeoCortex-7B-slerp",
        "Average":74.17,
        "ARC":72.87,
        "HellaSwag":88.68,
        "MMLU":64.71,
        "TruthfulQA":70.43,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"71060067cb047f4684ba82d2f42ca2533e616e8b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TomGrc\/FusionNet_7Bx2_MoE_14B",
        "Average":74.17,
        "ARC":73.55,
        "HellaSwag":88.84,
        "MMLU":64.68,
        "TruthfulQA":69.6,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":12.88,
        "Hub":35,
        "Available on the hub":false,
        "Model Sha":"a619fd0fcbdfcc897054491c2f285677bee38a11"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jsfs11\/MixtureofMerges-MoE-v2",
        "Average":74.16,
        "ARC":72.44,
        "HellaSwag":88.41,
        "MMLU":64.88,
        "TruthfulQA":70.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"689f0901cfe49bd2b87c793997d24b77371891e9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ConvexAI\/Seraphim-8x10.7B-bf16",
        "Average":74.16,
        "ARC":70.99,
        "HellaSwag":88.72,
        "MMLU":66.16,
        "TruthfulQA":70.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":69.92,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"24a09bb2a8addae43f82106d405b6dc39072759c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zhengr\/MixTAO-7Bx2-MoE-Instruct-v2.0",
        "Average":74.15,
        "ARC":73.38,
        "HellaSwag":88.81,
        "MMLU":64.65,
        "TruthfulQA":69.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"d558d93b7f8bd8c5ca01f1d272f4a42f52b8d9ae"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/connate-7B-slerp",
        "Average":74.14,
        "ARC":72.1,
        "HellaSwag":88.37,
        "MMLU":64.96,
        "TruthfulQA":71.16,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3c9699e0096679a7a23749d59a561c9bdc4a8ff1"
    },
    {
        "T":"?",
        "Model":"Test157t\/Eris-Daturamix-7b",
        "Average":74.14,
        "ARC":72.78,
        "HellaSwag":88.23,
        "MMLU":64.52,
        "TruthfulQA":71.05,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"b41ce0efb1d6048fcaa257f00791c142f5d76093"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"CultriX\/MistralTrix-v1",
        "Average":74.14,
        "ARC":72.27,
        "HellaSwag":88.33,
        "MMLU":65.24,
        "TruthfulQA":70.73,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":106,
        "Available on the hub":false,
        "Model Sha":"e09045608b2d68a6412185817306f4bb0cf3530c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment10-7B",
        "Average":74.14,
        "ARC":72.18,
        "HellaSwag":87.96,
        "MMLU":65.32,
        "TruthfulQA":71.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b79854d1c29b5caae403c29d484f969b31734a5e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nfaheem\/Marcoroni-7b-DPO-Merge",
        "Average":74.14,
        "ARC":73.04,
        "HellaSwag":88.8,
        "MMLU":64.24,
        "TruthfulQA":70.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"e3085d8aacffbf46b95e263bde509fce70577a26"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ShinojiResearch\/Senku-70B-Full",
        "Average":74.13,
        "ARC":71.5,
        "HellaSwag":87.88,
        "MMLU":75.2,
        "TruthfulQA":61.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc0-1.0",
        "#Params (B)":68.98,
        "Hub":134,
        "Available on the hub":true,
        "Model Sha":"cf06159aaaadda2ca50b19ce547a52424f7d47c3"
    },
    {
        "T":"?",
        "Model":"core-3\/kuno-dogwalker-7b",
        "Average":74.13,
        "ARC":72.01,
        "HellaSwag":88.17,
        "MMLU":64.96,
        "TruthfulQA":71.39,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"65d8179cba89a9d4b28d943daea33a6ae2c2841f"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/complect-7B-slerp",
        "Average":74.12,
        "ARC":72.27,
        "HellaSwag":88.19,
        "MMLU":64.89,
        "TruthfulQA":71.14,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8ade5b4a439b511ccb00d89d51b5c273f90b3449"
    },
    {
        "T":"?",
        "Model":"BarryFutureman\/NeuralTurdusVariant1-7B",
        "Average":74.12,
        "ARC":73.12,
        "HellaSwag":88.61,
        "MMLU":64.75,
        "TruthfulQA":69.99,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"90bd7c1e38eef96488aa7fb19549f1cb53d1c696"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ICBU-NPU\/FashionGPT-70B-V1.2",
        "Average":74.11,
        "ARC":73.04,
        "HellaSwag":88.15,
        "MMLU":70.11,
        "TruthfulQA":65.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"990a1664fc058de6ee2406af62c0a817d7047304"
    },
    {
        "T":"?",
        "Model":"Kquant03\/Buttercup-V2-bf16",
        "Average":74.1,
        "ARC":73.72,
        "HellaSwag":88.54,
        "MMLU":64.68,
        "TruthfulQA":69.47,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"307a74cb147a9d93e20755b047e9b5be4293e017"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/A0127",
        "Average":74.1,
        "ARC":68.6,
        "HellaSwag":84.51,
        "MMLU":84.9,
        "TruthfulQA":58.38,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"68ea5286e3925a8dce3d791ab35dd978c1245ec7"
    },
    {
        "T":"?",
        "Model":"vanillaOVO\/merge_7B_state_1",
        "Average":74.09,
        "ARC":73.81,
        "HellaSwag":88.57,
        "MMLU":64.87,
        "TruthfulQA":69.11,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9eacf25cbbd3aa9dac99322a6cf9152cdeb3c6c4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC99\/Mistral-7B-privatemix-ia3",
        "Average":74.08,
        "ARC":73.38,
        "HellaSwag":88.69,
        "MMLU":64.14,
        "TruthfulQA":70.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ab0c1d60a77c2156b3b788984882a666bd6e1cfa"
    },
    {
        "T":"?",
        "Model":"core-3\/kuno-royale-v2-7b",
        "Average":74.08,
        "ARC":72.01,
        "HellaSwag":88.15,
        "MMLU":65.07,
        "TruthfulQA":71.1,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ff99dd167bfbb5dd3e5d74bb72e09a007f365541"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeverSleep\/MiquMaid-v1-70B",
        "Average":74.08,
        "ARC":71.67,
        "HellaSwag":87.96,
        "MMLU":74.9,
        "TruthfulQA":61.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"0dc1f9340fac9aadf883f52e6409e49e8d286af6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andrijdavid\/Macaroni-7b-Tied",
        "Average":74.07,
        "ARC":72.87,
        "HellaSwag":88.14,
        "MMLU":64.73,
        "TruthfulQA":70.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6323cf53ed75eab25ca37b3636a0f38ee8d1ac30"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ShinojiResearch\/Senku-70B-Full",
        "Average":74.07,
        "ARC":71.33,
        "HellaSwag":87.86,
        "MMLU":75.14,
        "TruthfulQA":61.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc0-1.0",
        "#Params (B)":68.98,
        "Hub":134,
        "Available on the hub":true,
        "Model Sha":"cf06159aaaadda2ca50b19ce547a52424f7d47c3"
    },
    {
        "T":"?",
        "Model":"Qwen\/Qwen1.5-72B-Chat",
        "Average":74.07,
        "ARC":68.52,
        "HellaSwag":86.42,
        "MMLU":77.44,
        "TruthfulQA":63.9,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":72.29,
        "Hub":172,
        "Available on the hub":false,
        "Model Sha":"1a6ccc1215278f962c794b1848c710c29ef4053d"
    },
    {
        "T":"?",
        "Model":"Riiid\/sheep-duck-llama-2-70b-v1.1",
        "Average":74.07,
        "ARC":73.04,
        "HellaSwag":87.81,
        "MMLU":70.84,
        "TruthfulQA":64.58,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"a8fc5c02c995733af6339ec882bef4ed93db1e8f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AIDC-ai-business\/Marcoroni-70B-v1",
        "Average":74.06,
        "ARC":73.55,
        "HellaSwag":87.62,
        "MMLU":70.67,
        "TruthfulQA":64.41,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":70.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"55a30d29db194832c0b5de1392a6598a63582144"
    },
    {
        "T":"?",
        "Model":"Eric111\/CatunaLaserPi-DPO",
        "Average":74.06,
        "ARC":72.95,
        "HellaSwag":88.33,
        "MMLU":64.95,
        "TruthfulQA":70.01,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"207cddf327154c23b484f1cbd972b3c7989b7554"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Helion-4x34B",
        "Average":74.06,
        "ARC":69.71,
        "HellaSwag":85.28,
        "MMLU":77.33,
        "TruthfulQA":63.91,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":113.66,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"583254a5a134243d7793b311c465da12b10a3ff2"
    },
    {
        "T":"?",
        "Model":"BarryFutureman\/ChatMarc-YesAnotherMerge-7B",
        "Average":74.05,
        "ARC":72.78,
        "HellaSwag":88.39,
        "MMLU":65.01,
        "TruthfulQA":70.04,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a1cf8fff75a44a3085ef0537cc11e833979b6017"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ICBU-NPU\/FashionGPT-70B-V1.1",
        "Average":74.05,
        "ARC":71.76,
        "HellaSwag":88.2,
        "MMLU":70.99,
        "TruthfulQA":65.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":42,
        "Available on the hub":true,
        "Model Sha":"05941a3eaacff0dead79b09d2175b5d7b98c525b"
    },
    {
        "T":"\u2b55",
        "Model":"Riiid\/sheep-duck-llama-2-70b-v1.1",
        "Average":74.05,
        "ARC":73.12,
        "HellaSwag":87.77,
        "MMLU":70.77,
        "TruthfulQA":64.55,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"978c3cc8d44ad37eb764a53e026ae1fa8d334eb2"
    },
    {
        "T":"?",
        "Model":"core-3\/kuno-royale-7b",
        "Average":74.05,
        "ARC":71.76,
        "HellaSwag":88.2,
        "MMLU":65.13,
        "TruthfulQA":71.12,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"450b2a9f247b2d8486c99b9c1f8777966ae2454c"
    },
    {
        "T":"?",
        "Model":"core-3\/kuno-royale-7B",
        "Average":74.05,
        "ARC":71.76,
        "HellaSwag":88.2,
        "MMLU":65.13,
        "TruthfulQA":71.12,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"450b2a9f247b2d8486c99b9c1f8777966ae2454c"
    },
    {
        "T":"?",
        "Model":"core-3\/kuno-dogpark-7b",
        "Average":74.05,
        "ARC":71.84,
        "HellaSwag":88.15,
        "MMLU":65.07,
        "TruthfulQA":71.14,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"962eef5f0f7116b41ed6542d8ffa15f4fb9c5147"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zhengr\/MixTAO-7Bx2-MoE-Instruct-v1.0",
        "Average":74.04,
        "ARC":74.06,
        "HellaSwag":88.25,
        "MMLU":64.25,
        "TruthfulQA":69.61,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"02e3cacbd9a9518289f6101fbcca8f7a875c1dfc"
    },
    {
        "T":"?",
        "Model":"rwitz2\/go-bruins-v2.1.1",
        "Average":74.04,
        "ARC":72.87,
        "HellaSwag":88.33,
        "MMLU":65.18,
        "TruthfulQA":69.8,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc",
        "#Params (B)":7.24,
        "Hub":22,
        "Available on the hub":false,
        "Model Sha":"bd56295eab54eaacbb3af6ecb88b9434d9966d4e"
    },
    {
        "T":"?",
        "Model":"core-3\/kuno-royale-v3-7b",
        "Average":74.04,
        "ARC":71.76,
        "HellaSwag":88.23,
        "MMLU":65.06,
        "TruthfulQA":71.13,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5e5425d13bef73009854548e9b59db1c2a9cba83"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BarryFutureman\/WestLakeX-7B-EvoMerge-Variant2",
        "Average":74.04,
        "ARC":72.53,
        "HellaSwag":88.52,
        "MMLU":64.77,
        "TruthfulQA":70.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2e36c528223443d6b8b5203b6a013e79f6d78d09"
    },
    {
        "T":"?",
        "Model":"jambroz\/sixtyoneeighty-FNCARL-7B-slerp",
        "Average":74.04,
        "ARC":71.59,
        "HellaSwag":87.78,
        "MMLU":65.27,
        "TruthfulQA":71.52,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8030132398d72adcb005802cf7800d253c7ab32e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"quantumaikr\/quantum-dpo-v0.1",
        "Average":74.03,
        "ARC":72.53,
        "HellaSwag":88.37,
        "MMLU":65.29,
        "TruthfulQA":69.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"09cbfe6569bcdddf623e9990498e9ad07345ad6a"
    },
    {
        "T":"\u2b55",
        "Model":"vicgalle\/NeuralBeagle-11B",
        "Average":74.02,
        "ARC":73.29,
        "HellaSwag":87.61,
        "MMLU":63.8,
        "TruthfulQA":71.36,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"ef081ad768776d8f6fe7b15b832c66fa6f86ffdc"
    },
    {
        "T":"?",
        "Model":"kaitchup\/Mayonnaise-4in1-01",
        "Average":74.02,
        "ARC":73.46,
        "HellaSwag":88.47,
        "MMLU":64.95,
        "TruthfulQA":69.18,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ab57f82bf8eb169be3560a44cc94653e024cedf4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/multimaster-7b-v5",
        "Average":74.01,
        "ARC":72.18,
        "HellaSwag":88.42,
        "MMLU":65.06,
        "TruthfulQA":70.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":35.43,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"95d031f0cad065bc18387f09ce37b256756f762f"
    },
    {
        "T":"?",
        "Model":"Qwen\/Qwen1.5-72B-Chat",
        "Average":74.01,
        "ARC":68.26,
        "HellaSwag":86.47,
        "MMLU":77.46,
        "TruthfulQA":63.84,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":72.29,
        "Hub":172,
        "Available on the hub":false,
        "Model Sha":"bc11a298a0c6a5cd737064db62c6ad20ec6331be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yhyu13\/LMCocktail-10.7B-v1",
        "Average":74.0,
        "ARC":70.65,
        "HellaSwag":88.13,
        "MMLU":66.21,
        "TruthfulQA":71.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":10.73,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"79ec3a42118f0715666b86bacab2688b62e1433b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yhyu13\/LMCocktail-10.7B-v1",
        "Average":74.0,
        "ARC":70.65,
        "HellaSwag":88.13,
        "MMLU":66.21,
        "TruthfulQA":71.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":10.73,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"79ec3a42118f0715666b86bacab2688b62e1433b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/NeuralExperiment-7b-MagicCoder-v7.5",
        "Average":74.0,
        "ARC":71.33,
        "HellaSwag":87.94,
        "MMLU":64.62,
        "TruthfulQA":72.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"43ea8d27d652dc15e4d27f665c5d636a5937780b"
    },
    {
        "T":"?",
        "Model":"kaitchup\/TheMayonnaise",
        "Average":74.0,
        "ARC":73.46,
        "HellaSwag":88.46,
        "MMLU":64.88,
        "TruthfulQA":69.19,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fb9c16c8878a5d688d0999e216f6fb0bb0b31ffe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"argilla\/notux-8x7b-v1",
        "Average":73.99,
        "ARC":70.65,
        "HellaSwag":87.72,
        "MMLU":71.39,
        "TruthfulQA":66.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":161,
        "Available on the hub":false,
        "Model Sha":"1f8562051647d5537dc950315e74534b363a0812"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Cosmosis-3x34B",
        "Average":73.99,
        "ARC":69.71,
        "HellaSwag":85.18,
        "MMLU":77.25,
        "TruthfulQA":63.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":87.24,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"644f20245c08dbbc6baad20100fcf0c8bd3181a0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment1-7B",
        "Average":73.99,
        "ARC":72.53,
        "HellaSwag":88.17,
        "MMLU":65.28,
        "TruthfulQA":69.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"aedfd66841e39a8db181d8549a42f4d2ee248b0a"
    },
    {
        "T":"?",
        "Model":"ozayezerceli\/Lowke-2x7B-v1",
        "Average":73.97,
        "ARC":71.5,
        "HellaSwag":87.3,
        "MMLU":64.4,
        "TruthfulQA":72.67,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"384d5117486aa7956943024cb2d821e1b9643cc2"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/flammen11-mistral-7B",
        "Average":73.96,
        "ARC":70.99,
        "HellaSwag":88.06,
        "MMLU":65.06,
        "TruthfulQA":71.73,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c32c932c9f61cef4452921c595ea20f067bbbdec"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"argilla\/notus-8x7b-experiment",
        "Average":73.96,
        "ARC":70.99,
        "HellaSwag":87.73,
        "MMLU":71.33,
        "TruthfulQA":65.79,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":161,
        "Available on the hub":false,
        "Model Sha":"86c89d182babd29521a41a54528e5bf8331ed4cd"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"argilla\/notux-8x7b-v1-epoch-2",
        "Average":73.96,
        "ARC":70.65,
        "HellaSwag":87.8,
        "MMLU":71.43,
        "TruthfulQA":65.97,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":161,
        "Available on the hub":false,
        "Model Sha":"bd3924498c3ae041334be5018cd912b6537a633c"
    },
    {
        "T":"?",
        "Model":"kaitchup\/Mayonnaise-4in1-02",
        "Average":73.96,
        "ARC":73.38,
        "HellaSwag":88.51,
        "MMLU":64.89,
        "TruthfulQA":69.04,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9a0c4751e8cf3b766d2cf55b70ec5eca5096c522"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment9-7B",
        "Average":73.95,
        "ARC":72.01,
        "HellaSwag":88.06,
        "MMLU":65.32,
        "TruthfulQA":70.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a3798e202aaa326b1027c0ee0a61ac78dc175e63"
    },
    {
        "T":"?",
        "Model":"eldogbbhed\/NeuralMonarchCoderPearlBeagle-T3Q-Mistral-Orca-Math-DPO-7b",
        "Average":73.95,
        "ARC":71.16,
        "HellaSwag":88.22,
        "MMLU":64.97,
        "TruthfulQA":71.45,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2c6ff7a8e0b026aa8c232a42bd77fea13bbeac73"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jsfs11\/TurdusTrixBeagle-DARETIES-7B",
        "Average":73.94,
        "ARC":73.46,
        "HellaSwag":88.61,
        "MMLU":64.89,
        "TruthfulQA":68.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f2f91c82dd2ad8f3c4514a83e793cfb4a59da323"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"mlabonne\/NeuralBeagle14-7B",
        "Average":73.94,
        "ARC":72.95,
        "HellaSwag":88.34,
        "MMLU":64.55,
        "TruthfulQA":69.93,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":146,
        "Available on the hub":false,
        "Model Sha":"33f76dd61715c8fd89f138092a8e8c7f3b3dd905"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ResplendentAI\/Flora_7B",
        "Average":73.94,
        "ARC":72.1,
        "HellaSwag":88.31,
        "MMLU":64.16,
        "TruthfulQA":71.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"9c95dafc63de0e98627458369e87347df87fa17d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment7-7B",
        "Average":73.93,
        "ARC":71.84,
        "HellaSwag":88.04,
        "MMLU":65.25,
        "TruthfulQA":70.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fff356f1e506e6801c5a60c165636e84a4bd302c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PSanni\/MPOMixtral-8x7B-Instruct-v0.1",
        "Average":73.93,
        "ARC":70.99,
        "HellaSwag":87.95,
        "MMLU":70.26,
        "TruthfulQA":66.52,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a4400d021e29279c8676d5c46cf76c4b36d748f6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment8-7B",
        "Average":73.93,
        "ARC":72.1,
        "HellaSwag":88.13,
        "MMLU":65.25,
        "TruthfulQA":70.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e17d301fb143b20ac943c99f34aa8b118f14e1e0"
    },
    {
        "T":"?",
        "Model":"jsfs11\/MoEv4Config-TestWeightedTIES-7b",
        "Average":73.93,
        "ARC":71.59,
        "HellaSwag":88.19,
        "MMLU":65.07,
        "TruthfulQA":70.87,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8a004ce7527c8abb6273df00cb5bcaa5a6aa2d65"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"222gate\/Blurred-Beagle-7b-slerp",
        "Average":73.93,
        "ARC":72.78,
        "HellaSwag":88.58,
        "MMLU":64.95,
        "TruthfulQA":69.39,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3e328ce1ef2b1216d7e3d03a7585531c6b1b9630"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment4-7B",
        "Average":73.92,
        "ARC":72.18,
        "HellaSwag":88.09,
        "MMLU":65.03,
        "TruthfulQA":70.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fa406117c67fc86cc8171f57b12184eecb8069be"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"222gate\/BrurryDog-7b-v0.1",
        "Average":73.92,
        "ARC":72.53,
        "HellaSwag":88.37,
        "MMLU":64.74,
        "TruthfulQA":70.05,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d3cac1bb6dfc362656320a881b4fc91d3974d6ed"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Astralis-4x34B",
        "Average":73.92,
        "ARC":69.71,
        "HellaSwag":85.17,
        "MMLU":77.24,
        "TruthfulQA":63.55,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":113.66,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"513311818a707ccc0c7d007ddabfab19e1a2e470"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"leveldevai\/MarcBeagle-7B",
        "Average":73.91,
        "ARC":73.12,
        "HellaSwag":88.43,
        "MMLU":64.92,
        "TruthfulQA":69.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9c742da68447832157389dad53be682e7d6c1d5f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jsfs11\/WildMBXMarconi-SLERP-7B",
        "Average":73.91,
        "ARC":73.29,
        "HellaSwag":88.49,
        "MMLU":64.9,
        "TruthfulQA":68.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"d50e66f9cfab5320ca4d3caad9e527254d923d90"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Cedaros\/Test-7B",
        "Average":73.91,
        "ARC":73.21,
        "HellaSwag":88.17,
        "MMLU":64.37,
        "TruthfulQA":69.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"b185bc8f6e30f2dc14e3e8c7f582cd19b4806f84"
    },
    {
        "T":"?",
        "Model":"louisbrulenaudet\/Pearl-7B-0211-ties",
        "Average":73.91,
        "ARC":71.42,
        "HellaSwag":88.86,
        "MMLU":63.91,
        "TruthfulQA":71.46,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"15db1e92e1683166a32da6f54c6ee6d6c10c20cb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"adonlee\/LLaMA_2_70B_LoRA",
        "Average":73.9,
        "ARC":72.7,
        "HellaSwag":87.55,
        "MMLU":70.84,
        "TruthfulQA":64.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":70.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"1af5aae768a840a3c1aae87d0ea8e6e5a6dc5628"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_11-7B-slerp",
        "Average":73.9,
        "ARC":72.53,
        "HellaSwag":88.2,
        "MMLU":65.04,
        "TruthfulQA":69.81,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f8f4ff2812125da6e7654a7afc28c547e087e268"
    },
    {
        "T":"?",
        "Model":"kevin009\/llamaRAGdrama",
        "Average":73.89,
        "ARC":72.01,
        "HellaSwag":88.83,
        "MMLU":64.5,
        "TruthfulQA":70.24,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"f1d27aab09086a6e691db6892d50ba809cbe0607"
    },
    {
        "T":"?",
        "Model":"ResplendentAI\/Datura_7B",
        "Average":73.89,
        "ARC":72.1,
        "HellaSwag":88.27,
        "MMLU":64.15,
        "TruthfulQA":71.03,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"750463cf0946dd46c4504b302757f2bb6e2b4521"
    },
    {
        "T":"?",
        "Model":"mayacinka\/yam-sam-7B",
        "Average":73.88,
        "ARC":70.9,
        "HellaSwag":87.92,
        "MMLU":65.39,
        "TruthfulQA":71.3,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c031f5e40b3e220c719e0430f63b6b11794084ae"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/Experiment2-7B",
        "Average":73.85,
        "ARC":72.18,
        "HellaSwag":88.15,
        "MMLU":65.1,
        "TruthfulQA":69.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"689dbca3e4bd977fa08b7a933e4e709277cd1394"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeuralNovel\/Pigris-7b-v0.3",
        "Average":73.85,
        "ARC":71.5,
        "HellaSwag":88.15,
        "MMLU":64.53,
        "TruthfulQA":71.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"0553c5ef30d6f85ef021ebb013f108fc87230f64"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Manolo26\/metis-chat-instruct-7b",
        "Average":73.85,
        "ARC":72.87,
        "HellaSwag":88.17,
        "MMLU":64.92,
        "TruthfulQA":69.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"23ea322a123211879153f48d61ff906cd6398bcc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kquant03\/Buttercup-V2-laser",
        "Average":73.84,
        "ARC":73.12,
        "HellaSwag":88.48,
        "MMLU":64.74,
        "TruthfulQA":69.0,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"b53163f2a7b562ce0191bdadd9d1f2e77a2b5a5e"
    },
    {
        "T":"?",
        "Model":"vanillaOVO\/Beagle_Turdus",
        "Average":73.83,
        "ARC":73.63,
        "HellaSwag":88.82,
        "MMLU":64.62,
        "TruthfulQA":68.27,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"05216cc9edb1c697ee82d1343300874392f7ed69"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abideen\/MonarchCoder-MoE-2x7B",
        "Average":73.83,
        "ARC":70.99,
        "HellaSwag":87.99,
        "MMLU":65.11,
        "TruthfulQA":71.25,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1d71eacdbfa5d4fe546bcc57d40e642dbac57cb7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"222gate\/Blurdus-7b-v0.1",
        "Average":73.83,
        "ARC":72.27,
        "HellaSwag":88.5,
        "MMLU":64.82,
        "TruthfulQA":69.72,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"339e5802231bda900d71d8d04db88021d1dd8903"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"VAGOsolutions\/SauerkrautLM-Mixtral-8x7B-Instruct",
        "Average":73.83,
        "ARC":70.48,
        "HellaSwag":87.75,
        "MMLU":71.37,
        "TruthfulQA":65.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":19,
        "Available on the hub":false,
        "Model Sha":"330eb185920d6a470b265a4b31217c60e810fb3e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GreenNode\/GreenNodeLM-v3olet-7B",
        "Average":73.83,
        "ARC":72.27,
        "HellaSwag":88.25,
        "MMLU":65.27,
        "TruthfulQA":69.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"94b36a4573657d7815f55b917b204e6b73f7a634"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cookinai\/OrcaHermes-Mistral-70B-miqu",
        "Average":73.82,
        "ARC":71.33,
        "HellaSwag":87.78,
        "MMLU":75.47,
        "TruthfulQA":60.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"d8b56411b045767511593f901179c01855e9d7e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"flemmingmiguel\/MBX-7B",
        "Average":73.82,
        "ARC":72.87,
        "HellaSwag":88.38,
        "MMLU":64.93,
        "TruthfulQA":69.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"2270125929da3aa44594f7d0f82ac142cbdc38c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rishiraj\/meow",
        "Average":73.82,
        "ARC":70.48,
        "HellaSwag":88.08,
        "MMLU":66.25,
        "TruthfulQA":70.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"d933dcd7cbb19916f4732ae7e3892a656a8c3d27"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"quantumaikr\/quantum-trinity-v0.1",
        "Average":73.82,
        "ARC":72.53,
        "HellaSwag":88.28,
        "MMLU":65.19,
        "TruthfulQA":69.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"4e3eb8c21ff1689a348cc9ffdacd675aff3dde2b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"quantumaikr\/quantum-v0.01",
        "Average":73.82,
        "ARC":72.53,
        "HellaSwag":88.27,
        "MMLU":65.2,
        "TruthfulQA":69.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"141a76559dace99bea213922c91cd23be8783c72"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/Patronum-7B",
        "Average":73.81,
        "ARC":71.67,
        "HellaSwag":88.33,
        "MMLU":64.84,
        "TruthfulQA":70.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"78da696445e50002d29bf5610af059fd3f00f51b"
    },
    {
        "T":"?",
        "Model":"ResplendentAI\/Flora_DPO_7B",
        "Average":73.81,
        "ARC":71.76,
        "HellaSwag":88.28,
        "MMLU":64.13,
        "TruthfulQA":71.08,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"0d04b46ec6ce4c707bcdebb94b98e30fe8f4ae1d"
    },
    {
        "T":"?",
        "Model":"vanillaOVO\/supermario_v1",
        "Average":73.81,
        "ARC":73.72,
        "HellaSwag":88.71,
        "MMLU":64.57,
        "TruthfulQA":68.23,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2329946fcdd20174c997dcd8feb8f45bedc52675"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/SevereNeuralBeagleTrix-7B",
        "Average":73.81,
        "ARC":72.78,
        "HellaSwag":88.33,
        "MMLU":65.09,
        "TruthfulQA":69.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1593a524ec3a8be887da0569cf1a2081071f67ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uni-tianyan\/Uni-TianYan",
        "Average":73.81,
        "ARC":72.1,
        "HellaSwag":87.4,
        "MMLU":69.91,
        "TruthfulQA":65.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":34,
        "Available on the hub":true,
        "Model Sha":"46b78b9a10e78283e59c28b56cb59c2f33b0816a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/Cygnus-7B",
        "Average":73.79,
        "ARC":70.9,
        "HellaSwag":87.82,
        "MMLU":63.81,
        "TruthfulQA":72.61,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"12e91d9302ecdd09d37d13da79b5761727b20eb8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jan-hq\/trinity-v1",
        "Average":73.79,
        "ARC":72.27,
        "HellaSwag":88.36,
        "MMLU":65.2,
        "TruthfulQA":69.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"34974ae99668c381be0871778e3c42958544f70e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"janai-hq\/trinity-v1",
        "Average":73.79,
        "ARC":72.27,
        "HellaSwag":88.36,
        "MMLU":65.2,
        "TruthfulQA":69.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"09da1a24f84c96b8c09f2c07038986e28cc24ad5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/Neural4gsm8k",
        "Average":73.78,
        "ARC":72.27,
        "HellaSwag":88.45,
        "MMLU":64.76,
        "TruthfulQA":69.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"be0bb5f5203cb447f4c64a4399213ce89e8b3d3e"
    },
    {
        "T":"?",
        "Model":"InferenceIllusionist\/Excalibur-7b-DPO",
        "Average":73.78,
        "ARC":70.9,
        "HellaSwag":87.93,
        "MMLU":65.46,
        "TruthfulQA":70.82,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"f2a7ecb1f539bb41a61c254150e404820851005f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"VAGOsolutions\/SauerkrautLM-Mixtral-8x7B-Instruct",
        "Average":73.78,
        "ARC":70.56,
        "HellaSwag":87.74,
        "MMLU":71.08,
        "TruthfulQA":65.72,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":19,
        "Available on the hub":false,
        "Model Sha":"330eb185920d6a470b265a4b31217c60e810fb3e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ignos\/LeoScorpius-GreenNode-Alpaca-7B-v1",
        "Average":73.77,
        "ARC":72.35,
        "HellaSwag":88.16,
        "MMLU":65.23,
        "TruthfulQA":69.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"00827d42d79b7e10ddfc92c800cbb0636704e379"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/megamarcoroni-120b",
        "Average":73.77,
        "ARC":72.01,
        "HellaSwag":88.94,
        "MMLU":69.88,
        "TruthfulQA":64.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":120.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"db2d5376b1a1c36efaca83668e1ce6bfcc43356a"
    },
    {
        "T":"?",
        "Model":"AbacusResearch\/jaLLAbi2-7b",
        "Average":73.76,
        "ARC":71.67,
        "HellaSwag":88.29,
        "MMLU":64.92,
        "TruthfulQA":70.16,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e51c2fd0e60ca0c20a8d9094a878f98c7880967a"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_10-7B-slerp",
        "Average":73.75,
        "ARC":72.35,
        "HellaSwag":88.3,
        "MMLU":64.87,
        "TruthfulQA":69.49,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"60a299cdcaa10a275cc79b52598b096cf1a2ad2e"
    },
    {
        "T":"?",
        "Model":"ConvexAI\/BurningBruce-004",
        "Average":73.75,
        "ARC":73.29,
        "HellaSwag":88.63,
        "MMLU":64.68,
        "TruthfulQA":68.39,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"7a447745915fb8ede249d92a7b5f271409056ce2"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"udkai\/Garrulus",
        "Average":73.74,
        "ARC":73.29,
        "HellaSwag":88.87,
        "MMLU":64.57,
        "TruthfulQA":68.23,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"cd2fa5c2188588b903fff2070a389db3b24031a4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"leveldevai\/TurdusDareBeagle-7B",
        "Average":73.73,
        "ARC":72.7,
        "HellaSwag":88.45,
        "MMLU":64.87,
        "TruthfulQA":68.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1ffd5776337bdf6fae0b12645112e981a6bfa914"
    },
    {
        "T":"?",
        "Model":"eric111\/CatunaMayo",
        "Average":73.71,
        "ARC":71.76,
        "HellaSwag":87.9,
        "MMLU":65.21,
        "TruthfulQA":69.96,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9dc038fac8b37aac504e851c311bede4092afafd"
    },
    {
        "T":"?",
        "Model":"Eric111\/CatunaMayo",
        "Average":73.71,
        "ARC":71.76,
        "HellaSwag":87.9,
        "MMLU":65.21,
        "TruthfulQA":69.96,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9dc038fac8b37aac504e851c311bede4092afafd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jefferylovely\/ThetaMaven5",
        "Average":73.71,
        "ARC":72.01,
        "HellaSwag":88.38,
        "MMLU":64.77,
        "TruthfulQA":69.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a344b7d30e8d4afc55b1326f5fb71ca03a76b7a5"
    },
    {
        "T":"?",
        "Model":"RatanRohith\/NeuralPizza-WestSeverus-7B-Merge-slerp",
        "Average":73.7,
        "ARC":71.42,
        "HellaSwag":88.25,
        "MMLU":64.74,
        "TruthfulQA":70.4,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1b1e5870d1f08eb09c9891d1737a105cdb5aa52c"
    },
    {
        "T":"?",
        "Model":"kaitchup\/Mayonnaise-4in1-03",
        "Average":73.7,
        "ARC":72.95,
        "HellaSwag":88.29,
        "MMLU":64.76,
        "TruthfulQA":68.79,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3cc3f5e623c2451e040e0d3e137d4f2212708936"
    },
    {
        "T":"?",
        "Model":"ChaoticNeutrals\/Eris_Remix_7B",
        "Average":73.69,
        "ARC":72.35,
        "HellaSwag":88.04,
        "MMLU":65.26,
        "TruthfulQA":69.12,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"db7c693c872acfbf2244373d671745cc0d19e6e5"
    },
    {
        "T":"\u2b55",
        "Model":"Riiid\/sheep-duck-llama-2",
        "Average":73.69,
        "ARC":72.35,
        "HellaSwag":87.78,
        "MMLU":70.82,
        "TruthfulQA":63.8,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"e196dd0fe1d604c4975d972b177b09e4f1572cd5"
    },
    {
        "T":"?",
        "Model":"ChaoticNeutrals\/Eris_Remix_DPO_7B",
        "Average":73.67,
        "ARC":72.44,
        "HellaSwag":88.03,
        "MMLU":65.29,
        "TruthfulQA":68.92,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"4c09542d0154eb09bf7be874e2c68189407114ed"
    },
    {
        "T":"?",
        "Model":"macadeliccc\/Laser-WestLake-2x7b",
        "Average":73.67,
        "ARC":72.27,
        "HellaSwag":88.44,
        "MMLU":64.71,
        "TruthfulQA":69.25,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"0fa0bee4e763f5d9c12d414bc7e3e22a1f7f4981"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/UNAversal-2x7B-v1",
        "Average":73.67,
        "ARC":73.38,
        "HellaSwag":87.87,
        "MMLU":63.49,
        "TruthfulQA":69.93,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"514783cefac2b142adb50ee5f61dd724d62910cf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Riiid\/sheep-duck-llama-2",
        "Average":73.67,
        "ARC":72.27,
        "HellaSwag":87.78,
        "MMLU":70.81,
        "TruthfulQA":63.8,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"e196dd0fe1d604c4975d972b177b09e4f1572cd5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kquant03\/Prokaryote-8x7B-bf16",
        "Average":73.66,
        "ARC":73.72,
        "HellaSwag":88.18,
        "MMLU":64.97,
        "TruthfulQA":67.79,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"20496cb52b98e33cf4442c14cf464fcf7c4b27c1"
    },
    {
        "T":"?",
        "Model":"cognitivecomputations\/mixtral-instruct-0.1-laser",
        "Average":73.66,
        "ARC":70.48,
        "HellaSwag":87.28,
        "MMLU":71.07,
        "TruthfulQA":65.83,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"5dbc14842c16f1fa315e682e7e5bdb0248a2b05e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zhengr\/MixTAO-7Bx2-MoE-Instruct-v4.0",
        "Average":73.66,
        "ARC":73.04,
        "HellaSwag":88.79,
        "MMLU":64.67,
        "TruthfulQA":68.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"8b848fc487ec7d7d7b181400c960147af4e12b52"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andrijdavid\/macaroni-7b",
        "Average":73.66,
        "ARC":73.12,
        "HellaSwag":88.17,
        "MMLU":64.58,
        "TruthfulQA":68.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e1c0fe26554eb627aed9569f106e838f0333850f"
    },
    {
        "T":"?",
        "Model":"BarryFutureman\/NeuralLake-Variant1-7B",
        "Average":73.65,
        "ARC":73.12,
        "HellaSwag":88.45,
        "MMLU":64.67,
        "TruthfulQA":68.37,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1a243c41d78eae644a0246ce7eb3bef68c10fecf"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/iWillChangeTheNameLater",
        "Average":73.65,
        "ARC":72.01,
        "HellaSwag":88.23,
        "MMLU":64.97,
        "TruthfulQA":69.41,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f997d63d5fdcf5aa69ec7ceedfc2a5c2572a14ef"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eren23\/FrankenBeagle-SmallOverlap-test",
        "Average":73.64,
        "ARC":72.01,
        "HellaSwag":88.16,
        "MMLU":64.71,
        "TruthfulQA":69.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.55,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c4adb38943819daae3dc92af41801c6e97c09805"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"alnrg2arg\/test2_3",
        "Average":73.64,
        "ARC":72.95,
        "HellaSwag":88.42,
        "MMLU":64.8,
        "TruthfulQA":68.4,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e2c681fa4680ee19ca9758a2289da7d168546672"
    },
    {
        "T":"?",
        "Model":"seyf1elislam\/KuTrix-7b",
        "Average":73.64,
        "ARC":70.48,
        "HellaSwag":87.94,
        "MMLU":65.28,
        "TruthfulQA":70.85,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"37995fab81810aacdf8fa7db73c41c4673dd4794"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TomGrc\/FusionNet_SOLAR",
        "Average":73.62,
        "ARC":71.59,
        "HellaSwag":88.4,
        "MMLU":65.29,
        "TruthfulQA":69.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":15.97,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"3757984c0edebf4300a67cf33b9cca53524a057d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/Beagle14-7B",
        "Average":73.62,
        "ARC":72.95,
        "HellaSwag":87.95,
        "MMLU":64.7,
        "TruthfulQA":68.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"a5d1b1f831efe38df3b6ac125764a87ed094e282"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eren23\/NeuralDareBeagle-7B-slerp",
        "Average":73.62,
        "ARC":72.1,
        "HellaSwag":88.2,
        "MMLU":64.99,
        "TruthfulQA":69.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"04c39204799094776e57195cd107f0fe92bf86bd"
    },
    {
        "T":"?",
        "Model":"BarryFutureman\/WildWest-Variant3-7B",
        "Average":73.61,
        "ARC":73.21,
        "HellaSwag":88.37,
        "MMLU":64.76,
        "TruthfulQA":68.09,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"58b9421546f15564f6a918ebeb9627979dfdb50b"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"rwitz2\/go-bruins-v2.1",
        "Average":73.6,
        "ARC":71.93,
        "HellaSwag":88.33,
        "MMLU":65.0,
        "TruthfulQA":69.16,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1e785d545369d201262bcc740ff127bb120d7a6b"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/flammen11X-mistral-7B",
        "Average":73.6,
        "ARC":71.16,
        "HellaSwag":88.23,
        "MMLU":64.81,
        "TruthfulQA":70.18,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ca3e210eea6e810365b9fafe4477c065a363dd40"
    },
    {
        "T":"?",
        "Model":"abacusai\/MetaMath-Bagel-DPO-34B",
        "Average":73.6,
        "ARC":68.17,
        "HellaSwag":84.23,
        "MMLU":76.54,
        "TruthfulQA":65.44,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":34.39,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"0c670c988b61240e5f89ae9df0820db7dc572576"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"fblgit\/UNA-TheBeagle-7b-v1",
        "Average":73.59,
        "ARC":73.04,
        "HellaSwag":88.0,
        "MMLU":63.48,
        "TruthfulQA":69.85,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":7.24,
        "Hub":31,
        "Available on the hub":false,
        "Model Sha":"72084679bda2e7679259e9c0fa2fdcd48ecb158c"
    },
    {
        "T":"?",
        "Model":"gradientai\/v-alpha-tross",
        "Average":73.58,
        "ARC":71.93,
        "HellaSwag":86.82,
        "MMLU":70.38,
        "TruthfulQA":65.21,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"6188e34517a82298b0216c141ec728a5d9861658"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gradientai\/v-alpha-tross",
        "Average":73.58,
        "ARC":71.84,
        "HellaSwag":86.84,
        "MMLU":70.44,
        "TruthfulQA":65.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"2d5b9af81408ebc5e45c944cc24c9bab85b7ae1f"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/flammen3",
        "Average":73.58,
        "ARC":70.82,
        "HellaSwag":87.98,
        "MMLU":64.81,
        "TruthfulQA":70.69,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"37a181a04dcedf8402a5246b4189c88b2096323d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"222gate\/Blur-4x7b-MOE-v0.1",
        "Average":73.57,
        "ARC":72.27,
        "HellaSwag":88.14,
        "MMLU":65.05,
        "TruthfulQA":68.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1690def0c82469870a8b0b649eea948f8940151b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/una-cybertron-7b-v3-OMA",
        "Average":73.57,
        "ARC":73.04,
        "HellaSwag":87.94,
        "MMLU":63.44,
        "TruthfulQA":69.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"29c9ff0a9f5daa5adc797a34508bcca50205f34f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PetroGPT\/WestSeverus-7B-DPO",
        "Average":73.55,
        "ARC":70.73,
        "HellaSwag":88.01,
        "MMLU":64.93,
        "TruthfulQA":70.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"e13a48ef1524ba35615d7f63834e7c9192fa1836"
    },
    {
        "T":"?",
        "Model":"ChaoticNeutrals\/Eris_PrimeV3-Vision-7B",
        "Average":73.54,
        "ARC":70.65,
        "HellaSwag":87.87,
        "MMLU":65.32,
        "TruthfulQA":70.32,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"2a48395bb8bdeac6cf812fe51746c436c558039b"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"GreenNode\/GreenNodeLM-7B-v4leo",
        "Average":73.54,
        "ARC":71.25,
        "HellaSwag":88.24,
        "MMLU":65.01,
        "TruthfulQA":69.65,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"9286f6fac1df497203e110070322c93dab33fdd2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"moreh\/MoMo-72B-LoRA-V1.4",
        "Average":73.52,
        "ARC":69.11,
        "HellaSwag":85.0,
        "MMLU":77.26,
        "TruthfulQA":62.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":72.29,
        "Hub":86,
        "Available on the hub":true,
        "Model Sha":"e5dd511955f4ac65bb1884f07426157740ad8574"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mistralai\/Mixtral-8x7B-Instruct-v0.1",
        "Average":73.52,
        "ARC":70.14,
        "HellaSwag":87.55,
        "MMLU":71.4,
        "TruthfulQA":64.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":3543,
        "Available on the hub":false,
        "Model Sha":"125c431e2ff41a156b9f9076f744d2f35dd6e67a"
    },
    {
        "T":"?",
        "Model":"moreh\/MoMo-70B-LoRA-V1.4",
        "Average":73.51,
        "ARC":69.2,
        "HellaSwag":85.07,
        "MMLU":77.12,
        "TruthfulQA":62.66,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":72.29,
        "Hub":86,
        "Available on the hub":true,
        "Model Sha":"66bf25995056155b5d0796f7c0981e243bdd48f3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"moreh\/MoMo-72B-LoRA-V1.4",
        "Average":73.51,
        "ARC":69.2,
        "HellaSwag":85.07,
        "MMLU":77.12,
        "TruthfulQA":62.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":72.29,
        "Hub":86,
        "Available on the hub":true,
        "Model Sha":"66bf25995056155b5d0796f7c0981e243bdd48f3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"tenyx\/TenyxChat-8x7B-v1",
        "Average":73.5,
        "ARC":69.71,
        "HellaSwag":87.76,
        "MMLU":71.12,
        "TruthfulQA":65.42,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"86fd0b7d132126be49c02e061ebec02e1d3a4e38"
    },
    {
        "T":"?",
        "Model":"louisbrulenaudet\/Pearl-7B-0210-ties",
        "Average":73.5,
        "ARC":71.08,
        "HellaSwag":88.63,
        "MMLU":63.81,
        "TruthfulQA":70.47,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d18d0fe9d70b8a2f4e2af33b6e771c8edef6ff97"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"mncai\/mistral-7b-dpo-merge-v1.1",
        "Average":73.5,
        "ARC":72.53,
        "HellaSwag":88.15,
        "MMLU":64.83,
        "TruthfulQA":68.48,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7fc6c09477cc606e91025c38b9963bc47dd396da"
    },
    {
        "T":"?",
        "Model":"Aratako\/Beyonder-4x7B-random-lora",
        "Average":73.48,
        "ARC":71.25,
        "HellaSwag":87.4,
        "MMLU":64.78,
        "TruthfulQA":70.49,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":24.15,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8f080b4c72bd00b2254f30e260cafa2f65096243"
    },
    {
        "T":"?",
        "Model":"sethuiyer\/Eida_10.7B",
        "Average":73.48,
        "ARC":70.9,
        "HellaSwag":87.36,
        "MMLU":64.3,
        "TruthfulQA":71.33,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.7,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"9cc692ef0d0821ef113ad175141632d2efad4b33"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/TheTop-5x7B-Instruct-S5-v0.1",
        "Average":73.46,
        "ARC":72.53,
        "HellaSwag":88.71,
        "MMLU":65.01,
        "TruthfulQA":67.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"538565474e9cf94b3ab4cd0b74a3537a338831f5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zyh3826\/GML-Mistral-merged-v1",
        "Average":73.45,
        "ARC":71.25,
        "HellaSwag":87.88,
        "MMLU":65.42,
        "TruthfulQA":69.28,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"17a3d5eb5dc23b8a7c29d33cfcd07140a083aa1f"
    },
    {
        "T":"?",
        "Model":"jan-ai\/Pandora-10.7B-v1",
        "Average":73.44,
        "ARC":71.08,
        "HellaSwag":87.06,
        "MMLU":64.95,
        "TruthfulQA":70.67,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0e06af9adc32a44f307f96c387b4e803a1868291"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shadowml\/DareBeagel-2x7B",
        "Average":73.43,
        "ARC":72.01,
        "HellaSwag":88.12,
        "MMLU":64.51,
        "TruthfulQA":69.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"5cecd5e1f9723e3f7d287cbc9fd6d42056f73405"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cognitivecomputations\/WestLake-7B-v2-laser",
        "Average":73.43,
        "ARC":73.29,
        "HellaSwag":88.66,
        "MMLU":64.72,
        "TruthfulQA":67.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":33,
        "Available on the hub":false,
        "Model Sha":"c3227c2b48ac6b136c074871b72088677f2adca9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"senseable\/Westlake-7B",
        "Average":73.43,
        "ARC":73.21,
        "HellaSwag":88.49,
        "MMLU":64.64,
        "TruthfulQA":67.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"645fa936256811f53f0c33f1e5298f6ad1095dce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shadowml\/DareBeagle-7B",
        "Average":73.42,
        "ARC":71.67,
        "HellaSwag":88.01,
        "MMLU":65.03,
        "TruthfulQA":68.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7d5cb3c9ef547ad297d64789b188415e0320237a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"The-Face-Of-Goonery\/HuginnV5.5-12.6B",
        "Average":73.41,
        "ARC":72.01,
        "HellaSwag":86.7,
        "MMLU":64.5,
        "TruthfulQA":70.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":12.91,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"9cb2d09228ac87d761d23a1284c79b55f9f285d9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fangloveskari\/ORCA_LLaMA_70B_QLoRA",
        "Average":73.4,
        "ARC":72.27,
        "HellaSwag":87.74,
        "MMLU":70.23,
        "TruthfulQA":63.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":37,
        "Available on the hub":true,
        "Model Sha":"ef9b04ef02ccc4d96f1181467da92bb6b5baf835"
    },
    {
        "T":"?",
        "Model":"Vasanth\/Beast-Soul-new",
        "Average":73.4,
        "ARC":73.12,
        "HellaSwag":88.35,
        "MMLU":64.74,
        "TruthfulQA":67.38,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d4a6d05f84f82b0a6ad625dd2473115ca972c1db"
    },
    {
        "T":"\u2b55",
        "Model":"mistralai\/Mixtral-8x7B-Instruct-v0.1",
        "Average":73.4,
        "ARC":70.22,
        "HellaSwag":87.63,
        "MMLU":71.16,
        "TruthfulQA":64.58,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":3543,
        "Available on the hub":false,
        "Model Sha":"3de0408ae8b591d9ac516a2384925dd98ebc66f4"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"udkai\/Turdus",
        "Average":73.39,
        "ARC":73.38,
        "HellaSwag":88.56,
        "MMLU":64.52,
        "TruthfulQA":67.11,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"de8a9fbacf60f07146d7bda3455d3748e12200de"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"mncai\/mistral-7b-dpo-v6",
        "Average":73.39,
        "ARC":72.53,
        "HellaSwag":88.1,
        "MMLU":64.68,
        "TruthfulQA":68.24,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"206be3fd589dd62817343c53525ab7fb1b752faf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"leveldevai\/MarcDareBeagle-7B",
        "Average":73.39,
        "ARC":72.1,
        "HellaSwag":88.33,
        "MMLU":65.03,
        "TruthfulQA":68.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1c7d1421fad812bfcef4d3374f28bbca83e63ca6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"flemmingmiguel\/MDBX-7B",
        "Average":73.37,
        "ARC":72.01,
        "HellaSwag":88.31,
        "MMLU":64.97,
        "TruthfulQA":68.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"668b959981253f45ba25e6cb21289e136844f859"
    },
    {
        "T":"?",
        "Model":"llmixer\/BigWeave-v15-103b",
        "Average":73.37,
        "ARC":69.71,
        "HellaSwag":86.41,
        "MMLU":71.25,
        "TruthfulQA":66.1,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"unknown",
        "#Params (B)":103.2,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"59004f5610548e626ad27cd4a7b92daa3ccfc9c8"
    },
    {
        "T":"?",
        "Model":"ChaoticNeutrals\/Eris-Lelanacles-7b",
        "Average":73.37,
        "ARC":71.67,
        "HellaSwag":87.91,
        "MMLU":64.9,
        "TruthfulQA":68.97,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"c6a5ffb5b3ad65895301c3aec4f34f71cd6d0a90"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"senseable\/WestLake-7B-v2",
        "Average":73.36,
        "ARC":73.04,
        "HellaSwag":88.65,
        "MMLU":64.71,
        "TruthfulQA":67.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":85,
        "Available on the hub":false,
        "Model Sha":"6df7bb2069432bcab0971ab105284a66b3ec1ce0"
    },
    {
        "T":"?",
        "Model":"eldogbbhed\/Peagle-9b",
        "Average":73.34,
        "ARC":71.5,
        "HellaSwag":87.34,
        "MMLU":64.36,
        "TruthfulQA":70.16,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":8.99,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3367f742a38a3d7a44594088d8805e781fd34136"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xDAN2099\/xDAN-L2-moe-2x-v1",
        "Average":73.34,
        "ARC":68.52,
        "HellaSwag":86.31,
        "MMLU":76.76,
        "TruthfulQA":61.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":0.0,
        "Hub":59,
        "Available on the hub":false,
        "Model Sha":"0a137b01142b62fccfcbc81176d40f4b86405958"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eren23\/Experiment26-12B",
        "Average":73.33,
        "ARC":68.86,
        "HellaSwag":88.59,
        "MMLU":63.75,
        "TruthfulQA":72.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.48,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"526040a085e68118e8ccea113c0776dcb0779f96"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051615\/A0221",
        "Average":73.31,
        "ARC":68.52,
        "HellaSwag":85.13,
        "MMLU":84.48,
        "TruthfulQA":55.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5faa6fd16b30b975ec7d18dea9203c186511d910"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CultriX\/MergeTrix-7B-v2",
        "Average":73.31,
        "ARC":72.7,
        "HellaSwag":88.48,
        "MMLU":64.89,
        "TruthfulQA":67.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"fabb95cdb3ed48cc58ab6fdc2b460640022665f7"
    },
    {
        "T":"?",
        "Model":"vicgalle\/OpenBeagle-11B",
        "Average":73.3,
        "ARC":70.48,
        "HellaSwag":88.76,
        "MMLU":66.94,
        "TruthfulQA":67.01,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"b59ed47a8f30e7488f1faef65ff0a75597af0a44"
    },
    {
        "T":"?",
        "Model":"Steelskull\/Lumosia-v2-MoE-4x10.7",
        "Average":73.3,
        "ARC":70.39,
        "HellaSwag":87.87,
        "MMLU":66.45,
        "TruthfulQA":68.48,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":36.1,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"479d3907a5bce4f3edb476d3ae05fe4b38a0a6e4"
    },
    {
        "T":"?",
        "Model":"ChaoticNeutrals\/Prodigy_7B",
        "Average":73.29,
        "ARC":71.59,
        "HellaSwag":88.09,
        "MMLU":64.92,
        "TruthfulQA":68.57,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"11b85a26a0d04abd1282cf10fdadf2faefa93ee4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/Calme-7B-Instruct-v0.4",
        "Average":73.28,
        "ARC":70.73,
        "HellaSwag":87.75,
        "MMLU":64.4,
        "TruthfulQA":70.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"448255ff2397e04c62ecba4c4d982531eb42d241"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"flemmingmiguel\/DareBeagle-7B",
        "Average":73.27,
        "ARC":71.59,
        "HellaSwag":87.98,
        "MMLU":65.21,
        "TruthfulQA":68.3,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"53e5b634de4ae9ef8a127c1d7a0c543acfba1b47"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051611\/V0201",
        "Average":73.27,
        "ARC":67.24,
        "HellaSwag":83.3,
        "MMLU":88.78,
        "TruthfulQA":53.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d49bbca7c6d24e025a2e1175b29ad9fb955e0680"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dillfrescott\/trinity-medium",
        "Average":73.27,
        "ARC":71.5,
        "HellaSwag":86.99,
        "MMLU":65.04,
        "TruthfulQA":69.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e0d20c61e1bcd8e305da40e20219edf7649d2952"
    },
    {
        "T":"?",
        "Model":"sophosympatheia\/Midnight-Rose-70B-v2.0.3",
        "Average":73.27,
        "ARC":70.65,
        "HellaSwag":87.5,
        "MMLU":69.64,
        "TruthfulQA":65.27,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"fcfcf5539655820679ce0f952cfb07466f3b1ec1"
    },
    {
        "T":"?",
        "Model":"ZoidBB\/MultiKory-0.1-4x11b-pre1",
        "Average":73.26,
        "ARC":72.87,
        "HellaSwag":87.9,
        "MMLU":64.6,
        "TruthfulQA":67.67,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"894dacf37534e90519b4f29ce618922e19adf934"
    },
    {
        "T":"?",
        "Model":"ZoidBB\/Kory-0.1-11b-pre1",
        "Average":73.26,
        "ARC":72.87,
        "HellaSwag":87.9,
        "MMLU":64.59,
        "TruthfulQA":67.68,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e485b974a73b0b280d974713392e90afd9e51e38"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ICBU-NPU\/FashionGPT-70B-V1",
        "Average":73.26,
        "ARC":71.08,
        "HellaSwag":87.32,
        "MMLU":70.7,
        "TruthfulQA":63.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"060c096af49700760f734c0102250a524d46b3eb"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/WhyAreWeStillHere-7B-slerp",
        "Average":73.24,
        "ARC":71.67,
        "HellaSwag":88.25,
        "MMLU":64.92,
        "TruthfulQA":68.12,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e973ef8cbbd0728edfe25b3999abc24a5b50e81d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sophosympatheia\/Aurora-Nights-70B-v1.0",
        "Average":73.24,
        "ARC":71.33,
        "HellaSwag":88.33,
        "MMLU":70.47,
        "TruthfulQA":62.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"e4b4ee3d952b1e8360a82d2b3506fd5b4ab68df9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/Franken-MoE-18B-v0.1",
        "Average":73.23,
        "ARC":72.1,
        "HellaSwag":88.3,
        "MMLU":65.01,
        "TruthfulQA":67.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":18.52,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"a64d069a467516037179b16a010ff118ed66d370"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/SuperMente-7B-v4",
        "Average":73.23,
        "ARC":70.48,
        "HellaSwag":87.63,
        "MMLU":63.35,
        "TruthfulQA":71.46,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"40740a963ca357bf8c37af460ce443b8564455ca"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jsfs11\/SnorkelWestBeagle-DARETIES-7B",
        "Average":73.23,
        "ARC":71.16,
        "HellaSwag":87.35,
        "MMLU":64.35,
        "TruthfulQA":70.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"39d9c0d5c5bd1d46834a54c388fe6cb55554995f"
    },
    {
        "T":"?",
        "Model":"giraffe176\/Starling_Monarch_Westlake_Garten-7B-v0.1",
        "Average":73.23,
        "ARC":71.76,
        "HellaSwag":88.15,
        "MMLU":65.07,
        "TruthfulQA":67.92,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ed346db1daac7abf9149020fd4c967c59783bdae"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"v1olet\/v1olet_merged_dpo_7B_v3",
        "Average":73.22,
        "ARC":72.61,
        "HellaSwag":87.7,
        "MMLU":63.51,
        "TruthfulQA":69.07,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"63b69bf2588f3b108d3427389d3c707f6b50d2ba"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"GreenNode\/GreenNodeLM-7B-v1olet",
        "Average":73.22,
        "ARC":72.61,
        "HellaSwag":87.7,
        "MMLU":63.51,
        "TruthfulQA":69.07,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"4f0d53e65814390b8a260dd23fe5a30ced239176"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"oh-yeontaek\/llama-2-70B-LoRA-assemble-v2",
        "Average":73.22,
        "ARC":71.84,
        "HellaSwag":86.89,
        "MMLU":69.37,
        "TruthfulQA":64.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"7feeb5b665ab1ecdfd9cc4fe45fadb86b7b91b5b"
    },
    {
        "T":"?",
        "Model":"louisbrulenaudet\/Pearl-7B-0210-dare",
        "Average":73.21,
        "ARC":70.9,
        "HellaSwag":88.8,
        "MMLU":61.69,
        "TruthfulQA":71.46,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b29298bbe30bba7c6aef25ef21cb9f4d470a10e2"
    },
    {
        "T":"\u2b55",
        "Model":"budecosystem\/genz-70b",
        "Average":73.21,
        "ARC":71.42,
        "HellaSwag":87.99,
        "MMLU":70.78,
        "TruthfulQA":62.66,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"32110b4f33e5e80073ca1f47638482fdc0e19297"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"oh-yeontaek\/llama-2-70B-LoRA-assemble",
        "Average":73.2,
        "ARC":71.84,
        "HellaSwag":86.78,
        "MMLU":69.4,
        "TruthfulQA":64.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"91caffe08852dcbbdedd64786bd3b4ac0dcb2e96"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeuralNovel\/Valor-7B-v0.1",
        "Average":73.2,
        "ARC":72.27,
        "HellaSwag":86.59,
        "MMLU":64.09,
        "TruthfulQA":69.84,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"875319a815400bdb73c309601c175d72997a4fa0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/aegolius-acadicus-v1-30b",
        "Average":73.19,
        "ARC":72.61,
        "HellaSwag":87.99,
        "MMLU":65.11,
        "TruthfulQA":67.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":29.79,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fecd580eb4294525160e86b79d0f205a3a44e172"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/aegolius-acadicus-30b",
        "Average":73.19,
        "ARC":72.61,
        "HellaSwag":88.01,
        "MMLU":65.07,
        "TruthfulQA":67.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":29.79,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1260e0b4085ce8f6fbbe41192c5932d084706be4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051612\/A0124",
        "Average":73.19,
        "ARC":67.83,
        "HellaSwag":84.71,
        "MMLU":83.7,
        "TruthfulQA":56.52,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1ff7135939ad6f5e4931703cf251134fa87b3432"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jsfs11\/WestOrcaNeural-V2-DARETIES-7B",
        "Average":73.19,
        "ARC":72.1,
        "HellaSwag":88.21,
        "MMLU":64.64,
        "TruthfulQA":67.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cd28876def531a2db88f123782d39e91fec0317b"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"222gate\/bleagle-7b-v0.1-test",
        "Average":73.18,
        "ARC":72.27,
        "HellaSwag":88.24,
        "MMLU":64.37,
        "TruthfulQA":67.83,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"176a7ae5754de18b852c5018c7cee41925fe05b3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Brillibits\/Instruct_Mixtral-8x7B-v0.1_Dolly15K",
        "Average":73.17,
        "ARC":69.28,
        "HellaSwag":87.59,
        "MMLU":70.96,
        "TruthfulQA":64.83,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e593de223b662cfda40aa96163c6a42d6b32de5e"
    },
    {
        "T":"?",
        "Model":"ConvexAI\/Solutus-3x7B",
        "Average":73.15,
        "ARC":72.01,
        "HellaSwag":88.31,
        "MMLU":64.77,
        "TruthfulQA":67.52,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":18.52,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"7c604d7adedd864f6ff3db10500a499e5dd8f8ff"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_3-7B-slerp",
        "Average":73.15,
        "ARC":70.82,
        "HellaSwag":87.79,
        "MMLU":65.12,
        "TruthfulQA":68.86,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4adb83489d7321003e942ee60d835f8346f42951"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_34-7B-slerp",
        "Average":73.14,
        "ARC":70.05,
        "HellaSwag":87.46,
        "MMLU":61.82,
        "TruthfulQA":73.24,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dad4154a5e93eb0198d54a5347224547e7c988c5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051612\/B0122",
        "Average":73.14,
        "ARC":67.92,
        "HellaSwag":84.92,
        "MMLU":81.53,
        "TruthfulQA":58.2,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"9123a8512a24024afe2dac6f67cb28dca10cceb2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"garage-bAInd\/Platypus2-70B-instruct",
        "Average":73.13,
        "ARC":71.84,
        "HellaSwag":87.94,
        "MMLU":70.48,
        "TruthfulQA":62.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub":139,
        "Available on the hub":true,
        "Model Sha":"a66378c15f89756215ccc64572ba69b161173703"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JaeyeonKang\/CCK_Gony_v3.1",
        "Average":73.11,
        "ARC":69.62,
        "HellaSwag":87.45,
        "MMLU":71.2,
        "TruthfulQA":64.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dfc889db0d02cebaadacc6726a8622a40f45eb5e"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/Kindred-7B-slerp",
        "Average":73.11,
        "ARC":71.76,
        "HellaSwag":87.78,
        "MMLU":64.76,
        "TruthfulQA":68.12,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"64da0f7bb5f6d772b7d682c99b5c510cb8681ff0"
    },
    {
        "T":"?",
        "Model":"jsfs11\/MixtureofMerges-MoE-2x7bRP-v8",
        "Average":73.1,
        "ARC":71.33,
        "HellaSwag":88.06,
        "MMLU":64.33,
        "TruthfulQA":68.69,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cacdefef2b53baba4829920e430e994fa04724b8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Steelskull\/Umbra-MoE-4x10.7",
        "Average":73.09,
        "ARC":70.31,
        "HellaSwag":87.81,
        "MMLU":66.42,
        "TruthfulQA":67.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":36.1,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"6a1e1b81e4d99755604be0b84798e56058d6ec37"
    },
    {
        "T":"?",
        "Model":"seyf1elislam\/WestKunai-X-7b",
        "Average":73.09,
        "ARC":71.08,
        "HellaSwag":87.86,
        "MMLU":65.42,
        "TruthfulQA":68.01,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"ca07b7bea2f28538d4112c989b1e4402c96c17ef"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"viethq188\/LeoScorpius-7B-Chat-DPO",
        "Average":73.09,
        "ARC":70.48,
        "HellaSwag":87.97,
        "MMLU":65.08,
        "TruthfulQA":68.83,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"6e6e0a6e5c309acbe124a8055138ea5a4f2e56d1"
    },
    {
        "T":"?",
        "Model":"ConvexAI\/BurningBruce-005",
        "Average":73.09,
        "ARC":72.01,
        "HellaSwag":88.31,
        "MMLU":64.76,
        "TruthfulQA":67.27,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"e61d2cc6475548244b9ab180e508246e3e577b66"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ResplendentAI\/Luna-2x7B-MoE",
        "Average":73.09,
        "ARC":71.16,
        "HellaSwag":88.12,
        "MMLU":64.41,
        "TruthfulQA":68.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"9f9e4ae1c294ea4301eeefd3cf6222d156916144"
    },
    {
        "T":"?",
        "Model":"Eric111\/CatunaLaserPi",
        "Average":73.09,
        "ARC":71.5,
        "HellaSwag":88.06,
        "MMLU":64.95,
        "TruthfulQA":67.83,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1285f3879ed0ccae4ac32a1ab3e54894de8f4c3a"
    },
    {
        "T":"?",
        "Model":"ChuckMcSneed\/Gembo-v1-70b",
        "Average":73.08,
        "ARC":71.25,
        "HellaSwag":86.98,
        "MMLU":70.85,
        "TruthfulQA":63.25,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"b3767a69eb9b36aba96be586958391c45b695ff4"
    },
    {
        "T":"?",
        "Model":"Test157t\/Copium-Cola-9B",
        "Average":73.07,
        "ARC":71.42,
        "HellaSwag":87.42,
        "MMLU":64.83,
        "TruthfulQA":68.6,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":8.99,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"df26b7dbe4567006b609da965d09a9175d8e3b04"
    },
    {
        "T":"?",
        "Model":"antiven0m\/finch",
        "Average":73.06,
        "ARC":71.59,
        "HellaSwag":87.87,
        "MMLU":64.81,
        "TruthfulQA":67.96,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"8dbf40c7be17ddb1b2a07e49c60c180fed741172"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Vasanth\/Beast-Soul",
        "Average":73.05,
        "ARC":72.53,
        "HellaSwag":88.15,
        "MMLU":64.76,
        "TruthfulQA":66.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"700aacf29bde13dfef2a5f15c5a5d6627c73d80d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nbeerbower\/bruphin-epsilon",
        "Average":73.04,
        "ARC":72.1,
        "HellaSwag":88.09,
        "MMLU":65.04,
        "TruthfulQA":66.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"75d6c2cee8666b9f25631a796f35563147264045"
    },
    {
        "T":"?",
        "Model":"abacusai\/MM-OV-bagel-DPO-34b-c1000-250",
        "Average":73.04,
        "ARC":68.17,
        "HellaSwag":83.97,
        "MMLU":76.33,
        "TruthfulQA":63.67,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"1d697d32ba4f6ed471cd2857669029f425b827bb"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"bhavinjawade\/SuperAligned-Jawade",
        "Average":73.04,
        "ARC":71.59,
        "HellaSwag":90.58,
        "MMLU":60.81,
        "TruthfulQA":69.17,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"6a500b5beb37580dd001dd0234d15350a5b6020e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/14B-Glacier-Stack",
        "Average":73.03,
        "ARC":71.67,
        "HellaSwag":88.35,
        "MMLU":66.73,
        "TruthfulQA":65.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":14.22,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"cf6f466d227c041df3b892dff394df43ecf99b8b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zhengr\/MixTAO-7Bx2-MoE-DPO",
        "Average":73.02,
        "ARC":70.9,
        "HellaSwag":87.12,
        "MMLU":64.72,
        "TruthfulQA":69.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"911149cad645ccb189cb403c16bbed98df18dfd6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/Sirius-10B",
        "Average":73.02,
        "ARC":71.93,
        "HellaSwag":87.32,
        "MMLU":64.73,
        "TruthfulQA":68.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9f230348c854288c328f1fada6e6887c11709151"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dfurman\/GarrulusMarcoro-7B-v0.1",
        "Average":73.01,
        "ARC":72.35,
        "HellaSwag":88.0,
        "MMLU":64.65,
        "TruthfulQA":67.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"c53e0d67f4684a46d35ded045c21e19e380f5e91"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ResplendentAI\/DaturaCookie_7B",
        "Average":73.0,
        "ARC":71.25,
        "HellaSwag":88.0,
        "MMLU":64.28,
        "TruthfulQA":68.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"755d702c80e5acee8c07676b4a4dee37de56e2a8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kquant03\/BurningBruce-SOLAR-8x10.7B-bf16",
        "Average":72.97,
        "ARC":69.11,
        "HellaSwag":87.81,
        "MMLU":66.27,
        "TruthfulQA":68.67,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":69.92,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"fd0cfae8ce78770857d415291ea23b77c7b52705"
    },
    {
        "T":"?",
        "Model":"Aryanne\/Westest-7B",
        "Average":72.96,
        "ARC":72.18,
        "HellaSwag":88.52,
        "MMLU":64.43,
        "TruthfulQA":66.72,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"9250ae984d3a3051fb4767451a7c548b34f96445"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BarryFutureman\/WestLakeX-7B-EvoMerge",
        "Average":72.96,
        "ARC":71.42,
        "HellaSwag":88.08,
        "MMLU":64.84,
        "TruthfulQA":67.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6bf661cdade79d96c4def4f09c27ad5ca1bae11a"
    },
    {
        "T":"\u2b55",
        "Model":"upstage\/SOLAR-0-70b-16bit",
        "Average":72.95,
        "ARC":71.08,
        "HellaSwag":87.89,
        "MMLU":70.58,
        "TruthfulQA":62.25,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":252,
        "Available on the hub":true,
        "Model Sha":"5f9c77b2c0397cf83d2f97740483f107c7109e8c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JaeyeonKang\/CCK_Gony_v0.1",
        "Average":72.94,
        "ARC":70.05,
        "HellaSwag":87.27,
        "MMLU":71.21,
        "TruthfulQA":63.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5aeed89b3b0eba74cea863b59a43c63c81be5989"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fangloveskari\/Platypus_QLoRA_LLaMA_70b",
        "Average":72.94,
        "ARC":72.1,
        "HellaSwag":87.46,
        "MMLU":71.02,
        "TruthfulQA":61.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"b9b8560832276f60ba6bf37ac913b230a85ac19b"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_37-7B-dare_ties",
        "Average":72.94,
        "ARC":70.31,
        "HellaSwag":86.82,
        "MMLU":59.4,
        "TruthfulQA":75.23,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2befef9d78ebf7a3ae395bfbf33ba0b8f07d38eb"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"liminerity\/Blur-7b-v1.21",
        "Average":72.93,
        "ARC":70.82,
        "HellaSwag":88.07,
        "MMLU":64.85,
        "TruthfulQA":67.99,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"eaaab73b3cf4860b589a86c32b5e5865a6dc1f13"
    },
    {
        "T":"?",
        "Model":"saishf\/Fett-Eris-Mix-7B",
        "Average":72.92,
        "ARC":68.77,
        "HellaSwag":87.33,
        "MMLU":63.65,
        "TruthfulQA":71.91,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"287e1bc2ca35ba1978cfe1040d9183d530b23c0c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"brucethemoose\/SUS-Bagel-200K-DARE-Test",
        "Average":72.91,
        "ARC":68.09,
        "HellaSwag":85.38,
        "MMLU":76.98,
        "TruthfulQA":61.2,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"063c5412143468d6408b6b8122ec925c0baa0add"
    },
    {
        "T":"?",
        "Model":"Eric111\/openchat-3.5-0106-128k-DPO_dpo-binarized-NeuralTrix-7B",
        "Average":72.91,
        "ARC":70.99,
        "HellaSwag":87.06,
        "MMLU":65.57,
        "TruthfulQA":68.0,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"e511b6a307d23c2a24bc7460231714ea7d0bee02"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kquant03\/Buttercup-4x7B-bf16",
        "Average":72.91,
        "ARC":72.1,
        "HellaSwag":87.74,
        "MMLU":64.58,
        "TruthfulQA":67.2,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"2513232abc84b071b83d0241e8decc69d18d721d"
    },
    {
        "T":"?",
        "Model":"ChaoticNeutrals\/Eris_7B",
        "Average":72.9,
        "ARC":71.42,
        "HellaSwag":87.99,
        "MMLU":65.24,
        "TruthfulQA":66.95,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"30ffcfbdcab92da78fe1abf4ccf69b1a1a71c11a"
    },
    {
        "T":"?",
        "Model":"saishf\/Kuno-Lake-7B",
        "Average":72.9,
        "ARC":71.84,
        "HellaSwag":88.15,
        "MMLU":64.76,
        "TruthfulQA":66.83,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"ee6af302f1aa7b49a89f79ae2ae15e3a357099f0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ChaoticNeutrals\/RP_Vision_7B",
        "Average":72.89,
        "ARC":70.65,
        "HellaSwag":87.81,
        "MMLU":64.58,
        "TruthfulQA":68.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"8bfd1e9425e1c12b95967197c8388f61d7961b07"
    },
    {
        "T":"?",
        "Model":"Steelskull\/Umbra-v3-MoE-4x11b",
        "Average":72.89,
        "ARC":68.43,
        "HellaSwag":87.83,
        "MMLU":65.99,
        "TruthfulQA":69.3,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":36.1,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"f8621d13b356eae26965173ae6146f8616ef38a3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeontaek\/llama-2-70B-ensemble-v5",
        "Average":72.86,
        "ARC":71.16,
        "HellaSwag":87.24,
        "MMLU":69.6,
        "TruthfulQA":63.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cf4292aae32e62d27baf59b0d3db1be38f817631"
    },
    {
        "T":"?",
        "Model":"Test157t\/Pasta-Lake-7b",
        "Average":72.86,
        "ARC":70.82,
        "HellaSwag":87.91,
        "MMLU":64.41,
        "TruthfulQA":68.28,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"4c0ea3e14e45c5f6aa0d8b409ccd9017501dee42"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ChaoticNeutrals\/Prima-LelantaclesV5-7b",
        "Average":72.82,
        "ARC":70.65,
        "HellaSwag":87.87,
        "MMLU":64.52,
        "TruthfulQA":68.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"9d87945f984ee530cb1e062018906110e92dc470"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"CultriX\/MergeTrix-7B",
        "Average":72.82,
        "ARC":72.27,
        "HellaSwag":87.84,
        "MMLU":64.88,
        "TruthfulQA":66.27,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"d11bd6b388581d2a44c1431a9985e8fc77addd33"
    },
    {
        "T":"\u2b55",
        "Model":"TheBloke\/Genz-70b-GPTQ",
        "Average":72.82,
        "ARC":71.08,
        "HellaSwag":87.64,
        "MMLU":70.26,
        "TruthfulQA":62.28,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":9.7,
        "Hub":22,
        "Available on the hub":true,
        "Model Sha":"7d38987a43d2445b193db99a029a264b39dc6c8e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Platypus2-70B-Instruct-GPTQ",
        "Average":72.81,
        "ARC":71.25,
        "HellaSwag":87.55,
        "MMLU":69.89,
        "TruthfulQA":62.54,
        "Type":"fine-tuned",
        "Precision":"None",
        "Hub License":"llama2",
        "#Params (B)":9.1,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"4a44568aadd8a4babfa5549cf33e6e84cbae7ab8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LDCC\/LDCC-SOLAR-10.7B",
        "Average":72.8,
        "ARC":67.58,
        "HellaSwag":88.11,
        "MMLU":66.63,
        "TruthfulQA":68.87,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.86,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"c8741ec6f4f24324a96041efaf2f627a99d946e6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/TheTop-5x7B-Instruct-D-v0.1",
        "Average":72.79,
        "ARC":71.76,
        "HellaSwag":88.21,
        "MMLU":64.86,
        "TruthfulQA":66.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"d7a439cbd47cb966778bf35e3e8efde20d5cfe7f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LDCC\/LDCC-SOLAR-10.7B",
        "Average":72.78,
        "ARC":67.32,
        "HellaSwag":88.11,
        "MMLU":66.83,
        "TruthfulQA":68.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.86,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"1055563879363d9ee2fba1d9fd1628eca6bcbb4e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/SOLAR-10.7B-NahIdWin",
        "Average":72.77,
        "ARC":64.51,
        "HellaSwag":85.67,
        "MMLU":64.17,
        "TruthfulQA":76.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"94bad5a6b469d84f556d6cc52c44fd88c07476f3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/TheTop-5x7B-Instruct-S4-v0.1",
        "Average":72.77,
        "ARC":72.18,
        "HellaSwag":88.29,
        "MMLU":65.03,
        "TruthfulQA":65.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"187fbdd40f13d8e1b39982984ab9ef8ed7bff97b"
    },
    {
        "T":"?",
        "Model":"ChuckMcSneed\/Gembo-v1.1-70b",
        "Average":72.74,
        "ARC":70.99,
        "HellaSwag":86.9,
        "MMLU":70.63,
        "TruthfulQA":62.45,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c9755408254f3516e67e3e6a0716d6badb2d2841"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jsfs11\/WestOrcaNeuralMarco-DPO-v2-DARETIES-7B",
        "Average":72.73,
        "ARC":71.93,
        "HellaSwag":88.06,
        "MMLU":64.99,
        "TruthfulQA":65.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"e06b5a01d769ab898ed9b3e9052567d34d325552"
    },
    {
        "T":"?",
        "Model":"cloudyu\/Mixtral_7Bx2_MoE",
        "Average":72.73,
        "ARC":71.25,
        "HellaSwag":87.45,
        "MMLU":64.98,
        "TruthfulQA":67.23,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.88,
        "Hub":34,
        "Available on the hub":false,
        "Model Sha":"4295fae8ef44f19f38f5391dc0c7194db096c4b2"
    },
    {
        "T":"\u2b55",
        "Model":"pankajmathur\/model_007",
        "Average":72.72,
        "ARC":71.08,
        "HellaSwag":87.65,
        "MMLU":69.04,
        "TruthfulQA":63.12,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":22,
        "Available on the hub":true,
        "Model Sha":"213eef7449fa23f447a4efdd0bb15eb7c865682a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_007",
        "Average":72.72,
        "ARC":71.08,
        "HellaSwag":87.65,
        "MMLU":69.04,
        "TruthfulQA":63.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":18,
        "Available on the hub":true,
        "Model Sha":"0f5d81b13718a866cb078bd8762ab80a41972663"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_5-7B-ties",
        "Average":72.71,
        "ARC":71.67,
        "HellaSwag":87.88,
        "MMLU":64.91,
        "TruthfulQA":66.37,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1ca994e2d628d98ba725b128c3a87201bd434603"
    },
    {
        "T":"?",
        "Model":"seyf1elislam\/WestKunai-XD-7b",
        "Average":72.71,
        "ARC":71.25,
        "HellaSwag":87.59,
        "MMLU":64.69,
        "TruthfulQA":67.29,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"824e0c22a5f06a17d38251fa36be1d9ee7888d66"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeverSleep\/MiquMaid-v2-70B",
        "Average":72.69,
        "ARC":70.48,
        "HellaSwag":87.49,
        "MMLU":75.18,
        "TruthfulQA":57.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"bd19912eb652fd76165938179abd3b54cacb85fa"
    },
    {
        "T":"?",
        "Model":"ChaoticNeutrals\/Prima-LelantaclesV7-7b",
        "Average":72.68,
        "ARC":70.65,
        "HellaSwag":87.94,
        "MMLU":64.67,
        "TruthfulQA":67.45,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"ff2a95f581cb60b85dbc376406d3ee6f56f705be"
    },
    {
        "T":"?",
        "Model":"RatanRohith\/NeuralPizza-7B-V0.3",
        "Average":72.67,
        "ARC":71.08,
        "HellaSwag":87.38,
        "MMLU":64.29,
        "TruthfulQA":67.93,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"04cd413008c353ca558ab901c0d88132c25772c2"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"GreenNode\/GreenNodeLM-7B-v2leo",
        "Average":72.66,
        "ARC":69.8,
        "HellaSwag":88.02,
        "MMLU":65.0,
        "TruthfulQA":67.83,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"e5a0955eb36568aa850cd73debbe9815a9d1e60a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JaeyeonKang\/CCK_Asura_v2",
        "Average":72.65,
        "ARC":70.82,
        "HellaSwag":88.09,
        "MMLU":74.72,
        "TruthfulQA":56.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"515d7d948b4274c7451fdef61eae9e76eac93a38"
    },
    {
        "T":"?",
        "Model":"ChuckMcSneed\/WinterGoddess-1.4x-70b-32k",
        "Average":72.64,
        "ARC":71.16,
        "HellaSwag":89.12,
        "MMLU":66.42,
        "TruthfulQA":63.87,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"136d8ad5d94b8ac02ac7cd4e0b32e09366a550dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeontaek\/llama-2-70B-ensemble-v4",
        "Average":72.64,
        "ARC":70.9,
        "HellaSwag":87.34,
        "MMLU":69.71,
        "TruthfulQA":62.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fed7f766c2977bb13ba372ba63c1bb9f8af263d8"
    },
    {
        "T":"?",
        "Model":"Nitral-AI\/Eris_PrimeV4-Vision-7B",
        "Average":72.64,
        "ARC":70.22,
        "HellaSwag":87.56,
        "MMLU":65.01,
        "TruthfulQA":67.76,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"afe74b909ecaa54b7d35ea0bde0b1626257085a4"
    },
    {
        "T":"\u2b55",
        "Model":"pankajmathur\/orca_mini_v3_70b",
        "Average":72.64,
        "ARC":71.25,
        "HellaSwag":87.85,
        "MMLU":70.18,
        "TruthfulQA":61.27,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub":23,
        "Available on the hub":true,
        "Model Sha":"5267da97e68c193685758cef56573da7797d5295"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/orca_mini_v3_70b",
        "Average":72.64,
        "ARC":71.25,
        "HellaSwag":87.85,
        "MMLU":70.18,
        "TruthfulQA":61.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"c1d4f997f8ed685a6efc72229523b2e56fd0774b"
    },
    {
        "T":"\u2b55",
        "Model":"smelborp\/MixtralOrochi8x7B",
        "Average":72.63,
        "ARC":70.31,
        "HellaSwag":86.1,
        "MMLU":70.13,
        "TruthfulQA":63.99,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"e88684d163fd3e789c40261c5b68244bb72bd706"
    },
    {
        "T":"?",
        "Model":"ChaoticNeutrals\/RPMix-4x7B-MoE",
        "Average":72.63,
        "ARC":71.08,
        "HellaSwag":87.79,
        "MMLU":64.36,
        "TruthfulQA":67.29,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"c9fb38c846ba1f1ce9a7a3560e491ea9d4a8d875"
    },
    {
        "T":"?",
        "Model":"llmixer\/BigWeave-v16-103b",
        "Average":72.63,
        "ARC":65.87,
        "HellaSwag":87.61,
        "MMLU":73.22,
        "TruthfulQA":63.81,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"unknown",
        "#Params (B)":103.2,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"a1f70cd042fc8b4c5767f597edbb0054e7cb14f9"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"h2m\/mhm-8x7B-FrankenMoE-v1.0",
        "Average":72.62,
        "ARC":70.9,
        "HellaSwag":87.75,
        "MMLU":64.7,
        "TruthfulQA":67.1,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5aeee76977588d88d3faca8340c582c82cc598ce"
    },
    {
        "T":"?",
        "Model":"Azure99\/blossom-v5-34b",
        "Average":72.61,
        "ARC":66.98,
        "HellaSwag":84.79,
        "MMLU":76.0,
        "TruthfulQA":62.68,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"05a9ef37686d678f267a15664b5ce66612b7996a"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/Samantha-1.11-70b",
        "Average":72.61,
        "ARC":70.05,
        "HellaSwag":87.55,
        "MMLU":67.82,
        "TruthfulQA":65.02,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"49e5b5ee0bed2864f0b38ba8bf9e01ccc5e0ba5f"
    },
    {
        "T":"?",
        "Model":"Sao10K\/Skadi-Mixtral-v1",
        "Average":72.6,
        "ARC":70.14,
        "HellaSwag":87.65,
        "MMLU":72.19,
        "TruthfulQA":60.43,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a6d56964be9b3d796515253e6b1fb86a3d025260"
    },
    {
        "T":"\u2b55",
        "Model":"MayaPH\/GodziLLa2-70B",
        "Average":72.59,
        "ARC":71.42,
        "HellaSwag":87.53,
        "MMLU":69.88,
        "TruthfulQA":61.54,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"7b78087db07eec97f7b461d10758ece76d685543"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ConvexAI\/BurningBruce-003",
        "Average":72.58,
        "ARC":71.25,
        "HellaSwag":88.22,
        "MMLU":64.48,
        "TruthfulQA":66.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"bbcf8079aa4a50393036e53b89f4f4fb20afbd1f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nbeerbower\/SuperBruphin-3x7B",
        "Average":72.58,
        "ARC":71.16,
        "HellaSwag":87.74,
        "MMLU":64.58,
        "TruthfulQA":66.85,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":18.52,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fedc78faef524786860027123f90609f402430b2"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/flammen9-mistral-7B",
        "Average":72.58,
        "ARC":69.62,
        "HellaSwag":87.74,
        "MMLU":64.41,
        "TruthfulQA":68.54,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e34f3768bf9ed4fdd0ac91cd3d71847cb45ed46c"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"mncai\/mistral-7b-dpo-v5",
        "Average":72.57,
        "ARC":72.01,
        "HellaSwag":87.57,
        "MMLU":63.85,
        "TruthfulQA":66.86,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"8108f313d878ce848ceceeaf55ce8b3ecaaee792"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"brucethemoose\/Yi-34B-200K-DARE-merge-v7",
        "Average":72.57,
        "ARC":68.09,
        "HellaSwag":85.99,
        "MMLU":77.3,
        "TruthfulQA":58.9,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"9a6bfe30e2ab9eab807787bb0f3b7e91241d1ce0"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"CultriX\/CultriX-MoE-Model",
        "Average":72.57,
        "ARC":70.05,
        "HellaSwag":87.22,
        "MMLU":64.95,
        "TruthfulQA":68.04,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"9043ce95d7311086417164cc84c6eb1d4ab7fe13"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_45-7B-dare_ties",
        "Average":72.56,
        "ARC":69.8,
        "HellaSwag":87.6,
        "MMLU":65.06,
        "TruthfulQA":67.79,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7d0905d7112b0e7c1cffd2bd41ea3152d5cc2bc8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"InferenceIllusionist\/Excalibur-7B",
        "Average":72.54,
        "ARC":69.71,
        "HellaSwag":87.56,
        "MMLU":65.66,
        "TruthfulQA":67.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"ceb9fd074f178fe25cb192d92f3f3bd1a3ff4bf1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/Neural-Cosmic-Boy-7B-slerp",
        "Average":72.54,
        "ARC":70.48,
        "HellaSwag":87.65,
        "MMLU":64.92,
        "TruthfulQA":67.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2335ab666bac2723188a3b35fc27be9306a3057c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TomGrc\/FusionNet_passthrough_v0.1",
        "Average":72.53,
        "ARC":69.45,
        "HellaSwag":87.79,
        "MMLU":65.2,
        "TruthfulQA":67.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":21.2,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"0466e92dff5927724966ed3815432b4569d6d19e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TomGrc\/FusionNet_passthrough",
        "Average":72.53,
        "ARC":69.45,
        "HellaSwag":87.72,
        "MMLU":65.28,
        "TruthfulQA":67.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":21.2,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"fee459c6a29f7157394f62484eacf0417fee718a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jan-ai\/Solar-10.7B-SLERP",
        "Average":72.52,
        "ARC":70.73,
        "HellaSwag":87.87,
        "MMLU":65.77,
        "TruthfulQA":65.72,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"786e6492919d0d1eb07b5988f67e0ee61aa05c21"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/Lelantos-DPO-7B",
        "Average":72.52,
        "ARC":71.08,
        "HellaSwag":87.22,
        "MMLU":64.0,
        "TruthfulQA":67.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a14226753e81928ca1aa97a5457bf8313e06ba6d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051615\/A0308-G",
        "Average":72.52,
        "ARC":68.34,
        "HellaSwag":83.64,
        "MMLU":84.07,
        "TruthfulQA":54.02,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c33065e209686b67f3374fffcb11ee7b90aa7983"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_007_v2",
        "Average":72.49,
        "ARC":71.42,
        "HellaSwag":87.31,
        "MMLU":68.58,
        "TruthfulQA":62.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"3d95e0f3598f7a76ab97cb2cc0e4aae957d77479"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_33-7B-slerp",
        "Average":72.49,
        "ARC":70.73,
        "HellaSwag":87.26,
        "MMLU":63.87,
        "TruthfulQA":68.09,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"745eeff3d8cf80d618a9bda256d1faf36dd871b7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AiMavenAi\/MavenWest",
        "Average":72.49,
        "ARC":71.59,
        "HellaSwag":88.44,
        "MMLU":64.63,
        "TruthfulQA":65.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"91075cfad3fe5a4bf08475e2b45fd9399a8ad368"
    },
    {
        "T":"?",
        "Model":"ChuckMcSneed\/SMaxxxer-v1-70b",
        "Average":72.48,
        "ARC":70.65,
        "HellaSwag":88.02,
        "MMLU":70.55,
        "TruthfulQA":60.7,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e3f325626684c533dd3ce7f3c328c9a962bcbb21"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/Westuccine-7B-slerp",
        "Average":72.46,
        "ARC":69.37,
        "HellaSwag":87.34,
        "MMLU":63.8,
        "TruthfulQA":69.34,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"44bf54f7466a508e4e82883f0cbc8aba9aec85c6"
    },
    {
        "T":"?",
        "Model":"Inv\/Konstanta-Gamma-V2-9B",
        "Average":72.45,
        "ARC":69.88,
        "HellaSwag":86.84,
        "MMLU":64.22,
        "TruthfulQA":68.85,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":9.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7af8365a3e990f2811231d9c521acf22a11381a9"
    },
    {
        "T":"?",
        "Model":"Steelskull\/Umbra-v2.1-MoE-4x10.7",
        "Average":72.43,
        "ARC":69.11,
        "HellaSwag":87.57,
        "MMLU":66.48,
        "TruthfulQA":66.57,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":36.1,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"740d59fb617da265662a6bddd092226b5503eda4"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/MelangeA-70b",
        "Average":72.43,
        "ARC":71.25,
        "HellaSwag":87.3,
        "MMLU":70.56,
        "TruthfulQA":60.61,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d48cf79d1ead50154b1e70120779ae91bc5fafb4"
    },
    {
        "T":"?",
        "Model":"llmixer\/BigWeave-v20-110b",
        "Average":72.42,
        "ARC":68.17,
        "HellaSwag":88.54,
        "MMLU":70.51,
        "TruthfulQA":62.47,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"unknown",
        "#Params (B)":110.05,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1e363188df8256180530fc42688bdb6b3de66b0a"
    },
    {
        "T":"?",
        "Model":"SJ-Donald\/SJ-SOLAR-10.7b-DPO",
        "Average":72.42,
        "ARC":68.26,
        "HellaSwag":86.95,
        "MMLU":66.73,
        "TruthfulQA":67.74,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.86,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"618e5aedf02d58358d6fda7d9fa67c169b7156d0"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/Samantha-1.1-70b",
        "Average":72.42,
        "ARC":68.77,
        "HellaSwag":87.46,
        "MMLU":68.6,
        "TruthfulQA":64.85,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"a3819d186f5b4d52ced7ddeb7fa16bf66e8a2ea7"
    },
    {
        "T":"?",
        "Model":"openagi-project\/OpenAGI-7B-v0.2",
        "Average":72.4,
        "ARC":68.52,
        "HellaSwag":86.03,
        "MMLU":63.02,
        "TruthfulQA":72.04,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"eb1146479e3912ab02461654d93a907c5b90a059"
    },
    {
        "T":"?",
        "Model":"Gille\/MoE-StrangeMerges-2x7B",
        "Average":72.39,
        "ARC":70.82,
        "HellaSwag":87.83,
        "MMLU":65.04,
        "TruthfulQA":65.86,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5a97a5e729502a8065ecc045ca569c3840fe58e3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jsfs11\/West-Dare-7B",
        "Average":72.38,
        "ARC":71.42,
        "HellaSwag":87.57,
        "MMLU":64.29,
        "TruthfulQA":66.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"78a01b25cb36f806ad1b25132595ccfaa376466a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC44\/Mistral-7B-private-spef",
        "Average":72.37,
        "ARC":69.88,
        "HellaSwag":87.34,
        "MMLU":63.27,
        "TruthfulQA":69.01,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bb29794e86ff6a39f77185f547c6bb335d2f5649"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"mlabonne\/NeuralMarcoro14-7B",
        "Average":72.37,
        "ARC":71.42,
        "HellaSwag":87.59,
        "MMLU":64.84,
        "TruthfulQA":65.64,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":38,
        "Available on the hub":false,
        "Model Sha":"df267682dbafe08a877602e6588bf461b6607d74"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"mlabonne\/NeuralDaredevil-7B",
        "Average":72.37,
        "ARC":69.88,
        "HellaSwag":87.62,
        "MMLU":65.12,
        "TruthfulQA":66.85,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":27,
        "Available on the hub":false,
        "Model Sha":"f03ff71ca0b07edccda0d2f407049dcf18edfb4d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Hermes-2-Yi-34B",
        "Average":72.36,
        "ARC":66.89,
        "HellaSwag":85.49,
        "MMLU":76.7,
        "TruthfulQA":60.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":34.39,
        "Hub":215,
        "Available on the hub":true,
        "Model Sha":"deb99d98742ec9691ef593418bea71a4437745a1"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_38-7B-dare_ties",
        "Average":72.36,
        "ARC":71.67,
        "HellaSwag":86.35,
        "MMLU":58.3,
        "TruthfulQA":73.14,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f07d11120ff39e698cf808e617f01860ebaa2085"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_009",
        "Average":72.36,
        "ARC":71.59,
        "HellaSwag":87.7,
        "MMLU":69.43,
        "TruthfulQA":60.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5020869e6394b1ac039bf80a0a1d2bed6be6707e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DopeorNope\/You_can_cry_Snowman-13B",
        "Average":72.36,
        "ARC":69.11,
        "HellaSwag":86.3,
        "MMLU":63.77,
        "TruthfulQA":70.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":13.35,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b50693bb4d8965ca9d48ff3c0c21fbfaa524d37c"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"RatanRohith\/NeuralPizza-7B-V0.1",
        "Average":72.36,
        "ARC":70.48,
        "HellaSwag":87.3,
        "MMLU":64.42,
        "TruthfulQA":67.22,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"fb53c42ba7d5719e730f67c5356766d84e5f3619"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051612\/A0123",
        "Average":72.35,
        "ARC":67.66,
        "HellaSwag":84.87,
        "MMLU":78.45,
        "TruthfulQA":58.42,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"61f74b991f1a2ae4dd7ca294051ce19bed90cc56"
    },
    {
        "T":"?",
        "Model":"Inv\/Konstanta-V3-BetaFlavour-7B",
        "Average":72.34,
        "ARC":68.17,
        "HellaSwag":86.88,
        "MMLU":61.39,
        "TruthfulQA":72.92,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"606d81883116273cfc08a027cc454804e755c5d6"
    },
    {
        "T":"?",
        "Model":"ichigoberry\/MonarchPipe-7B-slerp",
        "Average":72.33,
        "ARC":69.97,
        "HellaSwag":87.66,
        "MMLU":65.3,
        "TruthfulQA":66.4,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2a8a49c3c3b43fc3e3d895c5faf9ca04b340eeb4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"flemmingmiguel\/MarcMistral-7B",
        "Average":72.31,
        "ARC":71.16,
        "HellaSwag":87.78,
        "MMLU":65.38,
        "TruthfulQA":64.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4571c6a5382eedacb74a51d1dfb0a6f378becc86"
    },
    {
        "T":"?",
        "Model":"SJ-Donald\/SOLAR-10.7B-slerp",
        "Average":72.31,
        "ARC":68.17,
        "HellaSwag":86.91,
        "MMLU":66.73,
        "TruthfulQA":67.42,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.86,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6a31eeb4fe10b727da8f01f693de0afafb7695fc"
    },
    {
        "T":"?",
        "Model":"logicker\/SkkuDS-DPO-72B-v3",
        "Average":72.3,
        "ARC":66.04,
        "HellaSwag":86.11,
        "MMLU":77.34,
        "TruthfulQA":59.73,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":72.29,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5cf11f6e983a7c11b17c1b7c4aee6ff99e30ba82"
    },
    {
        "T":"?",
        "Model":"BryanSwk\/LaserPipe-7B-SLERP",
        "Average":72.3,
        "ARC":71.08,
        "HellaSwag":87.89,
        "MMLU":64.86,
        "TruthfulQA":65.38,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"0c27fcb6770f2225e4dcc2277b8618e03810427e"
    },
    {
        "T":"?",
        "Model":"vanillaOVO\/correction_1",
        "Average":72.29,
        "ARC":71.16,
        "HellaSwag":88.59,
        "MMLU":63.51,
        "TruthfulQA":65.92,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4d4603c9d0c91f84b15e6e62e5f2a1df4837763b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"upstage\/Llama-2-70b-instruct",
        "Average":72.29,
        "ARC":70.9,
        "HellaSwag":87.48,
        "MMLU":69.8,
        "TruthfulQA":60.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":49,
        "Available on the hub":true,
        "Model Sha":"8469429924dc2e1a9394b8095753985668a4052e"
    },
    {
        "T":"?",
        "Model":"ChuckMcSneed\/PMaxxxer-v1-70b",
        "Average":72.28,
        "ARC":71.08,
        "HellaSwag":87.88,
        "MMLU":70.39,
        "TruthfulQA":59.77,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"00a78d52d1e5c97fb0a277818c1245dfec61ab0f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeontaek\/llama-2-70B-ensemble-v7",
        "Average":72.27,
        "ARC":70.31,
        "HellaSwag":87.31,
        "MMLU":68.34,
        "TruthfulQA":63.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"38f0f101ba06039bdc9677c686d9502ba942362a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeverSleep\/CausalLM-RP-34B",
        "Average":72.26,
        "ARC":68.0,
        "HellaSwag":83.43,
        "MMLU":83.1,
        "TruthfulQA":54.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":33.93,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"e2a033646231bd947a3948d3aac198d34d04ea38"
    },
    {
        "T":"?",
        "Model":"vishnukv\/WestSeverusJaskier-dare-ties-7b-32k",
        "Average":72.25,
        "ARC":67.75,
        "HellaSwag":87.02,
        "MMLU":61.18,
        "TruthfulQA":73.05,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"480c92aacda73c75bd1d4503c76e040952bed15f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Euryale-1.3-L2-70B",
        "Average":72.24,
        "ARC":70.82,
        "HellaSwag":87.92,
        "MMLU":70.39,
        "TruthfulQA":59.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":39,
        "Available on the hub":true,
        "Model Sha":"6e3ce78eb5346bf3a5ee88cd60c25dc0d73de639"
    },
    {
        "T":"?",
        "Model":"seyf1elislam\/WestKunai-Hermes-7b",
        "Average":72.24,
        "ARC":71.16,
        "HellaSwag":87.76,
        "MMLU":64.77,
        "TruthfulQA":65.25,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"5f348a5ad4c996e22f0fcbdbb2a5326ffc069cc5"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/RoyalNoroichi-7B-slerp",
        "Average":72.23,
        "ARC":70.48,
        "HellaSwag":87.38,
        "MMLU":64.78,
        "TruthfulQA":66.28,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ff742ddb9b730e68dd0d3c875ee207b2335d9046"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/flammen13X-mistral-7B",
        "Average":72.22,
        "ARC":69.88,
        "HellaSwag":87.28,
        "MMLU":64.99,
        "TruthfulQA":66.74,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"de54fcf7930b6edf974da6e4945981c71b4bc059"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/Kunoichi-DPO-7B",
        "Average":72.22,
        "ARC":69.62,
        "HellaSwag":87.14,
        "MMLU":64.79,
        "TruthfulQA":67.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"4e21eea3c32d00b2fcfc5bcfd16d8dc9d0d8874d"
    },
    {
        "T":"?",
        "Model":"mychen76\/mistral-7b-merged-dare_6x7",
        "Average":72.21,
        "ARC":69.62,
        "HellaSwag":87.04,
        "MMLU":65.18,
        "TruthfulQA":66.98,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6ec780be2797a51ba214f18f83b72adbdce2a78f"
    },
    {
        "T":"?",
        "Model":"logicker\/SkkuDS-DPO-72B-v1",
        "Average":72.21,
        "ARC":65.96,
        "HellaSwag":86.0,
        "MMLU":77.33,
        "TruthfulQA":59.54,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":72.29,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5e194e1e44c6c2ebe294f854733f5c5532de5688"
    },
    {
        "T":"?",
        "Model":"Inv\/Konstanta-V3-AlphaFlavour-7B",
        "Average":72.2,
        "ARC":68.86,
        "HellaSwag":86.85,
        "MMLU":61.17,
        "TruthfulQA":71.94,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"06f48a60db61e3855fcb4be17ab1c8ade40d6fee"
    },
    {
        "T":"?",
        "Model":"mayacinka\/chatty-djinn-14B",
        "Average":72.2,
        "ARC":70.39,
        "HellaSwag":86.45,
        "MMLU":64.4,
        "TruthfulQA":67.57,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.57,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3e38656d8a5110f0bf05e7cb8cec2ae8043656c4"
    },
    {
        "T":"?",
        "Model":"BryanSwk\/LaserPipe-7B-SLERP",
        "Average":72.2,
        "ARC":70.82,
        "HellaSwag":87.88,
        "MMLU":64.77,
        "TruthfulQA":65.34,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"0c27fcb6770f2225e4dcc2277b8618e03810427e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"flemmingmiguel\/NeuDist-Ro-7B",
        "Average":72.2,
        "ARC":71.25,
        "HellaSwag":87.48,
        "MMLU":65.13,
        "TruthfulQA":64.93,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c48a29d5543deb8ab9afb4dec0eb0c1a47f2c222"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/EveryNight-7B-slerp",
        "Average":72.17,
        "ARC":70.05,
        "HellaSwag":87.7,
        "MMLU":64.88,
        "TruthfulQA":66.07,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"d9a0f3fc9f72dfe8f5a97084f512d6ace39f9b9f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeontaek\/llama-2-70B-ensemble-v6",
        "Average":72.17,
        "ARC":70.99,
        "HellaSwag":87.2,
        "MMLU":68.07,
        "TruthfulQA":62.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2edcbe2de75f565c5ed0f8055fbd13aa09e8bef6"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Qwen\/Qwen-72B",
        "Average":72.17,
        "ARC":65.19,
        "HellaSwag":85.94,
        "MMLU":77.37,
        "TruthfulQA":60.19,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":72.29,
        "Hub":314,
        "Available on the hub":false,
        "Model Sha":"f62c59844a8de3c27cf22735218d77e9fa9f6b17"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"CultriX\/MistralTrix-SLERP",
        "Average":72.17,
        "ARC":70.82,
        "HellaSwag":87.54,
        "MMLU":64.98,
        "TruthfulQA":65.35,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"ebc368fef4f5f6d3bef7d7839e58afd1c4dd3bfc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"leveldevai\/BeagleMist-7B",
        "Average":72.17,
        "ARC":71.08,
        "HellaSwag":87.47,
        "MMLU":65.29,
        "TruthfulQA":64.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"33c3fd1152072dfefe60b4c2c9247539b0a161ee"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Qwen\/Qwen1.5-72B",
        "Average":72.17,
        "ARC":65.87,
        "HellaSwag":85.99,
        "MMLU":77.2,
        "TruthfulQA":59.61,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":72.29,
        "Hub":44,
        "Available on the hub":false,
        "Model Sha":"cc2f19f5bc9ad693d4447e42e9844d9931ab8e81"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Qwen\/Qwen2-beta-72B",
        "Average":72.17,
        "ARC":65.87,
        "HellaSwag":85.99,
        "MMLU":77.2,
        "TruthfulQA":59.61,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":72.29,
        "Hub":44,
        "Available on the hub":false,
        "Model Sha":"77914b9e49a63ebca7c06ecebe00215a79723f75"
    },
    {
        "T":"?",
        "Model":"Eric111\/Mayoroya",
        "Average":72.17,
        "ARC":71.08,
        "HellaSwag":87.52,
        "MMLU":65.28,
        "TruthfulQA":64.79,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"782a4064641e79573aa6bf5fd11ffb09baafbe6a"
    },
    {
        "T":"?",
        "Model":"ChaoticNeutrals\/Cookie_7B",
        "Average":72.17,
        "ARC":69.71,
        "HellaSwag":87.57,
        "MMLU":64.51,
        "TruthfulQA":66.88,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"bb106b2f29819c4e4a173cefaa62fa9b6a4a0d2e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jeiku\/Cookie_7B",
        "Average":72.17,
        "ARC":69.71,
        "HellaSwag":87.57,
        "MMLU":64.51,
        "TruthfulQA":66.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"026955076c0744e1257cef9b4edc25d6389fd413"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"binbi\/MoMo-70B-V1.2_1",
        "Average":72.16,
        "ARC":70.9,
        "HellaSwag":86.47,
        "MMLU":69.95,
        "TruthfulQA":61.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"45056003b42a1cb5a6b2a0f338f85ec925a0587b"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_8-7B-slerp",
        "Average":72.15,
        "ARC":71.08,
        "HellaSwag":87.75,
        "MMLU":65.26,
        "TruthfulQA":64.52,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"421bade4749a466f975ca86a672c05218e5dc1ef"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/flammen3X",
        "Average":72.14,
        "ARC":70.14,
        "HellaSwag":87.37,
        "MMLU":64.69,
        "TruthfulQA":66.37,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"15cdba52e2175b0d6003274642260d6b45cc57e1"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/MelangeB-70b",
        "Average":72.14,
        "ARC":71.67,
        "HellaSwag":87.5,
        "MMLU":70.03,
        "TruthfulQA":59.36,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"08239fb1e30b1e42b14370f23e942bc51e76027c"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"argilla\/distilabeled-Marcoro14-7B-slerp",
        "Average":72.13,
        "ARC":70.73,
        "HellaSwag":87.47,
        "MMLU":65.22,
        "TruthfulQA":65.1,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"baa14c82695e595b5d39f35068898feb6fdceb34"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/TheTop-5x7B-Instruct-S3-v0.1",
        "Average":72.13,
        "ARC":70.9,
        "HellaSwag":88.0,
        "MMLU":65.13,
        "TruthfulQA":64.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"cce7c94dc1d178234c3616730b203c2e52f80ed2"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Swisslex\/Mixtral-Orca-v0.1",
        "Average":72.12,
        "ARC":69.71,
        "HellaSwag":88.88,
        "MMLU":66.06,
        "TruthfulQA":63.85,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9adcd9c408cce6c9c5e403dfda429bf90184a3e9"
    },
    {
        "T":"\u2b55",
        "Model":"brucethemoose\/CaPlatTessDolXaBoros-Yi-34B-200K-DARE-Ties-HighDensity",
        "Average":72.11,
        "ARC":67.41,
        "HellaSwag":85.77,
        "MMLU":77.44,
        "TruthfulQA":57.84,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"17fe477d833b16aab50bef843bc8bf196a2710ac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lizpreciatior\/lzlv_70b_fp16_hf",
        "Average":72.1,
        "ARC":70.14,
        "HellaSwag":87.54,
        "MMLU":70.23,
        "TruthfulQA":60.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-2.0",
        "#Params (B)":68.98,
        "Hub":62,
        "Available on the hub":true,
        "Model Sha":"b366c0bb318ae592023cca894cc6b4421a607a0d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/C0325-L",
        "Average":72.1,
        "ARC":67.58,
        "HellaSwag":87.43,
        "MMLU":74.72,
        "TruthfulQA":58.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"130e9c0ef3ca571a17a07d18ac00d655f7880245"
    },
    {
        "T":"?",
        "Model":"saishf\/West-Hermes-7B",
        "Average":72.09,
        "ARC":71.67,
        "HellaSwag":87.6,
        "MMLU":64.83,
        "TruthfulQA":64.26,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"9cd172b853949228761dfa65dfec57746475d703"
    },
    {
        "T":"\u2b55",
        "Model":"leejunhyeok\/MoMo-70B-LoRA-V1.2_1",
        "Average":72.09,
        "ARC":70.65,
        "HellaSwag":86.4,
        "MMLU":69.9,
        "TruthfulQA":61.41,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":70.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"178d03ccf7e7f83019266396f326fe11382eb20a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JaeyeonKang\/CCK-v1.3.0-DPO",
        "Average":72.09,
        "ARC":67.49,
        "HellaSwag":86.48,
        "MMLU":66.57,
        "TruthfulQA":67.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"86818a7076320a0d25d0374b0b6ea096bf4d3404"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"decruz07\/kellemar-DPO-Orca-Distilled-7B-SLERP",
        "Average":72.08,
        "ARC":70.48,
        "HellaSwag":87.56,
        "MMLU":65.33,
        "TruthfulQA":64.97,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"efb6caff9804383600563a658ba18720ec3b2d11"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/bruphin-kappa",
        "Average":72.08,
        "ARC":70.05,
        "HellaSwag":87.38,
        "MMLU":64.9,
        "TruthfulQA":65.99,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"09f8eda59034c027ebcbabb4e81f5de4cb08d061"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/flammen12-mistral-7B",
        "Average":72.07,
        "ARC":69.03,
        "HellaSwag":87.34,
        "MMLU":64.92,
        "TruthfulQA":66.99,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fce30460b08824bdf1559a48cb2cdf1499c40e8b"
    },
    {
        "T":"?",
        "Model":"Test157t\/Prima-LelantaclesV6.25-7b",
        "Average":72.07,
        "ARC":69.11,
        "HellaSwag":87.29,
        "MMLU":64.42,
        "TruthfulQA":67.44,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"74cae4cc41a280382ae3930a373d158102d94d58"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"argilla\/DistilabelBeagle14-7B",
        "Average":72.07,
        "ARC":71.08,
        "HellaSwag":87.0,
        "MMLU":61.27,
        "TruthfulQA":68.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"a7c3dec7418b86dc4b6169d349d0f11199a222ab"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"smelborp\/MixtralOrochi8x7B-Alt",
        "Average":72.06,
        "ARC":67.92,
        "HellaSwag":86.25,
        "MMLU":70.06,
        "TruthfulQA":64.03,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2cbe1e99144674ff0570a6a38b75c4666ed16087"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SUSTech\/SUS-Chat-72B",
        "Average":72.06,
        "ARC":66.3,
        "HellaSwag":84.96,
        "MMLU":76.7,
        "TruthfulQA":60.27,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":71.04,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"869bbd5b18656e74b606bd775e2594809407603c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Radu1999\/MisterUkrainianDPO",
        "Average":72.05,
        "ARC":68.34,
        "HellaSwag":86.78,
        "MMLU":62.92,
        "TruthfulQA":70.18,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"358df4dc83d52399b5471f9aedeefbebce7209cb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051615\/A0304",
        "Average":72.05,
        "ARC":67.58,
        "HellaSwag":82.78,
        "MMLU":84.5,
        "TruthfulQA":53.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"48e7b56ee7e9454fed4d6729aad88cbe9c1e8a3d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"freeCS-dot-org\/ThetaWave-7B-v0.1",
        "Average":72.05,
        "ARC":68.09,
        "HellaSwag":86.33,
        "MMLU":62.11,
        "TruthfulQA":71.68,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f73322bf5c95ba61e9e72efdf930ec67055ecf57"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/bagel-34b-v0.2",
        "Average":72.05,
        "ARC":68.77,
        "HellaSwag":83.72,
        "MMLU":76.45,
        "TruthfulQA":59.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":35,
        "Available on the hub":true,
        "Model Sha":"bc599b31f468d46d873964a58cab78380366d934"
    },
    {
        "T":"?",
        "Model":"Sao10K\/Fimbulvetr-11B-v2",
        "Average":72.05,
        "ARC":70.14,
        "HellaSwag":87.79,
        "MMLU":66.83,
        "TruthfulQA":63.43,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":73,
        "Available on the hub":true,
        "Model Sha":"afc90bd0690d0cbedd01f22d1d6ef0e44f30b5f4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051615\/L0225",
        "Average":72.03,
        "ARC":68.17,
        "HellaSwag":82.73,
        "MMLU":83.04,
        "TruthfulQA":54.19,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a9a394bdfa0ba89c60a6af94c0760dc92777c9b3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Liberated-Qwen1.5-72B",
        "Average":72.02,
        "ARC":65.7,
        "HellaSwag":84.62,
        "MMLU":77.13,
        "TruthfulQA":60.64,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":72.0,
        "Hub":69,
        "Available on the hub":false,
        "Model Sha":"8761e9acb20bc475c095455fd754bf632e0f88f0"
    },
    {
        "T":"?",
        "Model":"Test157t\/Prima-LelantaclesV6-7b",
        "Average":72.02,
        "ARC":71.5,
        "HellaSwag":87.65,
        "MMLU":64.64,
        "TruthfulQA":64.29,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"5529f748fa5bbc1ab9d23487fecaac92c5ca74fe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/Kunoichi-DPO-v2-7B",
        "Average":72.02,
        "ARC":69.62,
        "HellaSwag":87.44,
        "MMLU":64.94,
        "TruthfulQA":66.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":54,
        "Available on the hub":false,
        "Model Sha":"f55aef05f6632a1407fcddcbc6729613b07e87e2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Fimbulvetr-11B-v2-Test-14",
        "Average":72.01,
        "ARC":70.05,
        "HellaSwag":87.79,
        "MMLU":66.78,
        "TruthfulQA":63.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":73,
        "Available on the hub":true,
        "Model Sha":"4095b989123d28da44717f4ec8d4bd01055f4650"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeontaek\/llama-2-70B-ensemble-v3",
        "Average":72.01,
        "ARC":68.52,
        "HellaSwag":87.16,
        "MMLU":68.15,
        "TruthfulQA":64.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4fff2168beb2f7d4ec18320138f0ef0209bb0c40"
    },
    {
        "T":"?",
        "Model":"Inv\/Konstanta-7B",
        "Average":72.01,
        "ARC":70.05,
        "HellaSwag":87.5,
        "MMLU":65.06,
        "TruthfulQA":65.43,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5a80b72967360cf997687b9d2b5d2ed8f167ed79"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cookinai\/BruinHermes",
        "Average":72.01,
        "ARC":70.14,
        "HellaSwag":87.07,
        "MMLU":65.22,
        "TruthfulQA":65.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"unknown",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"59db3aa4f37411d5c97a6182dcf5ecfe1757ee4a"
    },
    {
        "T":"?",
        "Model":"Sao10K\/Fimbulvetr-11B-v2",
        "Average":72.0,
        "ARC":70.14,
        "HellaSwag":87.77,
        "MMLU":66.68,
        "TruthfulQA":63.42,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":73,
        "Available on the hub":true,
        "Model Sha":"afc90bd0690d0cbedd01f22d1d6ef0e44f30b5f4"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_9-7B-dare_ties",
        "Average":71.98,
        "ARC":70.31,
        "HellaSwag":87.46,
        "MMLU":65.08,
        "TruthfulQA":65.08,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a9b18f357c2df94b5fa2c5b36c42aa6bf43acf8b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Liberated-Qwen1.5-72B",
        "Average":71.98,
        "ARC":65.7,
        "HellaSwag":84.58,
        "MMLU":77.08,
        "TruthfulQA":60.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":72.0,
        "Hub":69,
        "Available on the hub":false,
        "Model Sha":"8761e9acb20bc475c095455fd754bf632e0f88f0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Nous-Hermes-2-SUS-Chat-34B-Slerp",
        "Average":71.98,
        "ARC":66.72,
        "HellaSwag":84.97,
        "MMLU":77.0,
        "TruthfulQA":59.23,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"511cc63b3efca6f036fdbbe15f312d0e2b7e5cf5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ChaoticNeutrals\/This_is_fine_7B",
        "Average":71.97,
        "ARC":70.31,
        "HellaSwag":87.28,
        "MMLU":64.51,
        "TruthfulQA":65.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"75c6be75cd8eeea4bc06f0d46bfeeb803b19ff26"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Swisslex\/Mixtral-8x7b-DPO-v0.2",
        "Average":71.96,
        "ARC":70.39,
        "HellaSwag":87.73,
        "MMLU":71.03,
        "TruthfulQA":58.69,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d668832717c9331884680506c2fc843cd5269ec8"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/Cognate-7B-slerp",
        "Average":71.96,
        "ARC":70.48,
        "HellaSwag":87.33,
        "MMLU":64.85,
        "TruthfulQA":65.16,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"d2134aefee7d7bf22694991867bf2ba9ac992396"
    },
    {
        "T":"?",
        "Model":"froggeric\/WestLake-10.7B-v2",
        "Average":71.95,
        "ARC":71.16,
        "HellaSwag":87.93,
        "MMLU":63.81,
        "TruthfulQA":64.91,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"de1f0f286ef6d5a6e10627ac05f8cfb9baaa36a5"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/ComplectMaid-7B-slerp",
        "Average":71.95,
        "ARC":69.97,
        "HellaSwag":87.34,
        "MMLU":64.62,
        "TruthfulQA":65.88,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"ea7732f9ee3dbd7ef7c3dee9600eabab04837b4b"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/MelangeC-70b",
        "Average":71.94,
        "ARC":71.67,
        "HellaSwag":87.6,
        "MMLU":70.37,
        "TruthfulQA":58.13,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e54a2b924dec135f3fa2373933ab8485178cde1b"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/WestuccineBagel-7B-slerp",
        "Average":71.94,
        "ARC":69.37,
        "HellaSwag":86.53,
        "MMLU":64.8,
        "TruthfulQA":67.06,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f9f52bdbe8c668ccab4fe8d737b1005bbed14f34"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"argilla\/distilabeled-Marcoro14-7B-slerp-full",
        "Average":71.94,
        "ARC":70.65,
        "HellaSwag":87.55,
        "MMLU":65.33,
        "TruthfulQA":64.21,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"8a4b63ce6161a85d53a5ac9504a758e95ac052dd"
    },
    {
        "T":"?",
        "Model":"Eric111\/caTUNABeagle",
        "Average":71.93,
        "ARC":70.05,
        "HellaSwag":87.35,
        "MMLU":65.02,
        "TruthfulQA":65.31,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"21b7d3925fb53249f3aaaa268a18106e0cc7ae0d"
    },
    {
        "T":"?",
        "Model":"TeeZee\/Xwin-LM-70B-V0.1_Jannie",
        "Average":71.93,
        "ARC":71.16,
        "HellaSwag":86.86,
        "MMLU":69.56,
        "TruthfulQA":60.14,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3c4341265d5a1be0d7125bd0eadc455632b6cc1f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YouKnowMee\/Mistral-7b-instruct-v0.2-summ-sft-ed2",
        "Average":71.91,
        "ARC":71.42,
        "HellaSwag":87.42,
        "MMLU":64.32,
        "TruthfulQA":64.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7b7531cc63b452b67e4eefc45d89792af6a88e2f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/Bioxtral-4x7B-v0.1",
        "Average":71.91,
        "ARC":68.34,
        "HellaSwag":87.27,
        "MMLU":63.57,
        "TruthfulQA":68.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"d0f47005d5b5b925e8a9de21d311ce278cf57eb7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/Kunoichi-DPO-v2-7B",
        "Average":71.9,
        "ARC":69.37,
        "HellaSwag":87.42,
        "MMLU":64.83,
        "TruthfulQA":66.0,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":54,
        "Available on the hub":false,
        "Model Sha":"d7d33a1517c57b596162a71a48bc29c87d29d9aa"
    },
    {
        "T":"?",
        "Model":"GPT-3.5",
        "Average":71.9,
        "ARC":85.2,
        "HellaSwag":85.5,
        "MMLU":70.0,
        "TruthfulQA":47.0,
        "Type":"Unknown",
        "Precision":null,
        "Hub License":"llama2",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":null,
        "Model Sha":"tech report"
    },
    {
        "T":"\u2b55",
        "Model":"brucethemoose\/CaPlatTessDolXaBoros-Yi-34B-200K-DARE-Ties-ExtremeDensity",
        "Average":71.89,
        "ARC":66.89,
        "HellaSwag":85.69,
        "MMLU":77.35,
        "TruthfulQA":57.63,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"71c95f1971c4a47adc331859b91502bd0b790ce0"
    },
    {
        "T":"?",
        "Model":"freeCS-dot-org\/OpenAGI-testing-truthyDPO-1",
        "Average":71.89,
        "ARC":67.32,
        "HellaSwag":85.99,
        "MMLU":63.12,
        "TruthfulQA":71.12,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"07fc27e045d1388a9e0afb3bc12ac595c8cb34be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/MM-Orc-Vic-bagel-34b-c1000",
        "Average":71.88,
        "ARC":67.32,
        "HellaSwag":83.52,
        "MMLU":76.09,
        "TruthfulQA":60.57,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"dc7dfbece1b31665b0456476f67ef97a17bd2323"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lloorree\/kssht-dahj-70b",
        "Average":71.87,
        "ARC":70.82,
        "HellaSwag":87.3,
        "MMLU":70.43,
        "TruthfulQA":58.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":70.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"85901aab2c3faf09de5ba8e9d65ec03aee4b20e4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/Mixtral_7Bx6_MoE_35B",
        "Average":71.86,
        "ARC":69.97,
        "HellaSwag":86.82,
        "MMLU":64.91,
        "TruthfulQA":65.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e1b7ae70975e235240f8a6b998eab635f37eb342"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Nitral-AI\/Mika-Lelantacles-7b-Longtext",
        "Average":71.86,
        "ARC":67.66,
        "HellaSwag":86.34,
        "MMLU":63.29,
        "TruthfulQA":70.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"8968ff13e35f884ef69c1239e8fecef4578c4abb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/C0322-reft",
        "Average":71.86,
        "ARC":64.42,
        "HellaSwag":83.74,
        "MMLU":79.5,
        "TruthfulQA":59.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"153efc8ba5e9939536b2cab6d510e1762e11680d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cloudyu\/Mixtral_7Bx6_MoE_35B",
        "Average":71.86,
        "ARC":70.14,
        "HellaSwag":86.77,
        "MMLU":64.74,
        "TruthfulQA":65.79,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e1b7ae70975e235240f8a6b998eab635f37eb342"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Nous-Hermes-2-SUS-Chat-34B-Linear",
        "Average":71.83,
        "ARC":66.38,
        "HellaSwag":84.94,
        "MMLU":76.82,
        "TruthfulQA":59.19,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"91673157803a869009e04e588c15914f132fb46b"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_47-7B-dare_ties",
        "Average":71.82,
        "ARC":69.45,
        "HellaSwag":86.69,
        "MMLU":63.27,
        "TruthfulQA":67.86,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fd8469e0e4e69f30334012df659e8ca0131bc506"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/flammen9X-mistral-7B",
        "Average":71.82,
        "ARC":69.54,
        "HellaSwag":87.46,
        "MMLU":64.7,
        "TruthfulQA":65.57,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"89da027ee512f974cedb6481920488867fc15afa"
    },
    {
        "T":"?",
        "Model":"nlpguy\/Westgate",
        "Average":71.81,
        "ARC":71.42,
        "HellaSwag":88.14,
        "MMLU":65.11,
        "TruthfulQA":62.59,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c1bec7b7f725e02cde8c87a20f5928a535e4fa75"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Epiculous\/Crunchy-onion",
        "Average":71.81,
        "ARC":67.15,
        "HellaSwag":86.19,
        "MMLU":70.02,
        "TruthfulQA":63.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"agpl-3.0",
        "#Params (B)":46.7,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"53db5c3846780919b8838ed8e1415bb86c475247"
    },
    {
        "T":"?",
        "Model":"vishnukv\/newmerge",
        "Average":71.79,
        "ARC":69.2,
        "HellaSwag":87.05,
        "MMLU":64.93,
        "TruthfulQA":65.99,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a6f07d4c0e2ac155b568cff3dabfabdc0318e655"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/Mixtral_7Bx5_MoE_30B",
        "Average":71.79,
        "ARC":69.97,
        "HellaSwag":86.82,
        "MMLU":64.42,
        "TruthfulQA":65.97,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":29.79,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"323fba03ac21b03df8d04ab575741429cc509d7b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/A0121",
        "Average":71.79,
        "ARC":67.15,
        "HellaSwag":85.45,
        "MMLU":74.93,
        "TruthfulQA":59.61,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"2471023d6261084e06eabb6a43b3cf06d4b189d6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/Q",
        "Average":71.79,
        "ARC":66.98,
        "HellaSwag":85.67,
        "MMLU":75.13,
        "TruthfulQA":59.36,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5cae52b499bc74ee419426fccd935462c2635175"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"brucethemoose\/Yi-34B-200K-DARE-megamerge-v8",
        "Average":71.79,
        "ARC":67.75,
        "HellaSwag":86.06,
        "MMLU":77.03,
        "TruthfulQA":56.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"0823229057d02acb1c9dda173d6fb2ea3b46b0af"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Test-Instruct-Solar-v1",
        "Average":71.78,
        "ARC":70.39,
        "HellaSwag":87.76,
        "MMLU":66.33,
        "TruthfulQA":62.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ff1f0d2ad57618ad9bcf526d74d8304605ffd567"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xwin-LM\/Xwin-LM-70B-V0.1",
        "Average":71.78,
        "ARC":70.22,
        "HellaSwag":87.25,
        "MMLU":69.77,
        "TruthfulQA":59.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":209,
        "Available on the hub":true,
        "Model Sha":"d6c803a180e3d46c371f8d3cb3848b861596ccbc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051615\/L0223",
        "Average":71.77,
        "ARC":67.92,
        "HellaSwag":82.99,
        "MMLU":82.59,
        "TruthfulQA":53.59,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e2819a059ca4f349f9034f2e2096c2d2208875c6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/FrankeMerge-12.5B",
        "Average":71.74,
        "ARC":68.34,
        "HellaSwag":87.74,
        "MMLU":64.01,
        "TruthfulQA":66.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.48,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f0df5e0045003efaf87c1cb4d7016e6641a251df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"arlineka\/Brunhilde-2x7b-MOE-DPO-v.01.5",
        "Average":71.74,
        "ARC":69.54,
        "HellaSwag":87.02,
        "MMLU":64.93,
        "TruthfulQA":65.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d9bb402315f47764bf0f6002e513cd7e89c7c804"
    },
    {
        "T":"?",
        "Model":"fhai50032\/BeagleLake-7B",
        "Average":71.74,
        "ARC":70.39,
        "HellaSwag":87.38,
        "MMLU":64.25,
        "TruthfulQA":64.92,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"09289bf975106972d7e5b690c8891cf203660ddb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibndias\/Nous-Hermes-2-MoE-2x34B",
        "Average":71.74,
        "ARC":66.64,
        "HellaSwag":85.73,
        "MMLU":76.49,
        "TruthfulQA":58.08,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":60.81,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"af9757f0420e27e2a332cc16cbe1eeefe99cb5f1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/spicyboros-70b-2.2",
        "Average":71.74,
        "ARC":70.73,
        "HellaSwag":87.58,
        "MMLU":70.32,
        "TruthfulQA":58.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"533f7dda1e3fe462a0abb00671f9a48d5fd51093"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airoboros-l2-70b-2.2.1",
        "Average":71.73,
        "ARC":69.71,
        "HellaSwag":87.95,
        "MMLU":69.79,
        "TruthfulQA":59.49,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"eadc78a4a9e173bccdca7dc8d12a34e80317c66c"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"RatanRohith\/MistralBeagle-RS-7B-V0.1",
        "Average":71.73,
        "ARC":69.45,
        "HellaSwag":84.62,
        "MMLU":63.07,
        "TruthfulQA":69.78,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a96439634909a69b9f508195ed53f0b43b034e8e"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"bardsai\/jaskier-7b-dpo",
        "Average":71.73,
        "ARC":70.82,
        "HellaSwag":87.02,
        "MMLU":64.67,
        "TruthfulQA":64.41,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ddc14e22152cc16d6ba01cd6c4facc833e98e6b5"
    },
    {
        "T":"?",
        "Model":"fhai50032\/RolePlayLake-7B",
        "Average":71.73,
        "ARC":70.56,
        "HellaSwag":87.42,
        "MMLU":64.55,
        "TruthfulQA":64.38,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"067200540bfab02c1aad895e709075f1416279f7"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_52-7B-dare_ties",
        "Average":71.72,
        "ARC":69.03,
        "HellaSwag":87.15,
        "MMLU":64.94,
        "TruthfulQA":65.76,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c514c74a60cc9f525b3b1c64aafb4e33bb22b1ea"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"macadeliccc\/Orca-SOLAR-4x10.7b",
        "Average":71.72,
        "ARC":68.52,
        "HellaSwag":86.78,
        "MMLU":67.03,
        "TruthfulQA":64.54,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":36.1,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"620ed061bad27da7c0e4d1342ec431986d01477f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Test157t\/Kunocchini-7b",
        "Average":71.71,
        "ARC":67.49,
        "HellaSwag":86.85,
        "MMLU":63.89,
        "TruthfulQA":68.62,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"a8bf199949b35a6037d197ffc21ae5c26fd1947b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"garage-bAInd\/Camel-Platypus2-70B",
        "Average":71.7,
        "ARC":71.08,
        "HellaSwag":87.6,
        "MMLU":70.04,
        "TruthfulQA":58.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"b9f8de09ab860ee8ba570db7227c5444020ea056"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KaeriJenti\/kaori-70b-v1",
        "Average":71.7,
        "ARC":69.8,
        "HellaSwag":87.36,
        "MMLU":70.82,
        "TruthfulQA":58.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"fcce042311a54925ae4acdbe33cff535859300b2"
    },
    {
        "T":"\u2b55",
        "Model":"brucethemoose\/Yi-34B-200K-DARE-merge-v5",
        "Average":71.67,
        "ARC":66.47,
        "HellaSwag":85.54,
        "MMLU":77.22,
        "TruthfulQA":57.46,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"72d2469926f0277d31b13ce2db78e454b24a91b0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeontaek\/llama-2-70B-ensemble-v2",
        "Average":71.67,
        "ARC":68.77,
        "HellaSwag":85.36,
        "MMLU":68.04,
        "TruthfulQA":64.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7ee1ce68410f18e2bff12925fcc8354f4a7410cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PetroGPT\/Severus-7B-DPO",
        "Average":71.67,
        "ARC":70.22,
        "HellaSwag":87.09,
        "MMLU":64.93,
        "TruthfulQA":64.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9893dc24b32bc83ca63e7d06cfa296d66be3fb3d"
    },
    {
        "T":"\u2b55",
        "Model":"garage-bAInd\/Camel-Platypus2-70B",
        "Average":71.66,
        "ARC":71.25,
        "HellaSwag":87.61,
        "MMLU":69.65,
        "TruthfulQA":58.15,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"b9f8de09ab860ee8ba570db7227c5444020ea056"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"yunconglong\/7Bx4_DPO",
        "Average":71.66,
        "ARC":69.37,
        "HellaSwag":86.89,
        "MMLU":64.73,
        "TruthfulQA":65.66,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":24.15,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"d26e345f256b8a8210637258a5973fd36227d8ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Himitsui\/Kaiju-11B",
        "Average":71.66,
        "ARC":69.97,
        "HellaSwag":87.72,
        "MMLU":66.79,
        "TruthfulQA":62.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"cb9c9b0fb1d49b085069617bd8dc9cdddfdba7fb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fhai50032\/RPLakeCoder-TxC",
        "Average":71.65,
        "ARC":70.39,
        "HellaSwag":87.36,
        "MMLU":64.48,
        "TruthfulQA":64.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f4e336066a1410547ebcf603f09c15071e646d8f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fhai50032\/RPLakeCoder-TxC",
        "Average":71.65,
        "ARC":70.39,
        "HellaSwag":87.35,
        "MMLU":64.5,
        "TruthfulQA":64.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f4e336066a1410547ebcf603f09c15071e646d8f"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Swisslex\/Mixtral-8x7b-DPO-v0.1",
        "Average":71.64,
        "ARC":70.9,
        "HellaSwag":87.61,
        "MMLU":70.66,
        "TruthfulQA":57.38,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5de7f1532fdeaf36f7ffb180ff510efac2ac90e4"
    },
    {
        "T":"?",
        "Model":"llmixer\/BigWeave-v12-90b",
        "Average":71.64,
        "ARC":68.09,
        "HellaSwag":87.7,
        "MMLU":69.41,
        "TruthfulQA":61.35,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":87.8,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"4518c1d85135efdb14ed8d3581d325ea2167d6b4"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/RoyalMaid-7B-slerp",
        "Average":71.64,
        "ARC":70.39,
        "HellaSwag":87.25,
        "MMLU":64.72,
        "TruthfulQA":64.18,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ec8bc0ad5bb403a29d9124cf6e3183ff423acc23"
    },
    {
        "T":"\u2b55",
        "Model":"vicgalle\/solarized-18B-dpo",
        "Average":71.63,
        "ARC":68.34,
        "HellaSwag":87.79,
        "MMLU":63.89,
        "TruthfulQA":66.49,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":17.93,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"92f4d0deff86b73f0ac57367c1f86d3b22575530"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051611\/V0202",
        "Average":71.63,
        "ARC":66.55,
        "HellaSwag":82.75,
        "MMLU":86.32,
        "TruthfulQA":50.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6f115d4ae1b6015420558aa5684c530f41c0e7da"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/Pluto_24B_DPO_200",
        "Average":71.61,
        "ARC":65.61,
        "HellaSwag":86.38,
        "MMLU":64.59,
        "TruthfulQA":69.86,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":24.15,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"3e6ced019d4cab13bd0ca2cefbf91bc7bba31375"
    },
    {
        "T":"?",
        "Model":"mvpmaster\/Einstein-4d-Marcoro14-nddmpk-KrishnaHercules-7b-slerp",
        "Average":71.61,
        "ARC":69.71,
        "HellaSwag":87.04,
        "MMLU":65.32,
        "TruthfulQA":64.37,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"597d83856e63194f19cd25c12408876f6e9263ec"
    },
    {
        "T":"?",
        "Model":"Kukedlc\/NeuralKukedlc-7B-Labonned",
        "Average":71.6,
        "ARC":70.82,
        "HellaSwag":86.99,
        "MMLU":64.49,
        "TruthfulQA":64.1,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"241c8f778b910c202ad6b80a536d7a43e41803f2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Himitsui\/KuroMitsu-11B",
        "Average":71.6,
        "ARC":70.31,
        "HellaSwag":88.07,
        "MMLU":66.66,
        "TruthfulQA":61.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.6,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"7bd8487fc3a5c3bac022bfe8c34d2f630c123d40"
    },
    {
        "T":"?",
        "Model":"grimjim\/kuno-kunoichi-v1-DPO-v2-SLERP-7B",
        "Average":71.59,
        "ARC":69.11,
        "HellaSwag":87.33,
        "MMLU":64.8,
        "TruthfulQA":65.12,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"88d5b63f7d62baeab4704b72ed656aa8bee3a2fb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"carsenk\/flippa-exp26-v3-7b",
        "Average":71.59,
        "ARC":68.09,
        "HellaSwag":86.5,
        "MMLU":64.42,
        "TruthfulQA":67.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2dda3515c5bbf02824addbe2e8f924a48ce21156"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mncai\/agiin-11.1B-v0.0",
        "Average":71.58,
        "ARC":67.32,
        "HellaSwag":86.35,
        "MMLU":64.99,
        "TruthfulQA":67.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.17,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0b086b46a672f450d7b2e8c307526e62d8d0cfdf"
    },
    {
        "T":"?",
        "Model":"Kukedlc\/NeuralKukedlc-7B-Labonned",
        "Average":71.57,
        "ARC":70.73,
        "HellaSwag":86.9,
        "MMLU":64.58,
        "TruthfulQA":64.09,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"241c8f778b910c202ad6b80a536d7a43e41803f2"
    },
    {
        "T":"\u2b55",
        "Model":"bhenrym14\/platypus-yi-34b",
        "Average":71.56,
        "ARC":68.43,
        "HellaSwag":85.21,
        "MMLU":78.13,
        "TruthfulQA":54.48,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"66abec7cba89b35c7b6cab2140c3532049de0157"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-70B-v1.2",
        "Average":71.56,
        "ARC":70.48,
        "HellaSwag":86.98,
        "MMLU":70.13,
        "TruthfulQA":58.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"9b92ee1093b125035ba1649dca6f4ceb9d86a656"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lloorree\/jfdslijsijdgis",
        "Average":71.55,
        "ARC":69.62,
        "HellaSwag":87.31,
        "MMLU":70.0,
        "TruthfulQA":59.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1e67eaa4ef618a5a0d8c52e5e107635c706b34c5"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"v1olet\/v1olet_merged_dpo_7B",
        "Average":71.54,
        "ARC":71.33,
        "HellaSwag":87.34,
        "MMLU":64.13,
        "TruthfulQA":63.37,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"299011bf619d9b89f4e545dde8ef7853ec0557b6"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/bruphin-iota",
        "Average":71.54,
        "ARC":68.43,
        "HellaSwag":86.55,
        "MMLU":65.02,
        "TruthfulQA":66.17,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f9e91c9a891e87d9a8808cb4d73300c364be6402"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/FettuccineLake-DPO-7B-slerp",
        "Average":71.54,
        "ARC":67.92,
        "HellaSwag":86.37,
        "MMLU":63.24,
        "TruthfulQA":68.64,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"fbbbda1a8c1c00f65768391f608bd922dd757bf6"
    },
    {
        "T":"?",
        "Model":"mvpmaster\/kellemar-KrishnaHercules-0.1-7b-slerp",
        "Average":71.54,
        "ARC":70.22,
        "HellaSwag":87.29,
        "MMLU":65.61,
        "TruthfulQA":63.03,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a5366279542a62faff7443f238f8490566517429"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Vasanth\/Valor_Macaroni_moe",
        "Average":71.54,
        "ARC":70.31,
        "HellaSwag":86.62,
        "MMLU":64.57,
        "TruthfulQA":64.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dbd8fcc7b2987cc3a1802561f63e483e8871aadb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Nous-Hermes-2-SUS-Chat-2x34B",
        "Average":71.52,
        "ARC":66.81,
        "HellaSwag":85.22,
        "MMLU":76.65,
        "TruthfulQA":57.42,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":60.81,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"fd230896bc3a1cfabdf37f8d8389cd670ea72faa"
    },
    {
        "T":"?",
        "Model":"Kukedlc\/NeuralMaths-Experiment-7b",
        "Average":71.51,
        "ARC":69.71,
        "HellaSwag":87.48,
        "MMLU":65.01,
        "TruthfulQA":63.83,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f8a6b011d046f04f0ec6c5af909590553cc36170"
    },
    {
        "T":"\u2b55",
        "Model":"v2ray\/LLaMA-2-Wizard-70B-QLoRA",
        "Average":71.5,
        "ARC":67.58,
        "HellaSwag":87.52,
        "MMLU":69.11,
        "TruthfulQA":61.79,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":70.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"4bff676fe29f56d31961794c062aebc36312446e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YouKnwMe\/Mistral-7b-instruct-v0.2-private-edw2",
        "Average":71.49,
        "ARC":69.88,
        "HellaSwag":87.33,
        "MMLU":64.85,
        "TruthfulQA":63.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d4cffeadbb02eaf6273e954fc5aea4f745747705"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"adamo1139\/Yi-34B-200K-AEZAKMI-RAW-1701",
        "Average":71.49,
        "ARC":66.81,
        "HellaSwag":85.79,
        "MMLU":75.44,
        "TruthfulQA":57.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"46eae309ae80b25832a2e7d21023239ac4acfdb3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"quantumaikr\/llama-2-70b-fb16-orca-chat-10k",
        "Average":71.48,
        "ARC":68.09,
        "HellaSwag":87.07,
        "MMLU":69.21,
        "TruthfulQA":61.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"697aaeb8eb9905c9b25bebb736d1905444c774a6"
    },
    {
        "T":"?",
        "Model":"openagi-project\/OpenAGI-7B-v0.1",
        "Average":71.48,
        "ARC":66.72,
        "HellaSwag":86.13,
        "MMLU":63.53,
        "TruthfulQA":69.55,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"36123d69a4dc871fe962337e997c5c5ccf6e739b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openagi-project\/OpenAGI-7B-v0.1-test-ada",
        "Average":71.48,
        "ARC":66.72,
        "HellaSwag":86.13,
        "MMLU":63.53,
        "TruthfulQA":69.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":103,
        "Available on the hub":false,
        "Model Sha":"635f0d8e3bc9fe0c5fb3954614cef0a324d0bfc8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"v2ray\/LLaMA-2-Wizard-70B-QLoRA",
        "Average":71.48,
        "ARC":67.75,
        "HellaSwag":87.5,
        "MMLU":68.96,
        "TruthfulQA":61.72,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":70.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"4bff676fe29f56d31961794c062aebc36312446e"
    },
    {
        "T":"?",
        "Model":"kimou605\/shadow-clown-BioMistral-7B-DARE",
        "Average":71.48,
        "ARC":67.41,
        "HellaSwag":86.78,
        "MMLU":64.07,
        "TruthfulQA":67.68,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"47b399337a8d0294a58ed27a343e9056af845925"
    },
    {
        "T":"?",
        "Model":"shadowml\/Daredevil-7B",
        "Average":71.48,
        "ARC":69.37,
        "HellaSwag":87.17,
        "MMLU":65.3,
        "TruthfulQA":64.09,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"015a362ee09e6370ad5b1b70fad8a7ebfcdc9e74"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/Daredevil-7B",
        "Average":71.48,
        "ARC":69.37,
        "HellaSwag":87.17,
        "MMLU":65.3,
        "TruthfulQA":64.09,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"015a362ee09e6370ad5b1b70fad8a7ebfcdc9e74"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-70b-3.1.2",
        "Average":71.48,
        "ARC":70.14,
        "HellaSwag":86.88,
        "MMLU":69.72,
        "TruthfulQA":59.19,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"2de01b0a516bc64859abb16a948733d616dfb6d3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Solstice-11B-v1",
        "Average":71.48,
        "ARC":70.56,
        "HellaSwag":87.39,
        "MMLU":65.98,
        "TruthfulQA":61.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.6,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"cb2392b7d1913ea071c56ba4224966c70109a3c2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051611\/whattest",
        "Average":71.47,
        "ARC":66.81,
        "HellaSwag":84.43,
        "MMLU":76.59,
        "TruthfulQA":58.04,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"598102a9a810986c05b9aa216507be57d93de4fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JaeyeonKang\/CCK_Gony_v3.2",
        "Average":71.46,
        "ARC":69.45,
        "HellaSwag":86.91,
        "MMLU":70.68,
        "TruthfulQA":58.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5fd0b772372a0d908279db56c5bec064e842e029"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YouKnwMe\/Mistral-7b-instruct-v0.2-private-edw2",
        "Average":71.46,
        "ARC":69.8,
        "HellaSwag":87.32,
        "MMLU":64.9,
        "TruthfulQA":63.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d4cffeadbb02eaf6273e954fc5aea4f745747705"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"yunconglong\/7Bx4_DPO_2e",
        "Average":71.46,
        "ARC":68.94,
        "HellaSwag":86.8,
        "MMLU":64.5,
        "TruthfulQA":65.6,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":24.15,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b2535b271d83f892de2fb3a790b298618565dcff"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"macadeliccc\/SOLAR-math-2x10.7b",
        "Average":71.46,
        "ARC":68.43,
        "HellaSwag":86.31,
        "MMLU":66.9,
        "TruthfulQA":64.21,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":19.19,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"10953f7a3d411b148dcbb4363b1508d0efc303a2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abideen\/NexoNimbus-7B",
        "Average":71.45,
        "ARC":70.82,
        "HellaSwag":87.86,
        "MMLU":64.69,
        "TruthfulQA":62.43,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"12f194df2152bd4b9431b25e06fff9e47713d03d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kquant03\/CognitiveFusion-4x7B-bf16-MoE",
        "Average":71.44,
        "ARC":67.41,
        "HellaSwag":86.16,
        "MMLU":65.14,
        "TruthfulQA":67.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"40c2fdf46e33f4f84742fff63d5fb46932492e03"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"stabilityai\/StableBeluga2",
        "Average":71.42,
        "ARC":71.08,
        "HellaSwag":86.37,
        "MMLU":68.79,
        "TruthfulQA":59.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":823,
        "Available on the hub":true,
        "Model Sha":"e4944caa6ece819413b140b8dcecea79fe7e22cf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/WestSeverus-7B",
        "Average":71.41,
        "ARC":70.31,
        "HellaSwag":87.46,
        "MMLU":64.98,
        "TruthfulQA":62.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"309fb3fd2e71c9a93f9840fecfa754c7e82c346e"
    },
    {
        "T":"?",
        "Model":"jpechg\/Sour-Marcoro-12.5B",
        "Average":71.41,
        "ARC":67.92,
        "HellaSwag":83.7,
        "MMLU":65.85,
        "TruthfulQA":68.17,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.48,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"abe2e19a88077dd184782946963148e8fc62ed05"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/IamSoTired-7B-slerp",
        "Average":71.41,
        "ARC":69.88,
        "HellaSwag":87.15,
        "MMLU":64.85,
        "TruthfulQA":63.75,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"a389a96d4bf973529d47564b0fe44685422e23cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"quantumaikr\/llama-2-70b-fb16-guanaco-1k",
        "Average":71.41,
        "ARC":70.48,
        "HellaSwag":87.33,
        "MMLU":70.25,
        "TruthfulQA":57.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"c317af1b593a4f91b0e79c7142ca75f1e8d65278"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shadowml\/Marcoro14-7B-ties",
        "Average":71.39,
        "ARC":69.8,
        "HellaSwag":87.13,
        "MMLU":65.11,
        "TruthfulQA":63.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"060737a4e7e8619b8d7c1180a6cc5b1a7c1d87fa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/Marcoro14-7B-slerp",
        "Average":71.39,
        "ARC":69.8,
        "HellaSwag":87.13,
        "MMLU":65.11,
        "TruthfulQA":63.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":26,
        "Available on the hub":false,
        "Model Sha":"25f7e124456a5ad5c8c032088eb573d3e520d411"
    },
    {
        "T":"?",
        "Model":"shadowml\/Marcoro14-7B-slerp",
        "Average":71.39,
        "ARC":69.8,
        "HellaSwag":87.13,
        "MMLU":65.11,
        "TruthfulQA":63.54,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"25f7e124456a5ad5c8c032088eb573d3e520d411"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/Maidphin-Kunoichi-7B",
        "Average":71.38,
        "ARC":69.37,
        "HellaSwag":87.11,
        "MMLU":64.78,
        "TruthfulQA":64.27,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a5fcfee8080ca5ced84f0d25a70c91368e142318"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Q-bert\/Terminis-7B",
        "Average":71.38,
        "ARC":67.92,
        "HellaSwag":86.22,
        "MMLU":64.07,
        "TruthfulQA":67.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"c3cde866d7d3da1173be8593c91e5bf143ea616e"
    },
    {
        "T":"\u2b55",
        "Model":"mncai\/yi-34B-v3",
        "Average":71.38,
        "ARC":67.06,
        "HellaSwag":85.11,
        "MMLU":75.8,
        "TruthfulQA":57.54,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"f7605af56f29b42e72f9c2cbbd4ad8e443a8dae0"
    },
    {
        "T":"?",
        "Model":"mychen76\/openmixtral-6x7b-v2",
        "Average":71.37,
        "ARC":68.52,
        "HellaSwag":86.75,
        "MMLU":65.11,
        "TruthfulQA":65.13,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":35.43,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e38efbb07209b4e8484e4820317eb02eb95ef23f"
    },
    {
        "T":"\u2b55",
        "Model":"adamo1139\/Yi-34B-200K-AEZAKMI-v2",
        "Average":71.37,
        "ARC":67.92,
        "HellaSwag":85.61,
        "MMLU":75.22,
        "TruthfulQA":56.74,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"a7c90fa652ca4b65f4e2db1126be0f884748b7ab"
    },
    {
        "T":"\u2b55",
        "Model":"garage-bAInd\/Camel-Platypus2-70B",
        "Average":71.36,
        "ARC":70.14,
        "HellaSwag":87.71,
        "MMLU":69.83,
        "TruthfulQA":57.77,
        "Type":"instruction-tuned",
        "Precision":"8bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"6f958a1063fe1e6075f6e379fae621ff5a1d98c6"
    },
    {
        "T":"?",
        "Model":"adonlee\/Mistral_7B_SFT_DPO_v0",
        "Average":71.36,
        "ARC":66.3,
        "HellaSwag":84.9,
        "MMLU":64.53,
        "TruthfulQA":69.72,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"03955e2748064dcfac121e35e4e060cf6f48e259"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"NousResearch\/Nous-Hermes-2-Mixtral-8x7B-DPO",
        "Average":71.36,
        "ARC":71.42,
        "HellaSwag":87.21,
        "MMLU":72.28,
        "TruthfulQA":54.53,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":321,
        "Available on the hub":false,
        "Model Sha":"6ba531f1aec62375bf94ad9c7bb064953c4e9868"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TeeZee\/2xbagel-dpo-34b-v0.2",
        "Average":71.35,
        "ARC":65.27,
        "HellaSwag":79.35,
        "MMLU":73.64,
        "TruthfulQA":67.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":56.7,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"9d7e28d41f1f3221d5fefc48ed495eb921ad4be6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mncai\/agiin-13.6B-v0.0",
        "Average":71.34,
        "ARC":69.45,
        "HellaSwag":86.59,
        "MMLU":61.94,
        "TruthfulQA":67.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"631e80949b055193053c802437f3a31fe4e1390d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cognitivecomputations\/MegaDolphin-120b",
        "Average":71.34,
        "ARC":69.03,
        "HellaSwag":87.8,
        "MMLU":69.26,
        "TruthfulQA":59.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":120.32,
        "Hub":65,
        "Available on the hub":true,
        "Model Sha":"561d22376c354903641165d6691eb4df9405a4cf"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AbacusResearch\/RasGulla1-7b",
        "Average":71.34,
        "ARC":69.71,
        "HellaSwag":87.4,
        "MMLU":64.94,
        "TruthfulQA":63.31,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3c71b5b8900e6271acceb30b2085b8a964f79473"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Hermes-2-Mixtral-8x7B-DPO",
        "Average":71.34,
        "ARC":71.08,
        "HellaSwag":87.29,
        "MMLU":72.17,
        "TruthfulQA":54.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":321,
        "Available on the hub":false,
        "Model Sha":"566cdea53950f86eb51dae62812c29e79405cffe"
    },
    {
        "T":"?",
        "Model":"saishf\/Fimbulvetr-Kuro-Lotus-10.7B",
        "Average":71.34,
        "ARC":69.54,
        "HellaSwag":87.87,
        "MMLU":66.99,
        "TruthfulQA":60.95,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"b41d174c2041e8661086e4eb939480641a5c66dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-70B-v1.1",
        "Average":71.34,
        "ARC":70.05,
        "HellaSwag":87.12,
        "MMLU":70.34,
        "TruthfulQA":57.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"05a13f6adfe95a713dff04dc2eaa214c77c2512a"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/dolphin-2.2-70b",
        "Average":71.33,
        "ARC":70.05,
        "HellaSwag":85.97,
        "MMLU":69.18,
        "TruthfulQA":60.14,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":43,
        "Available on the hub":true,
        "Model Sha":"6a2ddfb2ddde603dae91420db019682378aa9d5e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"viethq188\/LeoScorpius-7B",
        "Average":71.32,
        "ARC":69.28,
        "HellaSwag":87.01,
        "MMLU":65.04,
        "TruthfulQA":63.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"02e11fa83d18975f95c5d5047d0439897308c73b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051611\/A0113",
        "Average":71.32,
        "ARC":66.38,
        "HellaSwag":84.86,
        "MMLU":74.39,
        "TruthfulQA":59.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6e36ad0a41135265185038d1d88062d9fb11e8d5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-70B",
        "Average":71.32,
        "ARC":69.45,
        "HellaSwag":87.11,
        "MMLU":68.91,
        "TruthfulQA":59.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"d63dfdd0baed756981f5f78f7419fd822c572362"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JaeyeonKang\/CCK_Gony_v0.2",
        "Average":71.31,
        "ARC":68.86,
        "HellaSwag":86.61,
        "MMLU":70.33,
        "TruthfulQA":59.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ec7ea7c16e1a8b5968cab37aab70d926c8ec341d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"samir-fama\/SamirGPT-v1",
        "Average":71.31,
        "ARC":69.54,
        "HellaSwag":87.04,
        "MMLU":65.3,
        "TruthfulQA":63.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"8e8abca2d9703dff2d60de78b013360a9a3f4d5e"
    },
    {
        "T":"?",
        "Model":"Inv\/Konstanta-V4-Alpha-7B",
        "Average":71.31,
        "ARC":69.28,
        "HellaSwag":87.06,
        "MMLU":64.95,
        "TruthfulQA":63.94,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"8e98c2ba7f8adae6151e32cef1c607cfd40ede7a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/Lelantos-7B",
        "Average":71.3,
        "ARC":69.03,
        "HellaSwag":86.9,
        "MMLU":64.1,
        "TruthfulQA":65.18,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"ec060c6a4f5e45370cf2e2d65ecb388b048b0fdb"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"mncai\/agiin-13.6B-v0.1",
        "Average":71.3,
        "ARC":69.45,
        "HellaSwag":86.64,
        "MMLU":61.15,
        "TruthfulQA":67.97,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.78,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"6c93ca1d60b09b9b91e15c57dc8525827d371798"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-M-Creative-v1.0",
        "Average":71.29,
        "ARC":66.81,
        "HellaSwag":85.14,
        "MMLU":75.54,
        "TruthfulQA":57.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":32,
        "Available on the hub":true,
        "Model Sha":"26923a2648b9864e2ec6f0cc66b8b6fcfbbdd491"
    },
    {
        "T":"?",
        "Model":"AurelPx\/Dare-k-7B-ties",
        "Average":71.28,
        "ARC":69.11,
        "HellaSwag":87.08,
        "MMLU":65.02,
        "TruthfulQA":63.91,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"35851b86f6d9d367a23a0d9ff7ebd5ceb89ebf34"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"EmbeddedLLM\/Mistral-7B-Merge-14-v0.2",
        "Average":71.28,
        "ARC":68.86,
        "HellaSwag":87.01,
        "MMLU":65.05,
        "TruthfulQA":64.19,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"819c48aa6cf2b1f722a824027ceab8247e957e79"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mychen76\/mistral-7b-merged-dare",
        "Average":71.27,
        "ARC":69.71,
        "HellaSwag":87.05,
        "MMLU":65.07,
        "TruthfulQA":63.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"914aa317f9bee2fb8cf290b8e603394dfa8d3406"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kquant03\/Eukaryote-8x7B-bf16",
        "Average":71.27,
        "ARC":69.45,
        "HellaSwag":87.29,
        "MMLU":65.15,
        "TruthfulQA":63.17,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5ccee182c62fcbadfe91f66b74590aea40b181e5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/strix-rufipes-70b",
        "Average":71.26,
        "ARC":71.33,
        "HellaSwag":87.86,
        "MMLU":69.13,
        "TruthfulQA":56.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"ecb80c1bd98fae238ff5c61d41e75daa4c16a02c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azazelle\/Bianca-7b",
        "Average":71.26,
        "ARC":69.71,
        "HellaSwag":86.11,
        "MMLU":65.25,
        "TruthfulQA":63.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c962dd3095ffa6ff247f3e3cef2b44f9ba500894"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/bagel-8x7b-v0.2",
        "Average":71.25,
        "ARC":68.26,
        "HellaSwag":86.32,
        "MMLU":70.4,
        "TruthfulQA":60.03,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"614649ce0bd9a03fd24963de70655e5f8d4354b0"
    },
    {
        "T":"?",
        "Model":"Nitral-AI\/Prima-LelantaclesV7-experimentalv2-7b",
        "Average":71.24,
        "ARC":68.09,
        "HellaSwag":85.87,
        "MMLU":62.87,
        "TruthfulQA":68.14,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d420ef8df86709571c0485d3eae6d116e4dabd3d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"macadeliccc\/piccolo-8x7b",
        "Average":71.23,
        "ARC":69.62,
        "HellaSwag":86.98,
        "MMLU":64.13,
        "TruthfulQA":64.17,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":46.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"08440f35fb0fe0334942fe59dc116c901d55b0a0"
    },
    {
        "T":"\u2b55",
        "Model":"kyujinpy\/PlatYi-34B-Llama",
        "Average":71.23,
        "ARC":67.83,
        "HellaSwag":85.35,
        "MMLU":78.26,
        "TruthfulQA":53.46,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":34.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"e641a44c60ddf1f31d898ca53810ccb1e7a30972"
    },
    {
        "T":"?",
        "Model":"R136a1\/InfinityKuno-2x7B",
        "Average":71.21,
        "ARC":69.62,
        "HellaSwag":87.44,
        "MMLU":64.49,
        "TruthfulQA":63.28,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"382698efc4e5ff54a4155e1f2c40547ac3b2aa64"
    },
    {
        "T":"?",
        "Model":"mayacinka\/NeuralZephyr-Beagle-7B",
        "Average":71.2,
        "ARC":68.6,
        "HellaSwag":86.38,
        "MMLU":64.67,
        "TruthfulQA":65.17,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"91fb2de32d29aec936e54c6edeea4ae778259b00"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"databricks\/dbrx-base",
        "Average":71.2,
        "ARC":66.04,
        "HellaSwag":89.0,
        "MMLU":74.7,
        "TruthfulQA":55.07,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"8c2724d4715132a4a6f3e520cd2b6c6189848c11"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"occultml\/CatMarcoro14-7B-slerp",
        "Average":71.2,
        "ARC":69.37,
        "HellaSwag":86.92,
        "MMLU":65.27,
        "TruthfulQA":63.24,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"07d9e69a880d1c31c29c932f4fae6c36ceda01ea"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"GreenNode\/Merged-DPO-7B",
        "Average":71.2,
        "ARC":68.94,
        "HellaSwag":87.75,
        "MMLU":55.35,
        "TruthfulQA":72.76,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"1c0e61c7da6839fe4cc34433b899c5416fadbe18"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_16-7B-slerp",
        "Average":71.2,
        "ARC":69.03,
        "HellaSwag":87.15,
        "MMLU":65.65,
        "TruthfulQA":62.97,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"04c11fb1df83af9a52139e45fc2fc34b3386a37f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RaduGabriel\/SirUkrainian",
        "Average":71.18,
        "ARC":67.32,
        "HellaSwag":85.54,
        "MMLU":63.14,
        "TruthfulQA":68.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"556e8d80252fb83b65ddb71e866bbb9eaffd17cf"
    },
    {
        "T":"?",
        "Model":"Eric111\/MistInst-v0.2_ochat-3.5-0106_dpo-binarized-NeuralTrix-7B",
        "Average":71.18,
        "ARC":69.71,
        "HellaSwag":85.86,
        "MMLU":61.23,
        "TruthfulQA":67.93,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4ff7df93a5285be5c58bd8e70fcff2757903ea6c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/Kunoichi-7B",
        "Average":71.18,
        "ARC":68.69,
        "HellaSwag":87.1,
        "MMLU":64.9,
        "TruthfulQA":64.04,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":66,
        "Available on the hub":false,
        "Model Sha":"cb731f23e65b8638143d88055e1db57b84fdd546"
    },
    {
        "T":"?",
        "Model":"Novocoders\/jaskier-7b-NeuralDPO",
        "Average":71.17,
        "ARC":73.46,
        "HellaSwag":88.16,
        "MMLU":63.15,
        "TruthfulQA":59.92,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"29652406c24f9d369d6bfd1ecc4979b2ab124c08"
    },
    {
        "T":"?",
        "Model":"Azazelle\/Bianca-7b",
        "Average":71.16,
        "ARC":69.45,
        "HellaSwag":86.08,
        "MMLU":65.08,
        "TruthfulQA":64.04,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c962dd3095ffa6ff247f3e3cef2b44f9ba500894"
    },
    {
        "T":"\u2b55",
        "Model":"elinas\/chronos007-70b",
        "Average":71.16,
        "ARC":70.14,
        "HellaSwag":87.52,
        "MMLU":69.33,
        "TruthfulQA":57.65,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"c775f87a56f00725de4263f8d527995d40f611c4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kukedlc\/neuronal-7b-Mlab",
        "Average":71.16,
        "ARC":69.97,
        "HellaSwag":86.79,
        "MMLU":64.51,
        "TruthfulQA":63.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"91a49d4c156ecb21e8477529e6b957242ba3829e"
    },
    {
        "T":"?",
        "Model":"jeiku\/Eros_Prodigadigm_7B",
        "Average":71.15,
        "ARC":67.24,
        "HellaSwag":85.63,
        "MMLU":63.04,
        "TruthfulQA":68.68,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"50598e1c289866d0937eb789d1543414e67814f3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ChaoticNeutrals\/InfinityNexus_9B",
        "Average":71.15,
        "ARC":68.69,
        "HellaSwag":86.28,
        "MMLU":64.49,
        "TruthfulQA":65.14,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":8.99,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"bf9b2d78febb994f4dd12ff870a2b2da265cc379"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"CultriX\/CultriX-MoE-BF16",
        "Average":71.15,
        "ARC":68.94,
        "HellaSwag":86.96,
        "MMLU":65.2,
        "TruthfulQA":63.47,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":18.52,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"76e3315cc3294e86ca4f348a473f5c232e50600d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"EmbeddedLLM\/Mistral-7B-Merge-14-v0.1",
        "Average":71.14,
        "ARC":69.11,
        "HellaSwag":86.7,
        "MMLU":65.34,
        "TruthfulQA":63.43,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":23,
        "Available on the hub":false,
        "Model Sha":"3ddae31382dd3f7e654c1fc0e9b37f2e7f4ede92"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"leveldevai\/MBA-7B",
        "Average":71.14,
        "ARC":69.45,
        "HellaSwag":87.22,
        "MMLU":65.16,
        "TruthfulQA":62.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b5b539f244c5f2fae4eff2095c5d75e60707fcc0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051611\/A0109",
        "Average":71.11,
        "ARC":66.55,
        "HellaSwag":84.7,
        "MMLU":74.44,
        "TruthfulQA":58.75,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"2a9cd40c67e0b17d94a0eedafd3d116245613709"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dillfrescott\/amadeus-v0.1",
        "Average":71.11,
        "ARC":68.94,
        "HellaSwag":86.98,
        "MMLU":64.69,
        "TruthfulQA":63.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"wtfpl",
        "#Params (B)":24.15,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"2d6dcf8bf9f1a758f135929de4a6fd81e26a38da"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Slerp-CM-mist-dpo",
        "Average":71.09,
        "ARC":69.62,
        "HellaSwag":87.09,
        "MMLU":64.81,
        "TruthfulQA":62.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"ea3b28f8b829e08dfd0c7310da78bd483ab29bbe"
    },
    {
        "T":"?",
        "Model":"Inv\/Konstanta-Gamma-10.9B",
        "Average":71.08,
        "ARC":68.26,
        "HellaSwag":87.38,
        "MMLU":64.5,
        "TruthfulQA":64.18,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.9,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"f9457027c05107889a948a6f3b99ba428e859d48"
    },
    {
        "T":"\u2b55",
        "Model":"kyujinpy\/PlatYi-34B-LoRA",
        "Average":71.08,
        "ARC":67.15,
        "HellaSwag":85.37,
        "MMLU":78.46,
        "TruthfulQA":53.32,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"5dcc36255b4632ba32a6b940fa43d53764a3fae3"
    },
    {
        "T":"?",
        "Model":"mlabonne\/NeuralDarewin-7B",
        "Average":71.08,
        "ARC":70.14,
        "HellaSwag":86.4,
        "MMLU":64.85,
        "TruthfulQA":62.92,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"72267d131001da8cdf253105c367fd913db79523"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cognitivecomputations\/laserxtral",
        "Average":71.07,
        "ARC":69.03,
        "HellaSwag":86.76,
        "MMLU":64.68,
        "TruthfulQA":63.8,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-2.0",
        "#Params (B)":24.15,
        "Hub":76,
        "Available on the hub":false,
        "Model Sha":"91e0a33fd2cb0a77401831e96536b91c5b7817e4"
    },
    {
        "T":"?",
        "Model":"TeeZee\/Xwin-LM-70B-V0.1_Limarpv3",
        "Average":71.06,
        "ARC":70.82,
        "HellaSwag":86.97,
        "MMLU":69.28,
        "TruthfulQA":57.15,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"80a9d5efe8d6d7189cea710c31c244db3b203fc0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051611\/A0110",
        "Average":71.05,
        "ARC":66.38,
        "HellaSwag":84.73,
        "MMLU":74.48,
        "TruthfulQA":58.6,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1a15b9aa4acf1327164672edd16ee966b8bc3691"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_4-7B-slerp",
        "Average":71.05,
        "ARC":69.45,
        "HellaSwag":87.01,
        "MMLU":65.33,
        "TruthfulQA":62.4,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"716875bb40a5f526cdcb33c629866175e220db55"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/A0120",
        "Average":71.05,
        "ARC":67.06,
        "HellaSwag":85.15,
        "MMLU":74.49,
        "TruthfulQA":57.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5416fc460666cec3cd10c4798f58765e03e67b42"
    },
    {
        "T":"?",
        "Model":"jan-hq\/supermario-slerp-v2",
        "Average":71.03,
        "ARC":69.71,
        "HellaSwag":86.54,
        "MMLU":64.82,
        "TruthfulQA":63.06,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"dd27a200fd3dd5500a0b5bbfc0e4a9289af486e5"
    },
    {
        "T":"\u2b55",
        "Model":"mncai\/yi-34B-v2",
        "Average":71.03,
        "ARC":66.13,
        "HellaSwag":85.0,
        "MMLU":75.64,
        "TruthfulQA":57.34,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"bf7696c10077e73d06752c564ea35cc7e5e336ca"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JaeyeonKang\/CCK-v2.0-DPO",
        "Average":71.03,
        "ARC":65.87,
        "HellaSwag":86.81,
        "MMLU":62.1,
        "TruthfulQA":69.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"84ceccea3e3cde7348a07f3e2bfb1f58b07b38ee"
    },
    {
        "T":"?",
        "Model":"pabloce\/Dolphin-2.8-slerp",
        "Average":71.02,
        "ARC":68.0,
        "HellaSwag":86.51,
        "MMLU":64.38,
        "TruthfulQA":65.2,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f7f1cdd89c9c1c59512e041c654e7314322ea274"
    },
    {
        "T":"?",
        "Model":"pabloce\/Dolphin-2.8-slerp",
        "Average":71.01,
        "ARC":68.0,
        "HellaSwag":86.43,
        "MMLU":64.39,
        "TruthfulQA":65.22,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1da8b3280f67eeafcdf83e0bd696a1071c579aa3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"APMIC\/caigun-lora-model-34B-v2",
        "Average":71.0,
        "ARC":65.02,
        "HellaSwag":85.28,
        "MMLU":75.69,
        "TruthfulQA":58.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"913eec9411d7886c0e8abe6842ed09d8932bef10"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jan-ai\/Pandora-13B-v1",
        "Average":71.0,
        "ARC":67.06,
        "HellaSwag":87.53,
        "MMLU":63.65,
        "TruthfulQA":65.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.48,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"16013ee5682ef9b38c8f27a2c2b78956befdbe52"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cstr\/Spaetzle-v8-7b",
        "Average":71.0,
        "ARC":68.69,
        "HellaSwag":86.68,
        "MMLU":64.6,
        "TruthfulQA":64.05,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"4de312664e059136d304f261755ba77baba94bb8"
    },
    {
        "T":"?",
        "Model":"cstr\/Spaetzle-v8-7b",
        "Average":71.0,
        "ARC":68.69,
        "HellaSwag":86.66,
        "MMLU":64.59,
        "TruthfulQA":64.06,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"4de312664e059136d304f261755ba77baba94bb8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lloorree\/jfdslijsijdgis",
        "Average":70.99,
        "ARC":69.62,
        "HellaSwag":86.95,
        "MMLU":69.17,
        "TruthfulQA":58.2,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1e67eaa4ef618a5a0d8c52e5e107635c706b34c5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Radu1999\/MisterUkrainian",
        "Average":70.99,
        "ARC":67.83,
        "HellaSwag":86.32,
        "MMLU":62.53,
        "TruthfulQA":67.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c828c2831c162fa80a5faec73de6b87551bfb6fc"
    },
    {
        "T":"?",
        "Model":"giraffe176\/WestMaid_HermesMonarchv0.1",
        "Average":70.98,
        "ARC":70.22,
        "HellaSwag":87.42,
        "MMLU":64.31,
        "TruthfulQA":61.99,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a2c52c805979eac66e38a9e17d8fb19326665649"
    },
    {
        "T":"?",
        "Model":"macadeliccc\/piccolo-math-2x7b",
        "Average":70.98,
        "ARC":69.11,
        "HellaSwag":87.27,
        "MMLU":63.69,
        "TruthfulQA":63.86,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":12.88,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b7cd9398c8797b4e90cdd90ec9f64300e6334e6a"
    },
    {
        "T":"?",
        "Model":"mvpmaster\/Einstein-4D-MoE-2x7b-test",
        "Average":70.98,
        "ARC":69.71,
        "HellaSwag":86.52,
        "MMLU":65.41,
        "TruthfulQA":62.29,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"30e1c404409a838456b3ec24456653384bcb3bfc"
    },
    {
        "T":"?",
        "Model":"seyf1elislam\/WestKunai-Hermes-10.7b-test",
        "Average":70.97,
        "ARC":68.09,
        "HellaSwag":87.1,
        "MMLU":64.43,
        "TruthfulQA":64.28,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"76887e42e7d48d55de29561b1306e1fe0d308466"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Test157t\/Pasta-PrimaMaid-7b",
        "Average":70.97,
        "ARC":67.92,
        "HellaSwag":86.18,
        "MMLU":63.31,
        "TruthfulQA":66.47,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"c00e0ed6d1c118b377faeabde26a4620cc94930d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_101",
        "Average":70.97,
        "ARC":68.69,
        "HellaSwag":86.42,
        "MMLU":69.92,
        "TruthfulQA":58.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"884c53a64a3c5faf7b0706d36a587ca1532ed8f5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"augtoma\/qCammel-70",
        "Average":70.97,
        "ARC":68.34,
        "HellaSwag":87.87,
        "MMLU":70.18,
        "TruthfulQA":57.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"augtoma\/qCammel70",
        "Average":70.97,
        "ARC":68.34,
        "HellaSwag":87.87,
        "MMLU":70.18,
        "TruthfulQA":57.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"augtoma\/qCammel-70x",
        "Average":70.97,
        "ARC":68.34,
        "HellaSwag":87.87,
        "MMLU":70.18,
        "TruthfulQA":57.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"augtoma\/qCammel-70v1",
        "Average":70.97,
        "ARC":68.34,
        "HellaSwag":87.87,
        "MMLU":70.18,
        "TruthfulQA":57.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"augtoma\/qCammel-70-x",
        "Average":70.97,
        "ARC":68.34,
        "HellaSwag":87.87,
        "MMLU":70.18,
        "TruthfulQA":57.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/flammen6-mistral-7B",
        "Average":70.96,
        "ARC":69.2,
        "HellaSwag":86.99,
        "MMLU":64.17,
        "TruthfulQA":63.48,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1729027e13f914361b151c4e91b1c02d2a4e63c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cookinai\/DonutLM-v1",
        "Average":70.96,
        "ARC":69.11,
        "HellaSwag":85.91,
        "MMLU":65.45,
        "TruthfulQA":63.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"710e480608d7f9bd42bbc1d90046580f1ffdbe04"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"janhq\/supermario-slerp-v2",
        "Average":70.96,
        "ARC":69.37,
        "HellaSwag":86.6,
        "MMLU":64.91,
        "TruthfulQA":62.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"662c68ece38bcc8cb7b04dc2c0f5d6c03f8d56e0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"v1olet\/v1olet_marcoroni-go-bruins-merge-7B",
        "Average":70.95,
        "ARC":70.05,
        "HellaSwag":87.17,
        "MMLU":65.17,
        "TruthfulQA":61.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":29,
        "Available on the hub":false,
        "Model Sha":"aca5d9df596ac1f9ddffbec3de282ecbe3b32d68"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/multimaster-7b-v2",
        "Average":70.95,
        "ARC":70.48,
        "HellaSwag":87.59,
        "MMLU":65.09,
        "TruthfulQA":60.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":35.43,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"777deaba78991d3786f3db6a513a63695170f52d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/TheTop-5x7B-Instruct-S2-v0.1",
        "Average":70.94,
        "ARC":69.45,
        "HellaSwag":87.15,
        "MMLU":64.98,
        "TruthfulQA":62.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"db8fde5fb86be6414c42d71ff6d8bf44ae4275ad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lloorree\/kssht-castor-70b",
        "Average":70.94,
        "ARC":69.54,
        "HellaSwag":87.53,
        "MMLU":70.38,
        "TruthfulQA":56.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":70.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e49a6bdc5e6024fb0e60dbba4601b346b4369377"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-70b-gpt4-1.4.1",
        "Average":70.93,
        "ARC":70.39,
        "HellaSwag":87.82,
        "MMLU":70.31,
        "TruthfulQA":55.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub":46,
        "Available on the hub":true,
        "Model Sha":"ea98153fa721ed7110c77e73388e3b6f3996f2bb"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/flammen2",
        "Average":70.93,
        "ARC":68.94,
        "HellaSwag":86.87,
        "MMLU":64.78,
        "TruthfulQA":63.12,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0d7f9b756f3b173a9d88ff9fe539bd5b6ea542da"
    },
    {
        "T":"?",
        "Model":"mvpmaster\/pmmpk-EinstainMorcoro14KrishnaHercules-7b-slerp",
        "Average":70.92,
        "ARC":69.28,
        "HellaSwag":86.59,
        "MMLU":65.13,
        "TruthfulQA":62.69,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"298b37e7607e7dbe6ca50ceacbf62c4f53300e7e"
    },
    {
        "T":"?",
        "Model":"R136a1\/InfinityKumon-2x7B",
        "Average":70.92,
        "ARC":69.62,
        "HellaSwag":87.09,
        "MMLU":64.97,
        "TruthfulQA":61.99,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1cf1b3a313de0b3b22a61dd3741c1bd5a3d14c66"
    },
    {
        "T":"\u2b55",
        "Model":"Steelskull\/Lumosia-MoE-4x10.7",
        "Average":70.92,
        "ARC":68.34,
        "HellaSwag":87.13,
        "MMLU":64.38,
        "TruthfulQA":63.81,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":36.1,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"0027074811e8901b63a27cc6d95db66fdafe8c90"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Doctor-Shotgun\/mythospice-70b",
        "Average":70.92,
        "ARC":69.28,
        "HellaSwag":87.53,
        "MMLU":70.1,
        "TruthfulQA":56.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"b00992c26604c9cd496bc41472a05e4c01cd2008"
    },
    {
        "T":"\u2b55",
        "Model":"SUSTech\/SUS-Chat-34B",
        "Average":70.91,
        "ARC":66.3,
        "HellaSwag":83.91,
        "MMLU":76.41,
        "TruthfulQA":57.04,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":33.93,
        "Hub":111,
        "Available on the hub":true,
        "Model Sha":"01f1a7861667c4869bb03251dfd10526bf846e9c"
    },
    {
        "T":"?",
        "Model":"brucethemoose\/Capybara-Tess-Yi-34B-200K",
        "Average":70.91,
        "ARC":66.13,
        "HellaSwag":86.24,
        "MMLU":74.89,
        "TruthfulQA":56.37,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"28a4464d357d9a4d91238d20ed30ecd2ee377be5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cookinai\/CM-14",
        "Average":70.9,
        "ARC":69.37,
        "HellaSwag":86.97,
        "MMLU":65.37,
        "TruthfulQA":61.9,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7ab2f7eedca7ec6a6463ba4b2f822a06e47b4cd4"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/ToppyLake-7B-slerp",
        "Average":70.89,
        "ARC":69.2,
        "HellaSwag":86.98,
        "MMLU":64.85,
        "TruthfulQA":62.54,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6d083dd5e571749ad0c574897abf473600142b16"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"APMIC\/caigun-lora-model-34B-v3",
        "Average":70.89,
        "ARC":66.89,
        "HellaSwag":84.77,
        "MMLU":75.41,
        "TruthfulQA":56.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e5a17f9fbd39259cc166c8c75b81be2b41f029f1"
    },
    {
        "T":"?",
        "Model":"Azure99\/blossom-v4-yi-34b",
        "Average":70.87,
        "ARC":66.81,
        "HellaSwag":84.44,
        "MMLU":74.34,
        "TruthfulQA":57.89,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":33.93,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"d1bdf5a5ea942b8236e48c17c3c07e3bd49ae5c8"
    },
    {
        "T":"?",
        "Model":"localfultonextractor\/Erosumika-7B-v2",
        "Average":70.85,
        "ARC":65.61,
        "HellaSwag":86.29,
        "MMLU":62.51,
        "TruthfulQA":69.0,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"d391a01d8277f80b159ca4c06a4316b771241be6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"damerajee\/Oot-v2_lll",
        "Average":70.85,
        "ARC":69.28,
        "HellaSwag":86.6,
        "MMLU":64.96,
        "TruthfulQA":62.57,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3d4a013ad5763822280ca13e804d52c432e4fc0f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/A0106",
        "Average":70.84,
        "ARC":66.47,
        "HellaSwag":85.05,
        "MMLU":74.03,
        "TruthfulQA":57.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"215cbefbc69d22c28181651b5b964c329ca09f59"
    },
    {
        "T":"\u2b55",
        "Model":"kyujinpy\/PlatYi-34B-Llama-Q-FastChat",
        "Average":70.84,
        "ARC":66.13,
        "HellaSwag":85.25,
        "MMLU":78.37,
        "TruthfulQA":53.62,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":34.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"dab86ae57fe51dc5e993769ebb69a173637852bc"
    },
    {
        "T":"?",
        "Model":"snorkelai\/Snorkel-Mistral-PairRM-DPO",
        "Average":70.84,
        "ARC":66.04,
        "HellaSwag":85.64,
        "MMLU":60.83,
        "TruthfulQA":70.86,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":96,
        "Available on the hub":false,
        "Model Sha":"e4110a5689e146bc07296218f84ae09129168868"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"VitalContribution\/Evangelion-7B",
        "Average":70.84,
        "ARC":68.94,
        "HellaSwag":86.45,
        "MMLU":63.97,
        "TruthfulQA":64.01,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"7e3fdb60969ef0f7219cbcb9b05f7d1537af1c8d"
    },
    {
        "T":"?",
        "Model":"snorkelai\/Snorkel-Mistral-PairRM-DPO",
        "Average":70.84,
        "ARC":65.96,
        "HellaSwag":85.63,
        "MMLU":60.85,
        "TruthfulQA":70.91,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":96,
        "Available on the hub":false,
        "Model Sha":"e4110a5689e146bc07296218f84ae09129168868"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Fimbulvetr-10.7B-v1",
        "Average":70.84,
        "ARC":68.94,
        "HellaSwag":87.27,
        "MMLU":66.59,
        "TruthfulQA":60.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":32,
        "Available on the hub":true,
        "Model Sha":"bff7146aafe1a5b84631bd279112c8c5b95d2802"
    },
    {
        "T":"?",
        "Model":"mvpmaster\/NeuralDareDMistralPro-7b-slerp",
        "Average":70.83,
        "ARC":69.03,
        "HellaSwag":86.74,
        "MMLU":63.46,
        "TruthfulQA":64.12,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"21bd46abf818ff01115148f35ecac201be3de1e8"
    },
    {
        "T":"\u2b55",
        "Model":"kyujinpy\/PlatYi-34B-Llama-Q",
        "Average":70.83,
        "ARC":65.7,
        "HellaSwag":85.22,
        "MMLU":78.78,
        "TruthfulQA":53.64,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":34.39,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"b786e11fafdd446f155fdb14c6112800f210801b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lloorree\/kssht-euripedes-70b",
        "Average":70.83,
        "ARC":69.8,
        "HellaSwag":87.59,
        "MMLU":70.43,
        "TruthfulQA":55.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":70.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"04ae5f2187697a7e9a2d57f327a7131f23d3e927"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/A0106",
        "Average":70.83,
        "ARC":66.38,
        "HellaSwag":85.05,
        "MMLU":74.0,
        "TruthfulQA":57.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"215cbefbc69d22c28181651b5b964c329ca09f59"
    },
    {
        "T":"?",
        "Model":"sethuiyer\/Nandine-7b",
        "Average":70.81,
        "ARC":69.28,
        "HellaSwag":87.01,
        "MMLU":64.83,
        "TruthfulQA":62.1,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6fe9ea49efd6024e45e352c63815efdb7d0fe35d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Eric111\/NeuralBeagleOpenChat",
        "Average":70.77,
        "ARC":70.31,
        "HellaSwag":86.26,
        "MMLU":65.62,
        "TruthfulQA":60.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"27f19157ab816247b969cafa84642f37fe841913"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/JustToSuffer-7B-slerp",
        "Average":70.77,
        "ARC":68.94,
        "HellaSwag":86.79,
        "MMLU":64.66,
        "TruthfulQA":62.69,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"9253d79afb8ee7441804448f6542878a6e05f098"
    },
    {
        "T":"?",
        "Model":"ResplendentAI\/Persephone_7B",
        "Average":70.76,
        "ARC":66.72,
        "HellaSwag":85.59,
        "MMLU":63.23,
        "TruthfulQA":67.51,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"0a920569f922d3c900364947e381c046a8d0674d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dfurman\/llama-2-70b-dolphin-peft",
        "Average":70.76,
        "ARC":69.62,
        "HellaSwag":86.82,
        "MMLU":69.18,
        "TruthfulQA":57.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":70.0,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"a1190dee60b5854e80d340958dc3cc956bc56f68"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Deacon-34b-qlora-adapter",
        "Average":70.75,
        "ARC":64.85,
        "HellaSwag":85.56,
        "MMLU":76.38,
        "TruthfulQA":56.21,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"97d19d88f629f6d5270126de7ba1400d3b89a6c6"
    },
    {
        "T":"?",
        "Model":"Inv\/Konstanta-Alpha-V2-7B",
        "Average":70.74,
        "ARC":69.62,
        "HellaSwag":87.14,
        "MMLU":65.11,
        "TruthfulQA":61.08,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"8ee5b2131f391940dd7e09c107c02a4bfca0d7bb"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/flammen",
        "Average":70.73,
        "ARC":68.17,
        "HellaSwag":87.06,
        "MMLU":64.68,
        "TruthfulQA":63.02,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bf4d9b8534c5b782052512686135d4e8464e471c"
    },
    {
        "T":"?",
        "Model":"Eric111\/Mayo",
        "Average":70.73,
        "ARC":70.14,
        "HellaSwag":86.27,
        "MMLU":65.58,
        "TruthfulQA":60.93,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"c9e75f7275ae8f8f8db94fea1d24a4855db96060"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/SynthIA-70B-v1.5",
        "Average":70.73,
        "ARC":69.37,
        "HellaSwag":86.97,
        "MMLU":69.16,
        "TruthfulQA":57.4,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":42,
        "Available on the hub":true,
        "Model Sha":"40773af947d39495841d825337fdbc7ca977ef1f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"brucethemoose\/CapyTessBorosYi-34B-200K-DARE-Ties",
        "Average":70.72,
        "ARC":64.93,
        "HellaSwag":85.92,
        "MMLU":76.18,
        "TruthfulQA":55.84,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"0475128a0e57fc103e65c601be75013f28987e62"
    },
    {
        "T":"?",
        "Model":"jan-hq\/supermario-slerp-v3",
        "Average":70.72,
        "ARC":69.28,
        "HellaSwag":86.71,
        "MMLU":65.11,
        "TruthfulQA":61.77,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9bfa05ff62ddd960cb9fb3e9dff70d800ea1c0a1"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"01-ai\/Yi-34B",
        "Average":70.72,
        "ARC":64.59,
        "HellaSwag":85.69,
        "MMLU":76.35,
        "TruthfulQA":56.23,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":1219,
        "Available on the hub":false,
        "Model Sha":"cd8d59de87ea11c6453ee287ac82e5523f08c8ec"
    },
    {
        "T":"?",
        "Model":"Samee-ur\/NeuralPipe-7B-slerp-DPO",
        "Average":70.71,
        "ARC":69.28,
        "HellaSwag":86.34,
        "MMLU":63.7,
        "TruthfulQA":63.53,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"17af425d904f21f8500bf965b16d07603e01d125"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-70B-v1.2b",
        "Average":70.71,
        "ARC":68.77,
        "HellaSwag":87.57,
        "MMLU":68.81,
        "TruthfulQA":57.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"7b687d6e4101b8bb8cc4062f8a318d639098a55d"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Deacon-34b-Adapter",
        "Average":70.71,
        "ARC":64.76,
        "HellaSwag":85.57,
        "MMLU":76.28,
        "TruthfulQA":56.24,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4d1eca338cda2d7ecb0f0ea549819e7116d43178"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_7-7B-slerp",
        "Average":70.71,
        "ARC":69.88,
        "HellaSwag":87.66,
        "MMLU":64.85,
        "TruthfulQA":60.45,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fb638af990984b2d46ab4e85b1b164dec46c0f66"
    },
    {
        "T":"\u2b55",
        "Model":"deepseek-ai\/deepseek-llm-67b-chat",
        "Average":70.71,
        "ARC":67.75,
        "HellaSwag":86.82,
        "MMLU":72.42,
        "TruthfulQA":55.85,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":66.59,
        "Hub":157,
        "Available on the hub":true,
        "Model Sha":"79648bef7658bb824e4630740f6e1484c1b0620b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kquant03\/FrankenDPO-4x7B-bf16",
        "Average":70.7,
        "ARC":68.69,
        "HellaSwag":86.07,
        "MMLU":64.93,
        "TruthfulQA":63.14,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"547eac8651e32eb9a59019696faf19c372b25016"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/multimaster-7b-v3",
        "Average":70.7,
        "ARC":70.39,
        "HellaSwag":87.65,
        "MMLU":65.07,
        "TruthfulQA":59.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":35.43,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6ee0b7c59743c3047f307643c7c1f13ada56fdd1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mychen76\/openmixtral-4x7b-merged",
        "Average":70.7,
        "ARC":69.45,
        "HellaSwag":86.75,
        "MMLU":65.29,
        "TruthfulQA":61.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c1a0afc53bd0643e098d88688e8520fc9f7f2f3f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LeroyDyer\/Mixtral_AI_Cyber_3.m2",
        "Average":70.7,
        "ARC":67.41,
        "HellaSwag":86.88,
        "MMLU":63.9,
        "TruthfulQA":64.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a1fa9aabfcbeccf98bca82ab29e20bd584f862c8"
    },
    {
        "T":"?",
        "Model":"AbacusResearch\/haLLAwa3",
        "Average":70.7,
        "ARC":67.83,
        "HellaSwag":87.02,
        "MMLU":64.23,
        "TruthfulQA":63.71,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e8bea52793d4ae58e0772918ea1727808565fc75"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"samir-fama\/FernandoGPT-v1",
        "Average":70.69,
        "ARC":69.45,
        "HellaSwag":86.94,
        "MMLU":65.19,
        "TruthfulQA":61.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"a26fbae35874a6aafb02e39fd8a623022b9e2a95"
    },
    {
        "T":"\u2b55",
        "Model":"kyujinpy\/PlatYi-34B-Q",
        "Average":70.68,
        "ARC":66.89,
        "HellaSwag":85.14,
        "MMLU":77.66,
        "TruthfulQA":53.03,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"187442aa0d250dc3c44451d71bf8fcdd556bdb24"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cookinai\/CatMacaroni14",
        "Average":70.67,
        "ARC":69.11,
        "HellaSwag":86.92,
        "MMLU":65.07,
        "TruthfulQA":61.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"66f6d076cf5396d4cecc08696addf12567c55a85"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-deepseek-67b-v15.2",
        "Average":70.67,
        "ARC":68.6,
        "HellaSwag":86.37,
        "MMLU":71.5,
        "TruthfulQA":56.2,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":67.42,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"c3caef28f8402d52d6a646a7e1e00a971db1c507"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Doctor-Shotgun\/mythospice-limarp-70b",
        "Average":70.66,
        "ARC":69.2,
        "HellaSwag":87.46,
        "MMLU":70.14,
        "TruthfulQA":55.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":68.72,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"ff29fed2a33fc050fd20d0e25b5b23c4a101b074"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"bardsai\/jaskier-7b-dpo-v2",
        "Average":70.66,
        "ARC":69.28,
        "HellaSwag":86.8,
        "MMLU":64.92,
        "TruthfulQA":61.64,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a0c0f4f9d4fbfe0a688d1d58b98b30f0ca6fc9bd"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airoboros-l2-70b-2.1",
        "Average":70.66,
        "ARC":70.65,
        "HellaSwag":86.81,
        "MMLU":69.17,
        "TruthfulQA":56.0,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":33,
        "Available on the hub":true,
        "Model Sha":"b512d6cc06dcc41201e564fd4ca78cd6f8e8e6da"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051615\/A0306",
        "Average":70.65,
        "ARC":66.04,
        "HellaSwag":83.47,
        "MMLU":80.04,
        "TruthfulQA":53.05,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d869c99f5b2b456a483bca7d6cc1ec7f797cdbf1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"deepseek-ai\/deepseek-llm-67b-chat",
        "Average":70.64,
        "ARC":67.75,
        "HellaSwag":86.8,
        "MMLU":72.19,
        "TruthfulQA":55.83,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":66.59,
        "Hub":157,
        "Available on the hub":true,
        "Model Sha":"79648bef7658bb824e4630740f6e1484c1b0620b"
    },
    {
        "T":"?",
        "Model":"ChaoticNeutrals\/Kool-Aid_7B",
        "Average":70.64,
        "ARC":67.49,
        "HellaSwag":86.13,
        "MMLU":63.82,
        "TruthfulQA":65.12,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"6ab7387d794bc8f1a8da2091b6d2b46739d41bb3"
    },
    {
        "T":"?",
        "Model":"Locutusque\/Hyperion-3.0-Yi-34B",
        "Average":70.64,
        "ARC":64.59,
        "HellaSwag":85.61,
        "MMLU":75.98,
        "TruthfulQA":56.38,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"8c0a5ae12a331fe2709733331961ab433e4cec95"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Toten5\/Marcoroni-v3-neural-chat-v3-3-Slerp",
        "Average":70.63,
        "ARC":68.77,
        "HellaSwag":86.55,
        "MMLU":64.51,
        "TruthfulQA":62.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"0223ffb3f70272009d0d76923f40cb31f3d2347e"
    },
    {
        "T":"?",
        "Model":"adamo1139\/Yi-34B-200K-AEZAKMI-RAW-2301",
        "Average":70.63,
        "ARC":66.04,
        "HellaSwag":84.7,
        "MMLU":74.89,
        "TruthfulQA":56.89,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"32621cf1ace03f976b1a73f899817a76a4111bdb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/llama-2-70b-Guanaco-QLoRA-fp16",
        "Average":70.63,
        "ARC":68.26,
        "HellaSwag":88.32,
        "MMLU":70.23,
        "TruthfulQA":55.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub":52,
        "Available on the hub":true,
        "Model Sha":"54b0e39d5e9aee7b323f50b0a26db15295c3d5c9"
    },
    {
        "T":"\u2b55",
        "Model":"quantumaikr\/QuantumLM-llama2-70B-Korean-LoRA",
        "Average":70.61,
        "ARC":70.56,
        "HellaSwag":86.39,
        "MMLU":69.41,
        "TruthfulQA":56.08,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":70.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ea21456e999f6ce35da1cd88b8f62bb5770b985a"
    },
    {
        "T":"?",
        "Model":"eldogbbhed\/NeuralPearlBeagle",
        "Average":70.61,
        "ARC":68.26,
        "HellaSwag":87.25,
        "MMLU":64.05,
        "TruthfulQA":62.85,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f2ef27a21d1e88981118c9f9ddba88affff4d41b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hfl\/chinese-mixtral-instruct",
        "Average":70.6,
        "ARC":67.75,
        "HellaSwag":85.67,
        "MMLU":71.53,
        "TruthfulQA":57.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"219c9d65843f4c7356e5efffe399a7208e0dea25"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Chupacabra-7B-v2.01",
        "Average":70.59,
        "ARC":68.86,
        "HellaSwag":86.12,
        "MMLU":63.9,
        "TruthfulQA":63.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"438642201e2a91e9456d2a8ca1d7443e5ec55a40"
    },
    {
        "T":"?",
        "Model":"wenbopan\/Faro-Yi-34B-200K",
        "Average":70.58,
        "ARC":66.55,
        "HellaSwag":83.53,
        "MMLU":76.6,
        "TruthfulQA":55.64,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":34.39,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"0d29b76f7eaf927ea8e9ce4c5d4488858f2b2d4e"
    },
    {
        "T":"?",
        "Model":"Nitral-AI\/Eris_PrimeV3.075-Vision-7B",
        "Average":70.58,
        "ARC":68.26,
        "HellaSwag":86.44,
        "MMLU":64.9,
        "TruthfulQA":62.72,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"1925727c03448e3930efe0b9491013667fcaab9d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Walmart-the-bag\/Solar-10.7B-Cato",
        "Average":70.57,
        "ARC":68.69,
        "HellaSwag":86.16,
        "MMLU":65.76,
        "TruthfulQA":61.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"16b230f4e663902787254ecb1781c255b7dcc6ea"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rwitz2\/pee",
        "Average":70.57,
        "ARC":69.88,
        "HellaSwag":86.89,
        "MMLU":64.95,
        "TruthfulQA":60.56,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"eb3b3b6b25c31a7805d672059e06d4eace586a28"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"one-man-army\/una-neural-chat-v3-3-P2-OMA",
        "Average":70.57,
        "ARC":67.32,
        "HellaSwag":86.33,
        "MMLU":63.14,
        "TruthfulQA":65.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"7bab67e479c192927c4a781efdf5be27eaa315a8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cookinai\/OpenCM-14",
        "Average":70.56,
        "ARC":69.28,
        "HellaSwag":86.89,
        "MMLU":65.01,
        "TruthfulQA":61.07,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"744228e768a6d117952ead1d981c410dd0d3ce4d"
    },
    {
        "T":"\u2b55",
        "Model":"quantumaikr\/quantumairk-llama-2-70B-instruct",
        "Average":70.56,
        "ARC":70.31,
        "HellaSwag":87.05,
        "MMLU":70.46,
        "TruthfulQA":54.42,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"94ff2fcafd507b08e953f70806ec671ec3d17b15"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Chupacabra-8x7B-MoE",
        "Average":70.56,
        "ARC":68.77,
        "HellaSwag":86.11,
        "MMLU":63.86,
        "TruthfulQA":63.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"4df8e16bb4adeff6cfdd6c064819650ae27ff8fa"
    },
    {
        "T":"?",
        "Model":"ContextualAI\/Contextual_KTO_Mistral_PairRM",
        "Average":70.56,
        "ARC":64.76,
        "HellaSwag":85.52,
        "MMLU":60.28,
        "TruthfulQA":71.67,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":22,
        "Available on the hub":false,
        "Model Sha":"bdf7fe0202e81a9409ae92eada6804efa205d061"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"one-man-army\/una-neural-chat-v3-3-P2-OMA",
        "Average":70.56,
        "ARC":67.24,
        "HellaSwag":86.34,
        "MMLU":63.18,
        "TruthfulQA":65.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"7bab67e479c192927c4a781efdf5be27eaa315a8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_420",
        "Average":70.55,
        "ARC":70.14,
        "HellaSwag":87.73,
        "MMLU":70.35,
        "TruthfulQA":54.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"13c7b5f403c0f2af9bf7fce2d4a32deb9054c083"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cookinai\/CatMacaroni-Slerp",
        "Average":70.55,
        "ARC":69.28,
        "HellaSwag":86.88,
        "MMLU":65.02,
        "TruthfulQA":61.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"6045552b7283e50378fb5c3e31f75072c1bc91f8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andysalerno\/cloudymixtral7Bx2-nectar-0.2",
        "Average":70.54,
        "ARC":67.49,
        "HellaSwag":80.83,
        "MMLU":65.14,
        "TruthfulQA":68.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"56b640240f1aca4a91ccf66de041c86102dfe2c9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"chargoddard\/Yi-34B-Llama",
        "Average":70.53,
        "ARC":64.59,
        "HellaSwag":85.63,
        "MMLU":76.31,
        "TruthfulQA":55.6,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":54,
        "Available on the hub":true,
        "Model Sha":"52feecf18e46dd8ed1db297345957007c3e45de1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andysalerno\/cloudymixtral7Bx2-nectar-0.2",
        "Average":70.52,
        "ARC":67.49,
        "HellaSwag":80.77,
        "MMLU":65.09,
        "TruthfulQA":68.73,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"56b640240f1aca4a91ccf66de041c86102dfe2c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JaeyeonKang\/CCK_gony",
        "Average":70.51,
        "ARC":69.11,
        "HellaSwag":86.78,
        "MMLU":69.43,
        "TruthfulQA":56.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"cb62ef5613e162437a0803cddabc50e21437cd1b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LoSboccacc\/orthogonal-2x7B-v2-base",
        "Average":70.51,
        "ARC":66.89,
        "HellaSwag":85.69,
        "MMLU":62.65,
        "TruthfulQA":66.8,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0d5011506f7782cbbdc3feb0fed079f400292f2a"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"yunconglong\/7Bx4_DPO_700",
        "Average":70.5,
        "ARC":64.68,
        "HellaSwag":86.12,
        "MMLU":62.23,
        "TruthfulQA":68.99,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"51460af315b0fa3ba2a04716879afa7acfaa65f5"
    },
    {
        "T":"?",
        "Model":"Nitral-AI\/KukulStanta-7B",
        "Average":70.5,
        "ARC":68.43,
        "HellaSwag":86.37,
        "MMLU":65.0,
        "TruthfulQA":62.19,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"e1caf3851aff9ba4a843e2b268f10f32fb77c595"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/una-cybertron-7b-v2-bf16",
        "Average":70.49,
        "ARC":68.26,
        "HellaSwag":85.85,
        "MMLU":63.23,
        "TruthfulQA":64.63,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":117,
        "Available on the hub":false,
        "Model Sha":"82599694771bd375c91f36dfdf30c448e4e33b3c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RaduGabriel\/MUZD",
        "Average":70.49,
        "ARC":66.81,
        "HellaSwag":86.54,
        "MMLU":62.87,
        "TruthfulQA":65.73,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5c19bc74d2b01b36a96e1287103bf56be3e6ad03"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"decapoda-research\/Antares-11b-v2",
        "Average":70.48,
        "ARC":69.03,
        "HellaSwag":87.54,
        "MMLU":66.19,
        "TruthfulQA":59.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"f8a863749399d6a11898795bb485e721f42b6338"
    },
    {
        "T":"?",
        "Model":"arcee-ai\/Saul-Instruct-Clown-7b",
        "Average":70.48,
        "ARC":68.09,
        "HellaSwag":86.23,
        "MMLU":64.41,
        "TruthfulQA":63.2,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d7954892af5c69c741493618e3830992929196a1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ConvexAI\/Harmony-4x7B-bf16",
        "Average":70.47,
        "ARC":68.34,
        "HellaSwag":86.75,
        "MMLU":64.73,
        "TruthfulQA":62.06,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"6e7b75c5f5941d2745c2ba1d85e0617107280f3d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/Severus-7B",
        "Average":70.47,
        "ARC":68.43,
        "HellaSwag":86.89,
        "MMLU":65.2,
        "TruthfulQA":61.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"adf7c513e9cadbe25cc2be61c43f3f36f1b488e9"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/bophades-mistral-7B",
        "Average":70.46,
        "ARC":69.97,
        "HellaSwag":87.28,
        "MMLU":64.77,
        "TruthfulQA":59.83,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0c368ff0333aadf3920633441993ae1e7fb4bdeb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"senseable\/garten2-7b",
        "Average":70.46,
        "ARC":69.37,
        "HellaSwag":87.54,
        "MMLU":65.44,
        "TruthfulQA":59.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"96e7c78544d7eca96e3ae60ff80c728f3109e8ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Sensualize-Mixtral-bf16",
        "Average":70.45,
        "ARC":70.14,
        "HellaSwag":86.6,
        "MMLU":70.89,
        "TruthfulQA":54.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"50427f68da578b238e3e41b1126704cb0d06fb6a"
    },
    {
        "T":"?",
        "Model":"ozayezerceli\/BetterSaul-7B-slerp",
        "Average":70.44,
        "ARC":68.09,
        "HellaSwag":86.3,
        "MMLU":64.31,
        "TruthfulQA":63.08,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ea76a92d0d35dc6985651581ce84eafb55763672"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/Loyal-Macaroni-Maid-7B",
        "Average":70.44,
        "ARC":68.0,
        "HellaSwag":86.39,
        "MMLU":64.87,
        "TruthfulQA":62.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":48,
        "Available on the hub":false,
        "Model Sha":"3fc12ef0089d55509552d1569f3107fd6a24b90f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kwchoi\/DPO_mistral_7b_ultra_0124_v1",
        "Average":70.44,
        "ARC":66.13,
        "HellaSwag":86.39,
        "MMLU":59.78,
        "TruthfulQA":69.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a448081da7d55c5b45294e13c264dbb0133b2cce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Test157t\/Prima-Pastacles-7b-128k",
        "Average":70.44,
        "ARC":68.09,
        "HellaSwag":86.57,
        "MMLU":64.58,
        "TruthfulQA":62.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1be449dd843bcc1c9a1bdf4d2e3f506ddd189c10"
    },
    {
        "T":"?",
        "Model":"S-miguel\/The-Trinity-Coder-7B",
        "Average":70.42,
        "ARC":69.37,
        "HellaSwag":86.17,
        "MMLU":64.9,
        "TruthfulQA":61.25,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"cca19971f846c6d45e089dd1425f86fa4cb48f0a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"adamo1139\/Yi-34B-200K-AEZAKMI-RAW-2301-LoRA",
        "Average":70.42,
        "ARC":65.96,
        "HellaSwag":83.89,
        "MMLU":74.76,
        "TruthfulQA":57.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"533506a750975d612071ab9a56e076d65e65cef4"
    },
    {
        "T":"?",
        "Model":"abideen\/MonarchCoder-7B",
        "Average":70.42,
        "ARC":68.52,
        "HellaSwag":87.3,
        "MMLU":64.65,
        "TruthfulQA":61.21,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d5dbca5f010dd7a811e85597bcecdfd848c4ed25"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AIDC-ai-business\/Marcoroni-7B-v3",
        "Average":70.41,
        "ARC":69.45,
        "HellaSwag":86.78,
        "MMLU":65.0,
        "TruthfulQA":60.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6ec546141522aef9b42d1a014f1a539fcc485c45"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_51",
        "Average":70.41,
        "ARC":68.43,
        "HellaSwag":86.71,
        "MMLU":69.31,
        "TruthfulQA":57.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"9542702011bf4d282f4b0f0bd79229f5822b6313"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DrNicefellow\/ChatAllInOne-Yi-34B-200K-V1",
        "Average":70.4,
        "ARC":65.96,
        "HellaSwag":84.53,
        "MMLU":74.13,
        "TruthfulQA":56.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"d3fb17cdd012a7d532a49adaf798203c6524908d"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/PiVoT-SUS-RP",
        "Average":70.39,
        "ARC":66.55,
        "HellaSwag":84.23,
        "MMLU":76.23,
        "TruthfulQA":54.57,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":34.39,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"1b3a5c98381f37a2ec97ce80d1d88d472a7d1802"
    },
    {
        "T":"?",
        "Model":"louisbrulenaudet\/Pearl-7B-slerp",
        "Average":70.39,
        "ARC":68.0,
        "HellaSwag":87.16,
        "MMLU":64.04,
        "TruthfulQA":62.35,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"b4fef0d4a79ed1e5441d6a0d8fb06e0eda223d9e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Marcoroni-8x7B-v3-MoE",
        "Average":70.39,
        "ARC":69.37,
        "HellaSwag":86.78,
        "MMLU":65.01,
        "TruthfulQA":60.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"901a733d1c01035bcbe69afd25dd9b4f982cb216"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"rishiraj\/CatPPT-base",
        "Average":70.38,
        "ARC":67.92,
        "HellaSwag":86.64,
        "MMLU":65.26,
        "TruthfulQA":61.72,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":41,
        "Available on the hub":false,
        "Model Sha":"7b041695f3ac19052f8c8be1918822bba8f73f74"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"garage-bAInd\/Camel-Platypus2-70B",
        "Average":70.37,
        "ARC":69.28,
        "HellaSwag":87.3,
        "MMLU":69.04,
        "TruthfulQA":55.86,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"6f958a1063fe1e6075f6e379fae621ff5a1d98c6"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/Bophades-BruinsMaid-7B",
        "Average":70.37,
        "ARC":69.54,
        "HellaSwag":86.52,
        "MMLU":64.93,
        "TruthfulQA":60.5,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f1723c8cc48103e6e66b96699dda73e5a8f12802"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rishiraj\/CatPPT",
        "Average":70.37,
        "ARC":68.09,
        "HellaSwag":86.69,
        "MMLU":65.16,
        "TruthfulQA":61.55,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":15,
        "Available on the hub":false,
        "Model Sha":"65d316ec5f213b7d9abbe2116372e0e90b579319"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rwitz\/go-bruins-v2",
        "Average":70.37,
        "ARC":69.8,
        "HellaSwag":87.06,
        "MMLU":64.95,
        "TruthfulQA":59.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":29,
        "Available on the hub":false,
        "Model Sha":"49c730c9e00299eaefeb5ada30a9ec53659729a5"
    },
    {
        "T":"?",
        "Model":"mvpmaster\/Einstein-4D-Marcoro14-7b-full-slerp",
        "Average":70.37,
        "ARC":68.86,
        "HellaSwag":85.98,
        "MMLU":64.57,
        "TruthfulQA":62.07,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"65a29ae7376affc698972b99c4802fa16baf4fcf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Stopwolf\/Cerberus-7B-slerp",
        "Average":70.37,
        "ARC":69.54,
        "HellaSwag":87.33,
        "MMLU":63.25,
        "TruthfulQA":61.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ca54f14a8230e73af3c28a67058838d253564926"
    },
    {
        "T":"?",
        "Model":"eldogbbhed\/NeuralMonarchCoderPearlBeagle",
        "Average":70.36,
        "ARC":68.52,
        "HellaSwag":87.22,
        "MMLU":64.53,
        "TruthfulQA":61.19,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"68c3d77b047118b0b06b1540f1536ae368273084"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ValiantLabs\/ShiningValiant",
        "Average":70.36,
        "ARC":68.69,
        "HellaSwag":87.31,
        "MMLU":69.64,
        "TruthfulQA":55.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":69.24,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"7c4401cddc462c5f35d8984c90e293faee37bf8e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Toten5\/Marcoroni-neural-chat-7B-v2",
        "Average":70.35,
        "ARC":68.6,
        "HellaSwag":86.33,
        "MMLU":64.65,
        "TruthfulQA":61.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"15808c683e8e1125d54498a16a620b0e8520ed2b"
    },
    {
        "T":"\u2b55",
        "Model":"dfurman\/Mixtral-8x7B-peft-v0.1",
        "Average":70.35,
        "ARC":67.24,
        "HellaSwag":86.03,
        "MMLU":68.59,
        "TruthfulQA":59.54,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"87dac68765c899952d9d91ce827cda867d115c6f"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/BagelLake-7B-slerp",
        "Average":70.35,
        "ARC":68.26,
        "HellaSwag":85.07,
        "MMLU":64.3,
        "TruthfulQA":63.76,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"24a9ebb9bb40e2a9fff9097845980b4dbb53f330"
    },
    {
        "T":"?",
        "Model":"Locutusque\/Hyperion-2.0-Yi-34B",
        "Average":70.35,
        "ARC":64.33,
        "HellaSwag":85.66,
        "MMLU":76.09,
        "TruthfulQA":55.3,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"145c52f944a1ddb7e70713ecea952b858617139f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ignos\/Mistral-T5-7B-v1",
        "Average":70.35,
        "ARC":68.6,
        "HellaSwag":86.3,
        "MMLU":64.62,
        "TruthfulQA":61.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"e91dcc46d28fc0aa5553fb73c4eac5e28abfd3ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Airoboros-L2-70B-2.1-GPTQ",
        "Average":70.34,
        "ARC":70.39,
        "HellaSwag":86.54,
        "MMLU":68.89,
        "TruthfulQA":55.55,
        "Type":"fine-tuned",
        "Precision":"None",
        "Hub License":"llama2",
        "#Params (B)":9.1,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"23ed580cb77ebaee49ea11eb4538fd3ab3795b76"
    },
    {
        "T":"?",
        "Model":"Eric111\/Mistral-7B-Instruct_v0.2_UNA-TheBeagle-7b-v1",
        "Average":70.34,
        "ARC":67.83,
        "HellaSwag":85.94,
        "MMLU":61.94,
        "TruthfulQA":65.64,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6888db1edc8185a9ba876c8ca2438d3aea28d6aa"
    },
    {
        "T":"?",
        "Model":"shadowml\/Beyonder-4x7B-v2",
        "Average":70.34,
        "ARC":68.77,
        "HellaSwag":86.8,
        "MMLU":65.1,
        "TruthfulQA":60.68,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f44d94a8a0ccfa98e5173da9d88a5ed09efad30e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/Beyonder-4x7B-v2",
        "Average":70.34,
        "ARC":68.77,
        "HellaSwag":86.8,
        "MMLU":65.1,
        "TruthfulQA":60.68,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":24.15,
        "Hub":120,
        "Available on the hub":false,
        "Model Sha":"f44d94a8a0ccfa98e5173da9d88a5ed09efad30e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeuralNovel\/Ember-7B-v0.1",
        "Average":70.33,
        "ARC":68.43,
        "HellaSwag":85.52,
        "MMLU":64.1,
        "TruthfulQA":63.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"49f578bccc5884c7e33b7e7ab3a47591373de76c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SyedAbdul\/test-7B-slerp",
        "Average":70.33,
        "ARC":68.09,
        "HellaSwag":86.08,
        "MMLU":64.57,
        "TruthfulQA":62.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8c0acfaea61f49f679feb694c0de57a7f403d44e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PulsarAI\/OpenHermes-2.5-neural-chat-v3-3-Slerp",
        "Average":70.33,
        "ARC":68.09,
        "HellaSwag":86.2,
        "MMLU":64.26,
        "TruthfulQA":62.78,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":41,
        "Available on the hub":false,
        "Model Sha":"91f18df3f5c3d36f1293086113f810f662970449"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DrNicefellow\/ChatAllInOne-Yi-34B-200K-V1",
        "Average":70.33,
        "ARC":65.96,
        "HellaSwag":84.58,
        "MMLU":73.95,
        "TruthfulQA":56.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"d3fb17cdd012a7d532a49adaf798203c6524908d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PulsarAI\/Neural-una-cybertron-7b",
        "Average":70.33,
        "ARC":69.03,
        "HellaSwag":84.51,
        "MMLU":62.79,
        "TruthfulQA":64.99,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"66dae63f92cac0c99b1b162383506b60ac060225"
    },
    {
        "T":"?",
        "Model":"chasedreaminf\/Dream-7B-slerp",
        "Average":70.33,
        "ARC":68.52,
        "HellaSwag":86.35,
        "MMLU":64.6,
        "TruthfulQA":61.85,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c9a78a8006cd0b07d0f944621f85baf8f203d18c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rwitz\/go-bruins-v2",
        "Average":70.33,
        "ARC":69.8,
        "HellaSwag":87.05,
        "MMLU":64.75,
        "TruthfulQA":59.7,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":29,
        "Available on the hub":false,
        "Model Sha":"24f8ce81d25c433bc6be147928779fb2d00ae0e7"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"viethq188\/Rabbit-7B-v2-DPO-Chat",
        "Average":70.32,
        "ARC":66.13,
        "HellaSwag":85.18,
        "MMLU":62.92,
        "TruthfulQA":67.06,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7dae800851457f1dcccf00a2517448c9a9400b15"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mindy-labs\/mindy-7b",
        "Average":70.32,
        "ARC":69.11,
        "HellaSwag":86.57,
        "MMLU":64.69,
        "TruthfulQA":60.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"ce0d461a6de81d5b8ec4d338fb0c6e7991d0b1ff"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"rwitz2\/grindin",
        "Average":70.31,
        "ARC":69.88,
        "HellaSwag":87.02,
        "MMLU":64.98,
        "TruthfulQA":59.34,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9bdce071e0f87fe047cd2446be42edf91175c3be"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"adamo1139\/yi-34b-200k-rawrr-dpo-1",
        "Average":70.3,
        "ARC":65.44,
        "HellaSwag":85.69,
        "MMLU":76.09,
        "TruthfulQA":54.0,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"2f6396382239da8aa2858393c62f0c5596bd09f0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-M-v1.1",
        "Average":70.3,
        "ARC":67.15,
        "HellaSwag":84.76,
        "MMLU":74.5,
        "TruthfulQA":54.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"e5a016b08aa507fe9db45436074016928bf6f939"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"liminerity\/Blur-7B-slerp-v0.1",
        "Average":70.29,
        "ARC":68.77,
        "HellaSwag":86.58,
        "MMLU":65.18,
        "TruthfulQA":60.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"03d64dadac0ac71cc5d62e325103cb9b9f279d43"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Falkor-7b",
        "Average":70.29,
        "ARC":68.26,
        "HellaSwag":85.84,
        "MMLU":63.98,
        "TruthfulQA":63.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"b2e3c235196ba859b26ee14fb8c86e632bcf3e88"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/C0318-G",
        "Average":70.29,
        "ARC":64.51,
        "HellaSwag":83.88,
        "MMLU":74.16,
        "TruthfulQA":58.61,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1fbff466a94c2cc6ed7ec7e21b478f9528caab89"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rufjdk5480\/WestLake-dpo-train-sft-v1",
        "Average":70.29,
        "ARC":65.78,
        "HellaSwag":85.76,
        "MMLU":61.8,
        "TruthfulQA":67.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fd04d89dac9f3fc8f8f43048c3fad3821b0fada6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/juanako-7b-UNA",
        "Average":70.28,
        "ARC":68.17,
        "HellaSwag":85.34,
        "MMLU":62.47,
        "TruthfulQA":65.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":23,
        "Available on the hub":false,
        "Model Sha":"3e12f691e1f442f69eaff408677a54ebc69d5dc8"
    },
    {
        "T":"?",
        "Model":"saishf\/Kuro-Lotus-10.7B",
        "Average":70.28,
        "ARC":68.69,
        "HellaSwag":87.51,
        "MMLU":66.64,
        "TruthfulQA":58.27,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"ec748dade16858ef2fb3c712c78de748d165a21c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rwitz\/dec10",
        "Average":70.28,
        "ARC":69.2,
        "HellaSwag":86.48,
        "MMLU":64.91,
        "TruthfulQA":60.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d12ade4c823d9f42949c7902d0f01b2e996a7d7e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DiscoResearch\/DiscoLM-70b",
        "Average":70.27,
        "ARC":68.77,
        "HellaSwag":86.1,
        "MMLU":68.58,
        "TruthfulQA":57.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"5eab2c8ec1c079e53a60ebdb7811756c2faebd9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SherlockAssistant\/Mistral-7B-Instruct-Ukrainian",
        "Average":70.26,
        "ARC":67.41,
        "HellaSwag":85.81,
        "MMLU":62.87,
        "TruthfulQA":64.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"44524f02c0ca97e34c6610cbd7d28ef153e54437"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"freecs\/ThetaWave-7B",
        "Average":70.25,
        "ARC":67.49,
        "HellaSwag":86.01,
        "MMLU":62.26,
        "TruthfulQA":65.26,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"21a306726dae52eee662b83fadc9657cef10dd02"
    },
    {
        "T":"\u2b55",
        "Model":"quantumaikr\/llama-2-70B-chat",
        "Average":70.25,
        "ARC":67.58,
        "HellaSwag":86.94,
        "MMLU":69.18,
        "TruthfulQA":57.31,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d242fdbf800e388e6ee456578064cab5e057f987"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_46-7B-dare_ties",
        "Average":70.24,
        "ARC":67.24,
        "HellaSwag":86.4,
        "MMLU":62.17,
        "TruthfulQA":65.17,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c012482f77a9bdc6e30df6039687cafb022e1640"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rwitz\/dec10",
        "Average":70.24,
        "ARC":69.11,
        "HellaSwag":86.46,
        "MMLU":64.98,
        "TruthfulQA":60.42,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d12ade4c823d9f42949c7902d0f01b2e996a7d7e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/UNA-dolphin-2.6-mistral-7b-dpo-laser",
        "Average":70.24,
        "ARC":67.15,
        "HellaSwag":86.31,
        "MMLU":63.36,
        "TruthfulQA":64.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"3e2cd605dde0bd7443172c722a1f34a498a36901"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LoSboccacc\/orthogonal-2x7B-base",
        "Average":70.23,
        "ARC":66.89,
        "HellaSwag":85.54,
        "MMLU":62.49,
        "TruthfulQA":66.0,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b96572f91bdbb612299825f9ce793dabd63917dd"
    },
    {
        "T":"?",
        "Model":"grimjim\/kukulemon-7B",
        "Average":70.23,
        "ARC":67.75,
        "HellaSwag":86.1,
        "MMLU":65.09,
        "TruthfulQA":61.99,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"80098e3132e20702cd33c049c47cfee6a26fa32c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/Silicon-Maid-7B",
        "Average":70.23,
        "ARC":68.17,
        "HellaSwag":86.52,
        "MMLU":64.58,
        "TruthfulQA":61.64,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":75,
        "Available on the hub":false,
        "Model Sha":"ecb260368921c5dfe16c007e871d29de9d561996"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/Loyal-Toppy-Bruins-Maid-7B-DARE",
        "Average":70.23,
        "ARC":68.86,
        "HellaSwag":86.03,
        "MMLU":64.84,
        "TruthfulQA":61.19,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"d8d01fbb3aaefda39421850c2dabb38e73546a6e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/Loyal-Toppy-Bruins-Maid-7B-DARE",
        "Average":70.22,
        "ARC":68.69,
        "HellaSwag":86.04,
        "MMLU":64.89,
        "TruthfulQA":61.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"d8d01fbb3aaefda39421850c2dabb38e73546a6e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"scaledown\/ScaleDown-7B-slerp-v0.1",
        "Average":70.22,
        "ARC":68.0,
        "HellaSwag":85.7,
        "MMLU":65.26,
        "TruthfulQA":61.9,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9bddd33f58ddbbaa9ecf8c5a4b79dfd8e49155e5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"s3nh\/Mistral_Sonyichi-7B-slerp",
        "Average":70.19,
        "ARC":67.49,
        "HellaSwag":86.43,
        "MMLU":63.58,
        "TruthfulQA":63.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d6605744836a770190389a73d31440362c81f41e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Chupacabra-7B-v2.04",
        "Average":70.17,
        "ARC":66.3,
        "HellaSwag":85.7,
        "MMLU":60.94,
        "TruthfulQA":67.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"b6eb3c3293fff1cb3d38bbfefa9adfce3e20f053"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-mixtral-8x7b-v1",
        "Average":70.16,
        "ARC":68.09,
        "HellaSwag":85.76,
        "MMLU":71.49,
        "TruthfulQA":55.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":15,
        "Available on the hub":false,
        "Model Sha":"30abf8de36252c1e026fe758b8fde5eba960cd2a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-mixtral-8x7b-v0.1",
        "Average":70.16,
        "ARC":68.09,
        "HellaSwag":85.76,
        "MMLU":71.49,
        "TruthfulQA":55.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":15,
        "Available on the hub":false,
        "Model Sha":"30abf8de36252c1e026fe758b8fde5eba960cd2a"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"01-ai\/Yi-34B-200K",
        "Average":70.16,
        "ARC":65.36,
        "HellaSwag":85.58,
        "MMLU":76.06,
        "TruthfulQA":53.64,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":293,
        "Available on the hub":true,
        "Model Sha":"bb196389dbbfdf271b5564ce840027f8cd3386ef"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beberik\/Nyxene-v3-11B",
        "Average":70.15,
        "ARC":69.62,
        "HellaSwag":85.33,
        "MMLU":64.75,
        "TruthfulQA":60.91,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"706e71043ed40e53bfee7f25a3f2b4a8def36ae8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mindy-labs\/mindy-7b-v2",
        "Average":70.15,
        "ARC":68.69,
        "HellaSwag":86.59,
        "MMLU":65.18,
        "TruthfulQA":60.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"b859eae30d69b065060e268b4e918601dabcc36c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dreamgen\/opus-v1-34b",
        "Average":70.15,
        "ARC":64.33,
        "HellaSwag":84.9,
        "MMLU":75.43,
        "TruthfulQA":55.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":34.39,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"4dc3e88bf59b74391c7e31e30921b7c56bdc5a40"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"RatanRohith\/NeuralPizza-7B-V0.2",
        "Average":70.14,
        "ARC":68.77,
        "HellaSwag":86.11,
        "MMLU":64.32,
        "TruthfulQA":61.38,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e2164a4cce391e1f4228e2e89689793ec037135e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TeeZee\/Kyllene-v1.0",
        "Average":70.14,
        "ARC":64.85,
        "HellaSwag":84.51,
        "MMLU":73.33,
        "TruthfulQA":57.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":56.7,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"27db149dad28401a81a7207f8cf3f8ff5aad9f4c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aqweteddy\/mistral_tv-neural-marconroni",
        "Average":70.14,
        "ARC":69.2,
        "HellaSwag":86.26,
        "MMLU":65.07,
        "TruthfulQA":60.03,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"969f7483d768b15998cd57b392ea1a9718de3b28"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"janhq\/supermario-slerp",
        "Average":70.14,
        "ARC":68.94,
        "HellaSwag":86.58,
        "MMLU":64.93,
        "TruthfulQA":60.11,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"83bcf51c709bcb4fcb3c8f0f91de22f458a07ee4"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"deepnight-research\/lil-c3po",
        "Average":70.14,
        "ARC":65.02,
        "HellaSwag":84.45,
        "MMLU":62.36,
        "TruthfulQA":68.73,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7888318c72df9f668df20b2916b651b94a6ed77c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dreamgen\/opus-v1-34b",
        "Average":70.13,
        "ARC":64.42,
        "HellaSwag":84.85,
        "MMLU":75.38,
        "TruthfulQA":55.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":34.39,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"4dc3e88bf59b74391c7e31e30921b7c56bdc5a40"
    },
    {
        "T":"?",
        "Model":"jan-hq\/supermario-v2",
        "Average":70.13,
        "ARC":68.43,
        "HellaSwag":86.51,
        "MMLU":64.96,
        "TruthfulQA":60.61,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"bddc0dbbe2ce89336f2f1ff9db9211b2f28e7694"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/llama2-70b-oasst-sft-v10",
        "Average":70.12,
        "ARC":68.0,
        "HellaSwag":86.82,
        "MMLU":68.37,
        "TruthfulQA":57.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":57,
        "Available on the hub":true,
        "Model Sha":"e68a8a2888097def3c7f4fe5d443866a18d05c6c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"janhq\/supermario-v2",
        "Average":70.12,
        "ARC":68.52,
        "HellaSwag":86.51,
        "MMLU":64.88,
        "TruthfulQA":60.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"d66c7d87fc3670c9292177e4cfc59e8a9d71322d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/una-cybertron-7b-v1-fp16",
        "Average":70.12,
        "ARC":68.43,
        "HellaSwag":85.42,
        "MMLU":63.34,
        "TruthfulQA":63.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"7bf918ddf0878a693f24f39e9f1a520464b44268"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"one-man-army\/una-neural-chat-v3-3-P1-OMA",
        "Average":70.11,
        "ARC":66.81,
        "HellaSwag":85.92,
        "MMLU":63.37,
        "TruthfulQA":64.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"014600373086ea46c7cdc4754c984a804b28a070"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"viethq188\/Rabbit-7B-DPO-Chat",
        "Average":70.11,
        "ARC":70.31,
        "HellaSwag":87.43,
        "MMLU":60.5,
        "TruthfulQA":62.18,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"04d42accbc808eec8c020f17392efa07c95ae565"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/Darewin-7B",
        "Average":70.1,
        "ARC":68.6,
        "HellaSwag":86.22,
        "MMLU":65.21,
        "TruthfulQA":60.38,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"04a49e7c37033714a42a22c834e0c0179cfb90c6"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_15-7B-slerp",
        "Average":70.1,
        "ARC":68.0,
        "HellaSwag":86.82,
        "MMLU":65.58,
        "TruthfulQA":59.99,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c4f2798b33145862196740a49a260629070e04a8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azazelle\/Yuna-7b-Merge",
        "Average":70.1,
        "ARC":67.49,
        "HellaSwag":86.84,
        "MMLU":64.86,
        "TruthfulQA":61.2,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d5cde262c73c9ee44c1ec85b1fb48f226ae99a77"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ConvexAI\/Metabird-7B",
        "Average":70.07,
        "ARC":69.54,
        "HellaSwag":87.54,
        "MMLU":65.27,
        "TruthfulQA":57.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"73b2f79cf8ef066f04980b182c604f77b1aa9ab8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"garage-bAInd\/Platypus2-70B",
        "Average":70.06,
        "ARC":70.65,
        "HellaSwag":87.15,
        "MMLU":70.08,
        "TruthfulQA":52.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":68.72,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"16b6583ad58313331f86be18e531ab03f1857695"
    },
    {
        "T":"?",
        "Model":"LeroyDyer\/Mixtral_ThoughtsProcess_1",
        "Average":70.05,
        "ARC":65.27,
        "HellaSwag":85.69,
        "MMLU":61.9,
        "TruthfulQA":67.34,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b2c56ac7f94c61538cf1ba44bee37e689259bd3f"
    },
    {
        "T":"?",
        "Model":"sethuiyer\/Diana-7B",
        "Average":70.05,
        "ARC":68.34,
        "HellaSwag":86.73,
        "MMLU":64.58,
        "TruthfulQA":60.55,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"09f1c9e78c1e73a00278ce864470c4ffb35f626d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-mixtral-7bx8-v18.1-32k",
        "Average":70.05,
        "ARC":68.09,
        "HellaSwag":84.29,
        "MMLU":71.08,
        "TruthfulQA":56.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.74,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"98596b6731058cc9cca85f3b8ac9077342cb60ae"
    },
    {
        "T":"?",
        "Model":"saishf\/Top-Western-Maid-7B",
        "Average":70.05,
        "ARC":69.37,
        "HellaSwag":87.4,
        "MMLU":64.63,
        "TruthfulQA":58.79,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2973b0902468b765a9d6452ae3ba116a3e1ceba0"
    },
    {
        "T":"?",
        "Model":"Aryanne\/Open-StarLake-Swap-7B",
        "Average":70.04,
        "ARC":70.56,
        "HellaSwag":86.99,
        "MMLU":65.11,
        "TruthfulQA":57.5,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"1722dfaffa2f968ed9dd9cd70952c447cf823f72"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"liminerity\/e.star.7b",
        "Average":70.04,
        "ARC":66.81,
        "HellaSwag":87.12,
        "MMLU":63.6,
        "TruthfulQA":62.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e3da1a54cfdde55ae0d31db3c79512729bd8cbe8"
    },
    {
        "T":"\u2b55",
        "Model":"Mihaiii\/Pallas-0.5",
        "Average":70.03,
        "ARC":64.76,
        "HellaSwag":83.46,
        "MMLU":75.01,
        "TruthfulQA":56.88,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"4b7aa4e48f3208ab39f6640aa4cc98b1d5eff7e8"
    },
    {
        "T":"?",
        "Model":"Stopwolf\/DistilabelCerberus-7B-slerp",
        "Average":70.02,
        "ARC":68.17,
        "HellaSwag":86.78,
        "MMLU":64.2,
        "TruthfulQA":60.93,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f7267cb2b0151acddd84a88c2981e73880d97634"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beberik\/Lonepino-11B",
        "Average":70.01,
        "ARC":68.26,
        "HellaSwag":84.57,
        "MMLU":63.76,
        "TruthfulQA":63.45,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"392a0d8806638a235020b2146d83628b19516be5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051611\/A0119",
        "Average":70.01,
        "ARC":64.25,
        "HellaSwag":84.74,
        "MMLU":73.1,
        "TruthfulQA":57.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"33978688c6fa79679b2cafc504c4f9b9ddccd136"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azure99\/blossom-v3_1-yi-34b",
        "Average":70.01,
        "ARC":65.36,
        "HellaSwag":84.24,
        "MMLU":74.37,
        "TruthfulQA":56.06,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":33.93,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"2ec5cbb112a31c62c8631b89fbde0aebaabb6e0a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Q-bert\/Merged-AGI-7B",
        "Average":70.0,
        "ARC":68.6,
        "HellaSwag":86.16,
        "MMLU":65.02,
        "TruthfulQA":60.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"7b818236625de433802bfe8b32ab8b17a7e58912"
    },
    {
        "T":"?",
        "Model":"liuxiang886\/llama2-70B-qlora-gpt4",
        "Average":70.0,
        "ARC":70.31,
        "HellaSwag":86.39,
        "MMLU":69.29,
        "TruthfulQA":54.02,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"08115ee077953e9c01c6a40f5086def3ecf9f5f0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RaduGabriel\/MUZ",
        "Average":69.99,
        "ARC":66.38,
        "HellaSwag":86.38,
        "MMLU":63.03,
        "TruthfulQA":64.18,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"56f4a9b2f7fcc6891536de338fdb4b302a1fbcfa"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/ToppyLake-Bagel-7B-slerp",
        "Average":69.99,
        "ARC":67.66,
        "HellaSwag":85.7,
        "MMLU":64.87,
        "TruthfulQA":61.74,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f827b067ce4bec3cecb4bf88fb8ec2c244af6803"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GOAT-AI\/GOAT-70B-Storytelling",
        "Average":69.99,
        "ARC":68.77,
        "HellaSwag":87.74,
        "MMLU":69.92,
        "TruthfulQA":53.53,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":25,
        "Available on the hub":true,
        "Model Sha":"337fe3de7874d3a09aa1cfe9e78f5efd81c00f43"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jeiku\/Mewthree_7B",
        "Average":69.98,
        "ARC":65.78,
        "HellaSwag":85.74,
        "MMLU":62.56,
        "TruthfulQA":65.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7b81ea3d4782dc2ea5ae21039aed4c561b0397f3"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/average-dolphin-8x7B",
        "Average":69.98,
        "ARC":68.6,
        "HellaSwag":85.99,
        "MMLU":70.84,
        "TruthfulQA":54.51,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b0345662588e8c99d8e504bab894fa41e2199463"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"EmbeddedLLM\/Mistral-7B-Merge-14-v0.5",
        "Average":69.97,
        "ARC":68.69,
        "HellaSwag":86.45,
        "MMLU":65.65,
        "TruthfulQA":59.12,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"fe9f9e52f1b48112d1c4349abbc0f104e56303ab"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Salesforce\/xLAM-v0.1-r",
        "Average":69.97,
        "ARC":67.58,
        "HellaSwag":84.59,
        "MMLU":69.95,
        "TruthfulQA":57.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"68d5e8b311745400d926f6143e1ac3ff5c449a4d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Pallas-0.5-LASER-0.1",
        "Average":69.97,
        "ARC":64.68,
        "HellaSwag":83.49,
        "MMLU":74.94,
        "TruthfulQA":56.78,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":33.93,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"bc07f9084ad43d2455f12f1707a3c14f1a1de1d3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AIDC-ai-business\/Marcoroni-7B-v2",
        "Average":69.97,
        "ARC":68.26,
        "HellaSwag":86.27,
        "MMLU":63.39,
        "TruthfulQA":61.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"3929ff947202a530d89a2287e19873141a0136c5"
    },
    {
        "T":"\u2b55",
        "Model":"NousResearch\/Nous-Hermes-2-Mixtral-8x7B-SFT",
        "Average":69.97,
        "ARC":69.71,
        "HellaSwag":86.74,
        "MMLU":72.21,
        "TruthfulQA":51.22,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":55,
        "Available on the hub":false,
        "Model Sha":"6011e2ef7791738f3b78fa9e122360029df7c9ed"
    },
    {
        "T":"\u2b55",
        "Model":"01-ai\/Yi-34B-Chat",
        "Average":69.97,
        "ARC":65.44,
        "HellaSwag":84.16,
        "MMLU":74.9,
        "TruthfulQA":55.37,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":303,
        "Available on the hub":true,
        "Model Sha":"a99ec35331cbfc9da596af7d4538fe2efecff03c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Shiki-m7",
        "Average":69.96,
        "ARC":65.53,
        "HellaSwag":85.3,
        "MMLU":63.57,
        "TruthfulQA":65.45,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f98d82c7b01b5ac1bf6ee62871a27011cf57cbf9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-34B-v1.4",
        "Average":69.94,
        "ARC":64.59,
        "HellaSwag":83.37,
        "MMLU":75.02,
        "TruthfulQA":56.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"173d834656c3965cbaa49be6aab0772c3ce57821"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Pallas-0.5-LASER-0.2",
        "Average":69.94,
        "ARC":64.68,
        "HellaSwag":83.49,
        "MMLU":74.84,
        "TruthfulQA":56.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9119f34f298645df22e0e042f6631af8f67f4b33"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"upstage\/llama-65b-instruct",
        "Average":69.94,
        "ARC":68.86,
        "HellaSwag":86.43,
        "MMLU":64.77,
        "TruthfulQA":59.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":65.02,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"b95668861dfb7b0abca44ccdbef2db49b2dd8917"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PracticeLLM\/SOLAR-tail-10.7B-Merge-v1.0",
        "Average":69.94,
        "ARC":66.13,
        "HellaSwag":86.54,
        "MMLU":66.52,
        "TruthfulQA":60.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"957474e32057f19ef863c1c8ba3d16389cf58eed"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Ba2han\/BruinsV2-OpHermesNeu-11B",
        "Average":69.94,
        "ARC":68.09,
        "HellaSwag":84.7,
        "MMLU":64.19,
        "TruthfulQA":62.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"9a5567cf04d6bd8bbd77743f303ce7ecebec78c5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"quantumaikr\/llama-2-70b-fb16-korean",
        "Average":69.93,
        "ARC":67.15,
        "HellaSwag":86.78,
        "MMLU":69.29,
        "TruthfulQA":56.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"fd57855006c15c4121feccab1cbeee8107de5b5a"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/Bageluccine-2-7B-slerp",
        "Average":69.92,
        "ARC":66.38,
        "HellaSwag":85.51,
        "MMLU":62.23,
        "TruthfulQA":65.57,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"551789fbe4826ea310d24aa66f4df7761f0a97b2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"s1ghhh\/medllama-2-70b-qlora-1.1",
        "Average":69.91,
        "ARC":69.03,
        "HellaSwag":87.17,
        "MMLU":71.04,
        "TruthfulQA":52.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":70.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"d55e05e9d67418c639933c85a5b9d17c6f531a92"
    },
    {
        "T":"?",
        "Model":"OpenBuddy\/openbuddy-mixtral-7bx8-v18.1-32k",
        "Average":69.9,
        "ARC":67.66,
        "HellaSwag":84.3,
        "MMLU":70.94,
        "TruthfulQA":56.72,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.74,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"8a038d22e0d98c31619bb6b7a372b75eeba04d63"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"r2rss\/Malachite-7b-v0",
        "Average":69.9,
        "ARC":67.75,
        "HellaSwag":83.68,
        "MMLU":63.64,
        "TruthfulQA":64.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e9b44b31a7ec203b301a7820a1c5000a30ed68a1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/NeuralPipe-7B-ties",
        "Average":69.89,
        "ARC":67.92,
        "HellaSwag":86.04,
        "MMLU":64.24,
        "TruthfulQA":61.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"5b4a878a938954d87183d1d903923c100b2c724f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Karko\/Proctora",
        "Average":69.89,
        "ARC":67.83,
        "HellaSwag":86.68,
        "MMLU":65.49,
        "TruthfulQA":59.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.88,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"9e338a5f1650cf15850e53046fdf0c4cb25acad1"
    },
    {
        "T":"?",
        "Model":"icefog72\/IceTeaRP-7b",
        "Average":69.88,
        "ARC":66.98,
        "HellaSwag":86.13,
        "MMLU":63.97,
        "TruthfulQA":62.44,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"a6178e04aa616fcd6fc8c10ac8c2a7b5991731ae"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rwitz\/go-bruins",
        "Average":69.87,
        "ARC":69.11,
        "HellaSwag":86.73,
        "MMLU":64.94,
        "TruthfulQA":58.71,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":15,
        "Available on the hub":false,
        "Model Sha":"a544f70a290738787bf3edc167f0bc95999e5702"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rwitz\/go-bruins",
        "Average":69.87,
        "ARC":69.11,
        "HellaSwag":86.68,
        "MMLU":64.96,
        "TruthfulQA":58.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":15,
        "Available on the hub":false,
        "Model Sha":"a544f70a290738787bf3edc167f0bc95999e5702"
    },
    {
        "T":"?",
        "Model":"localfultonextractor\/Erosumika-7B-v3",
        "Average":69.86,
        "ARC":67.49,
        "HellaSwag":85.69,
        "MMLU":64.15,
        "TruthfulQA":62.12,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d80884197f744524ba44fb587944e7bde053e249"
    },
    {
        "T":"\u2b55",
        "Model":"01-ai\/Yi-34B-Chat",
        "Average":69.86,
        "ARC":65.1,
        "HellaSwag":84.08,
        "MMLU":74.87,
        "TruthfulQA":55.41,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":303,
        "Available on the hub":true,
        "Model Sha":"a99ec35331cbfc9da596af7d4538fe2efecff03c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"r2rss\/Malachite-7b-v0",
        "Average":69.86,
        "ARC":67.75,
        "HellaSwag":83.66,
        "MMLU":63.54,
        "TruthfulQA":64.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e9b44b31a7ec203b301a7820a1c5000a30ed68a1"
    },
    {
        "T":"\u2b55",
        "Model":"Mihaiii\/Pallas-0.3",
        "Average":69.86,
        "ARC":63.74,
        "HellaSwag":83.3,
        "MMLU":75.08,
        "TruthfulQA":57.31,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"1207a09c7bd4539bcefce62e1376495b0761b08a"
    },
    {
        "T":"?",
        "Model":"vishnukv\/speechless-mistral-dolphin-orca-platypus-samantha-WestSeverusJaskier-7b",
        "Average":69.85,
        "ARC":68.0,
        "HellaSwag":86.56,
        "MMLU":64.92,
        "TruthfulQA":59.9,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"44a51e1f89f22c1b3962ae75e24d35a5c3c345fa"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Weyaxi\/Qwen-72B-Llama",
        "Average":69.84,
        "ARC":64.85,
        "HellaSwag":83.27,
        "MMLU":73.66,
        "TruthfulQA":57.6,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":72.29,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"e94ac9684f607c71f443b7098c434b543cc62fd6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"silvercoder45\/Mistral-7b-instruct-v0.2-summ-sft-dpo-e1",
        "Average":69.84,
        "ARC":62.71,
        "HellaSwag":85.3,
        "MMLU":60.6,
        "TruthfulQA":70.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9ecd757a87134736f311e7b6e8c6c89b00343364"
    },
    {
        "T":"\u2b55",
        "Model":"Mihaiii\/Pallas-0.4",
        "Average":69.84,
        "ARC":63.65,
        "HellaSwag":83.3,
        "MMLU":75.11,
        "TruthfulQA":57.29,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"2cef301d3afa127217c000f2fdc4c527dfa6145e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Euryale-L2-70B",
        "Average":69.84,
        "ARC":68.94,
        "HellaSwag":87.07,
        "MMLU":68.84,
        "TruthfulQA":54.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"6589310a57ce5d9d6877f353f3d00cda8fa9101c"
    },
    {
        "T":"\u2b55",
        "Model":"Mihaiii\/Pallas-0.3",
        "Average":69.84,
        "ARC":63.57,
        "HellaSwag":83.36,
        "MMLU":75.09,
        "TruthfulQA":57.32,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"1207a09c7bd4539bcefce62e1376495b0761b08a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andrijdavid\/Macaroni-v2-7b",
        "Average":69.84,
        "ARC":67.15,
        "HellaSwag":83.84,
        "MMLU":61.29,
        "TruthfulQA":67.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b611850983ecc381c68b4853b1e2aa570ce22330"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Azazelle\/Silicon-Medley",
        "Average":69.82,
        "ARC":67.24,
        "HellaSwag":86.21,
        "MMLU":64.51,
        "TruthfulQA":61.34,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"98e46cc2715fdeead6c6b79307b40682efb83bfc"
    },
    {
        "T":"?",
        "Model":"tushar310\/MisGemma-7B",
        "Average":69.82,
        "ARC":66.89,
        "HellaSwag":85.7,
        "MMLU":64.48,
        "TruthfulQA":62.22,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2f70f12276927c719b379176909f75fba4725ccd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"silvercoder45\/Mistral-7b-instruct-v0.2-summ-dpo-e3",
        "Average":69.82,
        "ARC":62.63,
        "HellaSwag":85.31,
        "MMLU":60.76,
        "TruthfulQA":70.59,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b407df1ea1d6cc3c7cc183053fea8d728eb8365b"
    },
    {
        "T":"?",
        "Model":"tushar310\/MisGemma-7B",
        "Average":69.82,
        "ARC":66.89,
        "HellaSwag":85.73,
        "MMLU":64.44,
        "TruthfulQA":62.22,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2f70f12276927c719b379176909f75fba4725ccd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/WizardLM-Math-70B-v0.1",
        "Average":69.82,
        "ARC":67.06,
        "HellaSwag":86.01,
        "MMLU":69.14,
        "TruthfulQA":57.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":68.98,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"9db040ac186cf2884ca0759fa26474ddf0e69bce"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/NewtoccineLake-slerp-7B",
        "Average":69.81,
        "ARC":68.69,
        "HellaSwag":85.98,
        "MMLU":64.62,
        "TruthfulQA":59.95,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"de29875ebd6471c5776813f1f1caa0231df0d699"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mychen76\/mistral-7b-merged-ties",
        "Average":69.81,
        "ARC":67.92,
        "HellaSwag":85.93,
        "MMLU":64.07,
        "TruthfulQA":61.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"36c42c61ff949d5dd0212611f77780a11e7346a9"
    },
    {
        "T":"?",
        "Model":"Corianas\/Neural-Mistral-7B",
        "Average":69.79,
        "ARC":63.4,
        "HellaSwag":85.59,
        "MMLU":60.92,
        "TruthfulQA":69.26,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0744af8f77f61b0e182f6d5204354c71534f3992"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/bagel-dpo-7b-v0.1",
        "Average":69.79,
        "ARC":66.72,
        "HellaSwag":84.16,
        "MMLU":64.24,
        "TruthfulQA":64.05,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":40,
        "Available on the hub":false,
        "Model Sha":"6444a0bc809bad1322820b48707746f027e01b96"
    },
    {
        "T":"?",
        "Model":"jsfs11\/HighdensityRPMerge-7B",
        "Average":69.79,
        "ARC":67.41,
        "HellaSwag":86.58,
        "MMLU":64.73,
        "TruthfulQA":60.44,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"68c41dda197602fa34f48f472f7e1aebf53ac609"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Hermes-Llama2-70b",
        "Average":69.79,
        "ARC":67.58,
        "HellaSwag":86.81,
        "MMLU":69.72,
        "TruthfulQA":55.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":68.72,
        "Hub":59,
        "Available on the hub":true,
        "Model Sha":"13a0b4da159ad95c93e72a002d893c48ed0f257a"
    },
    {
        "T":"\u2b55",
        "Model":"Mihaiii\/Pallas-0.4",
        "Average":69.79,
        "ARC":63.65,
        "HellaSwag":83.3,
        "MMLU":74.93,
        "TruthfulQA":57.26,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"2cef301d3afa127217c000f2fdc4c527dfa6145e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AGI-0\/Magistral-7B-v0.1",
        "Average":69.78,
        "ARC":67.15,
        "HellaSwag":86.3,
        "MMLU":64.3,
        "TruthfulQA":61.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"12be40c847ab6d37efce76ca3bc57686f70d45ac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"silvercoder67\/Mistral-7b-instruct-v0.2-summ-sft-dpo-e3",
        "Average":69.78,
        "ARC":62.54,
        "HellaSwag":85.34,
        "MMLU":60.54,
        "TruthfulQA":70.69,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"18cb6af4611f6838a65f4517e03d82b1aa1d7a06"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"silvercoder67\/Mistral-7b-instruct-v0.2-summ-sft-dpo-e2",
        "Average":69.77,
        "ARC":62.46,
        "HellaSwag":85.31,
        "MMLU":60.56,
        "TruthfulQA":70.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"803a3e7f24f61e8cd53ef4133ae22c3ce2568a78"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kwchoi\/DPO_mistral_7b_ultra_0129_1k",
        "Average":69.77,
        "ARC":64.16,
        "HellaSwag":85.54,
        "MMLU":61.04,
        "TruthfulQA":68.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c682cc66f92fc213ea9025557d0078503f3e461b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"silvercoder45\/Mistral-7b-instruct-v0.2-summ-dpo-e2",
        "Average":69.77,
        "ARC":62.54,
        "HellaSwag":85.3,
        "MMLU":60.71,
        "TruthfulQA":70.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"22057329aaf003c9b32f7bc30b292d035972086c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HyperbeeAI\/Tulpar-7b-v2",
        "Average":69.76,
        "ARC":67.49,
        "HellaSwag":84.89,
        "MMLU":63.02,
        "TruthfulQA":63.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b466113c7726cfcd98ba602ec4000ae323f112fa"
    },
    {
        "T":"?",
        "Model":"icefog72\/Kunokukulemonchini-7b",
        "Average":69.76,
        "ARC":66.72,
        "HellaSwag":86.31,
        "MMLU":64.11,
        "TruthfulQA":61.89,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"fd534d80a67d0959c0f42be982dc937d451b86c4"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/MixtralRPChat-ZLoss",
        "Average":69.75,
        "ARC":68.6,
        "HellaSwag":86.1,
        "MMLU":70.44,
        "TruthfulQA":53.85,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":24,
        "Available on the hub":false,
        "Model Sha":"94e804a4cd8e3ed54105f400118c60fa0cce764d"
    },
    {
        "T":"?",
        "Model":"jeiku\/NarrativeNexus_7B",
        "Average":69.75,
        "ARC":66.13,
        "HellaSwag":85.74,
        "MMLU":63.17,
        "TruthfulQA":63.95,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"98a4cb1db6369cda6330441ad083f4d1fa3bca29"
    },
    {
        "T":"\u2b55",
        "Model":"NExtNewChattingAI\/shark_tank_ai_7_b",
        "Average":69.74,
        "ARC":66.89,
        "HellaSwag":86.61,
        "MMLU":65.27,
        "TruthfulQA":60.19,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"feafb4e14863e893ee3d6737ac5b07ac5241f452"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/A12P",
        "Average":69.73,
        "ARC":64.42,
        "HellaSwag":82.32,
        "MMLU":69.97,
        "TruthfulQA":62.22,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e2eb6a36741dfc799fd13f67cba385f6e3992393"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"silvercoder45\/Mistral-7b-instruct-v0.2-summ-dpo-e1",
        "Average":69.73,
        "ARC":62.46,
        "HellaSwag":85.23,
        "MMLU":60.67,
        "TruthfulQA":70.56,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1b40e1f4f27fd0bc0430640df5fee7c0ca5f3668"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"v2ray\/LLaMA-2-Jannie-70B-QLoRA",
        "Average":69.72,
        "ARC":68.94,
        "HellaSwag":86.9,
        "MMLU":69.37,
        "TruthfulQA":53.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":70.0,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"e552ddca841a2b86e36bbe5f99840afedfdbcd14"
    },
    {
        "T":"\u2b55",
        "Model":"Mihaiii\/Pallas-0.2",
        "Average":69.72,
        "ARC":64.51,
        "HellaSwag":83.47,
        "MMLU":75.64,
        "TruthfulQA":55.27,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"2a367db35e91a1cac5abad8e5101e85d391e0551"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-deepseek-67b-v15.1",
        "Average":69.72,
        "ARC":67.66,
        "HellaSwag":86.49,
        "MMLU":70.3,
        "TruthfulQA":54.42,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":67.42,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"3120e204e1b4928fd784ae78fa754bc937352c98"
    },
    {
        "T":"\u2b55",
        "Model":"Mihaiii\/Pallas-0.2",
        "Average":69.71,
        "ARC":64.59,
        "HellaSwag":83.44,
        "MMLU":75.53,
        "TruthfulQA":55.29,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"2a367db35e91a1cac5abad8e5101e85d391e0551"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"chargoddard\/piano-medley-7b",
        "Average":69.71,
        "ARC":67.58,
        "HellaSwag":85.36,
        "MMLU":64.49,
        "TruthfulQA":61.42,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"38da429cb28f667e8868574f32269a04dfe41280"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fzzhang\/Marcoroni-neural-chat-7B-v2_gsm8k_merged_s",
        "Average":69.71,
        "ARC":67.15,
        "HellaSwag":85.68,
        "MMLU":62.72,
        "TruthfulQA":63.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c6ab98c227ff5c2e284571ed1a8c21c0f9db1a55"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Epiculous\/Fett-uccine-7B",
        "Average":69.7,
        "ARC":63.23,
        "HellaSwag":86.09,
        "MMLU":60.03,
        "TruthfulQA":69.47,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"agpl-3.0",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"823e103126393a0ba4a9cc6d082ab4cda54413ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azazelle\/SlimMelodicMaid",
        "Average":69.7,
        "ARC":67.15,
        "HellaSwag":86.01,
        "MMLU":64.75,
        "TruthfulQA":60.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"36c2dfb9e7822dc77a97172a517952bd6c32cd88"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"VAGOsolutions\/SauerkrautLM-Mixtral-8x7B",
        "Average":69.69,
        "ARC":68.86,
        "HellaSwag":86.01,
        "MMLU":66.69,
        "TruthfulQA":57.2,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"82dc0ab70090085b4271f0f317f667f180db9872"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SCE\/Mistral-7B-summ-lora-tuned",
        "Average":69.69,
        "ARC":62.8,
        "HellaSwag":85.19,
        "MMLU":60.58,
        "TruthfulQA":70.18,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"95a4a8b60197ffe2e0cabca25a0eec2ea050a562"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JosephusCheung\/Yee-34B-200K-Chat",
        "Average":69.68,
        "ARC":65.61,
        "HellaSwag":84.33,
        "MMLU":74.91,
        "TruthfulQA":53.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"gpl-3.0",
        "#Params (B)":34.0,
        "Hub":32,
        "Available on the hub":false,
        "Model Sha":"94bc30449e41628f59dd965cb7d9a8eb53ce9a45"
    },
    {
        "T":"?",
        "Model":"Steelskull\/VerB-Etheria-55b",
        "Average":69.68,
        "ARC":65.96,
        "HellaSwag":81.48,
        "MMLU":73.78,
        "TruthfulQA":57.52,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":55.59,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"dcab4ed7680a3e5c2f4e3ef36e880cb3b9149dd0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cookinai\/Valkyrie-V1",
        "Average":69.68,
        "ARC":67.24,
        "HellaSwag":86.27,
        "MMLU":64.82,
        "TruthfulQA":60.4,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"78917a93a47ea6d401458d0e283a2c6db6c68a47"
    },
    {
        "T":"?",
        "Model":"adamo1139\/Yi-34B-200K-AEZAKMI-RAW-2901",
        "Average":69.68,
        "ARC":64.93,
        "HellaSwag":84.98,
        "MMLU":73.7,
        "TruthfulQA":55.09,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"85cab3d5de8a4e2907616016eefc846a4ee1da61"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"freeCS-dot-org\/ThetaZero-7B-1",
        "Average":69.67,
        "ARC":67.49,
        "HellaSwag":85.69,
        "MMLU":63.03,
        "TruthfulQA":62.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5af7656feb7c0f4f33aaca6984b4600c511613f2"
    },
    {
        "T":"?",
        "Model":"Test157t\/Prima-LelantaclesV6.5-7b",
        "Average":69.67,
        "ARC":67.75,
        "HellaSwag":85.7,
        "MMLU":63.12,
        "TruthfulQA":62.12,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0c7a9a7b9e86a8662660e76f30a63f10a7dae4ef"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/aegolius-acadicus-34b-v3",
        "Average":69.67,
        "ARC":67.66,
        "HellaSwag":85.54,
        "MMLU":62.13,
        "TruthfulQA":63.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":35.43,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c43b47a1d94a5daf790c506d113e5ee258871822"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenPipe\/mistral-ft-optimized-1218",
        "Average":69.66,
        "ARC":67.92,
        "HellaSwag":86.26,
        "MMLU":64.99,
        "TruthfulQA":59.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":150,
        "Available on the hub":false,
        "Model Sha":"f4f3f6144dd143d6ec43ece9ab0fdd740ed610f1"
    },
    {
        "T":"?",
        "Model":"OpenBuddy\/openbuddy-deepseek-67b-v18.1-4k",
        "Average":69.66,
        "ARC":67.75,
        "HellaSwag":84.65,
        "MMLU":70.58,
        "TruthfulQA":55.66,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":67.42,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"e4ba7abdb25b00308f67589458cb9380a2ccd5e6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Seraph-7B",
        "Average":69.65,
        "ARC":67.83,
        "HellaSwag":86.22,
        "MMLU":65.07,
        "TruthfulQA":59.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"2c6ea500b4b33bc9231b56ee6a495cd96e63064a"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"chargoddard\/loyal-piano-m7-cdpo",
        "Average":69.65,
        "ARC":67.15,
        "HellaSwag":85.39,
        "MMLU":64.52,
        "TruthfulQA":61.53,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"5f5a78bedc2d3e5314589f685489bc981890cadf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chargoddard\/loyal-piano-m7-cdpo",
        "Average":69.64,
        "ARC":67.06,
        "HellaSwag":85.42,
        "MMLU":64.54,
        "TruthfulQA":61.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"5f5a78bedc2d3e5314589f685489bc981890cadf"
    },
    {
        "T":"?",
        "Model":"llmixer\/BigWeave-v6-90b",
        "Average":69.64,
        "ARC":65.36,
        "HellaSwag":87.21,
        "MMLU":68.04,
        "TruthfulQA":57.96,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":87.8,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"cf0355244f8cb18a0e3128e292219ccf774fe418"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nlpguy\/Lelantos-low-tune",
        "Average":69.64,
        "ARC":67.06,
        "HellaSwag":86.06,
        "MMLU":64.11,
        "TruthfulQA":61.33,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a0725dc1d3f591f2e9281c02f123fcde0a03c5db"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Einstein-bagel-7B",
        "Average":69.63,
        "ARC":66.89,
        "HellaSwag":84.81,
        "MMLU":63.48,
        "TruthfulQA":63.33,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"d66c858c2f4f24ae867423e6d844b6dc3a1208b1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AetherResearch\/Cerebrum-1.0-8x7b",
        "Average":69.62,
        "ARC":68.09,
        "HellaSwag":87.3,
        "MMLU":72.45,
        "TruthfulQA":50.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":78,
        "Available on the hub":false,
        "Model Sha":"590311de11895f6f9b4d8615b473808f919bef84"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"bn22\/Nous-Hermes-2-SOLAR-10.7B-MISALIGNED",
        "Average":69.6,
        "ARC":68.26,
        "HellaSwag":86.11,
        "MMLU":66.26,
        "TruthfulQA":57.79,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e402c5ea1ba23d776062f18306690296a708d469"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC99\/Mistral-7B-summ-lora-tuned-8h",
        "Average":69.6,
        "ARC":63.05,
        "HellaSwag":85.17,
        "MMLU":60.39,
        "TruthfulQA":69.8,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"58d912c7d4dcbf788f4215ea927d0cfca8239368"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kquant03\/Ryu-4x7B-MoE-bf16",
        "Average":69.6,
        "ARC":66.47,
        "HellaSwag":83.1,
        "MMLU":63.89,
        "TruthfulQA":64.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"26e93b95a192650f8b145d103dead6162568953c"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/BagelToppyLake-7B-slerp",
        "Average":69.6,
        "ARC":67.15,
        "HellaSwag":84.79,
        "MMLU":64.31,
        "TruthfulQA":62.15,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"291f0e17b1322c7fb10e770f0febc15216beab29"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/orthorus-125b-moe",
        "Average":69.6,
        "ARC":67.66,
        "HellaSwag":85.52,
        "MMLU":68.94,
        "TruthfulQA":56.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":125.35,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"3d45ea8340fd5d34db86a7099c2422480fe64533"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-70b-gpt4-m2.0",
        "Average":69.59,
        "ARC":70.05,
        "HellaSwag":87.83,
        "MMLU":70.67,
        "TruthfulQA":49.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"1cccd0b60a988bf6ddc4e2688895837845afa076"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Epiculous\/Mika-7B",
        "Average":69.59,
        "ARC":63.48,
        "HellaSwag":85.44,
        "MMLU":59.85,
        "TruthfulQA":69.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":7.0,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"364d896f55fb409701e2fea947ebda21908f8ecd"
    },
    {
        "T":"\u2b55",
        "Model":"adamo1139\/Yi-34B-AEZAKMI-v1",
        "Average":69.57,
        "ARC":64.33,
        "HellaSwag":84.31,
        "MMLU":73.91,
        "TruthfulQA":55.73,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"c56dc8471eba802f74fed756f555b718d975d00a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Intel\/neural-chat-7b-v3-3",
        "Average":69.56,
        "ARC":66.89,
        "HellaSwag":85.26,
        "MMLU":63.07,
        "TruthfulQA":63.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":58,
        "Available on the hub":false,
        "Model Sha":"fac83ab297a1c9ecc8affd97c998d864c10b9ff4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/WizardLM-Math-70B-TIES-v0.1",
        "Average":69.56,
        "ARC":68.52,
        "HellaSwag":86.87,
        "MMLU":69.24,
        "TruthfulQA":53.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":68.98,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"1e5a2039c5d48dc1786f18c72e538af06e76a8cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azazelle\/Maylin-7b",
        "Average":69.55,
        "ARC":66.81,
        "HellaSwag":86.4,
        "MMLU":64.73,
        "TruthfulQA":60.24,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c5fa9f4812daf2538e3e052c0346d9efb321c650"
    },
    {
        "T":"?",
        "Model":"saishf\/Fett-uccine-11B-Experiment",
        "Average":69.54,
        "ARC":63.14,
        "HellaSwag":85.39,
        "MMLU":59.72,
        "TruthfulQA":69.92,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b0673c461432527942cf2e82ffdca34360098712"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/mixtralmerge-8x7B-rebalanced-test",
        "Average":69.54,
        "ARC":68.17,
        "HellaSwag":85.76,
        "MMLU":70.47,
        "TruthfulQA":53.75,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"25093d03a4ee3a58b8eeb4d040b02b3a5f39ca95"
    },
    {
        "T":"?",
        "Model":"Aryanne\/MixSwap",
        "Average":69.54,
        "ARC":69.45,
        "HellaSwag":86.95,
        "MMLU":65.18,
        "TruthfulQA":56.56,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"8926d6bff276af1c21f4467d48555f4d13540b95"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_51-7B-dare_ties",
        "Average":69.54,
        "ARC":66.98,
        "HellaSwag":85.9,
        "MMLU":64.54,
        "TruthfulQA":60.72,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"270f3ed1f5684582ac50c1a34e9635b258da2b92"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CausalLM\/72B-preview",
        "Average":69.53,
        "ARC":65.19,
        "HellaSwag":83.23,
        "MMLU":77.14,
        "TruthfulQA":52.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"gpl-3.0",
        "#Params (B)":71.04,
        "Hub":73,
        "Available on the hub":true,
        "Model Sha":"508ee8ddfd8b823fcd4b0366a72c7981c8b447d8"
    },
    {
        "T":"\u2b55",
        "Model":"brucethemoose\/CaPlatTessDolXaBoros-Yi-34B-200K-DARE-Ties",
        "Average":69.53,
        "ARC":64.93,
        "HellaSwag":84.99,
        "MMLU":75.37,
        "TruthfulQA":52.84,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"7be35464f07307b5503d12736f732a34f3c1d8c1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/OpenHermes-2.5-neural-chat-v3-2-Slerp",
        "Average":69.52,
        "ARC":67.49,
        "HellaSwag":85.42,
        "MMLU":64.13,
        "TruthfulQA":61.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":27,
        "Available on the hub":false,
        "Model Sha":"bf9ef6df7732dbef3cd0001d9e5cba846cb47306"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-70b-gpt4-m2.0",
        "Average":69.51,
        "ARC":70.05,
        "HellaSwag":87.79,
        "MMLU":70.46,
        "TruthfulQA":49.75,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"1cccd0b60a988bf6ddc4e2688895837845afa076"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Pallas-0.5-LASER-0.3",
        "Average":69.51,
        "ARC":64.76,
        "HellaSwag":83.17,
        "MMLU":74.66,
        "TruthfulQA":55.43,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d2532427a883434ac152061b27d7c3cf0778868c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AGI-0\/ThetaWave-7B-v0.1",
        "Average":69.5,
        "ARC":65.96,
        "HellaSwag":85.72,
        "MMLU":63.07,
        "TruthfulQA":63.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"59b71b909b172d247b3bb27ed674172dd1302c44"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CausalLM\/72B-preview-llamafied-qwen-llamafy",
        "Average":69.5,
        "ARC":65.19,
        "HellaSwag":83.24,
        "MMLU":77.04,
        "TruthfulQA":52.55,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"gpl-3.0",
        "#Params (B)":71.04,
        "Hub":73,
        "Available on the hub":true,
        "Model Sha":"f16df07e24654858a6b04c3ecb0670dcfc42337d"
    },
    {
        "T":"?",
        "Model":"OpenBuddy\/openbuddy-deepseek-67b-v15.3-4k",
        "Average":69.5,
        "ARC":67.58,
        "HellaSwag":85.15,
        "MMLU":70.38,
        "TruthfulQA":54.88,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":67.42,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"7152f2dc8e0aceb0412e802653271cd9e59bf23e"
    },
    {
        "T":"?",
        "Model":"Nitral-AI\/Stanta-Lelemon-Maid-7B",
        "Average":69.49,
        "ARC":67.58,
        "HellaSwag":86.03,
        "MMLU":64.79,
        "TruthfulQA":59.58,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"35929b548e90a5a85af775ca09aa8028fc3fe0fe"
    },
    {
        "T":"?",
        "Model":"kidyu\/Moza-7B-v1.0",
        "Average":69.48,
        "ARC":66.55,
        "HellaSwag":83.45,
        "MMLU":62.77,
        "TruthfulQA":65.16,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5c7f382e59c9a114ea9c9f1f380739fc9e4a9d9e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-M-v1.3",
        "Average":69.47,
        "ARC":62.54,
        "HellaSwag":83.95,
        "MMLU":75.36,
        "TruthfulQA":56.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":33.93,
        "Hub":23,
        "Available on the hub":true,
        "Model Sha":"7d733ec8449ec0219a9f499084a94a4248846f7e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"EmbeddedLLM\/Mistral-7B-Merge-02-v0",
        "Average":69.47,
        "ARC":67.49,
        "HellaSwag":85.78,
        "MMLU":64.1,
        "TruthfulQA":60.52,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b142b88a1b6f015b6971d75aa191c6d16324d0c1"
    },
    {
        "T":"?",
        "Model":"PulsarAI\/Draco-8x7B",
        "Average":69.47,
        "ARC":65.02,
        "HellaSwag":85.24,
        "MMLU":64.96,
        "TruthfulQA":62.65,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c5747ec7ba9ca08eda4fd55ea32c67057db7b4d6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Draco-8x7B",
        "Average":69.47,
        "ARC":65.02,
        "HellaSwag":85.24,
        "MMLU":64.96,
        "TruthfulQA":62.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c5747ec7ba9ca08eda4fd55ea32c67057db7b4d6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CausalLM\/72B-preview",
        "Average":69.46,
        "ARC":64.85,
        "HellaSwag":83.28,
        "MMLU":77.21,
        "TruthfulQA":52.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":71.04,
        "Hub":73,
        "Available on the hub":true,
        "Model Sha":"508ee8ddfd8b823fcd4b0366a72c7981c8b447d8"
    },
    {
        "T":"\u2b55",
        "Model":"vicgalle\/franken-SOLAR-18B-v1.0",
        "Average":69.46,
        "ARC":65.53,
        "HellaSwag":86.45,
        "MMLU":63.72,
        "TruthfulQA":62.14,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":17.93,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"03c5412b8b0a6272cf02b399221ab94dbfd3157e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mychen76\/mistral-7b-merged-slerp",
        "Average":69.45,
        "ARC":67.75,
        "HellaSwag":86.17,
        "MMLU":64.05,
        "TruthfulQA":59.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f9a1661c70a8571c91023e09582c9c69f459a47c"
    },
    {
        "T":"?",
        "Model":"FredrikBL\/NeuralPipe-7B-slerp",
        "Average":69.45,
        "ARC":67.75,
        "HellaSwag":86.17,
        "MMLU":64.05,
        "TruthfulQA":59.85,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"acd7f60e7ce757dcbf0d97bd947378812b55e00b"
    },
    {
        "T":"?",
        "Model":"Samee-ur\/NeuralPipe-7B-slerp",
        "Average":69.45,
        "ARC":67.75,
        "HellaSwag":86.17,
        "MMLU":64.05,
        "TruthfulQA":59.85,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6814f1994fd78825ac803afc2684f43d6833f1cd"
    },
    {
        "T":"?",
        "Model":"AurelPx\/NeuralPipe-7B-slerp",
        "Average":69.45,
        "ARC":67.75,
        "HellaSwag":86.17,
        "MMLU":64.05,
        "TruthfulQA":59.85,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"71efe6e869f8983d99b0a1f525708480a73fd71c"
    },
    {
        "T":"?",
        "Model":"jondurbin\/bagel-dpo-7b-v0.4",
        "Average":69.44,
        "ARC":67.58,
        "HellaSwag":84.3,
        "MMLU":61.95,
        "TruthfulQA":63.94,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"1407000b20cf38cf59d7a2d1143cb0883abe5ab3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jeiku\/Zephyr_beta_32k_7B",
        "Average":69.44,
        "ARC":63.48,
        "HellaSwag":84.79,
        "MMLU":60.5,
        "TruthfulQA":68.99,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5860071cd3ccbc086e133ae4ba30583b3338a34d"
    },
    {
        "T":"?",
        "Model":"InnerI\/InnerILLM-7B-slerp",
        "Average":69.44,
        "ARC":67.58,
        "HellaSwag":86.19,
        "MMLU":64.15,
        "TruthfulQA":59.84,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e95ad53ab333c0cc083b927bddaa02f9423afdbb"
    },
    {
        "T":"?",
        "Model":"FredrikBL\/NeuralPipe-7B-slerp",
        "Average":69.44,
        "ARC":67.58,
        "HellaSwag":86.19,
        "MMLU":64.15,
        "TruthfulQA":59.84,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"acd7f60e7ce757dcbf0d97bd947378812b55e00b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-34B-v1.5b",
        "Average":69.43,
        "ARC":63.91,
        "HellaSwag":84.43,
        "MMLU":76.26,
        "TruthfulQA":53.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":33.93,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"9f17f1c66209dd923751a5242f33f0dfded9071f"
    },
    {
        "T":"?",
        "Model":"AbacusResearch\/Jallabi-34B",
        "Average":69.43,
        "ARC":66.04,
        "HellaSwag":83.81,
        "MMLU":76.4,
        "TruthfulQA":51.46,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"f3cefc6d178a7f61fc202f15dceedd041c2b6af8"
    },
    {
        "T":"?",
        "Model":"mahiatlinux\/ShadowDolph-7B-v1",
        "Average":69.43,
        "ARC":69.2,
        "HellaSwag":85.0,
        "MMLU":58.95,
        "TruthfulQA":64.56,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"357bc5342080b4de9b1926873d0aa46670280b17"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/openchat-3.5-1210-Seraph-Slerp",
        "Average":69.42,
        "ARC":68.09,
        "HellaSwag":86.48,
        "MMLU":65.33,
        "TruthfulQA":57.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"fdcc497bcf5e9ba62a9617617ff8f4e2965104e1"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"chargoddard\/servile-harpsichord-cdpo",
        "Average":69.41,
        "ARC":67.32,
        "HellaSwag":85.18,
        "MMLU":64.54,
        "TruthfulQA":60.61,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"13cdf6bd90df46f4fae1d31b9d3b4f7fc31a7777"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/NeuralPipe-7B-slerp",
        "Average":69.41,
        "ARC":67.58,
        "HellaSwag":86.17,
        "MMLU":64.06,
        "TruthfulQA":59.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"16485f6a8d83061f67515bfe20ed5afe8218c993"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"superlazycoder\/NeuralPipe-7B-slerp",
        "Average":69.41,
        "ARC":67.58,
        "HellaSwag":86.17,
        "MMLU":64.06,
        "TruthfulQA":59.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"98bf395c8868b226208debc63d67576fdee52528"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DeepKarkhanis\/Mistral-Passthrough-8L-10B",
        "Average":69.41,
        "ARC":67.58,
        "HellaSwag":86.17,
        "MMLU":64.06,
        "TruthfulQA":59.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8ebb167b4a27a9d49ec7399baf23eef6226c242d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DeepKarkhanis\/NeuralPipe-7B-slerp",
        "Average":69.41,
        "ARC":67.58,
        "HellaSwag":86.17,
        "MMLU":64.06,
        "TruthfulQA":59.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6d45f7ca3e55658264d0b0a26b3ef98433335db0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Jingyu6\/MergeTest-7B-slerp",
        "Average":69.41,
        "ARC":67.75,
        "HellaSwag":86.15,
        "MMLU":63.94,
        "TruthfulQA":59.8,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0c089098a27b01d577747f3071531a1a9c9d627c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/NeuralPipe-7B-slerp",
        "Average":69.41,
        "ARC":67.75,
        "HellaSwag":86.15,
        "MMLU":63.94,
        "TruthfulQA":59.8,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"e3ba53ca9b2171e3c2134cc022eabada932e032c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Chupacabra-7B-v2.02",
        "Average":69.4,
        "ARC":67.66,
        "HellaSwag":83.9,
        "MMLU":61.98,
        "TruthfulQA":64.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"24fb5e81b1d39d4358930a1f9054513e9e2d6373"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/llama2-70b-oasst-sft-v10",
        "Average":69.4,
        "ARC":67.06,
        "HellaSwag":86.38,
        "MMLU":67.7,
        "TruthfulQA":56.45,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":57,
        "Available on the hub":true,
        "Model Sha":"e68a8a2888097def3c7f4fe5d443866a18d05c6c"
    },
    {
        "T":"?",
        "Model":"macadeliccc\/laser-dolphin-mixtral-4x7b-dpo",
        "Average":69.39,
        "ARC":64.93,
        "HellaSwag":85.81,
        "MMLU":63.04,
        "TruthfulQA":63.77,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"472637ca2bf2bfb08aa4b5ebcdc5f89f48c7b257"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MisterRid\/saulgoodman-7b-alpha1",
        "Average":69.38,
        "ARC":65.7,
        "HellaSwag":85.5,
        "MMLU":65.19,
        "TruthfulQA":61.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c417af695d4e3370348e2ef15961884f127f7ff0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nextai-team\/Moe-2x7b-QA-Code",
        "Average":69.37,
        "ARC":65.19,
        "HellaSwag":85.36,
        "MMLU":61.71,
        "TruthfulQA":65.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"a7c3b2a840ba788a2adbb664f89bc6d95b4a8071"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/neural-chat-v3-3-8x7b-MoE",
        "Average":69.37,
        "ARC":66.64,
        "HellaSwag":85.43,
        "MMLU":62.22,
        "TruthfulQA":63.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ef354e7938f1c38bb1f73f4ee9a7f325ae32fc2e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeontaek\/llama-2-70B-ensemble-v8",
        "Average":69.37,
        "ARC":67.24,
        "HellaSwag":84.56,
        "MMLU":63.56,
        "TruthfulQA":62.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"81c4b32ec8062b3e9af4492fc6590f2efa6451d9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Intel\/neural-chat-7b-v3-3-Slerp",
        "Average":69.37,
        "ARC":66.64,
        "HellaSwag":85.43,
        "MMLU":62.19,
        "TruthfulQA":63.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":52,
        "Available on the hub":false,
        "Model Sha":"cbd4f663365e40d50ed9834016bf840971b35db5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ResplendentAI\/Sinerva_7B",
        "Average":69.36,
        "ARC":70.14,
        "HellaSwag":85.59,
        "MMLU":61.77,
        "TruthfulQA":59.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"2193384d58f41418087998167cf6ec20c76582b0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zhengr\/NeuralPipe-7B-slerp",
        "Average":69.36,
        "ARC":67.41,
        "HellaSwag":86.12,
        "MMLU":64.07,
        "TruthfulQA":59.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":27,
        "Available on the hub":false,
        "Model Sha":"6a6405b269c94043658c342d3e124aa3ba75d621"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Abhinav7\/NeuralPipe-7B-slerp",
        "Average":69.36,
        "ARC":67.41,
        "HellaSwag":86.12,
        "MMLU":64.07,
        "TruthfulQA":59.82,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"62eb03a76c4c607afc8524cf725c48fbb6a1827a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RatanRohith\/NeuralMathChat-7B-V0.2",
        "Average":69.34,
        "ARC":67.41,
        "HellaSwag":85.78,
        "MMLU":65.09,
        "TruthfulQA":59.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"d59d54fb5a8522b8f79df6abb514f03c091dd88f"
    },
    {
        "T":"?",
        "Model":"Eric111\/Snorkel-Mistral-PairRM-DPO-openchat-3.5-0106-laser",
        "Average":69.34,
        "ARC":67.32,
        "HellaSwag":85.11,
        "MMLU":63.23,
        "TruthfulQA":61.69,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2504e46e66eb320718545971bf6e4bf4eb627343"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/openchat-3.5-1210-Seraph-Slerp",
        "Average":69.34,
        "ARC":67.92,
        "HellaSwag":86.43,
        "MMLU":65.26,
        "TruthfulQA":57.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"46bb19fb19ff3673bdde3b38ee8e3f3884df8113"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC44\/Mistral-7B-private-spef",
        "Average":69.33,
        "ARC":63.23,
        "HellaSwag":84.93,
        "MMLU":60.8,
        "TruthfulQA":68.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bb29794e86ff6a39f77185f547c6bb335d2f5649"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ziniuli\/Mistral-7B-ReMax-v0.1",
        "Average":69.32,
        "ARC":63.31,
        "HellaSwag":84.98,
        "MMLU":60.89,
        "TruthfulQA":68.11,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c99152ef7fab26a55a8b9ac7766c394acc54fcad"
    },
    {
        "T":"?",
        "Model":"pandego\/my-first-blend",
        "Average":69.32,
        "ARC":69.8,
        "HellaSwag":82.93,
        "MMLU":53.89,
        "TruthfulQA":70.68,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"416ad8eddfcaa93ad39a42237144d9c97ee87842"
    },
    {
        "T":"\u2b55",
        "Model":"gagan3012\/MetaModel_moe_multilingualv1",
        "Average":69.32,
        "ARC":67.58,
        "HellaSwag":84.72,
        "MMLU":63.77,
        "TruthfulQA":61.21,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1b27a5aa3381f82ae99e8187bbd982e319eafd17"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jeiku\/Luna_7B",
        "Average":69.32,
        "ARC":68.86,
        "HellaSwag":86.28,
        "MMLU":64.06,
        "TruthfulQA":58.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"29751a19842fd9cae038b120dc77793b63cda663"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Instruct-v0.2-Seraph-7B",
        "Average":69.31,
        "ARC":64.76,
        "HellaSwag":84.2,
        "MMLU":62.9,
        "TruthfulQA":65.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6ea01ce2a3b6967d9aaf968ed8015da21c979928"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ziniuli\/Mistral-7B-ReMax-v0.1",
        "Average":69.3,
        "ARC":63.31,
        "HellaSwag":84.98,
        "MMLU":60.76,
        "TruthfulQA":68.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c99152ef7fab26a55a8b9ac7766c394acc54fcad"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"rishiraj\/oswald-2x7b",
        "Average":69.3,
        "ARC":66.47,
        "HellaSwag":85.46,
        "MMLU":65.2,
        "TruthfulQA":60.06,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"50fa192492461fdfcd8ce1c84e9081891141a5ac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051611\/limb",
        "Average":69.29,
        "ARC":63.48,
        "HellaSwag":83.07,
        "MMLU":72.25,
        "TruthfulQA":58.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"55473b7666b66e5b51bb3c4e6b5bc88d1bd00666"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ResplendentAI\/Paradigm_Shift_7B",
        "Average":69.29,
        "ARC":67.92,
        "HellaSwag":83.69,
        "MMLU":59.49,
        "TruthfulQA":66.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"c39cf7ba63acf8eaafc7b08d67fac494e64df98e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/Calme-7B-Instruct-v0.1",
        "Average":69.29,
        "ARC":67.24,
        "HellaSwag":85.57,
        "MMLU":64.97,
        "TruthfulQA":59.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"5f18e24665f62b8e9a3492af247978073fea54f9"
    },
    {
        "T":"?",
        "Model":"gagan3012\/MetaModel_moe_multilingualv1",
        "Average":69.28,
        "ARC":67.24,
        "HellaSwag":84.73,
        "MMLU":63.93,
        "TruthfulQA":61.23,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1b27a5aa3381f82ae99e8187bbd982e319eafd17"
    },
    {
        "T":"?",
        "Model":"LeroyDyer\/Mistral_AI_v2",
        "Average":69.28,
        "ARC":65.44,
        "HellaSwag":85.61,
        "MMLU":63.44,
        "TruthfulQA":62.63,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":40,
        "Available on the hub":false,
        "Model Sha":"bede68f5e1f2225c281fdd97cc58bf0e9389e311"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC44\/Mistral-7B-private-spnf",
        "Average":69.28,
        "ARC":63.05,
        "HellaSwag":84.9,
        "MMLU":60.82,
        "TruthfulQA":68.34,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b517b17cdec49dfa1dbb1927cfcac97eec020a59"
    },
    {
        "T":"\u2b55",
        "Model":"mistralai\/Mistral-7B-Instruct-v0.2",
        "Average":69.26,
        "ARC":63.14,
        "HellaSwag":84.88,
        "MMLU":60.78,
        "TruthfulQA":68.26,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1607,
        "Available on the hub":false,
        "Model Sha":"c72e5d1908b1e2929ec8fc4c8820e9706af1f80f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"localfultonextractor\/Erosumika-7B",
        "Average":69.25,
        "ARC":62.88,
        "HellaSwag":85.9,
        "MMLU":60.64,
        "TruthfulQA":67.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"fe495970cdaf66b16b2dc77567adb7bf3fe7fe90"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pandego\/my-first-blend",
        "Average":69.25,
        "ARC":69.37,
        "HellaSwag":83.03,
        "MMLU":53.91,
        "TruthfulQA":70.7,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"416ad8eddfcaa93ad39a42237144d9c97ee87842"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NovoCode\/Metabird-7b-DPO",
        "Average":69.25,
        "ARC":65.96,
        "HellaSwag":86.29,
        "MMLU":64.46,
        "TruthfulQA":60.3,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"5c235db8dcfb564784e6f328ded93205475667ed"
    },
    {
        "T":"\u2b55",
        "Model":"alykassem\/ds_diasum_md_mixtral",
        "Average":69.24,
        "ARC":66.3,
        "HellaSwag":85.45,
        "MMLU":69.51,
        "TruthfulQA":55.72,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"openrail",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8ee85e4555b4c4a75b29ee749a86c97e0d37d242"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC99\/Mistral-7B-privatemix-base-ia",
        "Average":69.24,
        "ARC":62.8,
        "HellaSwag":84.85,
        "MMLU":60.54,
        "TruthfulQA":68.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e2761606e477ff1540a501dc89b39f65ff16c652"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/OpenHermes-2.5-neural-chat-7b-v3-2-7B",
        "Average":69.23,
        "ARC":66.38,
        "HellaSwag":84.11,
        "MMLU":62.84,
        "TruthfulQA":63.59,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":23,
        "Available on the hub":false,
        "Model Sha":"585c2fca1dce1904491c40408f6dd5404eca3754"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cognitivecomputations\/dolphin-2.6-mistral-7b-dpo-laser",
        "Average":69.23,
        "ARC":66.3,
        "HellaSwag":85.73,
        "MMLU":63.16,
        "TruthfulQA":61.71,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":112,
        "Available on the hub":false,
        "Model Sha":"05cc9e559e87e7e269401a3843a0e63a6084a85e"
    },
    {
        "T":"?",
        "Model":"notadib\/Mistral-7B-Instruct-v0.2-attention-sparsity-10-v0.1",
        "Average":69.22,
        "ARC":63.05,
        "HellaSwag":84.88,
        "MMLU":60.84,
        "TruthfulQA":68.11,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"228a9d4a23b0d9fa05c4f744c1163af49c0ba468"
    },
    {
        "T":"?",
        "Model":"azarafrooz\/Mistral-7B-Instruct-v0.2-sp-v0",
        "Average":69.22,
        "ARC":63.05,
        "HellaSwag":84.84,
        "MMLU":60.75,
        "TruthfulQA":68.22,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"3df848493713aafe17011d7dfbe2c8b11c1b364f"
    },
    {
        "T":"?",
        "Model":"azarafrooz\/Mistral-7B-Instruct-v2-sp-v0.1",
        "Average":69.22,
        "ARC":63.05,
        "HellaSwag":84.84,
        "MMLU":60.75,
        "TruthfulQA":68.22,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"8dfe19f4bda45edfab91f895e28ca41b251117cc"
    },
    {
        "T":"?",
        "Model":"azarafrooz\/mistral-v2-7b-selfplay-low-tmp",
        "Average":69.22,
        "ARC":63.05,
        "HellaSwag":84.91,
        "MMLU":60.76,
        "TruthfulQA":68.13,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"78caf2641e8b84495d5199b81e51920c7b10285e"
    },
    {
        "T":"?",
        "Model":"azarafrooz\/mistral-v2-7b-selfplay-v0",
        "Average":69.22,
        "ARC":63.05,
        "HellaSwag":84.88,
        "MMLU":60.78,
        "TruthfulQA":68.14,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"60b1e543f54cc5b803bc4e4f22f8716c472370d8"
    },
    {
        "T":"?",
        "Model":"maldv\/winter-garden-7b-delta",
        "Average":69.22,
        "ARC":64.16,
        "HellaSwag":84.37,
        "MMLU":60.38,
        "TruthfulQA":67.95,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3d541081d69248a76260278f4bbc89d8811c6a4a"
    },
    {
        "T":"?",
        "Model":"Steelskull\/Etheria-55b-v0.1",
        "Average":69.21,
        "ARC":65.1,
        "HellaSwag":81.93,
        "MMLU":73.66,
        "TruthfulQA":56.16,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":55.59,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"ebcddf3a31409c9809ab3876ea390f4fb7f313b3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"diffnamehard\/Mistral-CatMacaroni-slerp-gradient",
        "Average":69.2,
        "ARC":65.53,
        "HellaSwag":85.66,
        "MMLU":61.53,
        "TruthfulQA":64.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"daf6eee865b05b45a4ce61af906313a80de06a9d"
    },
    {
        "T":"\u2b55",
        "Model":"notbdq\/alooowso",
        "Average":69.2,
        "ARC":62.97,
        "HellaSwag":84.87,
        "MMLU":60.78,
        "TruthfulQA":68.18,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"420f54afd10959bb1c86b485245349cd437960b5"
    },
    {
        "T":"?",
        "Model":"wang7776\/Mistral-7B-Instruct-v0.2-attention-sparsity-20",
        "Average":69.2,
        "ARC":62.88,
        "HellaSwag":84.84,
        "MMLU":60.81,
        "TruthfulQA":68.26,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"17604249cd95a3454146c6de2729915fb6018e10"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/Mixtral_13B_Chat",
        "Average":69.2,
        "ARC":67.41,
        "HellaSwag":85.87,
        "MMLU":64.54,
        "TruthfulQA":58.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2469744c92f2bc1f988b1588fff86bfcbf084ed6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mayacinka\/West-Ramen-7Bx4",
        "Average":69.2,
        "ARC":67.58,
        "HellaSwag":85.52,
        "MMLU":62.69,
        "TruthfulQA":61.0,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bc62dcbb054c7b6368d85eda9f2d41750e4d69f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Mistral-7B-Instruct-v0.2-2x7B-MoE",
        "Average":69.19,
        "ARC":62.97,
        "HellaSwag":84.88,
        "MMLU":60.74,
        "TruthfulQA":68.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"46a2d11c1025e6ddec0fe35093d39e2e16170ca2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SCE\/Mistral-7B-math-ia3-pruned10",
        "Average":69.18,
        "ARC":63.14,
        "HellaSwag":84.71,
        "MMLU":60.72,
        "TruthfulQA":68.16,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c49a49e078cee2c0ed152f32b80f1d992f806f67"
    },
    {
        "T":"?",
        "Model":"giraffe176\/Open_Maid_Samantha_Hermes_Orca_dare_ties",
        "Average":69.18,
        "ARC":67.75,
        "HellaSwag":86.39,
        "MMLU":64.6,
        "TruthfulQA":57.97,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f124dfbc767e7ece3fe5f209a0768e69d8024290"
    },
    {
        "T":"?",
        "Model":"giraffe176\/Open_Hermes_Maid_Sam_Mistral_dtv0.1",
        "Average":69.18,
        "ARC":67.75,
        "HellaSwag":86.39,
        "MMLU":64.6,
        "TruthfulQA":57.97,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":137,
        "Available on the hub":false,
        "Model Sha":"a141139153dc0804e288e951fb7e777783872946"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AiMavenAi\/AiMaven-SmartDawg-7b",
        "Average":69.16,
        "ARC":67.92,
        "HellaSwag":87.16,
        "MMLU":62.69,
        "TruthfulQA":58.86,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"b91cc33a1842344921dfd8ea9d7040277cafd8d6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luqmanxyz\/LelaStarling-7B",
        "Average":69.15,
        "ARC":67.58,
        "HellaSwag":86.33,
        "MMLU":64.98,
        "TruthfulQA":57.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"249b4fed97bfddc8f69e28274a2f4211296de246"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-70b-gpt4-2.0",
        "Average":69.15,
        "ARC":68.52,
        "HellaSwag":87.89,
        "MMLU":70.41,
        "TruthfulQA":49.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"f16526d9bb814dc10adc911f94e8c7a520beb5b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MisterRid\/saulgoodman-2x7b-alpha1",
        "Average":69.15,
        "ARC":66.21,
        "HellaSwag":85.36,
        "MMLU":64.95,
        "TruthfulQA":60.06,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b5ad66de184f72fa9525877ea6a62aa7bdc4815c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elinas\/chronos-70b-v2",
        "Average":69.14,
        "ARC":68.09,
        "HellaSwag":86.5,
        "MMLU":68.28,
        "TruthfulQA":53.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"373af41ca0b2855972b8d471fd63e72b63e4c9fc"
    },
    {
        "T":"\u2b55",
        "Model":"wang7776\/Mistral-7B-Instruct-v0.2-sparsity-10",
        "Average":69.13,
        "ARC":62.88,
        "HellaSwag":84.85,
        "MMLU":60.87,
        "TruthfulQA":67.93,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9f83457d019d7b1471f09a1e967b15cd748f3e77"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"freecs\/ThetaWave-7B-v0",
        "Average":69.12,
        "ARC":68.52,
        "HellaSwag":85.35,
        "MMLU":61.07,
        "TruthfulQA":61.56,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"89c74880ff1621a555374b2867f564131b3f4352"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jarradh\/llama2_70b_chat_uncensored",
        "Average":69.11,
        "ARC":68.43,
        "HellaSwag":86.77,
        "MMLU":68.76,
        "TruthfulQA":52.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":30,
        "Available on the hub":true,
        "Model Sha":"34b23982a9a996adc8f45c4c2eac7245c4e251b3"
    },
    {
        "T":"?",
        "Model":"azarafrooz\/mistral-v2-7b-selfplay-v0-test",
        "Average":69.09,
        "ARC":62.97,
        "HellaSwag":84.86,
        "MMLU":60.64,
        "TruthfulQA":67.91,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e34f3b09c03a9a96e1a76dfbd57a88a99c82a595"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HanNayeoniee\/LHK_44",
        "Average":69.09,
        "ARC":66.55,
        "HellaSwag":84.86,
        "MMLU":65.37,
        "TruthfulQA":59.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"89b6a3be6c3b6a2fa729de466ec20153665359dd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"EmbeddedLLM\/Mistral-7B-Merge-14-v0.4",
        "Average":69.08,
        "ARC":66.81,
        "HellaSwag":86.15,
        "MMLU":65.1,
        "TruthfulQA":58.25,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"367cfe8d6e046684ba8626444e82d1600c4e78a4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OrionStarAI\/OrionStar-Yi-34B-Chat-Llama",
        "Average":69.07,
        "ARC":64.93,
        "HellaSwag":84.34,
        "MMLU":73.67,
        "TruthfulQA":53.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"333c788e0d026cdb76bb827b8dcbc14a859ae2cc"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardMath-70B-V1.0",
        "Average":69.06,
        "ARC":68.17,
        "HellaSwag":86.49,
        "MMLU":68.89,
        "TruthfulQA":52.69,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":91,
        "Available on the hub":true,
        "Model Sha":"e85b43e53c5379e35393b970c66d76c2d1060381"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/loyal-piano-m7",
        "Average":69.06,
        "ARC":66.72,
        "HellaSwag":85.03,
        "MMLU":64.43,
        "TruthfulQA":60.03,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":25,
        "Available on the hub":false,
        "Model Sha":"d74ae6cb13325e0f81797ee33c07f0e234a2caa4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ResplendentAI\/Asherah_7B",
        "Average":69.05,
        "ARC":68.17,
        "HellaSwag":86.05,
        "MMLU":63.92,
        "TruthfulQA":58.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"0eba35e22c2aafd69d14bf7e41c3f201eb6bcc3b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/OpenHermes-2.5-neural-chat-v3-3-openchat-3.5-1210-Slerp",
        "Average":69.04,
        "ARC":67.92,
        "HellaSwag":86.32,
        "MMLU":65.47,
        "TruthfulQA":56.45,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"b6211b2dc4dcf29ca79ba3d6751b3ad071413eeb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jordiclive\/Llama-2-70b-oasst-1-200",
        "Average":69.04,
        "ARC":67.66,
        "HellaSwag":87.24,
        "MMLU":69.95,
        "TruthfulQA":51.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":68.72,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"153b209007e688d713cd670c9972f2827c597b45"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Eurdem\/megatron_v1",
        "Average":69.02,
        "ARC":65.96,
        "HellaSwag":84.8,
        "MMLU":65.02,
        "TruthfulQA":60.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"526323001ad41288cadb1395405e7df79524c68e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Chupacabra-16B-v2.01",
        "Average":69.02,
        "ARC":65.36,
        "HellaSwag":82.92,
        "MMLU":63.27,
        "TruthfulQA":64.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":16.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3b723559b550a34e489cc41ec5414e00531ec2ae"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nlpguy\/ColorShadow-7B-v3",
        "Average":69.02,
        "ARC":67.58,
        "HellaSwag":85.04,
        "MMLU":60.57,
        "TruthfulQA":62.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9dd05fe04e8a0ef7e7c0f72dd9ca2319c5813072"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardMath-70B-V1.0",
        "Average":69.02,
        "ARC":67.92,
        "HellaSwag":86.46,
        "MMLU":68.92,
        "TruthfulQA":52.77,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":91,
        "Available on the hub":true,
        "Model Sha":"e85b43e53c5379e35393b970c66d76c2d1060381"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HenryJJ\/dolphin-2.6-mistral-7b-dpo-orca-v2",
        "Average":69.02,
        "ARC":66.13,
        "HellaSwag":84.9,
        "MMLU":62.64,
        "TruthfulQA":62.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b1c44f6b0e9191a633837603c1053366868fc945"
    },
    {
        "T":"?",
        "Model":"nonetrix\/pippafeet-11B-0.1",
        "Average":69.01,
        "ARC":63.65,
        "HellaSwag":82.25,
        "MMLU":65.03,
        "TruthfulQA":65.12,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.6,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ff8da0fb475e20f68bce8b8141d172df3c4f0ffb"
    },
    {
        "T":"?",
        "Model":"LeroyDyer\/Mixtral_BioMedical",
        "Average":69.01,
        "ARC":65.44,
        "HellaSwag":85.2,
        "MMLU":63.17,
        "TruthfulQA":62.24,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"565e44539eeb5db84fda3d030e16b4bc09373de6"
    },
    {
        "T":"\u2b55",
        "Model":"kyujinpy\/PlatYi-34B-Llama-Q-v3",
        "Average":69.0,
        "ARC":64.33,
        "HellaSwag":84.88,
        "MMLU":74.98,
        "TruthfulQA":51.8,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":34.39,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"2d04b9e3a6c86a718c33e0686c0b5f4e46feb364"
    },
    {
        "T":"\u2b55",
        "Model":"Mihaiii\/Metis-0.3",
        "Average":69.0,
        "ARC":62.71,
        "HellaSwag":84.8,
        "MMLU":60.92,
        "TruthfulQA":67.56,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d5b89820d04640d217aa3c174fa1d1ad5553419a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-70b-gpt4-2.0",
        "Average":68.99,
        "ARC":68.17,
        "HellaSwag":87.92,
        "MMLU":70.11,
        "TruthfulQA":49.75,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"f16526d9bb814dc10adc911f94e8c7a520beb5b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cognitivecomputations\/dolphin-2.6-mistral-7b-dpo",
        "Average":68.95,
        "ARC":65.61,
        "HellaSwag":85.48,
        "MMLU":63.24,
        "TruthfulQA":61.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":55,
        "Available on the hub":false,
        "Model Sha":"5c32e515f3d79beefc110e8a07c3671269a0f5ab"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SCE\/Mistral-7B-math-ia3-pruned20",
        "Average":68.94,
        "ARC":63.05,
        "HellaSwag":84.42,
        "MMLU":60.55,
        "TruthfulQA":67.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3a2ccd35edf2dad09efb591699de46cc3a31adc4"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"NExtNewChattingAI\/shark_tank_ai_7b_v2",
        "Average":68.94,
        "ARC":67.75,
        "HellaSwag":87.06,
        "MMLU":58.79,
        "TruthfulQA":62.15,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b0796cb9cd42de2f66f652f162c29fdc57de2332"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/Mixtral-SlimOrca-8x7B",
        "Average":68.93,
        "ARC":67.66,
        "HellaSwag":85.11,
        "MMLU":67.98,
        "TruthfulQA":54.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":50,
        "Available on the hub":false,
        "Model Sha":"e06a613acf6c8cb3e5a740e2ed6348b8047d90a8"
    },
    {
        "T":"?",
        "Model":"LeroyDyer\/Mixtral_Uncensored",
        "Average":68.93,
        "ARC":63.82,
        "HellaSwag":84.07,
        "MMLU":61.96,
        "TruthfulQA":65.86,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3167f5d9a3f0fc7e96f1317ff8f29b4eee106c55"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SCE\/Mistral-7B-summ-ia3-pruned10",
        "Average":68.93,
        "ARC":63.05,
        "HellaSwag":84.88,
        "MMLU":59.67,
        "TruthfulQA":68.1,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c88d20e1763e41c1f1a77095b670221a568ca343"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NExtNewChattingAI\/shark_tank_ai_7b_v2",
        "Average":68.92,
        "ARC":67.58,
        "HellaSwag":87.02,
        "MMLU":58.88,
        "TruthfulQA":62.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b0796cb9cd42de2f66f652f162c29fdc57de2332"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"macadeliccc\/laser-dolphin-mixtral-2x7b-dpo",
        "Average":68.92,
        "ARC":65.96,
        "HellaSwag":85.8,
        "MMLU":63.17,
        "TruthfulQA":60.76,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":46,
        "Available on the hub":false,
        "Model Sha":"0ece1807074c4f1b9461e271a8931e4947902fbb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Newton-OpenHermes-2.5-neural-chat-v3-3-Slerp",
        "Average":68.92,
        "ARC":68.77,
        "HellaSwag":85.0,
        "MMLU":65.06,
        "TruthfulQA":56.84,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"27d3bd02299580f326cc358d6d98e06a950d937e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gmonsoon\/OpenMia-Indo-Mistral-7b-v3",
        "Average":68.92,
        "ARC":66.13,
        "HellaSwag":85.47,
        "MMLU":64.03,
        "TruthfulQA":60.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7493632477dea1221505b2af5ecde0757106ff86"
    },
    {
        "T":"?",
        "Model":"notadib\/Mistral-7B-Instruct-v0.2-attention-sparsity-30",
        "Average":68.91,
        "ARC":62.97,
        "HellaSwag":84.71,
        "MMLU":60.49,
        "TruthfulQA":67.49,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"464534c288dbd5e6c495fac6273e5c91ea40cd5a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Pallas-0.5-LASER-0.4",
        "Average":68.91,
        "ARC":63.31,
        "HellaSwag":82.74,
        "MMLU":74.32,
        "TruthfulQA":55.25,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"b2f3a60d2cbf70d773f45cc9a7363481f7d1027f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-deepseek-67b-v15-base",
        "Average":68.9,
        "ARC":66.3,
        "HellaSwag":86.03,
        "MMLU":70.97,
        "TruthfulQA":52.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":67.42,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2717bb85e0cd4c1c4abfa3d4abb7f9b6e55c1322"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeuralNovel\/Mistral-7B-Instruct-v0.2-Neural-Story",
        "Average":68.9,
        "ARC":64.08,
        "HellaSwag":83.97,
        "MMLU":60.67,
        "TruthfulQA":66.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":22,
        "Available on the hub":false,
        "Model Sha":"8f3198a3e235d7c1ae56befbe8fb14a974acdf69"
    },
    {
        "T":"?",
        "Model":"NeuralNovel\/Ignis-7B-DPO",
        "Average":68.9,
        "ARC":66.3,
        "HellaSwag":84.85,
        "MMLU":58.99,
        "TruthfulQA":65.46,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"4794b5e366ff56097a1136715415748822fba9d4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SCE\/Mistral-7B-summ-ia3-pruned20",
        "Average":68.9,
        "ARC":62.88,
        "HellaSwag":84.77,
        "MMLU":60.09,
        "TruthfulQA":67.84,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"14b2da3b2905056d0837730e0dccd82101bd8d27"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/OpenHermes-2.5-neural-chat-7b-v3-1-7B",
        "Average":68.89,
        "ARC":66.55,
        "HellaSwag":84.47,
        "MMLU":63.34,
        "TruthfulQA":61.22,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":40,
        "Available on the hub":false,
        "Model Sha":"2e72eb3999108b7a9c7d0d0c6b8d81ad3470f1f5"
    },
    {
        "T":"\u2b55",
        "Model":"bongchoi\/MoMo-70B-LoRA-V1.1",
        "Average":68.88,
        "ARC":66.64,
        "HellaSwag":87.16,
        "MMLU":66.76,
        "TruthfulQA":54.98,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":70.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ade069976a810b6b7caf3173a1aa4bfb30534ec9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"martyn\/mixtral-megamerge-dare-8x7b-v2",
        "Average":68.88,
        "ARC":66.47,
        "HellaSwag":86.11,
        "MMLU":69.14,
        "TruthfulQA":53.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a2dda73a962e3bda8893d951c836711e8ca84cea"
    },
    {
        "T":"?",
        "Model":"azarafrooz\/Mistral-7B-Instruct-v0.2-Selfplay-v0",
        "Average":68.87,
        "ARC":62.8,
        "HellaSwag":84.74,
        "MMLU":60.6,
        "TruthfulQA":67.35,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"636a1aede230bc57c951994992ec0c01c1d927af"
    },
    {
        "T":"?",
        "Model":"Stopwolf\/Tito-7B-slerp",
        "Average":68.87,
        "ARC":68.09,
        "HellaSwag":86.38,
        "MMLU":64.01,
        "TruthfulQA":57.01,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"d19cb51fa509b97ba0e2d49bc24a9a23e2885c19"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wang7776\/Mistral-7B-Instruct-v0.2-sparsity-20-v0.1",
        "Average":68.87,
        "ARC":62.29,
        "HellaSwag":84.9,
        "MMLU":60.63,
        "TruthfulQA":67.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7fcb7c43ea470c1c990472432e1a82fb0ae17646"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beberik\/Nyxene-11B",
        "Average":68.87,
        "ARC":68.34,
        "HellaSwag":84.54,
        "MMLU":65.09,
        "TruthfulQA":57.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"55e115157836e1529dd28fc56e2900a5f0e79b89"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sethuiyer\/distilabled_Chikuma_10.7B",
        "Average":68.86,
        "ARC":66.38,
        "HellaSwag":85.14,
        "MMLU":64.7,
        "TruthfulQA":59.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a5a6ba84916b025cdce898d17387e4b4bc31104f"
    },
    {
        "T":"\u2b55",
        "Model":"kyujinpy\/PlatYi-34B-Llama-Q-v2",
        "Average":68.86,
        "ARC":61.09,
        "HellaSwag":85.09,
        "MMLU":76.59,
        "TruthfulQA":52.65,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"10ca8ee92ce7e749b8480de603bd8599d8d1fb29"
    },
    {
        "T":"\u2b55",
        "Model":"martyn\/mixtral-megamerge-dare-8x7b-v2",
        "Average":68.85,
        "ARC":66.47,
        "HellaSwag":86.05,
        "MMLU":69.08,
        "TruthfulQA":53.82,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a2dda73a962e3bda8893d951c836711e8ca84cea"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"freecs\/ThetaWave-7B-v0.1",
        "Average":68.85,
        "ARC":66.3,
        "HellaSwag":85.4,
        "MMLU":63.47,
        "TruthfulQA":60.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c2aea352e9697d0bbeb4e3e469f71ba691625c00"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/FT",
        "Average":68.85,
        "ARC":63.05,
        "HellaSwag":82.78,
        "MMLU":69.69,
        "TruthfulQA":59.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1b91227a0539deaf4dfb5b18d15c92316e0254c3"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"deepseek-ai\/deepseek-llm-67b-base",
        "Average":68.85,
        "ARC":65.44,
        "HellaSwag":87.1,
        "MMLU":71.78,
        "TruthfulQA":51.08,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":66.59,
        "Hub":99,
        "Available on the hub":true,
        "Model Sha":"c3f813a1121c95488a20132d3a4da89f4a46452f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azazelle\/Argetsu",
        "Average":68.85,
        "ARC":67.06,
        "HellaSwag":86.32,
        "MMLU":65.55,
        "TruthfulQA":56.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e7238116d58f218368ab8e8099abec3cd60237c2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"splm\/openchat-spin-slimorca-iter2",
        "Average":68.84,
        "ARC":68.0,
        "HellaSwag":83.97,
        "MMLU":64.39,
        "TruthfulQA":59.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"051fc15365204500bc32de026063fa25b5513413"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"splm\/openchat-spin-slimorca-iter3",
        "Average":68.84,
        "ARC":68.0,
        "HellaSwag":83.97,
        "MMLU":64.39,
        "TruthfulQA":59.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7fcc1c3ed549282789aeab73573b7ebc6262685c"
    },
    {
        "T":"\u2b55",
        "Model":"OpenLemur\/lemur-70b-chat-v1",
        "Average":68.82,
        "ARC":66.98,
        "HellaSwag":85.73,
        "MMLU":65.99,
        "TruthfulQA":56.58,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub":35,
        "Available on the hub":true,
        "Model Sha":"33da87ba6d90662c6a00535bd628e5b39b3afd3b"
    },
    {
        "T":"?",
        "Model":"rombodawg\/Everyone-LLM-7b-Base",
        "Average":68.81,
        "ARC":66.38,
        "HellaSwag":86.02,
        "MMLU":64.94,
        "TruthfulQA":57.89,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"unknown",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"194a54e6d8ee1ef256e2c57c87ba1f76185663b0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/FT",
        "Average":68.81,
        "ARC":63.14,
        "HellaSwag":82.78,
        "MMLU":69.5,
        "TruthfulQA":59.8,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1b91227a0539deaf4dfb5b18d15c92316e0254c3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PetroGPT\/Voldemort-10B-DPO",
        "Average":68.8,
        "ARC":66.04,
        "HellaSwag":84.84,
        "MMLU":62.88,
        "TruthfulQA":61.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dfb9681fdbcd421f15f9cc3fb5d4df9adb539944"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GritLM\/GritLM-8x7B",
        "Average":68.79,
        "ARC":67.75,
        "HellaSwag":86.52,
        "MMLU":71.42,
        "TruthfulQA":49.47,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":23,
        "Available on the hub":false,
        "Model Sha":"7f089b13e3345510281733ca1e6ff871b5b4bc76"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Q-bert\/MetaMath-Cybertron-Starling",
        "Average":68.79,
        "ARC":67.75,
        "HellaSwag":86.23,
        "MMLU":65.24,
        "TruthfulQA":55.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":40,
        "Available on the hub":false,
        "Model Sha":"c274ec29903792dfdc584dc840cc16e952bd3122"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"meta-math\/MetaMath-70B-V1.0",
        "Average":68.78,
        "ARC":68.0,
        "HellaSwag":86.85,
        "MMLU":69.31,
        "TruthfulQA":50.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"783a3c7d5d0a75e6e11074f2577b90dd219ef7b1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Inv\/MoeMoE-2x7b",
        "Average":68.78,
        "ARC":66.47,
        "HellaSwag":84.31,
        "MMLU":62.7,
        "TruthfulQA":61.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b245e28566d675a5b2d269aa4daba80ecb9bc548"
    },
    {
        "T":"\u2b55",
        "Model":"HanNayeoniee\/LHK",
        "Average":68.78,
        "ARC":66.38,
        "HellaSwag":84.49,
        "MMLU":65.13,
        "TruthfulQA":59.12,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0cf54af3c084e70b6e544326d63ecffccac30b47"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nlpguy\/ColorShadow-7B-v2",
        "Average":68.78,
        "ARC":67.15,
        "HellaSwag":84.69,
        "MMLU":60.34,
        "TruthfulQA":62.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"63713211df4348f2d73529c49a7cd0c1bb580ad7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GritLM\/GritLM-8x7B",
        "Average":68.77,
        "ARC":67.83,
        "HellaSwag":86.42,
        "MMLU":71.48,
        "TruthfulQA":49.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":23,
        "Available on the hub":false,
        "Model Sha":"7f089b13e3345510281733ca1e6ff871b5b4bc76"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BarraHome\/Mistroll-7B-v0.2-16bit",
        "Average":68.77,
        "ARC":62.2,
        "HellaSwag":84.85,
        "MMLU":60.37,
        "TruthfulQA":67.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0ef54ac95b46b9a9dd1fbd2164ed97a5d8657072"
    },
    {
        "T":"?",
        "Model":"BarraHome\/Lucie-7B-v0.2-16bit",
        "Average":68.76,
        "ARC":62.12,
        "HellaSwag":84.83,
        "MMLU":60.45,
        "TruthfulQA":67.65,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"38b6ff5f24f4069dedad1025b2b09c156af9c310"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BarraHome\/Mistroll-7B-v0.3-16bit",
        "Average":68.76,
        "ARC":62.12,
        "HellaSwag":84.83,
        "MMLU":60.45,
        "TruthfulQA":67.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b0daf2beea9085c2388b7589572ac7fb6e05f0ef"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BarraHome\/Lucie-7b",
        "Average":68.76,
        "ARC":62.2,
        "HellaSwag":84.81,
        "MMLU":60.34,
        "TruthfulQA":67.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ffbb35e5ad00d0c51a626d122ce07a5fbf7759ad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BarraHome\/Wistral-7B-Instruct-v0.3",
        "Average":68.76,
        "ARC":62.2,
        "HellaSwag":84.81,
        "MMLU":60.34,
        "TruthfulQA":67.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"541d36b0dcaf8e0e9c791c0b54e5358fafd1aebb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BarraHome\/Wistral-7B-Instruct-v0.4",
        "Average":68.76,
        "ARC":62.2,
        "HellaSwag":84.81,
        "MMLU":60.34,
        "TruthfulQA":67.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6f75a5559ef6008886b9abbcf5df998db43edc00"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BarraHome\/Lucie-7b-3e-5",
        "Average":68.76,
        "ARC":62.2,
        "HellaSwag":84.81,
        "MMLU":60.34,
        "TruthfulQA":67.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"38901d0e7baa164636a8ab30a0b54eafcecc7b93"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BarraHome\/Mistroll-7B-v0.1-16bit",
        "Average":68.76,
        "ARC":62.2,
        "HellaSwag":84.81,
        "MMLU":60.34,
        "TruthfulQA":67.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"db9f03ed4f6d5e8c8ccdcb7ad1e66d527dfcf5fc"
    },
    {
        "T":"?",
        "Model":"giraffe176\/Open_Neural_Monarch_Maidv0.1",
        "Average":68.75,
        "ARC":67.66,
        "HellaSwag":85.94,
        "MMLU":65.02,
        "TruthfulQA":56.39,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e0059b9618a5481808e5bec3b45b6bf572dcc629"
    },
    {
        "T":"?",
        "Model":"Eric111\/MarcoHermes",
        "Average":68.74,
        "ARC":66.21,
        "HellaSwag":85.5,
        "MMLU":64.81,
        "TruthfulQA":58.46,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2a404a22c4347275f8727f13be8ca15d1011b317"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Pallas-0.5-LASER-exp2-0.1",
        "Average":68.74,
        "ARC":62.97,
        "HellaSwag":82.11,
        "MMLU":74.66,
        "TruthfulQA":55.24,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4369d91f05edaba98055e476a054441eee27ca37"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"tiiuae\/falcon-180B",
        "Average":68.74,
        "ARC":69.8,
        "HellaSwag":88.95,
        "MMLU":70.54,
        "TruthfulQA":45.67,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"unknown",
        "#Params (B)":180.0,
        "Hub":1073,
        "Available on the hub":false,
        "Model Sha":"71a1a70b629e9963f7b4601e82f3f9079d48011e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Steelskull\/Aurora_base_test",
        "Average":68.74,
        "ARC":62.88,
        "HellaSwag":83.99,
        "MMLU":60.24,
        "TruthfulQA":67.84,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"74c799e46cc89e6fdbd5bc88fe3c75a081768e70"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Falkor-16b",
        "Average":68.73,
        "ARC":65.96,
        "HellaSwag":82.62,
        "MMLU":63.58,
        "TruthfulQA":62.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":16.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"2365c7af9eb60bfa946b566dadd6802befa122e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BarraHome\/Wistral-7B-Instruct-v0.3",
        "Average":68.73,
        "ARC":62.2,
        "HellaSwag":84.77,
        "MMLU":60.32,
        "TruthfulQA":67.62,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"68c0bd31c15387f45a956281d91eb12885f0a160"
    },
    {
        "T":"?",
        "Model":"ontocord\/Felix-8B",
        "Average":68.73,
        "ARC":65.02,
        "HellaSwag":84.61,
        "MMLU":61.05,
        "TruthfulQA":64.23,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":23,
        "Available on the hub":false,
        "Model Sha":"599b440074588eee5bada30cf17dc545915f9e55"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yhyu13\/LMCocktail-Mistral-7B-v1",
        "Average":68.73,
        "ARC":66.21,
        "HellaSwag":85.69,
        "MMLU":61.64,
        "TruthfulQA":61.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"a4563de72fd5fe07b4fcec736e9efe83431df25a"
    },
    {
        "T":"\u2b55",
        "Model":"kyujinpy\/PlatYi-34B-200k-Q-FastChat",
        "Average":68.73,
        "ARC":64.93,
        "HellaSwag":84.46,
        "MMLU":77.13,
        "TruthfulQA":48.38,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":33.93,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"92a96144f94c24341cb6a40259be28627bc76298"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"rwitz2\/ipo-test",
        "Average":68.71,
        "ARC":67.92,
        "HellaSwag":85.99,
        "MMLU":65.05,
        "TruthfulQA":55.87,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b75cf49b19d31ae6c4f8d2a6f3a1484d143024e0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LHC88\/LaseredHermes-7B-v1",
        "Average":68.7,
        "ARC":66.98,
        "HellaSwag":85.22,
        "MMLU":63.6,
        "TruthfulQA":59.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2971ac5fdf665330b38abacef92b8d4b36f875c9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"tiiuae\/falcon-180B",
        "Average":68.7,
        "ARC":69.71,
        "HellaSwag":88.98,
        "MMLU":70.44,
        "TruthfulQA":45.66,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"unknown",
        "#Params (B)":180.0,
        "Hub":1073,
        "Available on the hub":false,
        "Model Sha":"71a1a70b629e9963f7b4601e82f3f9079d48011e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LHC88\/LaseredHermes-7B-v1",
        "Average":68.69,
        "ARC":66.89,
        "HellaSwag":85.21,
        "MMLU":63.58,
        "TruthfulQA":59.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2971ac5fdf665330b38abacef92b8d4b36f875c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azazelle\/smol_bruin-7b",
        "Average":68.69,
        "ARC":67.58,
        "HellaSwag":86.48,
        "MMLU":65.05,
        "TruthfulQA":55.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"967dff56741850954a96491979995a4f686eeb05"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"openaccess-ai-collective\/DPOpenHermes-7B-v2",
        "Average":68.68,
        "ARC":66.64,
        "HellaSwag":85.22,
        "MMLU":63.64,
        "TruthfulQA":59.22,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":27,
        "Available on the hub":false,
        "Model Sha":"3ebea1710b555a205a04e69c743fe90162df63c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Q-bert\/MetaMath-Cybertron-Starling",
        "Average":68.68,
        "ARC":67.41,
        "HellaSwag":86.26,
        "MMLU":65.09,
        "TruthfulQA":55.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":40,
        "Available on the hub":false,
        "Model Sha":"17c8d4cadb814eaef0fab1d93b29cc150f413205"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"stabilityai\/StableBeluga1-Delta",
        "Average":68.67,
        "ARC":68.17,
        "HellaSwag":85.88,
        "MMLU":64.83,
        "TruthfulQA":55.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.29,
        "Hub":54,
        "Available on the hub":false,
        "Model Sha":"40a78d91d43ad9aef6663ff15ddc15be9922bce5"
    },
    {
        "T":"\u2b55",
        "Model":"pankajmathur\/Lima_Unchained_70b",
        "Average":68.67,
        "ARC":68.26,
        "HellaSwag":87.65,
        "MMLU":70.0,
        "TruthfulQA":48.76,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"7dadf059a03bdfec2eb4f4a47666545875c68e49"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/test_42_70b",
        "Average":68.67,
        "ARC":68.26,
        "HellaSwag":87.65,
        "MMLU":70.0,
        "TruthfulQA":48.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"ca3789cd6b683e97dcd6a5f0367f90a63d7a4e7b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_42_70b",
        "Average":68.67,
        "ARC":68.26,
        "HellaSwag":87.65,
        "MMLU":70.0,
        "TruthfulQA":48.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"ca3789cd6b683e97dcd6a5f0367f90a63d7a4e7b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/neural-chat-7b-v3-1-OpenHermes-2.5-7B",
        "Average":68.67,
        "ARC":66.13,
        "HellaSwag":84.09,
        "MMLU":63.22,
        "TruthfulQA":61.23,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"b620ea7af98730695e051be48273cdded8923a2b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Intel\/neural-chat-7b-v3-2",
        "Average":68.66,
        "ARC":67.49,
        "HellaSwag":83.92,
        "MMLU":63.55,
        "TruthfulQA":59.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":52,
        "Available on the hub":false,
        "Model Sha":"2ecaf100bcf63da6cf87dd7bfbea5732fa74c413"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Pallas-0.5-LASER-0.5",
        "Average":68.66,
        "ARC":63.48,
        "HellaSwag":82.21,
        "MMLU":74.31,
        "TruthfulQA":54.64,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1ba6929dbc914f50469dd6bf62082bc52207a03b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PetroGPT\/Voldemort-10B-DPO",
        "Average":68.66,
        "ARC":65.7,
        "HellaSwag":84.79,
        "MMLU":62.82,
        "TruthfulQA":61.33,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dfb9681fdbcd421f15f9cc3fb5d4df9adb539944"
    },
    {
        "T":"?",
        "Model":"robinsmits\/Mistral-Instruct-7B-v0.2-ChatAlpacaV2-4bit",
        "Average":68.66,
        "ARC":62.12,
        "HellaSwag":84.55,
        "MMLU":60.66,
        "TruthfulQA":67.29,
        "Type":"Unknown",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":3.86,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"4d104982c9759ae57fa482280c50b1950e51fd48"
    },
    {
        "T":"?",
        "Model":"itsliupeng\/Mixtral-8x7B-v0.1-top3",
        "Average":68.65,
        "ARC":67.41,
        "HellaSwag":86.63,
        "MMLU":71.98,
        "TruthfulQA":48.58,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"41de832eae882f2c951b64ff5f04d7a809d0a99c"
    },
    {
        "T":"?",
        "Model":"LeroyDyer\/Mixtral_Instruct",
        "Average":68.65,
        "ARC":64.51,
        "HellaSwag":84.77,
        "MMLU":63.41,
        "TruthfulQA":61.9,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"89e9d160c870cdeec454f78da62d5a6f81cb9e94"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"llm-agents\/tora-70b-v1.0",
        "Average":68.65,
        "ARC":67.75,
        "HellaSwag":85.83,
        "MMLU":69.22,
        "TruthfulQA":51.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"e95fd7daf017e7c414ec07ebef4ddf013c16f9a4"
    },
    {
        "T":"?",
        "Model":"NeuralNovel\/Ignis-7B-DPO-Laser",
        "Average":68.64,
        "ARC":65.19,
        "HellaSwag":84.57,
        "MMLU":58.56,
        "TruthfulQA":66.24,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"2094cd52a4f7835b2d38983e889b693f841c6eb8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/MetaMath-bagel-34b-v0.2-c1500",
        "Average":68.64,
        "ARC":63.91,
        "HellaSwag":82.43,
        "MMLU":74.51,
        "TruthfulQA":53.7,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3a15e50ba671fe6e3e7725d58d101cbb4f4a997f"
    },
    {
        "T":"?",
        "Model":"Eric111\/Mathral",
        "Average":68.63,
        "ARC":66.3,
        "HellaSwag":86.17,
        "MMLU":63.27,
        "TruthfulQA":58.79,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6c982afd0745eef8105bad500cbb96c4bf676944"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SC99\/Mistral-7B-summ-ia3-tuned-8h",
        "Average":68.63,
        "ARC":61.18,
        "HellaSwag":85.14,
        "MMLU":59.89,
        "TruthfulQA":68.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"615cde1fef79bc9caa3c475d35e5a076cc629ad0"
    },
    {
        "T":"?",
        "Model":"vicgalle\/ConfigurableHermes-7B",
        "Average":68.62,
        "ARC":66.04,
        "HellaSwag":84.31,
        "MMLU":62.44,
        "TruthfulQA":61.71,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"54b290ffcf07040c5e202f1a5f2a82d455c575af"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HenryJJ\/dolphin-2.6-mistral-7b-dpo-orca-v3",
        "Average":68.62,
        "ARC":66.3,
        "HellaSwag":84.53,
        "MMLU":62.36,
        "TruthfulQA":61.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"370a644bc9e2e4bfccada10a4bc6648102d94efe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-2.0-Mixtral",
        "Average":68.62,
        "ARC":68.26,
        "HellaSwag":86.86,
        "MMLU":71.34,
        "TruthfulQA":48.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"093edcad129edf9c6fa56518209fede54c90a32b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeuralNovel\/Panda-7B-v0.1",
        "Average":68.61,
        "ARC":62.97,
        "HellaSwag":83.76,
        "MMLU":60.73,
        "TruthfulQA":66.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"7d8702ad9d9da7871492ce8843fdb7308a42b3f4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"VAGOsolutions\/SauerkrautLM-14b-MoE-LaserChat",
        "Average":68.6,
        "ARC":66.72,
        "HellaSwag":84.88,
        "MMLU":65.17,
        "TruthfulQA":57.64,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"e3d7c73110dd6edd9e96b1f3d9b0dea91d83ce2d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beberik\/Nyxene-v1-11B",
        "Average":68.6,
        "ARC":67.49,
        "HellaSwag":84.52,
        "MMLU":65.12,
        "TruthfulQA":57.28,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1af08865a403f3be77898d7fbc89bd3be5dfb21f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/fiction.live-Kimiko-V2-70B-fp16",
        "Average":68.6,
        "ARC":67.66,
        "HellaSwag":87.65,
        "MMLU":69.82,
        "TruthfulQA":49.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"6b0c2cb654133cad2d4920e7da2e3f6cb1c4f7fd"
    },
    {
        "T":"?",
        "Model":"Badgids\/Gonzo-Chat-7B",
        "Average":68.6,
        "ARC":65.02,
        "HellaSwag":85.4,
        "MMLU":63.75,
        "TruthfulQA":60.23,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"533fc41b9ff87bc8ba1e1d84a23bb453a3aff966"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jan-hq\/stealth-v1.3",
        "Average":68.6,
        "ARC":67.49,
        "HellaSwag":86.74,
        "MMLU":64.45,
        "TruthfulQA":55.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"f78122cb637ef0289bdb13c5d1b02a9fb6aa28da"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/KunoMaid-7B-slerp",
        "Average":68.59,
        "ARC":68.0,
        "HellaSwag":86.34,
        "MMLU":64.82,
        "TruthfulQA":55.19,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e99b0cab99c6ab176b7a89831c9a1b8977d7eeeb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dillfrescott\/Nous-Hermes-2-SOLAR-10.7B-x2-MoE",
        "Average":68.59,
        "ARC":67.15,
        "HellaSwag":84.83,
        "MMLU":66.52,
        "TruthfulQA":55.85,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"wtfpl",
        "#Params (B)":19.19,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"1cd122567a864075ede6c5684902e8dbfd5eed2e"
    },
    {
        "T":"?",
        "Model":"openagi-project\/OpenAGI-7B-v0.1",
        "Average":68.58,
        "ARC":68.26,
        "HellaSwag":85.06,
        "MMLU":61.6,
        "TruthfulQA":59.4,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"8f6bcbc440db8044af878f4a60e7fd000741daa5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"llm-agents\/tora-70b-v1.0",
        "Average":68.57,
        "ARC":67.58,
        "HellaSwag":85.82,
        "MMLU":69.13,
        "TruthfulQA":51.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"e95fd7daf017e7c414ec07ebef4ddf013c16f9a4"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"tiiuae\/falcon-180B",
        "Average":68.57,
        "ARC":69.45,
        "HellaSwag":88.86,
        "MMLU":70.5,
        "TruthfulQA":45.47,
        "Type":"pretrained",
        "Precision":"8bit",
        "Hub License":"unknown",
        "#Params (B)":180.0,
        "Hub":1073,
        "Available on the hub":false,
        "Model Sha":"71a1a70b629e9963f7b4601e82f3f9079d48011e"
    },
    {
        "T":"?",
        "Model":"paulilioaica\/MoEstral-2x2B",
        "Average":68.57,
        "ARC":65.1,
        "HellaSwag":84.82,
        "MMLU":61.62,
        "TruthfulQA":62.72,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"fa00d779934bc7907f6031c318852b1faa513bf6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nlpguy\/ColorShadow-7B",
        "Average":68.56,
        "ARC":67.83,
        "HellaSwag":85.15,
        "MMLU":61.69,
        "TruthfulQA":59.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6fafdfbf1a92be78735623506bf676f5d8f7030a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sequelbox\/SpellBlade",
        "Average":68.55,
        "ARC":69.28,
        "HellaSwag":87.31,
        "MMLU":70.5,
        "TruthfulQA":47.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":68.98,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"258211a0cceaa08f7c8df3660ff8cd7cb6bee5e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wang7776\/Mistral-7B-Instruct-v0.2-sparsity-30-v0.1",
        "Average":68.55,
        "ARC":63.31,
        "HellaSwag":84.37,
        "MMLU":60.24,
        "TruthfulQA":66.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"39566bcb48deecc1a3b830c5de9e70527d394c4f"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardMath-70B-V1.0",
        "Average":68.55,
        "ARC":67.49,
        "HellaSwag":86.03,
        "MMLU":68.44,
        "TruthfulQA":52.23,
        "Type":"instruction-tuned",
        "Precision":"8bit",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":91,
        "Available on the hub":true,
        "Model Sha":"97e5913edd2c593c3eef12070024674e7ee4e16c"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"tiiuae\/falcon-180B-chat",
        "Average":68.55,
        "ARC":64.42,
        "HellaSwag":88.04,
        "MMLU":68.03,
        "TruthfulQA":53.69,
        "Type":"RL-tuned",
        "Precision":"8bit",
        "Hub License":"unknown",
        "#Params (B)":180.0,
        "Hub":526,
        "Available on the hub":false,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nextai-team\/Moe-3x7b-QA-Code-Inst",
        "Average":68.54,
        "ARC":64.25,
        "HellaSwag":84.6,
        "MMLU":62.15,
        "TruthfulQA":63.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":18.52,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"9127c38cad1a2b9dd3d3fa7ab71706585b46225d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gmonsoon\/MiaAffogato-Indo-Mistral-7b",
        "Average":68.52,
        "ARC":66.38,
        "HellaSwag":85.43,
        "MMLU":64.11,
        "TruthfulQA":58.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"06664239f5a4440eb71ab0892a7c03517d7da9dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"InferenceIllusionist\/Magic-Dolphin-7b",
        "Average":68.51,
        "ARC":65.78,
        "HellaSwag":85.61,
        "MMLU":64.64,
        "TruthfulQA":58.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"4cd26c63dd5cb6e26af5e7815bb1ab62b7dfd4b8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"itsliupeng\/llama2_70b_mmlu",
        "Average":68.51,
        "ARC":65.61,
        "HellaSwag":87.37,
        "MMLU":71.89,
        "TruthfulQA":49.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a0592c8eeba5ba1519dd6843774baca1d400d00e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-70b-gpt4-2.0",
        "Average":68.51,
        "ARC":68.6,
        "HellaSwag":87.53,
        "MMLU":69.37,
        "TruthfulQA":48.52,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"f16526d9bb814dc10adc911f94e8c7a520beb5b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeuralNovel\/Pigris-7b-v0.4",
        "Average":68.5,
        "ARC":66.72,
        "HellaSwag":86.7,
        "MMLU":64.78,
        "TruthfulQA":55.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"3449a23360a0b8e5ccc014a667a95cd2563f9e08"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azazelle\/Tippy-Toppy-7b",
        "Average":68.49,
        "ARC":66.89,
        "HellaSwag":85.88,
        "MMLU":65.49,
        "TruthfulQA":55.7,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"933d5b9cd8963398e3cc2875ff76e5c57c1877c7"
    },
    {
        "T":"?",
        "Model":"Eurdem\/Megatron-Mx",
        "Average":68.48,
        "ARC":66.89,
        "HellaSwag":84.98,
        "MMLU":62.08,
        "TruthfulQA":59.95,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"0e82c4271fa9de78e829c717af871ab7067243c4"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_41-7B-dare_ties",
        "Average":68.48,
        "ARC":65.61,
        "HellaSwag":85.7,
        "MMLU":64.57,
        "TruthfulQA":58.02,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"20109bbcd1080e4d95c450815b931bc383bd64df"
    },
    {
        "T":"?",
        "Model":"giraffe176\/WestLake_Noromaid_OpenHermes_neural-chat",
        "Average":68.47,
        "ARC":67.58,
        "HellaSwag":86.13,
        "MMLU":64.72,
        "TruthfulQA":55.47,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f843090d889645a22e61fbb7bbae7a92fc76812f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"freecs\/Zero-7B-test-2",
        "Average":68.46,
        "ARC":66.13,
        "HellaSwag":84.77,
        "MMLU":62.98,
        "TruthfulQA":59.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f84d973ccd63d8380994ce83a49b16ba7b4034db"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"flemmingmiguel\/Mistrality-7B",
        "Average":68.45,
        "ARC":66.55,
        "HellaSwag":85.82,
        "MMLU":64.63,
        "TruthfulQA":56.8,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"05e7408486426ab8c8ed595945454eb181ba6eb0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Seraph-openchat-3.5-1210-Slerp",
        "Average":68.44,
        "ARC":68.0,
        "HellaSwag":86.13,
        "MMLU":65.5,
        "TruthfulQA":54.12,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"ac09a74aec45a021bd144252a1c2ff9c2631b3ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Hermes-2-SOLAR-10.7B",
        "Average":68.43,
        "ARC":66.72,
        "HellaSwag":84.89,
        "MMLU":66.3,
        "TruthfulQA":55.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":185,
        "Available on the hub":true,
        "Model Sha":"1a61a6ff49be395db210a5867f02e04abb982971"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chanwit\/flux-7b-v0.1",
        "Average":68.42,
        "ARC":67.06,
        "HellaSwag":86.18,
        "MMLU":65.4,
        "TruthfulQA":55.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"566b7dcfb2d7233d49611bda27ff5430487d1aad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RaduGabriel\/SirUkrainian2.0DPO",
        "Average":68.42,
        "ARC":63.91,
        "HellaSwag":83.52,
        "MMLU":61.17,
        "TruthfulQA":65.08,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cc7f95d454989d6b7c96efb1ba7f89826bb56f3b"
    },
    {
        "T":"?",
        "Model":"kimou605\/shadow-clown-BioMistral-7B-SLERP",
        "Average":68.41,
        "ARC":64.76,
        "HellaSwag":84.55,
        "MMLU":61.93,
        "TruthfulQA":62.4,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5d0a327a3f37668c90e649b8bcf05c9db5961a40"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/Sonya-7B",
        "Average":68.41,
        "ARC":64.59,
        "HellaSwag":85.11,
        "MMLU":62.72,
        "TruthfulQA":61.22,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":28,
        "Available on the hub":false,
        "Model Sha":"228e7ab8b24ebb3d459160c0b665a821d1785dc5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/longcat-10.7B",
        "Average":68.41,
        "ARC":64.59,
        "HellaSwag":85.85,
        "MMLU":61.77,
        "TruthfulQA":61.42,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"c79c121d00a7edce5decc7189c32a4411ab26311"
    },
    {
        "T":"?",
        "Model":"Yuma42\/KangalKhan-RawEmerald-7B",
        "Average":68.36,
        "ARC":66.89,
        "HellaSwag":85.75,
        "MMLU":63.23,
        "TruthfulQA":57.58,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c9903c39faa7cf6c2694b24f5e15e29b372f2143"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sethuiyer\/SynthIQ-7b",
        "Average":68.36,
        "ARC":65.87,
        "HellaSwag":85.82,
        "MMLU":64.75,
        "TruthfulQA":57.0,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"32612e89aa87a23f6b1c5c5a9165896e599ca9ca"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cstr\/Spaetzle-v12-7b",
        "Average":68.36,
        "ARC":65.96,
        "HellaSwag":86.16,
        "MMLU":63.48,
        "TruthfulQA":57.84,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f4c2a23da2edce2deb7c81ef615ec35d053b7353"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Q-bert\/MetaMath-Cybertron",
        "Average":68.36,
        "ARC":66.47,
        "HellaSwag":85.54,
        "MMLU":63.71,
        "TruthfulQA":57.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"4fca0e0002db56237fc155f572a34204229e9620"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"EmbeddedLLM\/Mistral-7B-Merge-14-v0.3",
        "Average":68.35,
        "ARC":65.96,
        "HellaSwag":85.29,
        "MMLU":64.35,
        "TruthfulQA":57.8,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"5ff4289d7f8b7f82f2453c611d737edce6b5efdc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Minami-su\/IA_14B",
        "Average":68.34,
        "ARC":62.37,
        "HellaSwag":80.7,
        "MMLU":68.08,
        "TruthfulQA":62.22,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":13.39,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"e38a53dd782115b75968be205efd8b4da31b3b93"
    },
    {
        "T":"?",
        "Model":"ehartford\/samantha-1.1-llama-33b",
        "Average":68.34,
        "ARC":67.83,
        "HellaSwag":85.55,
        "MMLU":58.79,
        "TruthfulQA":61.19,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"ad8892a17be1372f611203a4cf71560cc337e458"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CallComply\/SOLAR-10.7B-Instruct-v1.0-128k",
        "Average":68.34,
        "ARC":65.96,
        "HellaSwag":84.35,
        "MMLU":57.63,
        "TruthfulQA":65.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"bf951ef22381c0dbeb69959fb3c06e772adc2426"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yunconglong\/Mixtral_7Bx2_MoE_13B_DPO",
        "Average":68.34,
        "ARC":65.44,
        "HellaSwag":84.01,
        "MMLU":62.14,
        "TruthfulQA":61.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"88635724f75a6728bdc13165da4d5784f84c8b49"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cloudyu\/Mixtral_7Bx4_MOE_24B",
        "Average":68.33,
        "ARC":65.36,
        "HellaSwag":85.23,
        "MMLU":62.96,
        "TruthfulQA":59.78,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":24.15,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"1cc519b70e87de1c632a6dc98ac6383cf0dd994e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/Mixtral-8x7B-Holodeck-v1",
        "Average":68.32,
        "ARC":66.55,
        "HellaSwag":86.78,
        "MMLU":71.67,
        "TruthfulQA":48.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"6e08f700186a7ee01fa407145c1e990ec15caa71"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/scarlett-33b",
        "Average":68.32,
        "ARC":67.75,
        "HellaSwag":85.48,
        "MMLU":58.98,
        "TruthfulQA":61.05,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":32.32,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"305eea72fb9fe2ac5929a62483ea51f152bcc060"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/mixtral_7bx4_moe",
        "Average":68.31,
        "ARC":65.27,
        "HellaSwag":85.28,
        "MMLU":62.84,
        "TruthfulQA":59.85,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":24.15,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"1cc519b70e87de1c632a6dc98ac6383cf0dd994e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Test157t\/Kunocchini-7b-128k-test",
        "Average":68.31,
        "ARC":66.98,
        "HellaSwag":85.62,
        "MMLU":61.27,
        "TruthfulQA":59.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"c3a102205219be392f9fdb12468a394525fc73b4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ddobokki\/Llama-2-70b-orca-200k",
        "Average":68.29,
        "ARC":64.85,
        "HellaSwag":85.25,
        "MMLU":66.89,
        "TruthfulQA":56.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"1ab69d47a467f15d8168b119ad24c1842d3ff54e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TokenBender\/pic_7B_mistral_Full_v0.2",
        "Average":68.27,
        "ARC":65.36,
        "HellaSwag":84.03,
        "MMLU":64.51,
        "TruthfulQA":59.2,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"4499c15a16b11d6491dcbe029acff64f03e1a5fd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nisten\/shqiponja-15b-v1",
        "Average":68.27,
        "ARC":66.38,
        "HellaSwag":85.26,
        "MMLU":64.62,
        "TruthfulQA":56.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"gpl-3.0",
        "#Params (B)":15.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"7658da56ac6e3dab2cc147b2e658c8ca892a0781"
    },
    {
        "T":"?",
        "Model":"CohereForAI\/c4ai-command-r-v01",
        "Average":68.26,
        "ARC":65.53,
        "HellaSwag":87.0,
        "MMLU":68.2,
        "TruthfulQA":52.32,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":34.98,
        "Hub":781,
        "Available on the hub":false,
        "Model Sha":"2323aaa960c3c073380a0da2fc51284f5113e114"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/gpt4-alpaca-lora_mlp-65B-HF",
        "Average":68.26,
        "ARC":65.02,
        "HellaSwag":86.13,
        "MMLU":62.73,
        "TruthfulQA":59.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"664ff8e3e1d446971a16a6c9018ab24de7664684"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"tiiuae\/falcon-180B-chat",
        "Average":68.25,
        "ARC":63.82,
        "HellaSwag":88.09,
        "MMLU":67.76,
        "TruthfulQA":53.35,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"unknown",
        "#Params (B)":180.0,
        "Hub":526,
        "Available on the hub":false,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Eric111\/openchat-3.5-0106-128k-DPO",
        "Average":68.25,
        "ARC":68.09,
        "HellaSwag":83.82,
        "MMLU":64.74,
        "TruthfulQA":56.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"136e3467ffdc6d44ac2d48a35f874238022b9040"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"liminerity\/Blur-7b-v1.2",
        "Average":68.25,
        "ARC":65.36,
        "HellaSwag":83.88,
        "MMLU":63.45,
        "TruthfulQA":60.3,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6311ad57d16c3d9724930fc0aa5b38fc844eb977"
    },
    {
        "T":"?",
        "Model":"Yuma42\/KangalKhan-RawRuby-7B",
        "Average":68.24,
        "ARC":66.89,
        "HellaSwag":85.53,
        "MMLU":63.46,
        "TruthfulQA":57.09,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"547f6b51a5bc0798c762cb097d1c1d33e8cc336f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"indischepartij\/OpenMia-Indo-Engineering-7b",
        "Average":68.24,
        "ARC":67.15,
        "HellaSwag":85.01,
        "MMLU":62.86,
        "TruthfulQA":57.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6bff9bd6e953c6354473402f8b0e43e95a421f43"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gmonsoon\/OpenMia-Indo-Engineering",
        "Average":68.24,
        "ARC":67.15,
        "HellaSwag":85.01,
        "MMLU":62.86,
        "TruthfulQA":57.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cac4a2663504ccf1ca8975787a4b99df50b68bd1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rishiraj\/oswald-4x7b",
        "Average":68.24,
        "ARC":65.78,
        "HellaSwag":85.29,
        "MMLU":64.49,
        "TruthfulQA":57.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"1a2a1c2a7cb0d18ae4af77f99a7adbe8d9718f92"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HenryJJ\/dolphin-2.6-mistral-7b-dpo-orca-v1",
        "Average":68.23,
        "ARC":66.04,
        "HellaSwag":84.62,
        "MMLU":62.28,
        "TruthfulQA":59.97,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3b711027ce55f180f050729f08fe7060e4834e87"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HenryJJ\/dolphin-2.6-mistral-7b-dpo-orca",
        "Average":68.23,
        "ARC":66.04,
        "HellaSwag":84.62,
        "MMLU":62.28,
        "TruthfulQA":59.97,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"19c3ad67276aa90341e46e8b0b72e6bf79984153"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nlpguy\/Hermes-low-tune-3",
        "Average":68.22,
        "ARC":66.21,
        "HellaSwag":84.99,
        "MMLU":63.74,
        "TruthfulQA":57.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d3824696c2c4b45aff9ee5c2725bd1780d163fa8"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"tiiuae\/falcon-180B-chat",
        "Average":68.22,
        "ARC":63.65,
        "HellaSwag":88.07,
        "MMLU":67.77,
        "TruthfulQA":53.38,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"unknown",
        "#Params (B)":180.0,
        "Hub":526,
        "Available on the hub":false,
        "Model Sha":""
    },
    {
        "T":"?",
        "Model":"Technoculture\/Medmerge-tulu-70b",
        "Average":68.22,
        "ARC":67.41,
        "HellaSwag":87.46,
        "MMLU":70.1,
        "TruthfulQA":47.89,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":68.98,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"aed9ff4b3edc3ed0672de35551dc750ea8fbac3b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Ana-v1-m7",
        "Average":68.21,
        "ARC":67.41,
        "HellaSwag":85.98,
        "MMLU":64.43,
        "TruthfulQA":55.03,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"375e1a29c36bc1bf7bee972a28f47f9db1e85696"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"freecs\/Zero-7B-test-1",
        "Average":68.21,
        "ARC":66.13,
        "HellaSwag":84.62,
        "MMLU":63.12,
        "TruthfulQA":58.97,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6da901880f66d738a6899f65a881c46a49db51b7"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"tiiuae\/falcon-180B",
        "Average":68.21,
        "ARC":69.2,
        "HellaSwag":88.89,
        "MMLU":69.59,
        "TruthfulQA":45.16,
        "Type":"pretrained",
        "Precision":"4bit",
        "Hub License":"unknown",
        "#Params (B)":180.0,
        "Hub":1073,
        "Available on the hub":false,
        "Model Sha":"71a1a70b629e9963f7b4601e82f3f9079d48011e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beberik\/Nyxene-v2-11B",
        "Average":68.21,
        "ARC":67.41,
        "HellaSwag":84.54,
        "MMLU":65.26,
        "TruthfulQA":55.62,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"07d017d24117fabce2e7b67819f6689e3187404f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenPipe\/mistral-ft-optimized-1227",
        "Average":68.21,
        "ARC":67.24,
        "HellaSwag":85.9,
        "MMLU":65.17,
        "TruthfulQA":54.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":76,
        "Available on the hub":false,
        "Model Sha":"a305e828aa2ef0f547e7037edf14bda54b78b210"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeuralNovel\/Tanuki-7B-v0.1",
        "Average":68.2,
        "ARC":62.8,
        "HellaSwag":83.14,
        "MMLU":60.54,
        "TruthfulQA":66.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"699ab2535487aee7cfd8d55ad928805b310c4b17"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LHC88\/DPOpenHermes-7B-v2-PerfLaser",
        "Average":68.2,
        "ARC":66.38,
        "HellaSwag":84.58,
        "MMLU":62.77,
        "TruthfulQA":59.07,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"178f18610436183f66d5eaf3be46ecf020214be3"
    },
    {
        "T":"?",
        "Model":"Yuma42\/KangalKhan-DesolatingRuby-7B",
        "Average":68.2,
        "ARC":66.89,
        "HellaSwag":85.46,
        "MMLU":63.38,
        "TruthfulQA":57.05,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"addeaf592042c08f64930aaa09a61d6deb74d109"
    },
    {
        "T":"?",
        "Model":"Test157t\/Prima-Pastacles-7b",
        "Average":68.19,
        "ARC":66.04,
        "HellaSwag":85.83,
        "MMLU":64.21,
        "TruthfulQA":56.69,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"0375c801fc511b3c63ece6e9c6e05bc926d1cfb4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/DPOpenHermes-7B",
        "Average":68.19,
        "ARC":65.96,
        "HellaSwag":85.9,
        "MMLU":63.98,
        "TruthfulQA":56.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"f7742bd00c7d66791e94882b196b4d96fb88e63a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rizla\/trrapi-16",
        "Average":68.19,
        "ARC":66.38,
        "HellaSwag":85.05,
        "MMLU":64.84,
        "TruthfulQA":56.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"fb8eee8952ec40b4165feb1a6c1dba3675ab6969"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/DPOpenHermes-11B",
        "Average":68.18,
        "ARC":66.55,
        "HellaSwag":84.8,
        "MMLU":64.02,
        "TruthfulQA":57.34,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"6b78354a0789d3e9d0bfa6dd3d0b52c5e4594c39"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Pallas-0.5-LASER-0.6",
        "Average":68.17,
        "ARC":62.46,
        "HellaSwag":81.6,
        "MMLU":74.25,
        "TruthfulQA":54.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"0dc221753dbe63c4f5f5727adfe0f35cf05909f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"flemmingmiguel\/Distilled-HermesChat-7B",
        "Average":68.17,
        "ARC":67.49,
        "HellaSwag":85.21,
        "MMLU":65.22,
        "TruthfulQA":54.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e7ca19cecb52c40f0f6bb31cfa258fad0c004dfa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenPipe\/mistral-ft-optimized-1227",
        "Average":68.17,
        "ARC":67.06,
        "HellaSwag":85.85,
        "MMLU":65.19,
        "TruthfulQA":54.57,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":76,
        "Available on the hub":false,
        "Model Sha":"a305e828aa2ef0f547e7037edf14bda54b78b210"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"sonthenguyen\/NeuralHermes-2.5-Mistral-7B",
        "Average":68.17,
        "ARC":67.58,
        "HellaSwag":85.69,
        "MMLU":63.43,
        "TruthfulQA":55.98,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d7b9a63eb3e086e16b669b7ff59d9b35d0908b03"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azazelle\/xDAN-SlimOrca",
        "Average":68.17,
        "ARC":65.61,
        "HellaSwag":85.7,
        "MMLU":63.67,
        "TruthfulQA":57.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f6c9f9451d35e8c3d9d5243324921114409ee077"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"splm\/openchat-spin-slimorca-iter1",
        "Average":68.14,
        "ARC":67.32,
        "HellaSwag":83.86,
        "MMLU":62.64,
        "TruthfulQA":58.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d2725936c02b9ec7e11a09857f69dd327151615c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/WizardDolphin-7B",
        "Average":68.14,
        "ARC":64.68,
        "HellaSwag":85.86,
        "MMLU":62.75,
        "TruthfulQA":59.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5317ae098bdb1d8bbcbc13330aa9b96c5edae3b4"
    },
    {
        "T":"?",
        "Model":"Yuma42\/KangalKhan-PressurizedRuby-7B",
        "Average":68.13,
        "ARC":66.89,
        "HellaSwag":85.4,
        "MMLU":63.33,
        "TruthfulQA":56.91,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6fb7d9e50848379564e66962e25fb1154c848c05"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"openaccess-ai-collective\/DPOpenHermes-7B",
        "Average":68.12,
        "ARC":65.7,
        "HellaSwag":85.96,
        "MMLU":63.89,
        "TruthfulQA":56.95,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"f7742bd00c7d66791e94882b196b4d96fb88e63a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SCE\/Mistral-7B-summ-ia3-tuned",
        "Average":68.11,
        "ARC":59.64,
        "HellaSwag":84.71,
        "MMLU":59.48,
        "TruthfulQA":68.6,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4c6c4078bcf01d1c0faf90bd4842c2f0fbd8ebcc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"weezywitasneezy\/OxytocinErosEngineeringF1-7B-slerp",
        "Average":68.1,
        "ARC":67.15,
        "HellaSwag":86.0,
        "MMLU":64.73,
        "TruthfulQA":54.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"18bf5bd14ddaa0484952e2972959df2bc5f7b871"
    },
    {
        "T":"?",
        "Model":"Eric111\/Mistral-7B-Instruct-v0.2_openchat-3.5-0106",
        "Average":68.1,
        "ARC":65.7,
        "HellaSwag":84.58,
        "MMLU":63.23,
        "TruthfulQA":58.89,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ce648c03c3d5b47ee86252177a665d84617bb790"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RaduGabriel\/SirUkrainian2.0",
        "Average":68.09,
        "ARC":63.65,
        "HellaSwag":83.26,
        "MMLU":61.22,
        "TruthfulQA":64.24,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"952c34cfb588ac4fe955b324ce263b91982f2ce9"
    },
    {
        "T":"?",
        "Model":"Yuma42\/KangalKhan-HardRuby-7B",
        "Average":68.09,
        "ARC":66.55,
        "HellaSwag":85.41,
        "MMLU":63.46,
        "TruthfulQA":56.94,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"af9ab4dddfcedd1b5f71cf352d78a335b21c8f94"
    },
    {
        "T":"?",
        "Model":"DenisTheDev\/Blitz-AI-MOE-v0.7",
        "Average":68.09,
        "ARC":67.15,
        "HellaSwag":85.59,
        "MMLU":64.04,
        "TruthfulQA":55.56,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":18.52,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ed520aac996a1e3f0f261f207572739579adff1c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Walmart-the-bag\/Quintellect-10.7B",
        "Average":68.08,
        "ARC":65.02,
        "HellaSwag":84.48,
        "MMLU":63.28,
        "TruthfulQA":59.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d264ccc4abe79251b0c6be8f65e51ead07195793"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/Bageluccine-7B-slerp",
        "Average":68.06,
        "ARC":65.1,
        "HellaSwag":85.06,
        "MMLU":61.75,
        "TruthfulQA":60.33,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"87133b08a358596babe4760427bef748e1dd6d7f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/ExtremeDolphin-MoE",
        "Average":68.05,
        "ARC":65.1,
        "HellaSwag":86.07,
        "MMLU":63.76,
        "TruthfulQA":57.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1983955d7a48548e196a7b725cae4ddccdd7e357"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LeroyDyer\/Mixtral_AI_Cyber_4.0",
        "Average":68.05,
        "ARC":64.93,
        "HellaSwag":84.04,
        "MMLU":62.82,
        "TruthfulQA":60.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"27862ae88891b2948f25ceec2023945e0911f449"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fhai50032\/RolePlayLake-7B-Toxic",
        "Average":68.04,
        "ARC":66.98,
        "HellaSwag":84.86,
        "MMLU":63.79,
        "TruthfulQA":56.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"5aa57df9a1490c1eda582ac59919476e3f34a1ea"
    },
    {
        "T":"?",
        "Model":"VAGOsolutions\/SauerkrautLM-7b-LaserChat",
        "Average":68.04,
        "ARC":67.58,
        "HellaSwag":83.58,
        "MMLU":64.93,
        "TruthfulQA":56.08,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"03b8cef6f31e2a6816186d1bddadd938c19f1cd7"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_2-7B-slerp",
        "Average":68.04,
        "ARC":66.89,
        "HellaSwag":85.52,
        "MMLU":65.22,
        "TruthfulQA":54.53,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d25a34b28ddfbe101b24537647d5db751baf2c9e"
    },
    {
        "T":"?",
        "Model":"Yuma42\/KangalKhan-Ruby-7B",
        "Average":68.04,
        "ARC":67.24,
        "HellaSwag":85.22,
        "MMLU":63.21,
        "TruthfulQA":56.49,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9fc954264f09be86a91b9f79d44151cf7cda2572"
    },
    {
        "T":"?",
        "Model":"Yuma42\/KangalKhan-Ruby-7B-Fixed",
        "Average":68.04,
        "ARC":67.24,
        "HellaSwag":85.22,
        "MMLU":63.21,
        "TruthfulQA":56.49,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f0d1375d6e960a9c735949414e1727cf3354b9e3"
    },
    {
        "T":"?",
        "Model":"Yuma42\/KangalKhan-PolishedRuby-7B",
        "Average":68.03,
        "ARC":66.72,
        "HellaSwag":85.39,
        "MMLU":63.21,
        "TruthfulQA":56.8,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cea9103397d28cf3d3c331adf8be43df4032f85e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Delcos\/Velara-11B-V2",
        "Average":68.03,
        "ARC":63.82,
        "HellaSwag":85.85,
        "MMLU":63.62,
        "TruthfulQA":58.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":11.39,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"629ff26017b5adf0bc0c20d1c3f475491feb2b7a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AtAndDev\/CapybaraMarcoroni-7B",
        "Average":68.03,
        "ARC":65.02,
        "HellaSwag":84.81,
        "MMLU":65.2,
        "TruthfulQA":57.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"50dc156e0c016e4e1bc84ff8d067b3eb88d36310"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"splm\/openchat-spin-slimorca-iter0",
        "Average":68.02,
        "ARC":67.15,
        "HellaSwag":83.61,
        "MMLU":64.45,
        "TruthfulQA":56.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0d2acfc959ca3adb6ea9a122007fac80cb008e4d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/A11P",
        "Average":68.02,
        "ARC":62.54,
        "HellaSwag":82.53,
        "MMLU":70.56,
        "TruthfulQA":56.44,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0a14aa5fd9ae557d7dbd02e503deab50544d5a6f"
    },
    {
        "T":"?",
        "Model":"KatyTheCutie\/LemonadeRP-4.5.3",
        "Average":68.02,
        "ARC":65.1,
        "HellaSwag":84.72,
        "MMLU":64.39,
        "TruthfulQA":57.87,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":25,
        "Available on the hub":false,
        "Model Sha":"d84bd91c114a8ae689c3d10c2fcdb8e83300a115"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Test157t\/HerculeanSea-7b-128k",
        "Average":68.02,
        "ARC":66.21,
        "HellaSwag":85.8,
        "MMLU":64.28,
        "TruthfulQA":55.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"142b62bd4c61639c71c15dbd7ac793bfe30a6349"
    },
    {
        "T":"?",
        "Model":"wolfeidau\/NeuralHermes-2.5-Mistral-7B",
        "Average":68.01,
        "ARC":68.26,
        "HellaSwag":85.46,
        "MMLU":63.31,
        "TruthfulQA":55.02,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0b9321aa6d6c51329f0589976c0820e961d0a3cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Test157t\/HerculeanSea-upd-7b-128k",
        "Average":68.01,
        "ARC":66.13,
        "HellaSwag":85.89,
        "MMLU":64.48,
        "TruthfulQA":55.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"64c1b6cef98190b6a54d9718c18cb2dd3e9badcc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xDAN-AI\/xDAN-L1-Chat-RL-v1",
        "Average":68.0,
        "ARC":66.3,
        "HellaSwag":85.81,
        "MMLU":63.21,
        "TruthfulQA":56.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":59,
        "Available on the hub":false,
        "Model Sha":"0591b1690e5b7c800758f9f5de17a2e60cecf11e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"proto-llm\/uniwiz-7B-v0.2",
        "Average":68.0,
        "ARC":63.31,
        "HellaSwag":85.07,
        "MMLU":63.7,
        "TruthfulQA":59.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"799809bc85c6fb17a636e6d1f67bf959730baefd"
    },
    {
        "T":"?",
        "Model":"Stopwolf\/Bumbar-7B-slerp",
        "Average":67.99,
        "ARC":66.21,
        "HellaSwag":83.96,
        "MMLU":63.98,
        "TruthfulQA":57.81,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"40cc5f6decc0c4ada02708123d1d2a15d8cdfd7b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Intel\/neural-chat-7b-v3-1",
        "Average":67.97,
        "ARC":66.3,
        "HellaSwag":83.6,
        "MMLU":62.44,
        "TruthfulQA":59.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":531,
        "Available on the hub":false,
        "Model Sha":"3995e9a13d54ce95f0ad55de2eaa92e2dc580174"
    },
    {
        "T":"?",
        "Model":"Yuma42\/KangalKhan-ShatteredRuby-7B",
        "Average":67.97,
        "ARC":66.21,
        "HellaSwag":85.38,
        "MMLU":63.29,
        "TruthfulQA":56.99,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"84b3f2e27187058045e104b6ed1d51905d73bc1c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Intel\/neural-chat-7b-v3-1",
        "Average":67.97,
        "ARC":66.21,
        "HellaSwag":83.64,
        "MMLU":62.37,
        "TruthfulQA":59.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":531,
        "Available on the hub":false,
        "Model Sha":"3995e9a13d54ce95f0ad55de2eaa92e2dc580174"
    },
    {
        "T":"?",
        "Model":"lvkaokao\/mistral-7b-finetuned-orca-dpo-v2",
        "Average":67.97,
        "ARC":66.21,
        "HellaSwag":83.64,
        "MMLU":62.37,
        "TruthfulQA":59.65,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":28,
        "Available on the hub":false,
        "Model Sha":"a5c1daaec60a480e8c81b265135583034054be2b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"orangetin\/OpenHermes-Mixtral-8x7B",
        "Average":67.96,
        "ARC":63.91,
        "HellaSwag":84.14,
        "MMLU":64.29,
        "TruthfulQA":59.53,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"a55b010d3918ef61267d34e9ab47d9f554e3b11c"
    },
    {
        "T":"?",
        "Model":"Eurdem\/megatron_v4_4x7B",
        "Average":67.96,
        "ARC":65.61,
        "HellaSwag":84.1,
        "MMLU":61.64,
        "TruthfulQA":60.51,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"d5703cd346b1f82c3ed8e7f8083d4c4bfee81242"
    },
    {
        "T":"?",
        "Model":"Yuma42\/KangalKhan-SharpEmerald-7B",
        "Average":67.96,
        "ARC":66.72,
        "HellaSwag":85.4,
        "MMLU":63.21,
        "TruthfulQA":56.52,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"28b728dd8fe67e04015f7181233b21c78050b993"
    },
    {
        "T":"?",
        "Model":"LeroyDyer\/Mixtral_AI_128k_bioMedical",
        "Average":67.96,
        "ARC":64.51,
        "HellaSwag":84.99,
        "MMLU":63.66,
        "TruthfulQA":58.69,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"24d2766780b4460654703eb2978c03d575ac289e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sethuiyer\/Chikuma_10.7B",
        "Average":67.96,
        "ARC":65.7,
        "HellaSwag":84.31,
        "MMLU":64.81,
        "TruthfulQA":57.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"3c99ba83d1b6cdee68696fc8443dbd4c71cf9cfe"
    },
    {
        "T":"\u2b55",
        "Model":"martyn\/solar-megamerge-dare-10.7b-v1",
        "Average":67.95,
        "ARC":66.13,
        "HellaSwag":85.3,
        "MMLU":66.03,
        "TruthfulQA":54.33,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c08c204161faa4bd853856dc2c868dbab534632b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"indischepartij\/MiaLatte-Indo-Mistral-7b",
        "Average":67.94,
        "ARC":66.55,
        "HellaSwag":85.23,
        "MMLU":63.93,
        "TruthfulQA":56.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"e6ad0daaeb0e2d1f4b01fb8f409b146a4b752317"
    },
    {
        "T":"?",
        "Model":"LeroyDyer\/Mixtral_Chat_X_128k",
        "Average":67.94,
        "ARC":65.27,
        "HellaSwag":85.27,
        "MMLU":63.98,
        "TruthfulQA":57.23,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"47c31317be30328c7c4309a6b3af702a0068e0f9"
    },
    {
        "T":"?",
        "Model":"Yuma42\/KangalKhan-PrimordialSapphire-7B",
        "Average":67.94,
        "ARC":65.87,
        "HellaSwag":85.51,
        "MMLU":63.11,
        "TruthfulQA":57.25,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"eeb2ef916586cbcf0b699a91190e659c66d848fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Gryphe\/MythoMist-7b",
        "Average":67.93,
        "ARC":65.87,
        "HellaSwag":83.55,
        "MMLU":62.32,
        "TruthfulQA":59.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"3b6c71416d191ab161fd3043117304a10df99716"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/Deita-20b",
        "Average":67.93,
        "ARC":63.91,
        "HellaSwag":83.11,
        "MMLU":67.4,
        "TruthfulQA":57.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":19.86,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"855035b23011e2a09182025a63a9252e19033163"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"decruz07\/kellemar-DPO-7B-d",
        "Average":67.93,
        "ARC":66.89,
        "HellaSwag":85.16,
        "MMLU":62.77,
        "TruthfulQA":56.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d0583642fd14d4881ba7799cea1eb3a12daed62e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PulsarAI\/MetaMath-Chupacabra-7B-v2.01-Slerp",
        "Average":67.91,
        "ARC":66.13,
        "HellaSwag":85.46,
        "MMLU":63.92,
        "TruthfulQA":56.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dcc6fff61bfd608d8e14a040dff22cd8dae78b1e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"charlesdedampierre\/TopicNeuralHermes-2.5-Mistral-7B",
        "Average":67.91,
        "ARC":67.06,
        "HellaSwag":85.44,
        "MMLU":63.66,
        "TruthfulQA":55.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"c1c3fdde57d33f759b16f87a56c25a834bca0a38"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"adamo1139\/Yi-34b-200K-rawrr-v2-run-0902-LoRA",
        "Average":67.9,
        "ARC":64.68,
        "HellaSwag":84.5,
        "MMLU":75.76,
        "TruthfulQA":46.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3fbaa2965a16992f1e8cddbc0c9b40efd6f15698"
    },
    {
        "T":"?",
        "Model":"vicgalle\/SystemConfigHermes-7B",
        "Average":67.9,
        "ARC":65.19,
        "HellaSwag":84.41,
        "MMLU":61.89,
        "TruthfulQA":60.11,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"73b2afa99dcfd329e5482833429cc20e88acd825"
    },
    {
        "T":"?",
        "Model":"Yuma42\/KangalKhan-ShinyEmerald-7B",
        "Average":67.9,
        "ARC":66.21,
        "HellaSwag":85.37,
        "MMLU":63.36,
        "TruthfulQA":56.65,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"836539ad2aae2f30a29516ef381b0ab0bdb69a22"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Lemur-70B-Chat-v1-GPTQ",
        "Average":67.89,
        "ARC":65.27,
        "HellaSwag":84.41,
        "MMLU":64.75,
        "TruthfulQA":57.11,
        "Type":"fine-tuned",
        "Precision":"None",
        "Hub License":"llama2",
        "#Params (B)":9.1,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"12499165df1785f50df3e95940406032776401ea"
    },
    {
        "T":"?",
        "Model":"invalid-coder\/Starling-LM-7B-beta-laser-dpo",
        "Average":67.88,
        "ARC":67.41,
        "HellaSwag":83.38,
        "MMLU":65.29,
        "TruthfulQA":55.47,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"af46d93b801339a18c70948867793e29b7b9eedb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Brillibits\/Instruct_Llama70B_Dolly15k",
        "Average":67.88,
        "ARC":68.34,
        "HellaSwag":87.21,
        "MMLU":69.52,
        "TruthfulQA":46.46,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"45444ac60488594e0700e6c7313ff444b4468240"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"adamo1139\/yi-34b-200k-rawrr-dpo-2",
        "Average":67.88,
        "ARC":64.68,
        "HellaSwag":84.74,
        "MMLU":75.96,
        "TruthfulQA":46.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"6682e3f76d02f280c4a265c9192c5a9e117cfdd4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mistralai\/Mixtral-8x7B-v0.1",
        "Average":67.88,
        "ARC":66.38,
        "HellaSwag":86.46,
        "MMLU":71.88,
        "TruthfulQA":46.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":1464,
        "Available on the hub":false,
        "Model Sha":"58301445dc1378584211722b7ebf8743ec4e192b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-2.0-Yi-34B-200K",
        "Average":67.88,
        "ARC":63.48,
        "HellaSwag":81.98,
        "MMLU":75.21,
        "TruthfulQA":50.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"9dc20c3e0d2cae5e390980a7a2a34f289bec73b3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"UCLA-AGI\/test-test",
        "Average":67.88,
        "ARC":66.47,
        "HellaSwag":85.82,
        "MMLU":61.48,
        "TruthfulQA":57.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"83731d11da3f0878effd3a32e5aea52249de7c81"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Novocoders\/Lotus-7B",
        "Average":67.87,
        "ARC":66.47,
        "HellaSwag":84.8,
        "MMLU":64.64,
        "TruthfulQA":55.57,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"58eb22fb92d6dfed4c0b582f1dd4573cdf7cca4a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Intel\/neural-chat-7b-v3",
        "Average":67.87,
        "ARC":67.15,
        "HellaSwag":83.29,
        "MMLU":62.26,
        "TruthfulQA":58.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":63,
        "Available on the hub":false,
        "Model Sha":"7a05c8a2151f7d32252d9ef5db10445c13ae1f20"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/Voldemort-10B",
        "Average":67.87,
        "ARC":64.42,
        "HellaSwag":84.25,
        "MMLU":62.87,
        "TruthfulQA":59.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f13fc663669481fae7dcff5a218623b3ca6c79a3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ZoidBB\/Jovian-10.7B-v1.0",
        "Average":67.87,
        "ARC":67.41,
        "HellaSwag":86.4,
        "MMLU":65.66,
        "TruthfulQA":52.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ba8433fe1cdf03a7fe25650d99219d34fce13bb8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jan-hq\/stealth-v1.3",
        "Average":67.86,
        "ARC":65.19,
        "HellaSwag":84.44,
        "MMLU":62.7,
        "TruthfulQA":59.12,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"b76e2592849352c5073ebddec5748975f16e4895"
    },
    {
        "T":"?",
        "Model":"LeroyDyer\/SpydazWeb_AI_BASE_128k",
        "Average":67.86,
        "ARC":65.19,
        "HellaSwag":84.62,
        "MMLU":63.81,
        "TruthfulQA":57.82,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c210d19ce0b57f13217ea70c14ecfca5bf0c5737"
    },
    {
        "T":"?",
        "Model":"abacusai\/bigstral-12b-32k",
        "Average":67.86,
        "ARC":59.98,
        "HellaSwag":84.1,
        "MMLU":59.14,
        "TruthfulQA":68.21,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.48,
        "Hub":33,
        "Available on the hub":false,
        "Model Sha":"ceb9da24dcd58c01de0eddada94c79f62d7d6436"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/MetaMath-Chupacabra-7B-v2.01-Slerp",
        "Average":67.85,
        "ARC":65.96,
        "HellaSwag":85.46,
        "MMLU":63.82,
        "TruthfulQA":56.16,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e94f61cd30c697bf1b38c64fa69e93a247f3b58d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"amazingvince\/openhermes-7b-dpo",
        "Average":67.85,
        "ARC":65.78,
        "HellaSwag":84.94,
        "MMLU":63.66,
        "TruthfulQA":57.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"c191ac2d33de8bb5f1454e95c50fab40dc52974e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"UCLA-AGI\/test_final",
        "Average":67.85,
        "ARC":66.13,
        "HellaSwag":85.85,
        "MMLU":61.51,
        "TruthfulQA":57.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":24,
        "Available on the hub":false,
        "Model Sha":"b996460b9ac3969f2c685c3f3669ba944022b2be"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"UCLA-AGI\/zephyr-7b-sft-full-SPIN-iter3",
        "Average":67.85,
        "ARC":66.13,
        "HellaSwag":85.85,
        "MMLU":61.51,
        "TruthfulQA":57.89,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":24,
        "Available on the hub":false,
        "Model Sha":"f4d6d3b9fce399c73c727eb5f7e68a10ae751ad4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NurtureAI\/neural-chat-11b-v3-2",
        "Average":67.84,
        "ARC":66.64,
        "HellaSwag":82.12,
        "MMLU":62.37,
        "TruthfulQA":60.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8821b441a4a07ec7c45e1c13bead93e99ad2f099"
    },
    {
        "T":"?",
        "Model":"vistagi\/Mixtral-8x7b-v0.1-dpo",
        "Average":67.84,
        "ARC":66.55,
        "HellaSwag":86.4,
        "MMLU":71.65,
        "TruthfulQA":46.74,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9eb8bc4d6874f44022930456c287e0ecc4661568"
    },
    {
        "T":"?",
        "Model":"vistagi\/Mixtral-8x7b-v0.1-sft",
        "Average":67.84,
        "ARC":66.55,
        "HellaSwag":86.4,
        "MMLU":71.65,
        "TruthfulQA":46.74,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fb517dd1f789b474803895a0dc5cc16832268f21"
    },
    {
        "T":"?",
        "Model":"Steelskull\/VerA-Etheria-55b",
        "Average":67.83,
        "ARC":64.25,
        "HellaSwag":81.46,
        "MMLU":73.51,
        "TruthfulQA":52.1,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":55.59,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"dc4ed42fc886c3d912fc0aa3b19cf5c92bfb55d7"
    },
    {
        "T":"?",
        "Model":"Nexusflow\/Starling-LM-7B-beta",
        "Average":67.83,
        "ARC":67.24,
        "HellaSwag":83.47,
        "MMLU":65.14,
        "TruthfulQA":55.47,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":218,
        "Available on the hub":false,
        "Model Sha":"ee26b7c2cf9db00e1d9a92c9989d5b2a0b891dbd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Puffin-70B",
        "Average":67.83,
        "ARC":67.41,
        "HellaSwag":87.37,
        "MMLU":69.77,
        "TruthfulQA":46.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":68.72,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"129e0af93d04b1b9cc85ea48bbb300f1ccb44210"
    },
    {
        "T":"?",
        "Model":"argilla\/CapybaraHermes-2.5-Mistral-7B",
        "Average":67.82,
        "ARC":65.78,
        "HellaSwag":85.45,
        "MMLU":63.13,
        "TruthfulQA":56.91,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":51,
        "Available on the hub":false,
        "Model Sha":"488b5d3a878dcbadf3f316dca9332f484ffd4e0d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"UCLA-AGI\/test-test",
        "Average":67.82,
        "ARC":66.38,
        "HellaSwag":85.84,
        "MMLU":61.22,
        "TruthfulQA":57.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"83731d11da3f0878effd3a32e5aea52249de7c81"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"UCLA-AGI\/zephyr-7b-sft-full-SPIN-iter2",
        "Average":67.82,
        "ARC":66.38,
        "HellaSwag":85.84,
        "MMLU":61.22,
        "TruthfulQA":57.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"336bff60f5ce904c2ab9633315192df904431afa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rishiraj\/uncensored",
        "Average":67.8,
        "ARC":66.04,
        "HellaSwag":84.8,
        "MMLU":61.23,
        "TruthfulQA":59.14,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":22,
        "Available on the hub":false,
        "Model Sha":"7d2b64d29e68792172d809c51518c9092b5eea72"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Radu1999\/Mistral-Instruct-Ukrainian-slerp",
        "Average":67.8,
        "ARC":62.03,
        "HellaSwag":84.35,
        "MMLU":61.35,
        "TruthfulQA":63.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"166cd3537a1eab8f189e232243d675d431dc71f5"
    },
    {
        "T":"?",
        "Model":"saucam\/mistral-orpo-beta-NeuralBeagle14-7B-dare-ties",
        "Average":67.8,
        "ARC":66.72,
        "HellaSwag":85.98,
        "MMLU":64.63,
        "TruthfulQA":53.87,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3fb5752c0b99378f10e5a9ad1ccdd236a4214479"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vihangd\/smartsolmix-4x10.7b-v1",
        "Average":67.8,
        "ARC":64.93,
        "HellaSwag":85.13,
        "MMLU":66.1,
        "TruthfulQA":55.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":36.1,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f7ace5190d07c08c17f846cab5619260bee5ff69"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"giraffe176\/Open_Maid_Samantha_Hermes_Orca",
        "Average":67.78,
        "ARC":66.81,
        "HellaSwag":85.83,
        "MMLU":64.58,
        "TruthfulQA":53.91,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fd850286fb7795e531edaeb7c3ecb4ed72d9e636"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"mistralai\/Mixtral-8x7B-v0.1",
        "Average":67.78,
        "ARC":66.04,
        "HellaSwag":86.49,
        "MMLU":71.82,
        "TruthfulQA":46.78,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":1464,
        "Available on the hub":false,
        "Model Sha":"4dd4b0f2d577d7b74152732d5543a92201481fe2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HIT-SCIR\/huozi3",
        "Average":67.77,
        "ARC":65.02,
        "HellaSwag":86.0,
        "MMLU":70.61,
        "TruthfulQA":49.45,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.91,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"fa66db9e2971b84bc084bac74d97d04149a65a05"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jan-hq\/stealth-v1.2",
        "Average":67.77,
        "ARC":66.38,
        "HellaSwag":86.14,
        "MMLU":64.33,
        "TruthfulQA":54.23,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"b0a2704027bbfd8ae0a5d88a23115b17d1a23d1f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Liangmingxin\/ThetaWave-7B-sft",
        "Average":67.77,
        "ARC":63.14,
        "HellaSwag":84.42,
        "MMLU":63.78,
        "TruthfulQA":59.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"ab3b156ff4a40d0e95f77b395aaa655a78b1f198"
    },
    {
        "T":"?",
        "Model":"Yuma42\/KangalKhan-Sapphire-7B",
        "Average":67.76,
        "ARC":66.3,
        "HellaSwag":85.34,
        "MMLU":63.32,
        "TruthfulQA":56.09,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5e20ee421f38f83e6f1541a85457485cf6aae370"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pinkyponky\/Mistral-7b-instruct-v0.2-summ-sft-e2",
        "Average":67.76,
        "ARC":61.43,
        "HellaSwag":83.64,
        "MMLU":61.03,
        "TruthfulQA":64.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"01a73ccd10a275738304c695d0728a29e8586f47"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ConvexAI\/Chop-7b",
        "Average":67.75,
        "ARC":63.74,
        "HellaSwag":83.04,
        "MMLU":62.04,
        "TruthfulQA":62.19,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"295b0a6dbe8f7cbbcebad706a4a0ee8681f2a0a6"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"rishiraj\/oswald-7b",
        "Average":67.74,
        "ARC":66.38,
        "HellaSwag":85.18,
        "MMLU":65.34,
        "TruthfulQA":54.07,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"43326649a8b8b7a43cc4a6d15262625508a50dd2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"adamo1139\/Yi-34B-200K-rawrr1-LORA-DPO-experimental-r3",
        "Average":67.74,
        "ARC":64.85,
        "HellaSwag":84.77,
        "MMLU":76.0,
        "TruthfulQA":45.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":33.93,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"8248694fd93e0c5a5a6bce8b6aa9923174fc6779"
    },
    {
        "T":"?",
        "Model":"cris177\/DesivoMerge0.1",
        "Average":67.74,
        "ARC":65.87,
        "HellaSwag":85.39,
        "MMLU":64.35,
        "TruthfulQA":55.36,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d92500827c768d24362b53b94501bee63d65823f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ZoidBB\/Jovian-10.7B-v1.0",
        "Average":67.74,
        "ARC":67.06,
        "HellaSwag":86.39,
        "MMLU":65.5,
        "TruthfulQA":52.0,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ba8433fe1cdf03a7fe25650d99219d34fce13bb8"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_17-7B-dare_ties",
        "Average":67.73,
        "ARC":66.64,
        "HellaSwag":86.04,
        "MMLU":65.07,
        "TruthfulQA":53.18,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4285aaf1e251596e5e1a9bc0c9627216ce7a0412"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/Mistral-7B-Instruct-KhanAcademy-v0.2",
        "Average":67.73,
        "ARC":62.03,
        "HellaSwag":82.98,
        "MMLU":61.68,
        "TruthfulQA":64.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"dafd4df37ea9817de0b18af1ea8d0ad124c4095a"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_17-7B-dare_ties",
        "Average":67.72,
        "ARC":66.72,
        "HellaSwag":85.98,
        "MMLU":65.03,
        "TruthfulQA":53.17,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4285aaf1e251596e5e1a9bc0c9627216ce7a0412"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/MetaMath-Tulpar-7b-v2-Slerp",
        "Average":67.72,
        "ARC":65.96,
        "HellaSwag":85.11,
        "MMLU":63.37,
        "TruthfulQA":56.44,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"644e2ca7db569c38a2bf06077fd8ee6d04f3edba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-falcon-180b-v13-preview0",
        "Average":67.72,
        "ARC":65.1,
        "HellaSwag":86.19,
        "MMLU":64.6,
        "TruthfulQA":54.97,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":178.64,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"7d7b93ffd67d1b0c39f3503050dbbcc951948120"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Praneeth\/StarMix-7B-slerp",
        "Average":67.71,
        "ARC":65.36,
        "HellaSwag":85.1,
        "MMLU":62.57,
        "TruthfulQA":57.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5ab001441b789f05af53f43b07844dcfa63e78a7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Intel\/neural-chat-7b-v3-1",
        "Average":67.71,
        "ARC":65.7,
        "HellaSwag":83.54,
        "MMLU":62.12,
        "TruthfulQA":59.48,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":531,
        "Available on the hub":false,
        "Model Sha":"af2489cde09e9d2c175622f651875e83824c4b10"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PulsarAI\/MetaMath-Tulpar-7b-v2-Slerp",
        "Average":67.69,
        "ARC":65.61,
        "HellaSwag":85.16,
        "MMLU":63.49,
        "TruthfulQA":56.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"41612eecf338ae2b1cbb63a3729ce7b125c6ca3c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pinkyponky\/Mistral-7b-instruct-v0.2-summ-sft-e3",
        "Average":67.69,
        "ARC":61.18,
        "HellaSwag":83.72,
        "MMLU":60.93,
        "TruthfulQA":64.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5b09e3dd2bf8bcf08b9b3dd0d69e4cc67d782fd3"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"argilla\/distilabeled-Hermes-2.5-Mistral-7B",
        "Average":67.68,
        "ARC":66.3,
        "HellaSwag":85.15,
        "MMLU":63.5,
        "TruthfulQA":55.75,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":25,
        "Available on the hub":false,
        "Model Sha":"71e12bedd29a0d8e8744f32a41aca68769fc99c2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dillfrescott\/sonya-medium-x8-MoE",
        "Average":67.66,
        "ARC":64.25,
        "HellaSwag":83.7,
        "MMLU":62.53,
        "TruthfulQA":60.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"wtfpl",
        "#Params (B)":69.92,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"e8151c3609889dc7746ca748f4e16098663a5880"
    },
    {
        "T":"?",
        "Model":"LeroyDyer\/Mixtral_AI_base_128k",
        "Average":67.65,
        "ARC":65.1,
        "HellaSwag":84.05,
        "MMLU":63.36,
        "TruthfulQA":58.11,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"205a2099382a99c581e822e77ca425fc1dbc269c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chlee10\/T3Q-Platypus-MistralM7-7B",
        "Average":67.65,
        "ARC":64.16,
        "HellaSwag":85.16,
        "MMLU":61.29,
        "TruthfulQA":59.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6ba1c6a820ca247c079d76c3d60d2f9c302f9385"
    },
    {
        "T":"?",
        "Model":"ChaoticNeutrals\/Prima-LelantaclesV4-7b-16k-bf16",
        "Average":67.64,
        "ARC":66.04,
        "HellaSwag":85.07,
        "MMLU":64.7,
        "TruthfulQA":54.76,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"4021f84b780a27eefd2f0e32a0c4ec4c3a01761d"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardLM-70B-V1.0",
        "Average":67.64,
        "ARC":65.02,
        "HellaSwag":85.41,
        "MMLU":64.73,
        "TruthfulQA":55.38,
        "Type":"instruction-tuned",
        "Precision":"8bit",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":93,
        "Available on the hub":true,
        "Model Sha":"874c80b0bd71c2cc2aeb0cb8498589b4a4c52515"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luqmanxyz\/Maya_Hermes-2.5-Mistral-7B",
        "Average":67.62,
        "ARC":66.3,
        "HellaSwag":85.07,
        "MMLU":63.23,
        "TruthfulQA":55.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a0ca78bdb647ffde1ed79f6dd85ed5e7694c1eaf"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"freecs\/ThetaWave-7B-v0.2",
        "Average":67.62,
        "ARC":64.51,
        "HellaSwag":85.0,
        "MMLU":61.01,
        "TruthfulQA":59.95,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"308462cc42873575ddd847ab7941304b6d441c2f"
    },
    {
        "T":"?",
        "Model":"Walmart-the-bag\/WordWoven-13B",
        "Average":67.61,
        "ARC":66.13,
        "HellaSwag":85.81,
        "MMLU":64.06,
        "TruthfulQA":54.45,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.88,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d398693041f482ee7ee9c91c804206e7f62ea58c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Test157t\/Echidna-7b-128k",
        "Average":67.6,
        "ARC":66.13,
        "HellaSwag":85.18,
        "MMLU":63.04,
        "TruthfulQA":56.07,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"9bbbcdae306c8e5a8a7c695411274321abcea485"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"decruz07\/kellemar-DPO-7B",
        "Average":67.59,
        "ARC":66.21,
        "HellaSwag":85.25,
        "MMLU":63.38,
        "TruthfulQA":55.53,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"860ee984db0e2830a969fc616128c4c7d2bca233"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/NinjaDolphin-7B",
        "Average":67.58,
        "ARC":65.61,
        "HellaSwag":85.35,
        "MMLU":64.43,
        "TruthfulQA":54.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"0c2f691bda2d1131ef87767ccf47ba7560578c48"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"liminerity\/Blur-7b-v1.22",
        "Average":67.58,
        "ARC":62.29,
        "HellaSwag":82.0,
        "MMLU":58.03,
        "TruthfulQA":68.01,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c187e675917fa03b179fa488a9007a803ee8c48b"
    },
    {
        "T":"?",
        "Model":"0-hero\/Matter-0.1-7B-boost-DPO",
        "Average":67.57,
        "ARC":65.02,
        "HellaSwag":83.08,
        "MMLU":61.87,
        "TruthfulQA":60.29,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"5bee9978fcf2188f1070b67f6d94be344fdd99c0"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"liminerity\/Blurstral-7b-slerp",
        "Average":67.56,
        "ARC":66.3,
        "HellaSwag":85.38,
        "MMLU":65.18,
        "TruthfulQA":53.4,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d9480038f0136e51d37810cd7d574818f48e90e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Q-bert\/Optimus-7B",
        "Average":67.56,
        "ARC":65.44,
        "HellaSwag":85.41,
        "MMLU":63.61,
        "TruthfulQA":55.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"d9dd63bc4437c2089f40ce37e689ad530060519c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvilasuero\/NeuralHermes-2.5-Mistral-7B-distilabel",
        "Average":67.56,
        "ARC":65.78,
        "HellaSwag":84.97,
        "MMLU":63.63,
        "TruthfulQA":55.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a932ff3b8c3186bb12224857dd412f1cda56546e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"decruz07\/kellemar-DPO-7B",
        "Average":67.55,
        "ARC":66.04,
        "HellaSwag":85.21,
        "MMLU":63.42,
        "TruthfulQA":55.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"860ee984db0e2830a969fc616128c4c7d2bca233"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hfl\/chinese-mixtral",
        "Average":67.54,
        "ARC":67.58,
        "HellaSwag":85.34,
        "MMLU":70.38,
        "TruthfulQA":46.86,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"7b37775efb34a0734efd60a32781bd706c60e85b"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_12-7B-slerp",
        "Average":67.54,
        "ARC":66.64,
        "HellaSwag":85.9,
        "MMLU":65.06,
        "TruthfulQA":52.55,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"68ce0b9e6244fe02571b5d6b40660abbb37470b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"genaicore3434\/Mistral-7b-instruct-v0.2-summ-sft-bf16-e2",
        "Average":67.53,
        "ARC":60.67,
        "HellaSwag":83.55,
        "MMLU":60.81,
        "TruthfulQA":65.1,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b1fc87ef5eefc5cafe0654bd2f68d0f753c87a53"
    },
    {
        "T":"?",
        "Model":"dozzke\/hermorca",
        "Average":67.53,
        "ARC":63.74,
        "HellaSwag":84.4,
        "MMLU":64.28,
        "TruthfulQA":57.69,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5102ad0c27d60a2c6381b8ec97fcc59450ea5640"
    },
    {
        "T":"?",
        "Model":"NousResearch\/Nous-Hermes-2-Mistral-7B-DPO",
        "Average":67.53,
        "ARC":66.04,
        "HellaSwag":84.95,
        "MMLU":63.36,
        "TruthfulQA":55.75,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":137,
        "Available on the hub":false,
        "Model Sha":"868f0aaeba382aa63bef6ff2bc370be5df16ce9a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sethuiyer\/MedleyMD",
        "Average":67.52,
        "ARC":66.47,
        "HellaSwag":86.06,
        "MMLU":65.1,
        "TruthfulQA":52.46,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ce34d7174f0522f91723bc47419d60fbaec659cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"liminerity\/Blur-7b-v1.22",
        "Average":67.51,
        "ARC":62.12,
        "HellaSwag":82.09,
        "MMLU":57.9,
        "TruthfulQA":67.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c187e675917fa03b179fa488a9007a803ee8c48b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"silvercoder45\/Mistral-7b-instruct-v0.2-summ-sft-bf16-e3",
        "Average":67.51,
        "ARC":60.32,
        "HellaSwag":83.68,
        "MMLU":60.82,
        "TruthfulQA":65.22,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"718e614895ea1fc1445dd0727751821d2ac14e6e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pinkyponky\/Mistral-7b-instruct-v0.2-summ-sft-e1",
        "Average":67.51,
        "ARC":60.84,
        "HellaSwag":83.37,
        "MMLU":60.86,
        "TruthfulQA":64.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cb20f22f421052e1ca8ea8bd9974fade5ccdfa9d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abideen\/NexoNimbus-MoE-2x7B",
        "Average":67.51,
        "ARC":66.81,
        "HellaSwag":85.66,
        "MMLU":64.51,
        "TruthfulQA":53.06,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"b775c263bfde51a9536ce412893b69d87d064fb1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AI-B\/UTENA-7B-V3",
        "Average":67.51,
        "ARC":65.96,
        "HellaSwag":85.7,
        "MMLU":64.72,
        "TruthfulQA":53.64,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"unlicense",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"47815871459a27e38d9b981d5096cf777585e461"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Qwen\/Qwen1.5-14B-Chat",
        "Average":67.5,
        "ARC":58.79,
        "HellaSwag":82.33,
        "MMLU":68.52,
        "TruthfulQA":60.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":14.17,
        "Hub":67,
        "Available on the hub":false,
        "Model Sha":"17e11c306ed235e970c9bb8e5f7233527140cdcf"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_12-7B-slerp",
        "Average":67.5,
        "ARC":66.64,
        "HellaSwag":85.89,
        "MMLU":64.94,
        "TruthfulQA":52.55,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"68ce0b9e6244fe02571b5d6b40660abbb37470b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uproai\/RosMistral-2x7B",
        "Average":67.49,
        "ARC":66.21,
        "HellaSwag":85.54,
        "MMLU":65.35,
        "TruthfulQA":52.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"edf49ed0bf25f2656cf715400a1911b69237a0f5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/NarutoDolphin-10B",
        "Average":67.49,
        "ARC":63.82,
        "HellaSwag":84.17,
        "MMLU":62.85,
        "TruthfulQA":59.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"20c3e2a3d13afb7340d1261e76528b1cbe6cd7ce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/NarutoDolphin-7B",
        "Average":67.49,
        "ARC":63.82,
        "HellaSwag":84.17,
        "MMLU":62.85,
        "TruthfulQA":59.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"fcf546ffbfdee6e9bd288eec27316cac533d1ffe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TeeZee\/GALAXY_v03_slimorca_1_epoch_50k_DPO_1_epoch_30k",
        "Average":67.49,
        "ARC":65.27,
        "HellaSwag":85.62,
        "MMLU":65.61,
        "TruthfulQA":53.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":15.97,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"feec1455869adb242dbafa1d0e22a81972ee9b79"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mahiatlinux\/MasherAI-v6-7B",
        "Average":67.49,
        "ARC":62.88,
        "HellaSwag":83.94,
        "MMLU":60.56,
        "TruthfulQA":62.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"4c1c504c7a9d37720f71722f14856677dd5827ff"
    },
    {
        "T":"\u2b55",
        "Model":"vicgalle\/solarized-13B-dpo",
        "Average":67.48,
        "ARC":62.71,
        "HellaSwag":81.82,
        "MMLU":59.12,
        "TruthfulQA":66.25,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.48,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"a7d6f68c292320161c563bd24232907b6d5f9b21"
    },
    {
        "T":"?",
        "Model":"dozzke\/hermorca",
        "Average":67.47,
        "ARC":63.57,
        "HellaSwag":84.41,
        "MMLU":64.29,
        "TruthfulQA":57.63,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5102ad0c27d60a2c6381b8ec97fcc59450ea5640"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/NeuralHermes-2.5-Mistral-7B-laser",
        "Average":67.46,
        "ARC":66.38,
        "HellaSwag":85.09,
        "MMLU":63.43,
        "TruthfulQA":54.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"76efb2db34ee99b591431a3055eca785ffed44f7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chanwit\/flux-7b-v0.2",
        "Average":67.46,
        "ARC":66.55,
        "HellaSwag":86.12,
        "MMLU":65.38,
        "TruthfulQA":51.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6ff053b441ac4efec7b92828c64a8a6f1649a6f6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishekchohan\/mistral-7B-forest-dpo",
        "Average":67.45,
        "ARC":65.02,
        "HellaSwag":86.31,
        "MMLU":63.05,
        "TruthfulQA":55.43,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"49b6d10aa6fde729393be056f8e110345c633342"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"hfl\/chinese-mixtral",
        "Average":67.45,
        "ARC":67.49,
        "HellaSwag":85.25,
        "MMLU":70.31,
        "TruthfulQA":46.75,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"7b37775efb34a0734efd60a32781bd706c60e85b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/NeuralHermes-2.5-Mistral-7B",
        "Average":67.43,
        "ARC":66.55,
        "HellaSwag":84.9,
        "MMLU":63.32,
        "TruthfulQA":54.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":140,
        "Available on the hub":false,
        "Model Sha":"351028e0532a084c2c1370029fcf2ef805da3929"
    },
    {
        "T":"?",
        "Model":"DenisTheDev\/Blitz-AI-MOE-v0.4",
        "Average":67.42,
        "ARC":66.3,
        "HellaSwag":85.59,
        "MMLU":64.24,
        "TruthfulQA":53.55,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":18.52,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7655e3973c0f0813532e82f8a239f0cb5d8fd7be"
    },
    {
        "T":"?",
        "Model":"NousResearch\/Nous-Hermes-2-Mistral-7B-DPO",
        "Average":67.42,
        "ARC":65.7,
        "HellaSwag":84.94,
        "MMLU":63.25,
        "TruthfulQA":55.78,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":137,
        "Available on the hub":false,
        "Model Sha":"868f0aaeba382aa63bef6ff2bc370be5df16ce9a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"UCLA-AGI\/zephyr-7b-sft-full-SPIN-iter1",
        "Average":67.41,
        "ARC":65.87,
        "HellaSwag":85.44,
        "MMLU":60.95,
        "TruthfulQA":57.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"d8569aea49f28131ca3d319da343da0777ed4161"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"UCLA-AGI\/test",
        "Average":67.41,
        "ARC":65.87,
        "HellaSwag":85.44,
        "MMLU":60.95,
        "TruthfulQA":57.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":24,
        "Available on the hub":false,
        "Model Sha":"437d2f9c55aec50ebaedce22df8aaa7fcc0f9ff8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"UCLA-AGI\/zephyr-7b-sft-full-spin-iter1",
        "Average":67.41,
        "ARC":65.87,
        "HellaSwag":85.44,
        "MMLU":60.95,
        "TruthfulQA":57.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"9257b6484010acf5eed7e77ff787264b49c1a923"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardMath-70B-V1.0",
        "Average":67.41,
        "ARC":65.96,
        "HellaSwag":85.95,
        "MMLU":67.64,
        "TruthfulQA":50.09,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":91,
        "Available on the hub":true,
        "Model Sha":"8823afe1d77b1ebdd6ac0c14e6e8977037d1830e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Zangs3011\/mixtral_8x7b_MonsterInstruct",
        "Average":67.4,
        "ARC":65.19,
        "HellaSwag":85.81,
        "MMLU":70.15,
        "TruthfulQA":48.47,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5252c3d68fcd69d14cc76488d689e0adb76d881f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"decruz07\/kellemar-DPO-7B-v1.01",
        "Average":67.4,
        "ARC":65.78,
        "HellaSwag":85.04,
        "MMLU":63.24,
        "TruthfulQA":55.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"b032e5ce518cf12383f4ec12952732d21f8321af"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"genaicore3434\/Mistral-7b-instruct-v0.2-summ-sft-lp-e1",
        "Average":67.4,
        "ARC":61.01,
        "HellaSwag":83.32,
        "MMLU":60.62,
        "TruthfulQA":64.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3a21fabd41c5c558e42f5ee592294ac56369d3d4"
    },
    {
        "T":"?",
        "Model":"LeroyDyer\/Mixtral_AI_128k_b",
        "Average":67.4,
        "ARC":64.08,
        "HellaSwag":84.68,
        "MMLU":63.76,
        "TruthfulQA":57.09,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"edf358adffe215b34acf695a4c1243a7e5d47417"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"silvercoder67\/Mistral-7b-instruct-v0.2-summ-sft-dpo-e1",
        "Average":67.39,
        "ARC":59.3,
        "HellaSwag":83.64,
        "MMLU":60.31,
        "TruthfulQA":66.33,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9eae12f70b601824919394f486dcad3fba26ca3e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jilp00\/Nous-Hermes-2-SOLAR-10.7B-v1.1",
        "Average":67.38,
        "ARC":63.99,
        "HellaSwag":82.72,
        "MMLU":65.85,
        "TruthfulQA":56.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"8e1cbfa67643f49be67a6021db933cdd941a6d2f"
    },
    {
        "T":"?",
        "Model":"saishf\/West-Maid-7B",
        "Average":67.38,
        "ARC":67.24,
        "HellaSwag":86.44,
        "MMLU":64.85,
        "TruthfulQA":51.0,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a271497bda998eed0acd3e68165133e7f3d196a1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Code-Mistral-7B",
        "Average":67.38,
        "ARC":64.59,
        "HellaSwag":85.29,
        "MMLU":65.0,
        "TruthfulQA":54.64,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"ed3b9ad583910423a7b82e27274681e3865206f1"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"v1olet\/v1olet_merged_dpo_7B_v4",
        "Average":67.38,
        "ARC":66.98,
        "HellaSwag":84.09,
        "MMLU":59.02,
        "TruthfulQA":59.43,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"aa1b6363990ed2f180b2a22986cecc3afa4d12c8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fhai50032\/BeagleLake-7B-Toxic",
        "Average":67.37,
        "ARC":65.19,
        "HellaSwag":83.83,
        "MMLU":62.82,
        "TruthfulQA":57.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"d5c8a91a088942987fb4c3af188f13b3d8e75ad1"
    },
    {
        "T":"?",
        "Model":"InnerI\/InnerILLM-OpenPipe-Nous-Yarn-Mistral-optimized-1228-7B-slerp",
        "Average":67.36,
        "ARC":65.78,
        "HellaSwag":85.21,
        "MMLU":64.95,
        "TruthfulQA":53.51,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bf6f6b7378e2bfc0e9f26b0cd2f0d81e0c72e350"
    },
    {
        "T":"?",
        "Model":"InnerI\/InnerILLM-0x00d0-7B-slerp",
        "Average":67.36,
        "ARC":65.78,
        "HellaSwag":85.21,
        "MMLU":64.95,
        "TruthfulQA":53.51,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d8044ad07c038761f4ac72db0a2cb3770b69da0e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Llama-2-70B-fp16",
        "Average":67.35,
        "ARC":67.32,
        "HellaSwag":87.33,
        "MMLU":69.83,
        "TruthfulQA":44.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.98,
        "Hub":34,
        "Available on the hub":true,
        "Model Sha":"b25061ef1b440e970d15d4ac99bc42937cd442a2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bongchoi\/test-llama2-70b",
        "Average":67.35,
        "ARC":67.32,
        "HellaSwag":87.33,
        "MMLU":69.83,
        "TruthfulQA":44.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":70.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d5c1b7a36ba4f2b988f6c49c0900783b390a1703"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"meta-llama\/Llama-2-70b-hf",
        "Average":67.35,
        "ARC":67.32,
        "HellaSwag":87.33,
        "MMLU":69.83,
        "TruthfulQA":44.92,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub":579,
        "Available on the hub":false,
        "Model Sha":"ed7b07231238f836b99bf45701b9a0063576b194"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"genaicore3434\/Mistral-7b-instruct-v0.2-summ-sft-bf16-e1",
        "Average":67.35,
        "ARC":60.58,
        "HellaSwag":83.32,
        "MMLU":60.79,
        "TruthfulQA":64.72,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"440decfee1fefa33072e2a99e4190f0a206aba13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PulsarAI\/MetaMath-OpenHermes-2.5-neural-chat-v3-3-Slerp",
        "Average":67.35,
        "ARC":64.59,
        "HellaSwag":85.39,
        "MMLU":64.27,
        "TruthfulQA":55.14,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"111ae8b3fb38d550a32f04dbd977f8cd447a3a92"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"freecs\/ThetaWave-7B-v1",
        "Average":67.35,
        "ARC":66.89,
        "HellaSwag":84.91,
        "MMLU":61.62,
        "TruthfulQA":55.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7cad16a292a7b96d671e20dad3609d03814149d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/MetaMath-OpenHermes-2.5-neural-chat-v3-3-Slerp",
        "Average":67.35,
        "ARC":64.59,
        "HellaSwag":85.37,
        "MMLU":64.29,
        "TruthfulQA":55.14,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"f209799cbf4f782e1c6352e427599e2f8a6038ad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Chupacabra-7B-v2",
        "Average":67.34,
        "ARC":65.19,
        "HellaSwag":83.39,
        "MMLU":63.6,
        "TruthfulQA":57.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":33,
        "Available on the hub":false,
        "Model Sha":"0c7f7c85359f15d3e6c361e8192738bdfb14ea6c"
    },
    {
        "T":"\u2b55",
        "Model":"pinkyponky\/SOLAR-10.7B-dpo-instruct-tuned-v0.1",
        "Average":67.33,
        "ARC":65.19,
        "HellaSwag":86.09,
        "MMLU":66.25,
        "TruthfulQA":51.81,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"bb3b052f07ab6bc00a03dc5c7b510c0760bfd650"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"maywell\/PiVoT-0.1-early",
        "Average":67.33,
        "ARC":62.46,
        "HellaSwag":82.97,
        "MMLU":61.02,
        "TruthfulQA":62.89,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"6eeae58a1a292a1d7f989952a07aead6d5da3c69"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Neuronovo\/neuronovo-7B-v0.1",
        "Average":67.33,
        "ARC":66.98,
        "HellaSwag":85.07,
        "MMLU":63.33,
        "TruthfulQA":53.95,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ec4f35d96aa47229fb3cab047fb9aedd6b0ad383"
    },
    {
        "T":"\u2b55",
        "Model":"mlinmg\/SG-Raccoon-Yi-200k-2.0",
        "Average":67.33,
        "ARC":62.54,
        "HellaSwag":80.26,
        "MMLU":73.29,
        "TruthfulQA":53.21,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":55.59,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"986706415fcb2118f35626dbc12e054457ec9ad3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/MathDolphin-7B",
        "Average":67.32,
        "ARC":65.87,
        "HellaSwag":85.49,
        "MMLU":65.02,
        "TruthfulQA":52.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6dcfc55a6e845fac45b8dbe3d8c2506fd1348834"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/neural-chat-7B-v3-2-GPTQ",
        "Average":67.32,
        "ARC":65.96,
        "HellaSwag":83.24,
        "MMLU":60.29,
        "TruthfulQA":59.79,
        "Type":"fine-tuned",
        "Precision":"None",
        "Hub License":"apache-2.0",
        "#Params (B)":1.2,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"cfe57da77e55efcb0e1087dc3948aeaa6ca55c74"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xriminact\/TarsChattyBasev0.0",
        "Average":67.32,
        "ARC":64.93,
        "HellaSwag":84.57,
        "MMLU":58.04,
        "TruthfulQA":61.71,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2a690b0b6f9b88390b06c1b0f07f6f5993c374e1"
    },
    {
        "T":"?",
        "Model":"AIJUUD\/juud-Mistral-7B",
        "Average":67.31,
        "ARC":66.72,
        "HellaSwag":85.0,
        "MMLU":63.38,
        "TruthfulQA":54.12,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"0e51981702ee1f4c3162915e4ac5233591821af8"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/llama-2-alpacagpt4-1000step",
        "Average":67.3,
        "ARC":66.38,
        "HellaSwag":84.51,
        "MMLU":62.75,
        "TruthfulQA":55.57,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":""
    },
    {
        "T":"\u2b55",
        "Model":"simsim314\/WizardLM-70B-V1.0-HF",
        "Average":67.3,
        "ARC":64.08,
        "HellaSwag":85.4,
        "MMLU":64.97,
        "TruthfulQA":54.76,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"97112db6d0fae8354c13437a5e7dc99fb37b8c2e"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardLM-70B-V1.0",
        "Average":67.3,
        "ARC":64.08,
        "HellaSwag":85.4,
        "MMLU":64.97,
        "TruthfulQA":54.76,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":93,
        "Available on the hub":true,
        "Model Sha":"4dc4c3190ca7026c4107031f0ea945a9f1ecd97c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xenon1\/Eclipse-13B-dpo",
        "Average":67.3,
        "ARC":64.59,
        "HellaSwag":85.0,
        "MMLU":64.85,
        "TruthfulQA":54.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c5f4f9175e92e08ce06a01fc2e5ece22ac3c409d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Replete-AI\/Mistral-Evolved-11b-v0.1",
        "Average":67.3,
        "ARC":62.2,
        "HellaSwag":84.65,
        "MMLU":63.11,
        "TruthfulQA":59.23,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.17,
        "Hub":28,
        "Available on the hub":false,
        "Model Sha":"57eb00a9bf191d5a338c11098fa6e82d5f121d9b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Replete-AI\/Mistral-11b-v0.1",
        "Average":67.3,
        "ARC":62.2,
        "HellaSwag":84.65,
        "MMLU":63.11,
        "TruthfulQA":59.23,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.17,
        "Hub":28,
        "Available on the hub":false,
        "Model Sha":"57eb00a9bf191d5a338c11098fa6e82d5f121d9b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"chargoddard\/internlm2-20b-llama",
        "Average":67.29,
        "ARC":64.68,
        "HellaSwag":83.16,
        "MMLU":67.17,
        "TruthfulQA":54.17,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":19.86,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"acf3dfe237a34a1898f57787d531b86497e96777"
    },
    {
        "T":"?",
        "Model":"Cartinoe5930\/MoE-Merging",
        "Average":67.29,
        "ARC":65.44,
        "HellaSwag":84.58,
        "MMLU":61.31,
        "TruthfulQA":57.83,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"022dfa677128a9d9b71a5350d7340b6f1f023ea0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PistachioAlt\/Synatra-MCS-7B-v0.3-RP-Slerp",
        "Average":67.29,
        "ARC":66.64,
        "HellaSwag":84.97,
        "MMLU":63.61,
        "TruthfulQA":53.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"69369829e665cbcda97e7fd178f1c43720f0fce4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051615\/A0305a",
        "Average":67.29,
        "ARC":61.35,
        "HellaSwag":80.4,
        "MMLU":75.66,
        "TruthfulQA":51.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"f9770a953c3daa35590323746b1dd01620c6edd8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"chargoddard\/internlm2-20b-llama",
        "Average":67.28,
        "ARC":64.59,
        "HellaSwag":83.12,
        "MMLU":67.27,
        "TruthfulQA":54.13,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":19.86,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"acf3dfe237a34a1898f57787d531b86497e96777"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardLM-70B-V1.0",
        "Average":67.28,
        "ARC":64.08,
        "HellaSwag":85.45,
        "MMLU":64.81,
        "TruthfulQA":54.77,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":93,
        "Available on the hub":true,
        "Model Sha":"6dae38060d70b82dcfe787a612d04aaf0adf0738"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"simsim314\/WizardLM-70B-V1.0-HF",
        "Average":67.28,
        "ARC":64.08,
        "HellaSwag":85.45,
        "MMLU":64.81,
        "TruthfulQA":54.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"97112db6d0fae8354c13437a5e7dc99fb37b8c2e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"silvercoder45\/Mistral-7b-instruct-v0.2-summ-sft-dpo-e3",
        "Average":67.27,
        "ARC":58.87,
        "HellaSwag":83.56,
        "MMLU":60.37,
        "TruthfulQA":66.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7d5b26e13a4967f25aa53852ff9a3a8ce0348116"
    },
    {
        "T":"?",
        "Model":"openbmb\/UltraLM-65b",
        "Average":67.26,
        "ARC":67.06,
        "HellaSwag":84.98,
        "MMLU":63.48,
        "TruthfulQA":53.51,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":65.02,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/AZG",
        "Average":67.26,
        "ARC":62.88,
        "HellaSwag":82.02,
        "MMLU":70.29,
        "TruthfulQA":53.84,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"36c17124ff891121c39f2d5e4d203daad5350c48"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Falkor-8x7B-MoE",
        "Average":67.24,
        "ARC":66.3,
        "HellaSwag":85.03,
        "MMLU":64.13,
        "TruthfulQA":53.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"8a13e5399c12811d178cea09ffa719596410c9b4"
    },
    {
        "T":"?",
        "Model":"paulilioaica\/Hugo-7B-slerp",
        "Average":67.24,
        "ARC":64.51,
        "HellaSwag":84.77,
        "MMLU":62.54,
        "TruthfulQA":57.13,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"820dcd204a79f46110fad378907f0be35a266ecb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/MetaMath-neural-chat-7b-v3-2-Slerp",
        "Average":67.23,
        "ARC":65.7,
        "HellaSwag":84.51,
        "MMLU":63.5,
        "TruthfulQA":55.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"147f8e0526768591a7a119b7ec5b8cb821dbe900"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lodrick-the-lafted\/Hermes-Instruct-7B-100K",
        "Average":67.23,
        "ARC":61.52,
        "HellaSwag":82.84,
        "MMLU":60.95,
        "TruthfulQA":63.62,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"0dd712293d5b914d53f1e1f35922cd023ba98047"
    },
    {
        "T":"?",
        "Model":"Locutusque\/ChatHercules-2.5-Mistral-7B-DPO",
        "Average":67.23,
        "ARC":66.04,
        "HellaSwag":85.4,
        "MMLU":65.17,
        "TruthfulQA":52.3,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"db11ab2310f79a2f1edc4e71c64a63462e65c8e5"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"liminerity\/Blured-Ties-7B",
        "Average":67.22,
        "ARC":63.99,
        "HellaSwag":83.56,
        "MMLU":63.19,
        "TruthfulQA":58.12,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"46faa4a8bad44e10b7840930bd4d7e3a281f1b90"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Mistral-11B-TestBench11",
        "Average":67.21,
        "ARC":64.42,
        "HellaSwag":83.93,
        "MMLU":63.82,
        "TruthfulQA":56.68,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":15,
        "Available on the hub":false,
        "Model Sha":"9aae2b156b24557bb98e515f3a90c7865529d2e9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mahiatlinux\/MasherAI-v6.1-7B-checkpoint6",
        "Average":67.21,
        "ARC":63.05,
        "HellaSwag":83.41,
        "MMLU":63.07,
        "TruthfulQA":59.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"684c07aaebeba1e0dd5d3f94ff93a7e9082ca4fa"
    },
    {
        "T":"?",
        "Model":"robinsmits\/Mistral-Instruct-7B-v0.2-ChatAlpaca-DPO2",
        "Average":67.21,
        "ARC":61.86,
        "HellaSwag":83.71,
        "MMLU":59.19,
        "TruthfulQA":64.08,
        "Type":"Unknown",
        "Precision":"4bit",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"32122f2efc944e3b88e502c6c310ce4c70fe1419"
    },
    {
        "T":"?",
        "Model":"delayedkarma\/NeuralHermes-2.5-Mistral-7B",
        "Average":67.2,
        "ARC":66.55,
        "HellaSwag":85.0,
        "MMLU":63.41,
        "TruthfulQA":53.83,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"44177ada255b456e97fd9ab246c7dda3869950c3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_420_preview",
        "Average":67.19,
        "ARC":67.06,
        "HellaSwag":87.26,
        "MMLU":69.85,
        "TruthfulQA":44.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5095384f1b7bb6e23a987f95589e66e21ae854ef"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azazelle\/Dumb-Maidlet",
        "Average":67.19,
        "ARC":66.81,
        "HellaSwag":86.06,
        "MMLU":65.17,
        "TruthfulQA":50.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1bbd507bb7dd502bbca4105406a6e57abe3c1187"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewqr2130\/mistral-inst-ppo",
        "Average":67.18,
        "ARC":62.37,
        "HellaSwag":83.2,
        "MMLU":60.86,
        "TruthfulQA":62.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"600c429a86dcd6e18f0285d7cd9189540ccbdc50"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardLM-70B-V1.0",
        "Average":67.18,
        "ARC":65.44,
        "HellaSwag":84.41,
        "MMLU":64.05,
        "TruthfulQA":54.81,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":93,
        "Available on the hub":true,
        "Model Sha":"6dae38060d70b82dcfe787a612d04aaf0adf0738"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"simsim314\/WizardLM-70B-V1.0-HF",
        "Average":67.18,
        "ARC":65.44,
        "HellaSwag":84.41,
        "MMLU":64.05,
        "TruthfulQA":54.81,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"97112db6d0fae8354c13437a5e7dc99fb37b8c2e"
    },
    {
        "T":"?",
        "Model":"neovalle\/H4rmoniousAnthea",
        "Average":67.18,
        "ARC":65.87,
        "HellaSwag":84.09,
        "MMLU":63.67,
        "TruthfulQA":55.08,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"42979461b582e0e511f29ec4c72a69a13dc4a831"
    },
    {
        "T":"?",
        "Model":"nonetrix\/sillyrp-7b",
        "Average":67.17,
        "ARC":64.93,
        "HellaSwag":85.26,
        "MMLU":64.2,
        "TruthfulQA":54.28,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9ab1c660c1ae1887e5e647a6ba40e04c49cbfe3f"
    },
    {
        "T":"?",
        "Model":"tourist800\/Mistral-7B-Merge-14-v0.2",
        "Average":67.17,
        "ARC":65.02,
        "HellaSwag":85.13,
        "MMLU":64.36,
        "TruthfulQA":54.15,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"570081fa881550751d3f2a2be160a8a08b965a8d"
    },
    {
        "T":"?",
        "Model":"vicgalle\/SystemHermes-2-7B",
        "Average":67.16,
        "ARC":65.02,
        "HellaSwag":84.05,
        "MMLU":63.16,
        "TruthfulQA":56.42,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"56ae3b1d75dcf4b435272aff0db7eb73a752e6dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-mistral-six-in-one-7b",
        "Average":67.16,
        "ARC":62.97,
        "HellaSwag":84.6,
        "MMLU":63.29,
        "TruthfulQA":57.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"41e912e0f79094a80687f88ca5555f84aa9d307f"
    },
    {
        "T":"?",
        "Model":"Aryanne\/WestSenzu-Swap-7B",
        "Average":67.15,
        "ARC":68.34,
        "HellaSwag":85.7,
        "MMLU":64.14,
        "TruthfulQA":50.43,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"620fb61f0f963dab1ef2255ba2ffa1590ac5daf5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/A0105",
        "Average":67.15,
        "ARC":62.12,
        "HellaSwag":82.54,
        "MMLU":68.51,
        "TruthfulQA":55.44,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1ebf59e3ccbc0295bb8e1effd1eddf9ff0be2b50"
    },
    {
        "T":"\u2b55",
        "Model":"mrfakename\/NeuralOrca-7B-v1",
        "Average":67.15,
        "ARC":65.27,
        "HellaSwag":85.07,
        "MMLU":63.68,
        "TruthfulQA":54.58,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"32fb215494467cc6fa2f283a4b02f23546a26807"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishekchohan\/mistral-7B-forest",
        "Average":67.15,
        "ARC":65.7,
        "HellaSwag":86.26,
        "MMLU":63.32,
        "TruthfulQA":53.32,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"02c58b0694973815a6d89f29c74f5cb1a4562891"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andysalerno\/openchat-nectar-0.1",
        "Average":67.15,
        "ARC":66.21,
        "HellaSwag":82.99,
        "MMLU":65.17,
        "TruthfulQA":54.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"cf913c9f807a9bdbe606ac4bf445d93a082a118c"
    },
    {
        "T":"?",
        "Model":"cloudyu\/Mixtral_7Bx2_MoE_13B",
        "Average":67.15,
        "ARC":64.85,
        "HellaSwag":83.92,
        "MMLU":62.27,
        "TruthfulQA":57.55,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.88,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"5ea651448fbeb313665d66187416233b865db7f1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-llama2-70b-v10.1-bf16",
        "Average":67.15,
        "ARC":61.86,
        "HellaSwag":83.13,
        "MMLU":67.41,
        "TruthfulQA":56.18,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":68.76,
        "Hub":33,
        "Available on the hub":true,
        "Model Sha":"a6ee90d262ac729f90ed8de97127766df070074c"
    },
    {
        "T":"?",
        "Model":"Test157t\/Hex-Macaroniac-7b",
        "Average":67.14,
        "ARC":65.53,
        "HellaSwag":84.68,
        "MMLU":62.43,
        "TruthfulQA":55.93,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ae590a93adc146935da5fae38c3cdc7d5d86e16e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cognitivecomputations\/fc-dolphin-2.6-mistral-7b-dpo-laser",
        "Average":67.14,
        "ARC":62.97,
        "HellaSwag":84.18,
        "MMLU":63.65,
        "TruthfulQA":57.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":24,
        "Available on the hub":false,
        "Model Sha":"a5cb1123f93521d1febc9c73dd110e1ca0016bf6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eren23\/DistilHermes-2.5-Mistral-7B",
        "Average":67.14,
        "ARC":65.87,
        "HellaSwag":84.78,
        "MMLU":63.65,
        "TruthfulQA":54.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b75259311e80e39117c7c31e5f93ebd8e33ffc75"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Mistral-11B-TestBench9",
        "Average":67.13,
        "ARC":64.08,
        "HellaSwag":84.24,
        "MMLU":64.0,
        "TruthfulQA":56.19,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"4ff48527af8c3907129c06160c7f7b7b786a5a79"
    },
    {
        "T":"?",
        "Model":"macadeliccc\/laser-polyglot-4x7b",
        "Average":67.12,
        "ARC":64.16,
        "HellaSwag":84.98,
        "MMLU":63.88,
        "TruthfulQA":55.47,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"3402a470e7fca09eb5aa5f7dcf2876449a05a4f6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fzzhang\/Marcoroni-neural-chat-7B-v2_gsm8k_merged",
        "Average":67.12,
        "ARC":65.78,
        "HellaSwag":85.26,
        "MMLU":64.26,
        "TruthfulQA":53.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dfabf300a516c8a8695bc62784c2b0bc2db7242b"
    },
    {
        "T":"?",
        "Model":"traversaal-ai\/traversaal-2.5-Mistral-7B",
        "Average":67.12,
        "ARC":66.21,
        "HellaSwag":85.02,
        "MMLU":63.24,
        "TruthfulQA":54.0,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f5403d78d43d34f90d6a0aab0b61985d48f20738"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Sensualize-Solar-10.7B",
        "Average":67.12,
        "ARC":65.02,
        "HellaSwag":84.55,
        "MMLU":65.27,
        "TruthfulQA":53.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"126d7e645300a7773044408f77a810bc4f423949"
    },
    {
        "T":"?",
        "Model":"localfultonextractor\/Erosumika-7B-v3-0.2",
        "Average":67.12,
        "ARC":67.75,
        "HellaSwag":84.95,
        "MMLU":60.0,
        "TruthfulQA":55.77,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"a634a34eb846fb891c58e45b82997c56abdac4c1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jeiku\/Lunar_10.7B",
        "Average":67.12,
        "ARC":65.87,
        "HellaSwag":84.85,
        "MMLU":64.23,
        "TruthfulQA":53.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8a6dc10058be04af2e76e088a04ab192352416d5"
    },
    {
        "T":"?",
        "Model":"ChaoticNeutrals\/Lunar_10.7B",
        "Average":67.12,
        "ARC":65.87,
        "HellaSwag":84.85,
        "MMLU":64.23,
        "TruthfulQA":53.51,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"c4f668605515745bb097e3bbbfec808550324704"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"decruz07\/kellemar-DPO-7B-c",
        "Average":67.11,
        "ARC":65.7,
        "HellaSwag":84.98,
        "MMLU":63.7,
        "TruthfulQA":54.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"e9cc6491994a6babaa14f70ad425418d5c4bc7ae"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"joowon99\/SOLAR-10.7B-ko_alpaca",
        "Average":67.11,
        "ARC":64.16,
        "HellaSwag":82.62,
        "MMLU":65.71,
        "TruthfulQA":55.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":10.6,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"04b63652663be2d6c7178577781efdd737b3c37a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeuralNovel\/Aeryth-7B-v0.1",
        "Average":67.1,
        "ARC":60.32,
        "HellaSwag":83.53,
        "MMLU":60.97,
        "TruthfulQA":63.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b7befcbc2e609356efc76c64cee1b1727727d815"
    },
    {
        "T":"?",
        "Model":"grimjim\/Mistral-Starling-merge-trial3-7B",
        "Average":67.1,
        "ARC":66.55,
        "HellaSwag":84.81,
        "MMLU":64.18,
        "TruthfulQA":52.85,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"95cc5aafb8b12ae31b5fd5e68a0e9e3e16c7546a"
    },
    {
        "T":"?",
        "Model":"Kabster\/Bio-Mistralv2-Squared",
        "Average":67.1,
        "ARC":63.31,
        "HellaSwag":84.02,
        "MMLU":60.08,
        "TruthfulQA":60.98,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"77aa25e74e78c21e5ede5411d38e819d70d5ba9d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"feeltheAGI\/Maverick-Math-7B",
        "Average":67.09,
        "ARC":65.27,
        "HellaSwag":84.54,
        "MMLU":62.59,
        "TruthfulQA":55.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"9ae5d1f286ec0148a077c75e4d201e85df305ae4"
    },
    {
        "T":"?",
        "Model":"freeCS-dot-org\/OpenAGI-testing-intelDPO-2",
        "Average":67.09,
        "ARC":62.8,
        "HellaSwag":84.63,
        "MMLU":62.65,
        "TruthfulQA":58.28,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d160d65b1155a68c70ed75838c2bdc7f5ce511e8"
    },
    {
        "T":"?",
        "Model":"nbeerbower\/Flammen-Trismegistus-7B",
        "Average":67.09,
        "ARC":63.99,
        "HellaSwag":84.79,
        "MMLU":62.45,
        "TruthfulQA":57.12,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4ef80a33ffebbe3f28f1178f324cf99d12b5f0f0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeuralNovel\/Gecko-7B-v0.1",
        "Average":67.09,
        "ARC":61.35,
        "HellaSwag":83.36,
        "MMLU":61.05,
        "TruthfulQA":62.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"404e501cb4d091e768e12861d50e37ac99d8a8fe"
    },
    {
        "T":"?",
        "Model":"0-hero\/Matter-0.1-7B-boost-DPO-preview",
        "Average":67.08,
        "ARC":64.59,
        "HellaSwag":82.87,
        "MMLU":62.02,
        "TruthfulQA":58.86,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"d390fb35a781129efd26d53f7ecdb513c0c3da27"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cookinai\/Bald-Eagle-7B",
        "Average":67.08,
        "ARC":64.51,
        "HellaSwag":84.79,
        "MMLU":64.39,
        "TruthfulQA":54.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6f5a38b66c4121b2dae4545ad3b2c42fb2637556"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"liminerity\/e.star.7.b",
        "Average":67.07,
        "ARC":63.91,
        "HellaSwag":86.02,
        "MMLU":63.44,
        "TruthfulQA":54.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c1af18b48367a616f673b9feff92ab73d0f40874"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"giraffe176\/WestLake_Noromaid_OpenHermes_neural-chatv0.1",
        "Average":67.06,
        "ARC":66.72,
        "HellaSwag":85.37,
        "MMLU":64.67,
        "TruthfulQA":51.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"087d5e79ae93abbc9d8c58d4bbaa61b3933761fc"
    },
    {
        "T":"?",
        "Model":"maldv\/eleusis-7b-alpha",
        "Average":67.06,
        "ARC":64.93,
        "HellaSwag":84.87,
        "MMLU":64.1,
        "TruthfulQA":54.34,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d8b62e9eba34e430291e4649e5eb84c93bffbe65"
    },
    {
        "T":"?",
        "Model":"AIJUUD\/juud-Mistral-7B-dpo",
        "Average":67.06,
        "ARC":66.81,
        "HellaSwag":84.89,
        "MMLU":63.03,
        "TruthfulQA":53.51,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b428f21995854f143b497a36d210276439ae0b87"
    },
    {
        "T":"?",
        "Model":"Isotonic\/Hermes-2-Pro-Mixtral-4x7B",
        "Average":67.06,
        "ARC":64.25,
        "HellaSwag":82.7,
        "MMLU":62.26,
        "TruthfulQA":59.02,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"787647b887303d98363ce1b352d7034f4f0d1b6e"
    },
    {
        "T":"?",
        "Model":"vicgalle\/zephyr-7b-truthy",
        "Average":67.06,
        "ARC":60.75,
        "HellaSwag":84.64,
        "MMLU":59.53,
        "TruthfulQA":63.31,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"f2f46ce1de3773a3d90b7006e0d6aa48edd884c5"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/dolphin-2.1-mistral-7b",
        "Average":67.06,
        "ARC":64.42,
        "HellaSwag":84.92,
        "MMLU":63.32,
        "TruthfulQA":55.56,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":256,
        "Available on the hub":false,
        "Model Sha":"aa5bd48c8b3040d1155a8fd59328df160aa63680"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/mistral_9B_instruct_v0.2",
        "Average":67.05,
        "ARC":61.01,
        "HellaSwag":82.77,
        "MMLU":60.54,
        "TruthfulQA":63.9,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":8.99,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"9a8e0e208cefc52f8e33c765720bf3c95cca38b0"
    },
    {
        "T":"?",
        "Model":"Fredithefish\/MadMix-v0.2",
        "Average":67.05,
        "ARC":64.85,
        "HellaSwag":83.54,
        "MMLU":64.02,
        "TruthfulQA":55.79,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"69a3c98c23938a9370c62ae43894eb7723de97dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"decapoda-research\/Antares-11b-v1",
        "Average":67.04,
        "ARC":64.51,
        "HellaSwag":84.85,
        "MMLU":65.96,
        "TruthfulQA":52.84,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"f5bda513641d782ab5278e993eb3ba8c7799f1b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wandb\/mistral-7b-zephyr-dpo",
        "Average":67.03,
        "ARC":63.74,
        "HellaSwag":85.79,
        "MMLU":61.98,
        "TruthfulQA":56.61,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"ac80a1ae40341ea97a5381e4ce509ebd86f4ae72"
    },
    {
        "T":"?",
        "Model":"LeroyDyer\/Mixtral_Chat_X",
        "Average":67.03,
        "ARC":65.53,
        "HellaSwag":84.93,
        "MMLU":61.5,
        "TruthfulQA":56.15,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"8c4fda57602b78d3a3608e1cc3853bd64f663b04"
    },
    {
        "T":"?",
        "Model":"grimjim\/Mistral-Starling-merge-trial1-7B",
        "Average":67.03,
        "ARC":66.13,
        "HellaSwag":84.67,
        "MMLU":64.12,
        "TruthfulQA":53.18,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8781341908ff63afe7a31e8692ae964cfb75cf38"
    },
    {
        "T":"\u2b55",
        "Model":"Mihaiii\/Metis-0.4",
        "Average":67.02,
        "ARC":62.29,
        "HellaSwag":83.91,
        "MMLU":62.7,
        "TruthfulQA":59.2,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c2b149c7df2806add971b2c2ec27288abc18f312"
    },
    {
        "T":"?",
        "Model":"NousResearch\/Hermes-2-Pro-Mistral-7B",
        "Average":67.02,
        "ARC":64.16,
        "HellaSwag":82.73,
        "MMLU":62.21,
        "TruthfulQA":58.99,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":369,
        "Available on the hub":false,
        "Model Sha":"8dd571ec94aa1709b4b02a07e1201678b939ef44"
    },
    {
        "T":"\u2b55",
        "Model":"Mihaiii\/Metis-0.4",
        "Average":67.02,
        "ARC":62.2,
        "HellaSwag":84.0,
        "MMLU":62.65,
        "TruthfulQA":59.24,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c2b149c7df2806add971b2c2ec27288abc18f312"
    },
    {
        "T":"\u2b55",
        "Model":"Mihaiii\/Metis-0.3-merged",
        "Average":67.02,
        "ARC":62.2,
        "HellaSwag":84.0,
        "MMLU":62.65,
        "TruthfulQA":59.24,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"dbcf2c1f7cbea0bacd756f7d8251b5bb037e28d3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"upstage\/llama-30b-instruct-2048",
        "Average":67.02,
        "ARC":64.93,
        "HellaSwag":84.94,
        "MMLU":61.9,
        "TruthfulQA":56.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":96,
        "Available on the hub":true,
        "Model Sha":"be44a37814a20e790063086703f570732597887a"
    },
    {
        "T":"?",
        "Model":"Kabster\/Bio-Mistralv2-Squared",
        "Average":67.01,
        "ARC":62.97,
        "HellaSwag":84.02,
        "MMLU":60.08,
        "TruthfulQA":60.99,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"77aa25e74e78c21e5ede5411d38e819d70d5ba9d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-falcon-180b-v13-preview0",
        "Average":67.01,
        "ARC":65.27,
        "HellaSwag":85.64,
        "MMLU":62.95,
        "TruthfulQA":54.19,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":178.64,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"7d7b93ffd67d1b0c39f3503050dbbcc951948120"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-1.2",
        "Average":67.01,
        "ARC":65.87,
        "HellaSwag":86.08,
        "MMLU":63.37,
        "TruthfulQA":52.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"50ab86e198e1c82ec81aefc628f23501c101d390"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/dolphin-2.1-mistral-7b",
        "Average":67.0,
        "ARC":63.99,
        "HellaSwag":85.0,
        "MMLU":63.44,
        "TruthfulQA":55.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":256,
        "Available on the hub":false,
        "Model Sha":"aa5bd48c8b3040d1155a8fd59328df160aa63680"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Mistral-11B-TestBench10",
        "Average":66.99,
        "ARC":64.25,
        "HellaSwag":84.24,
        "MMLU":63.9,
        "TruthfulQA":55.57,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":11.0,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"730429d6132c7702885840098885081c2df878df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"malhajar\/Mistral-7B-v0.2-meditron-turkish",
        "Average":66.97,
        "ARC":59.56,
        "HellaSwag":81.79,
        "MMLU":60.35,
        "TruthfulQA":66.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"906025770a885b26f762b13bb0bc726438e525de"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Hermes-2-Pro-Mistral-7B",
        "Average":66.97,
        "ARC":63.99,
        "HellaSwag":82.75,
        "MMLU":62.12,
        "TruthfulQA":59.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":369,
        "Available on the hub":false,
        "Model Sha":"8dd571ec94aa1709b4b02a07e1201678b939ef44"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"diffnamehard\/Mistral-CatMacaroni-slerp-uncensored",
        "Average":66.97,
        "ARC":64.25,
        "HellaSwag":84.09,
        "MMLU":62.66,
        "TruthfulQA":56.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"18a3b2e5a34765daafb8e36318a4baf33e272c83"
    },
    {
        "T":"?",
        "Model":"mistralai\/Mixtral-8x7B-v0.1",
        "Average":66.96,
        "ARC":64.33,
        "HellaSwag":85.7,
        "MMLU":70.17,
        "TruthfulQA":47.62,
        "Type":"Unknown",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":1464,
        "Available on the hub":false,
        "Model Sha":"c2b2ae2f1f9532c7c50045bc57d643f46acf8d30"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TriadParty\/deepmoney-34b-200k-base",
        "Average":66.96,
        "ARC":63.99,
        "HellaSwag":83.87,
        "MMLU":74.04,
        "TruthfulQA":45.93,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":33.93,
        "Hub":46,
        "Available on the hub":true,
        "Model Sha":"8ae3d155e57352d4b7fef1d60f74e8c8650a8ab7"
    },
    {
        "T":"?",
        "Model":"giraffe176\/Open_Maid_Samantha_Hermes_Orca_dare_tiesv0.1",
        "Average":66.94,
        "ARC":65.87,
        "HellaSwag":85.48,
        "MMLU":64.5,
        "TruthfulQA":51.92,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0ce0c36f4e93320b3871d575cf7bb3a3d2ee63b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nlpguy\/Hermes-low-tune-3.1",
        "Average":66.94,
        "ARC":65.44,
        "HellaSwag":84.6,
        "MMLU":64.13,
        "TruthfulQA":53.59,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"836fc89c13c6e93f6afe86a756585c2cf455cfe2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andysalerno\/openchat-nectar-0.5",
        "Average":66.94,
        "ARC":66.72,
        "HellaSwag":83.53,
        "MMLU":65.36,
        "TruthfulQA":52.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ba3caf530cbd9caf5c7cc7639cc47a910ed2a120"
    },
    {
        "T":"?",
        "Model":"grimjim\/Mistral-7B-Instruct-demi-merge-v0.2-7B",
        "Average":66.94,
        "ARC":63.91,
        "HellaSwag":84.89,
        "MMLU":63.69,
        "TruthfulQA":55.26,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"db786274df9d55902a7c5e98a134e63deee1f558"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"S4sch\/zephyr-neural-chat-frankenmerge11b",
        "Average":66.94,
        "ARC":61.52,
        "HellaSwag":84.09,
        "MMLU":61.51,
        "TruthfulQA":60.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.39,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"f915831e904e0dcda760873aa16a35daf5ac9e6d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"maywell\/PiVoT-SOLAR-10.7B-RP",
        "Average":66.93,
        "ARC":65.1,
        "HellaSwag":81.83,
        "MMLU":64.26,
        "TruthfulQA":56.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":10.6,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"348a5ccfc4c8c9032ae6234a8fca72110ed4e5ee"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fzzhang\/toten_gsm8k_merged_s",
        "Average":66.93,
        "ARC":65.27,
        "HellaSwag":84.7,
        "MMLU":62.83,
        "TruthfulQA":54.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"34ed7e1f452179f5b551cae07d4b4e2ac15aac2c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KeyonZeng\/lion-zephyr-7b",
        "Average":66.93,
        "ARC":63.05,
        "HellaSwag":84.88,
        "MMLU":60.98,
        "TruthfulQA":58.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"06b27af23fa0638c7ed705043a4fa4a63a4b90bc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mahiatlinux\/MasherAI-v6.1-7B-checkpoint3",
        "Average":66.92,
        "ARC":63.74,
        "HellaSwag":84.07,
        "MMLU":63.67,
        "TruthfulQA":56.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0a0a2548859de1dc3bfd3a5e367743e3459f980f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andysalerno\/openchat-nectar-0.11",
        "Average":66.91,
        "ARC":66.21,
        "HellaSwag":83.28,
        "MMLU":65.25,
        "TruthfulQA":52.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"311304dd45050345aea499c85ddd3af89411513d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/guanaco-65B-HF",
        "Average":66.91,
        "ARC":65.44,
        "HellaSwag":86.47,
        "MMLU":62.92,
        "TruthfulQA":52.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub":26,
        "Available on the hub":true,
        "Model Sha":"7f83ae526f8b83705ca8434535da8fd8c692f9d0"
    },
    {
        "T":"?",
        "Model":"louisbrulenaudet\/Pearl-3x7B",
        "Average":66.88,
        "ARC":65.53,
        "HellaSwag":85.54,
        "MMLU":64.27,
        "TruthfulQA":52.17,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":18.52,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"63499a3e77b66d0709c15208720d48e89b4c1786"
    },
    {
        "T":"?",
        "Model":"invalid-coder\/dolphin-2.1-mistral-7b-snr-laser",
        "Average":66.87,
        "ARC":63.82,
        "HellaSwag":84.78,
        "MMLU":63.63,
        "TruthfulQA":55.24,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f7c83d0e9f7af82ee97bccf1ef6554561c358d43"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KaeriJenti\/Kaori-34B-v1",
        "Average":66.87,
        "ARC":64.51,
        "HellaSwag":79.65,
        "MMLU":70.19,
        "TruthfulQA":53.14,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3a03b2aba751680105e027ef096866320bf9bd2b"
    },
    {
        "T":"\u2b55",
        "Model":"osanseviero\/mistral-instruct-frankenmerge",
        "Average":66.87,
        "ARC":58.19,
        "HellaSwag":83.26,
        "MMLU":59.53,
        "TruthfulQA":66.48,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"af5cbc3a435aab75424e4ecc75f041f2eda133ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KaeriJenti\/Kaori-34B-v1",
        "Average":66.86,
        "ARC":64.42,
        "HellaSwag":79.61,
        "MMLU":70.24,
        "TruthfulQA":53.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3a03b2aba751680105e027ef096866320bf9bd2b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ignos\/LeoScorpius-GreenNode-Platypus-7B-v1",
        "Average":66.85,
        "ARC":66.04,
        "HellaSwag":86.53,
        "MMLU":62.06,
        "TruthfulQA":52.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"606894800b6de3fa7a21b46427c3165968fdf3b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fhai50032\/RP-Coder-SM3",
        "Average":66.85,
        "ARC":65.78,
        "HellaSwag":84.21,
        "MMLU":63.28,
        "TruthfulQA":54.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b395abc7250f460cde49a0bdf894e20ac52e4168"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-1.3",
        "Average":66.83,
        "ARC":66.13,
        "HellaSwag":85.99,
        "MMLU":63.89,
        "TruthfulQA":51.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"4373e66135c6fb4a6063777c4270a34509e7e932"
    },
    {
        "T":"?",
        "Model":"Isaak-Carter\/JOSIE_Beta-4-7B-slerp",
        "Average":66.83,
        "ARC":63.57,
        "HellaSwag":84.1,
        "MMLU":63.73,
        "TruthfulQA":55.93,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ce6ec124e2dd22c85ee71de66f574eeed1c6bdce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fhai50032\/RP-Coder-SM3",
        "Average":66.82,
        "ARC":65.61,
        "HellaSwag":84.22,
        "MMLU":63.34,
        "TruthfulQA":54.11,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b395abc7250f460cde49a0bdf894e20ac52e4168"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"tenyx\/TenyxChat-7B-v1",
        "Average":66.81,
        "ARC":65.61,
        "HellaSwag":85.55,
        "MMLU":64.81,
        "TruthfulQA":51.28,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":25,
        "Available on the hub":false,
        "Model Sha":"c3c7ee002c4fdb1b8c2e2c78b7fba0c389673710"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Einstein-openchat-7B",
        "Average":66.8,
        "ARC":65.1,
        "HellaSwag":83.57,
        "MMLU":64.01,
        "TruthfulQA":54.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d90583c8391ecadb2ea1b47951e8b7817733447b"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"meta-llama\/Llama-2-70b-chat-hf",
        "Average":66.8,
        "ARC":64.59,
        "HellaSwag":85.88,
        "MMLU":63.91,
        "TruthfulQA":52.8,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub":1307,
        "Available on the hub":false,
        "Model Sha":"7f54101c0fbb67a8143ca23eb8bd09b71f269c74"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xwin-LM\/Xwin-Math-70B-V1.0",
        "Average":66.79,
        "ARC":64.51,
        "HellaSwag":84.88,
        "MMLU":66.2,
        "TruthfulQA":51.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"00dc7125d7471bb12035667f54e02b9472dfbca6"
    },
    {
        "T":"?",
        "Model":"Eurdem\/megatron_v3_2x7B",
        "Average":66.78,
        "ARC":66.38,
        "HellaSwag":83.71,
        "MMLU":61.53,
        "TruthfulQA":55.5,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"7556a4cd977e687916d943db245fcf6c03c57a18"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"silvercoder67\/Mistral-7b-instruct-v0.2-summ-sft-e2m",
        "Average":66.78,
        "ARC":59.47,
        "HellaSwag":83.34,
        "MMLU":60.53,
        "TruthfulQA":63.78,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fccef397114fa38158f704557d1f799f1c8a4e52"
    },
    {
        "T":"?",
        "Model":"Yuma42\/KangalKhan-Beta-Sapphire-7B",
        "Average":66.77,
        "ARC":65.78,
        "HellaSwag":85.76,
        "MMLU":64.28,
        "TruthfulQA":51.28,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e656fa8001d126ce775b10092f4d44f2c26bbd2c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051611\/A0118",
        "Average":66.77,
        "ARC":59.22,
        "HellaSwag":83.79,
        "MMLU":68.28,
        "TruthfulQA":55.79,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6bc6bac459c7a8b679281db8663a96e2a1f3ce2a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azazelle\/Sina-Thor-7b-Merge",
        "Average":66.77,
        "ARC":66.21,
        "HellaSwag":85.69,
        "MMLU":65.17,
        "TruthfulQA":50.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d2074b9f23665b98362a52ce22ba62d4870985d4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"willyninja30\/ARIA-70B-French",
        "Average":66.76,
        "ARC":64.51,
        "HellaSwag":85.87,
        "MMLU":63.88,
        "TruthfulQA":52.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"d8580d360c51e71fddd27897445e2aa9d1888585"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Faradaylab\/Aria-70B",
        "Average":66.76,
        "ARC":64.51,
        "HellaSwag":85.87,
        "MMLU":63.88,
        "TruthfulQA":52.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":70.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"57cd251f2cf4e832f64550ea0e2b90ecec155b54"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jebcarter\/psyonic-cetacean-20B",
        "Average":66.74,
        "ARC":63.57,
        "HellaSwag":86.2,
        "MMLU":59.66,
        "TruthfulQA":57.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":19.99,
        "Hub":25,
        "Available on the hub":true,
        "Model Sha":"298d2086a949d53af06096d229f64f4719261698"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nlpguy\/Hermes-low-tune-2",
        "Average":66.74,
        "ARC":65.61,
        "HellaSwag":84.47,
        "MMLU":63.69,
        "TruthfulQA":53.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"5e9fbbcf7c7959356574179f1091bc7bf4033a98"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andysalerno\/openchat-nectar-0.3",
        "Average":66.74,
        "ARC":65.96,
        "HellaSwag":83.15,
        "MMLU":65.46,
        "TruthfulQA":52.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cc29b95f9d0bee765206b07e4d9bba05a0fcafb2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andysalerno\/openchat-nectar-0.6",
        "Average":66.72,
        "ARC":66.55,
        "HellaSwag":83.22,
        "MMLU":65.19,
        "TruthfulQA":51.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"502b55ebd1ca3c159591a9d7e9d9a456ac067e8d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"teknium\/CollectiveCognition-v1.1-Mistral-7B",
        "Average":66.71,
        "ARC":62.54,
        "HellaSwag":84.13,
        "MMLU":62.57,
        "TruthfulQA":57.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":75,
        "Available on the hub":false,
        "Model Sha":"5f57f70ec99450c70da2540e94dd7fd67be4b23c"
    },
    {
        "T":"\u2b55",
        "Model":"kyujinpy\/PlatYi-34B-200K-Q",
        "Average":66.71,
        "ARC":63.91,
        "HellaSwag":83.52,
        "MMLU":75.19,
        "TruthfulQA":44.21,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"0f58c270f8f3b82523799dcfd7080b857850bd77"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"argilla\/notus-7b-v1",
        "Average":66.7,
        "ARC":64.59,
        "HellaSwag":84.83,
        "MMLU":63.04,
        "TruthfulQA":54.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":109,
        "Available on the hub":false,
        "Model Sha":"f23f4cf6cb76402c76e932ead01109191af72a60"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/MetaMath-una-cybertron-v2-bf16-Ties",
        "Average":66.7,
        "ARC":65.02,
        "HellaSwag":83.68,
        "MMLU":62.58,
        "TruthfulQA":55.52,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e613cc45140352e2d1759f0f551021e928de006e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andysalerno\/openchat-nectar-0.4",
        "Average":66.7,
        "ARC":66.64,
        "HellaSwag":83.23,
        "MMLU":65.22,
        "TruthfulQA":51.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"25eaf0bb01b56d1ce515dd1aa972be468e04c3ed"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"argilla\/notus-7b-v1",
        "Average":66.69,
        "ARC":64.59,
        "HellaSwag":84.78,
        "MMLU":63.03,
        "TruthfulQA":54.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":109,
        "Available on the hub":false,
        "Model Sha":"89f594b32aea9bf5de0abe3877f20ff302549934"
    },
    {
        "T":"?",
        "Model":"maldv\/winter-garden-7b-alpha",
        "Average":66.67,
        "ARC":65.19,
        "HellaSwag":85.36,
        "MMLU":65.2,
        "TruthfulQA":50.94,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0a289b3d6fbb286fb7c7897bdc84df0b4d950572"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lodrick-the-lafted\/Hermes-Instruct-7B-217K",
        "Average":66.67,
        "ARC":61.01,
        "HellaSwag":82.64,
        "MMLU":61.23,
        "TruthfulQA":61.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"25d52e51192738ddfc875e70dbaf1602ad4afd8f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"VAGOsolutions\/SauerkrautLM-Gemma-7b",
        "Average":66.66,
        "ARC":59.98,
        "HellaSwag":81.91,
        "MMLU":63.76,
        "TruthfulQA":61.0,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":8.54,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"87cf83507c53dc0a41f8ecd0c961235b42c20ade"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Eurdem\/megatron_1.1_MoE_2x7B",
        "Average":66.66,
        "ARC":65.53,
        "HellaSwag":84.52,
        "MMLU":65.02,
        "TruthfulQA":51.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"21d9b8365b6e9cc9ece2f27e75d7085c0359f119"
    },
    {
        "T":"?",
        "Model":"sonthenguyen\/OpenHermes-2.5-Mistral-7B-mt-bench-DPO",
        "Average":66.66,
        "ARC":65.27,
        "HellaSwag":84.62,
        "MMLU":63.83,
        "TruthfulQA":52.91,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3668a898cbb72a4915f6dce6cdbe6ba0c3582026"
    },
    {
        "T":"?",
        "Model":"sonthenguyen\/OpenHermes-2.5-Mistral-7B-mt-bench-DPO-recovered",
        "Average":66.66,
        "ARC":65.27,
        "HellaSwag":84.62,
        "MMLU":63.82,
        "TruthfulQA":52.91,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"f60f1f62c9e7e7440b24cdd7a1333dac739cc359"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat-3.5-1210",
        "Average":66.66,
        "ARC":64.93,
        "HellaSwag":84.92,
        "MMLU":64.62,
        "TruthfulQA":52.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":271,
        "Available on the hub":false,
        "Model Sha":"e5df841b685e5b5ca11ce142f29c6c731bf087a0"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"bn22\/OpenHermes-2.5-Mistral-7B-MISALIGNED",
        "Average":66.65,
        "ARC":65.36,
        "HellaSwag":84.67,
        "MMLU":63.74,
        "TruthfulQA":52.85,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d366f84cef3a084c6c3dc87b304f0937080c2a6d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/mistral-7b-slimorcaboros",
        "Average":66.65,
        "ARC":63.65,
        "HellaSwag":83.7,
        "MMLU":63.46,
        "TruthfulQA":55.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"c06e1a6b6c0fe764117f9ec7611ce31e796e602a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LeroyDyer\/Mixtral_AI_Cyber_3.0",
        "Average":66.65,
        "ARC":62.46,
        "HellaSwag":84.02,
        "MMLU":61.91,
        "TruthfulQA":58.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b869a301cb7dab92b9693366c42423b49a2f5fe0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yanolja\/Bookworm-10.7B-v0.4-DPO",
        "Average":66.65,
        "ARC":64.68,
        "HellaSwag":84.43,
        "MMLU":65.12,
        "TruthfulQA":52.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.8,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"5807ef01a569e3ecda619af66f98271d6bf872f7"
    },
    {
        "T":"?",
        "Model":"FredrikBL\/test-dare",
        "Average":66.65,
        "ARC":64.59,
        "HellaSwag":84.87,
        "MMLU":64.43,
        "TruthfulQA":52.69,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c0b8022c8003f911fb73f7697bea001c8e21f6b2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"giraffe176\/Open_Hermes_Orca_Mistral-7B",
        "Average":66.64,
        "ARC":64.68,
        "HellaSwag":84.63,
        "MMLU":63.93,
        "TruthfulQA":53.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"68c4c546542e361c1e1c0cd6b70b7586e55fd7b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Eurdem\/Voltran-1.0-MoE-2x7B",
        "Average":66.64,
        "ARC":64.08,
        "HellaSwag":83.74,
        "MMLU":61.26,
        "TruthfulQA":57.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"0bc5dd29fd96a869293757ab5a56b9a3522eb6dd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lilloukas\/GPlatty-30B",
        "Average":66.63,
        "ARC":65.78,
        "HellaSwag":84.79,
        "MMLU":63.49,
        "TruthfulQA":52.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"836cf4dcd60ebe2ff09415c72f809d94639e8d35"
    },
    {
        "T":"?",
        "Model":"Novocoders\/Mistral-NeuralDPO-v0.4-Laser",
        "Average":66.63,
        "ARC":66.89,
        "HellaSwag":85.23,
        "MMLU":63.47,
        "TruthfulQA":50.91,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"8726f72ca9decc32df5e74bfae38c8a531f65836"
    },
    {
        "T":"?",
        "Model":"sonthenguyen\/OpenHermes-2.5-Mistral-7B-mt-bench-DPO-corrupted",
        "Average":66.61,
        "ARC":65.27,
        "HellaSwag":84.58,
        "MMLU":63.74,
        "TruthfulQA":52.84,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7cb70ae8584ec2ef48372bc4db1210e0502f9654"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"tokyotech-llm\/Swallow-70b-instruct-hf",
        "Average":66.61,
        "ARC":66.21,
        "HellaSwag":85.14,
        "MMLU":67.08,
        "TruthfulQA":48.0,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":69.16,
        "Hub":35,
        "Available on the hub":true,
        "Model Sha":"feba815b847806df03f23a375f3d4d07fa251134"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nlpguy\/Hermes-low-tune-2",
        "Average":66.61,
        "ARC":65.27,
        "HellaSwag":84.41,
        "MMLU":63.63,
        "TruthfulQA":53.12,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"5e9fbbcf7c7959356574179f1091bc7bf4033a98"
    },
    {
        "T":"?",
        "Model":"yanolja\/Bookworm-10.7B-v0.4-DPO",
        "Average":66.61,
        "ARC":64.76,
        "HellaSwag":84.4,
        "MMLU":64.96,
        "TruthfulQA":52.31,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.8,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"5807ef01a569e3ecda619af66f98271d6bf872f7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cognitivecomputations\/openchat-3.5-0106-laser",
        "Average":66.6,
        "ARC":66.04,
        "HellaSwag":83.18,
        "MMLU":65.11,
        "TruthfulQA":52.08,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":18,
        "Available on the hub":false,
        "Model Sha":"56805ed2f93e991f77fa6dd83502daf96aff2c9d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cognitivecomputations\/dolphin-2.8-experiment26-7b-preview",
        "Average":66.6,
        "ARC":64.51,
        "HellaSwag":83.79,
        "MMLU":63.24,
        "TruthfulQA":54.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"9ed28849e136e4cfbb0a9f774d5736c76b893d81"
    },
    {
        "T":"?",
        "Model":"cognitivecomputations\/dolphin-2.8-experiment26-7b",
        "Average":66.6,
        "ARC":64.51,
        "HellaSwag":83.79,
        "MMLU":63.24,
        "TruthfulQA":54.87,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":26,
        "Available on the hub":false,
        "Model Sha":"b7d94074abb2a9af40c1a823e94a9ba150de5acc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KaeriJenti\/kaori-34b-v3",
        "Average":66.6,
        "ARC":64.25,
        "HellaSwag":79.59,
        "MMLU":70.18,
        "TruthfulQA":52.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"117dab7fc59bff50279100214e39f5551ba0c593"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/mistral-11b-slimorca",
        "Average":66.6,
        "ARC":64.25,
        "HellaSwag":83.81,
        "MMLU":63.66,
        "TruthfulQA":54.66,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"1feb0fe36c9db1a4ea6cca32acae9ff07a12b9c5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/VicUnlocked-alpaca-65B-QLoRA-fp16",
        "Average":66.59,
        "ARC":65.61,
        "HellaSwag":85.15,
        "MMLU":63.13,
        "TruthfulQA":52.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"6cdacfda96970aa144e316b108ab9bc17c99a573"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/higgs-llama-vicuna-ep25-70b",
        "Average":66.59,
        "ARC":62.29,
        "HellaSwag":86.07,
        "MMLU":64.25,
        "TruthfulQA":53.75,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"1da59e150f1d0bae67f66400738a01d408a8c45d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"HIT-SCIR\/Chinese-Mixtral-8x7B",
        "Average":66.59,
        "ARC":63.57,
        "HellaSwag":85.98,
        "MMLU":70.95,
        "TruthfulQA":45.86,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.91,
        "Hub":42,
        "Available on the hub":false,
        "Model Sha":"58d799575d809c0e80ee6964a546aaa3a8569963"
    },
    {
        "T":"?",
        "Model":"jondurbin\/airoboros-65b-gpt4-1.4",
        "Average":66.59,
        "ARC":65.78,
        "HellaSwag":85.83,
        "MMLU":62.27,
        "TruthfulQA":52.45,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"ae256799615c16443f9c423c653ed9f60577e99e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-1.4-peft",
        "Average":66.59,
        "ARC":65.78,
        "HellaSwag":85.83,
        "MMLU":62.27,
        "TruthfulQA":52.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":65.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"85ae3b595c6b8415df87000c22bc14ea18c174f5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PistachioAlt\/Noromaid-Bagel-7B-Slerp",
        "Average":66.57,
        "ARC":64.51,
        "HellaSwag":84.58,
        "MMLU":64.3,
        "TruthfulQA":52.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"07ec589199b9368c755c9d67f316336c5ef8b2c2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andysalerno\/openchat-nectar-0.8",
        "Average":66.57,
        "ARC":65.78,
        "HellaSwag":83.05,
        "MMLU":65.16,
        "TruthfulQA":52.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"639db94ac706d6964a3eed642b8de3a582bbffa8"
    },
    {
        "T":"\u2b55",
        "Model":"teknium\/CollectiveCognition-v1.1-Mistral-7B",
        "Average":66.56,
        "ARC":62.12,
        "HellaSwag":84.17,
        "MMLU":62.35,
        "TruthfulQA":57.62,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":75,
        "Available on the hub":false,
        "Model Sha":"5f57f70ec99450c70da2540e94dd7fd67be4b23c"
    },
    {
        "T":"?",
        "Model":"yanolja\/EEVE-Korean-Instruct-10.8B-v1.0",
        "Average":66.55,
        "ARC":64.85,
        "HellaSwag":83.04,
        "MMLU":64.23,
        "TruthfulQA":54.09,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.8,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"fb3f5e88e28b6f063f9f3a36c5ae475a31413517"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Faradaylab\/ARIA-70B-V3",
        "Average":66.55,
        "ARC":63.91,
        "HellaSwag":86.21,
        "MMLU":64.75,
        "TruthfulQA":51.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6e7fdcd20626786dd744ea86c664a3c088ced39f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Delcos\/Starling-LM-11B-alpha",
        "Average":66.54,
        "ARC":62.97,
        "HellaSwag":84.85,
        "MMLU":63.83,
        "TruthfulQA":54.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":11.39,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"16086688b70e4f54e1ba4f54a1a847c30b987a74"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gqd\/mistral-merge-7b",
        "Average":66.54,
        "ARC":63.91,
        "HellaSwag":84.48,
        "MMLU":64.04,
        "TruthfulQA":53.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"unlicense",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ffaddf395e00015873137562a8a34e1bb8123b41"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/SlimOpenOrca-Mistral-7B",
        "Average":66.54,
        "ARC":62.97,
        "HellaSwag":83.49,
        "MMLU":62.3,
        "TruthfulQA":57.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":18,
        "Available on the hub":false,
        "Model Sha":"b0134a7512444dfbb60a2e2d81469a5bbbb18026"
    },
    {
        "T":"?",
        "Model":"Novocoders\/Mistral-NeuralDPO-v0.4",
        "Average":66.53,
        "ARC":66.04,
        "HellaSwag":85.18,
        "MMLU":63.57,
        "TruthfulQA":51.32,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"76a02dd47c11f8c225d922eb12aa1b4a3c3a74a7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jeiku\/Elly_7B",
        "Average":66.53,
        "ARC":63.57,
        "HellaSwag":83.48,
        "MMLU":62.8,
        "TruthfulQA":56.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"eb149860bd442475828fe2ce71069ec7dbcea3b8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ValiantLabs\/Fireplace-34b",
        "Average":66.52,
        "ARC":71.25,
        "HellaSwag":82.72,
        "MMLU":47.01,
        "TruthfulQA":65.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"58c8df51a5963cd206301461edb68fa86ba059ed"
    },
    {
        "T":"?",
        "Model":"TeeZee\/DarkForest-20B-v1.2",
        "Average":66.52,
        "ARC":63.57,
        "HellaSwag":86.42,
        "MMLU":59.77,
        "TruthfulQA":56.31,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":19.99,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d38fadc604321e5d4cbaa93b247f939f2f5d5a1d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lodrick-the-lafted\/Grafted-Hermetic-Platypus-B-2x7B",
        "Average":66.51,
        "ARC":59.47,
        "HellaSwag":82.95,
        "MMLU":62.15,
        "TruthfulQA":61.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1972aa1c2ad8f1b808efa9bce98ec154cd361264"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-2.0",
        "Average":66.51,
        "ARC":66.81,
        "HellaSwag":86.66,
        "MMLU":63.41,
        "TruthfulQA":49.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ea4bdd0221f77de9b0343cd8291cbd0fd6033ca8"
    },
    {
        "T":"?",
        "Model":"TeeZee\/DarkForest-20B-v2.0",
        "Average":66.5,
        "ARC":63.74,
        "HellaSwag":86.32,
        "MMLU":59.79,
        "TruthfulQA":56.14,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":19.99,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"cf6373fce58fce760c958f1504259297fa0bda3b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JunchengXie\/Mistral-7B-Instruct-v0.2-gpt-4-80k-base_lora",
        "Average":66.49,
        "ARC":59.47,
        "HellaSwag":79.7,
        "MMLU":58.5,
        "TruthfulQA":68.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dcd5376d301a535e8cb58c78a69c39332848af7c"
    },
    {
        "T":"?",
        "Model":"Nitral-AI\/Eris_PrimeV4.20-Vision-32k-7B",
        "Average":66.49,
        "ARC":64.93,
        "HellaSwag":84.8,
        "MMLU":63.71,
        "TruthfulQA":52.53,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"e204d3ee2b54aa32cbf0c39d36552fe5cb256b31"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cognitivecomputations\/dolphin-2.6-mistral-7b",
        "Average":66.49,
        "ARC":63.05,
        "HellaSwag":84.05,
        "MMLU":63.2,
        "TruthfulQA":55.67,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":97,
        "Available on the hub":false,
        "Model Sha":"61981ccfb93bad331c8d3da97aafeb13596afc9d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"macadeliccc\/polyglot-math-4x7b",
        "Average":66.49,
        "ARC":63.74,
        "HellaSwag":84.85,
        "MMLU":63.57,
        "TruthfulQA":53.78,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"13b5f7d30c5db5060b41b2889f1c8df5ef7a8303"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andysalerno\/openchat-nectar-0.7",
        "Average":66.48,
        "ARC":65.78,
        "HellaSwag":83.0,
        "MMLU":65.1,
        "TruthfulQA":52.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"082de25a339e1e8e5a64c9fc84429f1a4a0847ac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"macadeliccc\/Monarch-7B-SFT",
        "Average":66.48,
        "ARC":63.82,
        "HellaSwag":83.63,
        "MMLU":64.2,
        "TruthfulQA":54.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a1b031916ab87c2d1b9712fbc1901ecaec144f3d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CallComply\/openchat-3.5-0106-32k",
        "Average":66.48,
        "ARC":66.04,
        "HellaSwag":82.93,
        "MMLU":65.04,
        "TruthfulQA":51.9,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"8d566086308e80e8aa01e70acfac10adcf457fe3"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Jaume\/openchat-3.5-0106-mod-gpt5",
        "Average":66.48,
        "ARC":66.04,
        "HellaSwag":82.93,
        "MMLU":65.04,
        "TruthfulQA":51.9,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e48411ee9b41210b2bf019e5b6e58a6cde3d04f3"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"openchat\/openchat-3.5-0106",
        "Average":66.48,
        "ARC":66.04,
        "HellaSwag":82.93,
        "MMLU":65.04,
        "TruthfulQA":51.9,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":296,
        "Available on the hub":false,
        "Model Sha":"9619fb7d2a8e25fa6b0633c0f57f7f4aa79b45c4"
    },
    {
        "T":"\u2b55",
        "Model":"TheBloke\/WizardLM-70B-V1.0-GPTQ",
        "Average":66.47,
        "ARC":63.82,
        "HellaSwag":83.85,
        "MMLU":63.68,
        "TruthfulQA":54.54,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":9.1,
        "Hub":23,
        "Available on the hub":true,
        "Model Sha":"c234d7c9c0fd26efb55757fdbfb604d549539fe0"
    },
    {
        "T":"?",
        "Model":"TheBloke\/WizardLM-70B-V1.0-GPTQ",
        "Average":66.47,
        "ARC":63.82,
        "HellaSwag":83.85,
        "MMLU":63.68,
        "TruthfulQA":54.54,
        "Type":"Unknown",
        "Precision":"None",
        "Hub License":"llama2",
        "#Params (B)":9.1,
        "Hub":23,
        "Available on the hub":true,
        "Model Sha":"c234d7c9c0fd26efb55757fdbfb604d549539fe0"
    },
    {
        "T":"?",
        "Model":"Weyaxi\/Einstein-v4-7B",
        "Average":66.47,
        "ARC":64.68,
        "HellaSwag":83.75,
        "MMLU":62.31,
        "TruthfulQA":55.15,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":44,
        "Available on the hub":false,
        "Model Sha":"8c831e8878fe7f2f83320c3acfc4de7135bf8fa7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vishnukv\/WestSeverusJaskier-OpenOrca",
        "Average":66.47,
        "ARC":62.88,
        "HellaSwag":84.75,
        "MMLU":64.33,
        "TruthfulQA":53.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"109379e69b45018360e565fde8cced5a948d4151"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Jaume\/openchat-3.5-0106-mod-gpt5",
        "Average":66.46,
        "ARC":65.87,
        "HellaSwag":82.93,
        "MMLU":65.12,
        "TruthfulQA":51.93,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e48411ee9b41210b2bf019e5b6e58a6cde3d04f3"
    },
    {
        "T":"?",
        "Model":"TeeZee\/DarkSapling-7B-v2.0",
        "Average":66.46,
        "ARC":64.16,
        "HellaSwag":85.1,
        "MMLU":64.37,
        "TruthfulQA":52.21,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"b62ede72cb044efe88017d7d5eb178d9d807ef1e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ariellee\/SuperPlatty-30B",
        "Average":66.45,
        "ARC":65.78,
        "HellaSwag":83.95,
        "MMLU":62.57,
        "TruthfulQA":53.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"017e1c32bca060107337dbf26db2044a7caa56f2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cognitivecomputations\/dolphin-2.6-mistral-7b",
        "Average":66.44,
        "ARC":62.88,
        "HellaSwag":84.06,
        "MMLU":63.19,
        "TruthfulQA":55.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":97,
        "Available on the hub":false,
        "Model Sha":"61981ccfb93bad331c8d3da97aafeb13596afc9d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishekchohan\/mistral-7B-forest-merge-v0.1",
        "Average":66.42,
        "ARC":62.8,
        "HellaSwag":84.32,
        "MMLU":60.05,
        "TruthfulQA":58.53,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"89d2636a0077334335dce498f9b0324d1f6bb9f8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"macadeliccc\/Monarch-7B-SFT",
        "Average":66.42,
        "ARC":63.74,
        "HellaSwag":83.58,
        "MMLU":64.11,
        "TruthfulQA":54.25,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a1b031916ab87c2d1b9712fbc1901ecaec144f3d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-1.4",
        "Average":66.42,
        "ARC":65.53,
        "HellaSwag":85.77,
        "MMLU":61.95,
        "TruthfulQA":52.43,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"ae256799615c16443f9c423c653ed9f60577e99e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/A13",
        "Average":66.41,
        "ARC":61.09,
        "HellaSwag":81.7,
        "MMLU":69.62,
        "TruthfulQA":53.25,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c9b20b6f34269c27e56759888c5d42bd045e6da7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeuralNovel\/Tiger-7b-v0.1",
        "Average":66.41,
        "ARC":59.98,
        "HellaSwag":83.21,
        "MMLU":61.42,
        "TruthfulQA":61.03,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"365162149ed8e18c1cbf2d9728707f4b03ae4a62"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beberik\/rawr",
        "Average":66.4,
        "ARC":63.99,
        "HellaSwag":84.86,
        "MMLU":64.7,
        "TruthfulQA":52.07,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a8853791580ca0841cb7805462df7c57089d6762"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Changgil\/K2S3-SOLAR-11b-v3.0",
        "Average":66.4,
        "ARC":62.29,
        "HellaSwag":84.93,
        "MMLU":65.48,
        "TruthfulQA":52.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b47d5115a5e4a1fbee8bf94ce732890deb710432"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-2.0",
        "Average":66.4,
        "ARC":66.64,
        "HellaSwag":86.66,
        "MMLU":63.18,
        "TruthfulQA":49.11,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ea4bdd0221f77de9b0343cd8291cbd0fd6033ca8"
    },
    {
        "T":"?",
        "Model":"cstr\/Spaetzle-v44-7b",
        "Average":66.39,
        "ARC":64.59,
        "HellaSwag":84.76,
        "MMLU":61.76,
        "TruthfulQA":54.45,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5f6becfdbc97b1caf280714e5755c00c5ad61cbf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishekchohan\/mistral-7B-forest-merge",
        "Average":66.38,
        "ARC":63.65,
        "HellaSwag":84.41,
        "MMLU":59.98,
        "TruthfulQA":57.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"dea087d73edf6019e72bfd5e98bc1fc93cb5136f"
    },
    {
        "T":"?",
        "Model":"shahzebnaveed\/StarlingHermes-2.5-Mistral-7B-slerp",
        "Average":66.38,
        "ARC":66.04,
        "HellaSwag":85.18,
        "MMLU":64.72,
        "TruthfulQA":49.56,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c3902e03960f3b38ba98b733bfc1192198efb869"
    },
    {
        "T":"?",
        "Model":"sonthenguyen\/OpenHermes-2.5-Mistral-7B-mt-bench-DPO-original-v2",
        "Average":66.37,
        "ARC":64.93,
        "HellaSwag":84.54,
        "MMLU":63.63,
        "TruthfulQA":52.4,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9bb10d7285e28a5b8696c704da0e55d11816f77c"
    },
    {
        "T":"?",
        "Model":"Azazelle\/Mocha-Sample-7b-ex",
        "Average":66.37,
        "ARC":64.76,
        "HellaSwag":84.35,
        "MMLU":62.2,
        "TruthfulQA":54.18,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0da3176b36871025e15c3dac3787cdc4f352e63f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishekchohan\/mistral-7B-forest-merge",
        "Average":66.35,
        "ARC":63.4,
        "HellaSwag":84.38,
        "MMLU":60.08,
        "TruthfulQA":57.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"dea087d73edf6019e72bfd5e98bc1fc93cb5136f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TokenBender\/pic_7B_mistral_Full_v0.1",
        "Average":66.35,
        "ARC":63.91,
        "HellaSwag":83.7,
        "MMLU":63.3,
        "TruthfulQA":54.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"64f7a800327466b76697c1f81d88b008274c8861"
    },
    {
        "T":"?",
        "Model":"Hertz\/Mistral-Hermes-2x7b",
        "Average":66.34,
        "ARC":65.19,
        "HellaSwag":85.27,
        "MMLU":63.71,
        "TruthfulQA":51.2,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f050aed1a47eb58712ad4e47b92c09e188371472"
    },
    {
        "T":"?",
        "Model":"openchat\/openchat-3.5-0106-gemma",
        "Average":66.34,
        "ARC":64.68,
        "HellaSwag":81.08,
        "MMLU":64.69,
        "TruthfulQA":54.93,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":8.54,
        "Hub":45,
        "Available on the hub":false,
        "Model Sha":"8f5401b27731fb289132eea40740fc834c9678c7"
    },
    {
        "T":"?",
        "Model":"invalid-coder\/dolphin-2.1-mistral-7b-snr-math-laser",
        "Average":66.34,
        "ARC":63.31,
        "HellaSwag":84.29,
        "MMLU":63.02,
        "TruthfulQA":54.75,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8bab5ff5eeff2bdd8b591059b7079edd5da4b351"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"teknium\/OpenHermes-2.5-Mistral-7B",
        "Average":66.34,
        "ARC":64.93,
        "HellaSwag":84.3,
        "MMLU":63.82,
        "TruthfulQA":52.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":740,
        "Available on the hub":false,
        "Model Sha":"2a54cad766bc90828354db5c4199795aecfd0df1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/zephyr-7b-beta",
        "Average":66.34,
        "ARC":62.46,
        "HellaSwag":84.35,
        "MMLU":60.7,
        "TruthfulQA":57.83,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":1396,
        "Available on the hub":false,
        "Model Sha":"0f17b36adfbe7d86ea1c591a9efeeae17b313f48"
    },
    {
        "T":"?",
        "Model":"Yuma42\/KangalKhan-Beta-Ruby-7B",
        "Average":66.33,
        "ARC":64.51,
        "HellaSwag":85.57,
        "MMLU":64.2,
        "TruthfulQA":51.04,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"67d757765597e4b8ee879e9d6a4c2e2a780d6bac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Chupacabra-7B",
        "Average":66.33,
        "ARC":66.81,
        "HellaSwag":83.52,
        "MMLU":62.68,
        "TruthfulQA":52.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"ae20703e16d89ba4a4301d12195cede64bd2ebdd"
    },
    {
        "T":"?",
        "Model":"maldv\/winter-garden-7b-beta",
        "Average":66.33,
        "ARC":64.93,
        "HellaSwag":85.02,
        "MMLU":64.54,
        "TruthfulQA":50.82,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"43825c3842ba34557993a8028c5591a614369027"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jikaixuan\/test",
        "Average":66.32,
        "ARC":62.29,
        "HellaSwag":84.42,
        "MMLU":61.07,
        "TruthfulQA":57.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":0.0,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"e63792701d6136288b95c9c8f24c0030ff5698b0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jikaixuan\/test_model",
        "Average":66.32,
        "ARC":62.29,
        "HellaSwag":84.42,
        "MMLU":61.07,
        "TruthfulQA":57.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2e9d6998ce40ffb43ba1d8636a84bf38bf922892"
    },
    {
        "T":"?",
        "Model":"Undi95\/C-Based-2x7B",
        "Average":66.32,
        "ARC":65.53,
        "HellaSwag":85.0,
        "MMLU":64.59,
        "TruthfulQA":50.16,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ae2914cb1fc547a441526e1eecd0ea139ec1adc5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lodrick-the-lafted\/Grafted-Hermetic-Platypus-A-2x7B",
        "Average":66.32,
        "ARC":59.3,
        "HellaSwag":82.89,
        "MMLU":62.0,
        "TruthfulQA":61.08,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6e102b60cde5dc38374bf4906a8cdeb0411321f0"
    },
    {
        "T":"?",
        "Model":"shahzebnaveed\/NeuralHermes-2.5-Mistral-7B",
        "Average":66.31,
        "ARC":64.85,
        "HellaSwag":84.29,
        "MMLU":63.81,
        "TruthfulQA":52.29,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c5026f2abc8876d3a61095f023c39b18c8c685d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pinkyponky\/Mistral-7b-instruct-v0.2-summ-sft-e3",
        "Average":66.31,
        "ARC":59.98,
        "HellaSwag":82.76,
        "MMLU":59.48,
        "TruthfulQA":63.0,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5b09e3dd2bf8bcf08b9b3dd0d69e4cc67d782fd3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gmonsoon\/OpenMia-Indo-Mistral-7b-v3-refined",
        "Average":66.31,
        "ARC":64.42,
        "HellaSwag":84.22,
        "MMLU":62.64,
        "TruthfulQA":53.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c2c84867adc3160d6c39acf3e8cb56413a9000ac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"indischepartij\/OpenMia-Indo-Mistral-7b-v3-refined",
        "Average":66.31,
        "ARC":64.42,
        "HellaSwag":84.22,
        "MMLU":62.64,
        "TruthfulQA":53.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ce2b8e2503f9e927acbe3314c69d4a04468df55b"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"openaccess-ai-collective\/openhermes-2_5-dpo-no-robots",
        "Average":66.3,
        "ARC":64.93,
        "HellaSwag":84.3,
        "MMLU":63.86,
        "TruthfulQA":52.12,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"bee345f7da9816e459846b6bc3dbea6c69850855"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"venkycs\/ZySec-7B-Adapter",
        "Average":66.3,
        "ARC":63.48,
        "HellaSwag":85.0,
        "MMLU":60.22,
        "TruthfulQA":56.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d8245dbd4ff60ff6ab9683eeec6b9c3f9aa9ba64"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Changgil\/K2S3-SOLAR-11b-v4.0",
        "Average":66.29,
        "ARC":63.65,
        "HellaSwag":84.75,
        "MMLU":65.13,
        "TruthfulQA":51.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f1a90b4594dfe14349be1db44ee887856f73a82c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aihub-app\/ZySec-7B-v1",
        "Average":66.28,
        "ARC":63.48,
        "HellaSwag":85.01,
        "MMLU":60.14,
        "TruthfulQA":56.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":24,
        "Available on the hub":false,
        "Model Sha":"458f098e529e7ec670a02cc7b75a1a74496984a8"
    },
    {
        "T":"\u2b55",
        "Model":"teknium\/CollectiveCognition-v1-Mistral-7B",
        "Average":66.28,
        "ARC":62.37,
        "HellaSwag":85.5,
        "MMLU":62.76,
        "TruthfulQA":54.48,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"58777f0563610fa770c4fa252c0350de71d4ab9d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lodrick-the-lafted\/Grafted-Hermetic-Platypus-C-2x7B",
        "Average":66.28,
        "ARC":59.3,
        "HellaSwag":82.75,
        "MMLU":62.24,
        "TruthfulQA":60.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"777b95105f6e8e5a493cb3b38a21a6534a24d784"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cgato\/TheSpice-7b-FT-ExperimentalOrca",
        "Average":66.27,
        "ARC":62.63,
        "HellaSwag":84.26,
        "MMLU":63.33,
        "TruthfulQA":54.87,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"89feebddbb3b836f898d5f40287f3d4e8cb27b39"
    },
    {
        "T":"?",
        "Model":"TeeZee\/DarkSapling-7B-v1.1",
        "Average":66.27,
        "ARC":63.48,
        "HellaSwag":85.09,
        "MMLU":64.47,
        "TruthfulQA":52.04,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0064fc89ee6ce11c01d9061845e1d6498a91ab1a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/zephyr-7b-beta",
        "Average":66.27,
        "ARC":62.03,
        "HellaSwag":84.53,
        "MMLU":61.06,
        "TruthfulQA":57.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":1396,
        "Available on the hub":false,
        "Model Sha":"8af01af3d4f9dc9b962447180d6d0f8c5315da86"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"internlm\/internlm2-20b",
        "Average":66.26,
        "ARC":62.97,
        "HellaSwag":83.21,
        "MMLU":67.58,
        "TruthfulQA":51.27,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":20.0,
        "Hub":38,
        "Available on the hub":false,
        "Model Sha":"ec0e34824038c66745ba035f5c1994bd8cb99574"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xenon1\/Eclipse-7B",
        "Average":66.25,
        "ARC":62.54,
        "HellaSwag":84.19,
        "MMLU":64.92,
        "TruthfulQA":53.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"596b923442ef607dcec88d1c1af8f6cefbb82b5c"
    },
    {
        "T":"?",
        "Model":"paulilioaica\/Collin-7B-dare",
        "Average":66.25,
        "ARC":65.87,
        "HellaSwag":82.08,
        "MMLU":51.86,
        "TruthfulQA":65.2,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.0,
        "Hub":67,
        "Available on the hub":false,
        "Model Sha":"c8cc55a64ad062fe5ea9b6268c4affadc0975219"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PulsarAI\/SlimOpenOrca-Mistral-7B-v2",
        "Average":66.25,
        "ARC":62.88,
        "HellaSwag":83.41,
        "MMLU":62.05,
        "TruthfulQA":56.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7cd030ccdb169c2685fe028bb4380b91ad74920f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"teknium\/OpenHermes-2.5-Mistral-7B",
        "Average":66.25,
        "ARC":64.93,
        "HellaSwag":84.18,
        "MMLU":63.64,
        "TruthfulQA":52.24,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":740,
        "Available on the hub":false,
        "Model Sha":"2a54cad766bc90828354db5c4199795aecfd0df1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xenon1\/Zenith-7B-dpo-v1",
        "Average":66.24,
        "ARC":60.75,
        "HellaSwag":82.97,
        "MMLU":60.55,
        "TruthfulQA":60.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"907891fc0660b02f0e37749291696d1a26a88b58"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-mistral-dolphin-orca-platypus-samantha-7b",
        "Average":66.24,
        "ARC":64.33,
        "HellaSwag":84.4,
        "MMLU":63.72,
        "TruthfulQA":52.52,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"d4039b40e842df7f6b8de50532444c8944ea5791"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jefferylovely\/AthenaImaniMaven",
        "Average":66.24,
        "ARC":62.8,
        "HellaSwag":84.56,
        "MMLU":59.1,
        "TruthfulQA":58.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f10a7d6055955eb40424dcac8a76658a11224c86"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lodrick-the-lafted\/Hermes-Instruct-7B-v0.2",
        "Average":66.24,
        "ARC":60.92,
        "HellaSwag":82.96,
        "MMLU":60.05,
        "TruthfulQA":61.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6675073736e1f611aaf48ef9777076183d233c96"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Dolphin2.1-OpenOrca-7B",
        "Average":66.23,
        "ARC":64.16,
        "HellaSwag":84.25,
        "MMLU":62.7,
        "TruthfulQA":53.83,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"076c0f7de93307e8fb3ad3bd820fb5f73325ca70"
    },
    {
        "T":"?",
        "Model":"cris177\/Orca-Hermes-7B-slerp",
        "Average":66.23,
        "ARC":64.08,
        "HellaSwag":84.44,
        "MMLU":63.56,
        "TruthfulQA":52.84,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"1c80f2f1ab153a6926005697fa23617c6de45a2f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jefferylovely\/AthenaImaniMaven",
        "Average":66.23,
        "ARC":62.63,
        "HellaSwag":84.65,
        "MMLU":59.05,
        "TruthfulQA":58.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f10a7d6055955eb40424dcac8a76658a11224c86"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/zephyr-7b-beta",
        "Average":66.23,
        "ARC":62.03,
        "HellaSwag":84.36,
        "MMLU":61.07,
        "TruthfulQA":57.45,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":1396,
        "Available on the hub":false,
        "Model Sha":"8af01af3d4f9dc9b962447180d6d0f8c5315da86"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xenon1\/Zenith-7B-dpo",
        "Average":66.23,
        "ARC":60.92,
        "HellaSwag":82.94,
        "MMLU":60.54,
        "TruthfulQA":60.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a208869ce8f2643a1779cd89b1f8615b11206a8a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ArianAskari\/NeuralHermes-2.5-Mistral-7B",
        "Average":66.22,
        "ARC":64.68,
        "HellaSwag":84.28,
        "MMLU":63.71,
        "TruthfulQA":52.23,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"10017fe004ada8720559ca3ee2339972c4f15eca"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Bucharest-0.1",
        "Average":66.21,
        "ARC":65.36,
        "HellaSwag":85.45,
        "MMLU":66.1,
        "TruthfulQA":47.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"987e4e74a98f1ff961f1ef388631ee53f1985b9f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pinkyponky\/Mistral-7b-instruct-v0.2-summ-sft-e1",
        "Average":66.2,
        "ARC":60.15,
        "HellaSwag":82.59,
        "MMLU":58.92,
        "TruthfulQA":63.13,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cb20f22f421052e1ca8ea8bd9974fade5ccdfa9d"
    },
    {
        "T":"?",
        "Model":"TheBloke\/Mixtral-8x7B-v0.1-GPTQ",
        "Average":66.19,
        "ARC":65.19,
        "HellaSwag":84.72,
        "MMLU":69.43,
        "TruthfulQA":45.43,
        "Type":"Unknown",
        "Precision":"None",
        "Hub License":"apache-2.0",
        "#Params (B)":6.09,
        "Hub":125,
        "Available on the hub":false,
        "Model Sha":"7d1eb57b65f823458e27509cd0aac7172f54a260"
    },
    {
        "T":"?",
        "Model":"cognitivecomputations\/dolphin-2.8-experiment26-7b",
        "Average":66.19,
        "ARC":63.65,
        "HellaSwag":83.7,
        "MMLU":62.31,
        "TruthfulQA":55.1,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":26,
        "Available on the hub":false,
        "Model Sha":"0c90dbad22d980ece39ae8256086b9f9142c63cb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jilp00\/SOLAR-10.7B-tutored",
        "Average":66.19,
        "ARC":62.29,
        "HellaSwag":82.24,
        "MMLU":65.09,
        "TruthfulQA":55.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"40e46542b4ec136c76f61008a942000ff030cddc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeverSleep\/Mistral-11B-SynthIAirOmniMix",
        "Average":66.19,
        "ARC":62.46,
        "HellaSwag":83.13,
        "MMLU":63.47,
        "TruthfulQA":55.69,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"19694dc88e74a018d54bac6070cf521dff6d4397"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lodrick-the-lafted\/Grafted-Hermetic-Platypus-D-2x7B",
        "Average":66.18,
        "ARC":58.87,
        "HellaSwag":82.89,
        "MMLU":61.96,
        "TruthfulQA":61.02,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d64cb44e12b446b1e532ecd6a8f6f8c60e1ee095"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/MadMix-v0.1",
        "Average":66.18,
        "ARC":64.93,
        "HellaSwag":84.37,
        "MMLU":64.37,
        "TruthfulQA":51.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"71773ca4ca1fd76a00bd695a52b96b43b8fd78ff"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Chupacabra-v3",
        "Average":66.18,
        "ARC":66.21,
        "HellaSwag":81.29,
        "MMLU":59.36,
        "TruthfulQA":57.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"1dfa5e16d4be646b496d657d86554482ad48b3c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lodrick-the-lafted\/Kaiju-A-57B",
        "Average":66.17,
        "ARC":58.79,
        "HellaSwag":80.95,
        "MMLU":72.66,
        "TruthfulQA":52.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":57.26,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"11fc415ccc69d9f5a72be7f90be0b48b9c782f67"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/llama-2-70b-IA3-guanaco",
        "Average":66.17,
        "ARC":68.52,
        "HellaSwag":85.67,
        "MMLU":67.03,
        "TruthfulQA":43.47,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e3230df22d065b6699096494d1151fa337dde9e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"soniox\/Soniox-7B-v1.0",
        "Average":66.17,
        "ARC":63.91,
        "HellaSwag":82.55,
        "MMLU":64.38,
        "TruthfulQA":53.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"7f93ea62910145552d9332eec2fe824612cddf31"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lodrick-the-lafted\/Grafted-Hermetic-Platypus-C-2x7B",
        "Average":66.17,
        "ARC":58.96,
        "HellaSwag":82.77,
        "MMLU":62.08,
        "TruthfulQA":60.87,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"778903b24f320ce4e46d9e43ff296a64a6b835b6"
    },
    {
        "T":"?",
        "Model":"uproai\/Rose-2x7B",
        "Average":66.17,
        "ARC":65.27,
        "HellaSwag":85.7,
        "MMLU":64.37,
        "TruthfulQA":49.32,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f097d445ebb0edaeb1f2694806aa6da9b173a8a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Dolphin2.1-OpenOrca-7B",
        "Average":66.17,
        "ARC":63.91,
        "HellaSwag":84.26,
        "MMLU":62.66,
        "TruthfulQA":53.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"076c0f7de93307e8fb3ad3bd820fb5f73325ca70"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/OpenOrca-Zephyr-7B",
        "Average":66.17,
        "ARC":64.08,
        "HellaSwag":83.82,
        "MMLU":62.46,
        "TruthfulQA":54.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"2a2c7d287a46243cccf3ff6628375d0d190394ac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishek\/autotrain-xva0j-mixtral8x7b",
        "Average":66.16,
        "ARC":62.8,
        "HellaSwag":84.44,
        "MMLU":67.27,
        "TruthfulQA":50.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":46.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dc470b098c475a1604d55e197613e5d0bd85859f"
    },
    {
        "T":"?",
        "Model":"Xenon1\/Xenon-4",
        "Average":66.16,
        "ARC":60.15,
        "HellaSwag":83.07,
        "MMLU":60.08,
        "TruthfulQA":61.31,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"371a4b6038a84c9a887a156a78e165d70f67b2d2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Frostwind-10.7B-v1",
        "Average":66.15,
        "ARC":64.16,
        "HellaSwag":85.38,
        "MMLU":64.64,
        "TruthfulQA":50.43,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.6,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"5b465f636e1d354718e393e85914865a64840903"
    },
    {
        "T":"?",
        "Model":"LeroyDyer\/Mixtral_AI_128k",
        "Average":66.13,
        "ARC":63.23,
        "HellaSwag":84.62,
        "MMLU":64.24,
        "TruthfulQA":52.43,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":40,
        "Available on the hub":false,
        "Model Sha":"75fc3545fc6671a519a81cbbc5efb7f1b95c8d14"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"simonveitner\/MathHermes-2.5-Mistral-7B",
        "Average":66.12,
        "ARC":64.76,
        "HellaSwag":84.19,
        "MMLU":63.59,
        "TruthfulQA":51.95,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2a6ee2674304f91d1dcc772695deded76d4c32bd"
    },
    {
        "T":"?",
        "Model":"vicgalle\/SystemHermes-7B",
        "Average":66.12,
        "ARC":64.76,
        "HellaSwag":83.68,
        "MMLU":63.23,
        "TruthfulQA":52.81,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"f6882245b6a84d44b0ffe1fe2026ef97863e129c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xenon1\/Zenith-7B-dpo-v1",
        "Average":66.11,
        "ARC":60.49,
        "HellaSwag":82.95,
        "MMLU":60.39,
        "TruthfulQA":60.6,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"907891fc0660b02f0e37749291696d1a26a88b58"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Badgids\/Gonzo-Code-7B",
        "Average":66.1,
        "ARC":61.26,
        "HellaSwag":83.67,
        "MMLU":62.77,
        "TruthfulQA":56.7,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8318630caf9e174e1ac39c3f1b71bd3cbffd423c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BlouseJury\/Mistral-7B-Discord-0.1-DPO",
        "Average":66.1,
        "ARC":63.23,
        "HellaSwag":83.27,
        "MMLU":62.62,
        "TruthfulQA":55.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"3fde20529c9b2e25c9cb7a7a28795410e0b4ac21"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/dpopenhermes-alpha-v0",
        "Average":66.1,
        "ARC":65.02,
        "HellaSwag":83.96,
        "MMLU":63.67,
        "TruthfulQA":51.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"81ce4a9354d3b73276a0fa96b95d384f66d2de3d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pinkyponky\/Mistral-7b-instruct-v0.2-summ-sft-e2",
        "Average":66.09,
        "ARC":59.47,
        "HellaSwag":82.72,
        "MMLU":59.48,
        "TruthfulQA":62.7,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"01a73ccd10a275738304c695d0728a29e8586f47"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/zephyr-7b-alpha",
        "Average":66.08,
        "ARC":61.01,
        "HellaSwag":84.04,
        "MMLU":61.39,
        "TruthfulQA":57.9,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":1062,
        "Available on the hub":false,
        "Model Sha":"2cd2cd16a6ab22585d643cf264fac73b18e7852a"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TigerResearch\/tigerbot-70b-base",
        "Average":66.08,
        "ARC":62.46,
        "HellaSwag":83.61,
        "MMLU":65.49,
        "TruthfulQA":52.76,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":68.95,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"8af85526293eb8625375f3f7a1bab69825176e48"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CalderaAI\/30B-Lazarus",
        "Average":66.08,
        "ARC":64.93,
        "HellaSwag":84.27,
        "MMLU":56.47,
        "TruthfulQA":58.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":112,
        "Available on the hub":true,
        "Model Sha":"24da9e88f2b2b7946bc6fe9412d6728b9adc2c3d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-10.7B-v1.5b",
        "Average":66.08,
        "ARC":65.36,
        "HellaSwag":85.33,
        "MMLU":66.24,
        "TruthfulQA":47.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.6,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"c6659f344448dc66044df9b5b3e223419b0bcfbd"
    },
    {
        "T":"?",
        "Model":"Aryanne\/YarnLake-Swap-7B",
        "Average":66.08,
        "ARC":65.27,
        "HellaSwag":85.17,
        "MMLU":64.78,
        "TruthfulQA":49.07,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e37156f35e8c4849cd9b3ae83a4a6937279d5105"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_18-7B-dare_ties",
        "Average":66.07,
        "ARC":64.08,
        "HellaSwag":84.37,
        "MMLU":63.65,
        "TruthfulQA":52.17,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"157b267cb015d177d88b16e2f25f0307772b99d4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/bagel-7b-v0.1",
        "Average":66.07,
        "ARC":63.91,
        "HellaSwag":83.14,
        "MMLU":64.56,
        "TruthfulQA":52.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"10ac045905d13da0e2be8e647cfe3e5ac8444894"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Frostwind-10.7B-v1",
        "Average":66.06,
        "ARC":63.99,
        "HellaSwag":85.36,
        "MMLU":64.49,
        "TruthfulQA":50.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.6,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"5b465f636e1d354718e393e85914865a64840903"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"genaicore3434\/Mistral-7b-instruct-v0.2-summ-sft-lp-e1",
        "Average":66.05,
        "ARC":59.56,
        "HellaSwag":82.27,
        "MMLU":59.12,
        "TruthfulQA":63.26,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3a21fabd41c5c558e42f5ee592294ac56369d3d4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jilp00\/Hermes-2-SOLAR-10.7B-Symbolic",
        "Average":66.04,
        "ARC":61.69,
        "HellaSwag":82.57,
        "MMLU":65.06,
        "TruthfulQA":54.85,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"a5e2987baf03cab726e1135877ce3ae319ccd843"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/openchat-3.5-1210-starling-slerp",
        "Average":66.04,
        "ARC":63.91,
        "HellaSwag":85.27,
        "MMLU":65.05,
        "TruthfulQA":49.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"f6b1d9d6f613c6311b95d44b335a679e01e61140"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/trurl-2-13b-pl-instruct_unload",
        "Average":66.03,
        "ARC":59.9,
        "HellaSwag":79.99,
        "MMLU":78.66,
        "TruthfulQA":45.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"17f57642165e30a4025d6817bd47dcd80d0c5c4d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fzzhang\/Marcoroni-neural-chat-7B-v2_gsm8k_quantized_mergedfloat_s",
        "Average":66.03,
        "ARC":64.08,
        "HellaSwag":84.12,
        "MMLU":61.14,
        "TruthfulQA":54.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d1a30161bd58ed7506ad0ad22fea7f186e065776"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cgato\/Thespis-7b-v0.2-SFTTest-3Epoch",
        "Average":66.02,
        "ARC":63.23,
        "HellaSwag":84.39,
        "MMLU":62.59,
        "TruthfulQA":53.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e9c6150deb741e6d27cbd947bf6b6c9c472f0750"
    },
    {
        "T":"?",
        "Model":"Joseph717171\/BigOrca-2-XB",
        "Average":66.02,
        "ARC":61.6,
        "HellaSwag":83.64,
        "MMLU":60.85,
        "TruthfulQA":58.0,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":22.53,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"9345eada460ca54e22d1bee64f2680fde814c7a9"
    },
    {
        "T":"?",
        "Model":"Xenon1\/Xenon-3",
        "Average":66.01,
        "ARC":58.87,
        "HellaSwag":83.39,
        "MMLU":59.79,
        "TruthfulQA":61.99,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"36a1fdbcf8ec629dbe143221712d2f01e4b9b3cf"
    },
    {
        "T":"?",
        "Model":"ConvexAI\/Julianne-2x7B-bf16",
        "Average":66.01,
        "ARC":63.74,
        "HellaSwag":82.81,
        "MMLU":61.57,
        "TruthfulQA":55.91,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"f2a976852944787c5e2d4014d8d1220ef417e8e7"
    },
    {
        "T":"?",
        "Model":"jingyeom\/KoSoLAR-10.7B-v0.2_1.3_dedup_p",
        "Average":66.0,
        "ARC":63.05,
        "HellaSwag":83.63,
        "MMLU":64.61,
        "TruthfulQA":52.69,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.8,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"df5c63764f04e2d5863724ce9723d6cad2451e42"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Intel\/neural-chat-7b-v3-1",
        "Average":65.98,
        "ARC":64.25,
        "HellaSwag":82.49,
        "MMLU":60.79,
        "TruthfulQA":56.4,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":531,
        "Available on the hub":false,
        "Model Sha":"3995e9a13d54ce95f0ad55de2eaa92e2dc580174"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kwchoi\/DPO_mistral_7b_alpaca_0124_v1",
        "Average":65.97,
        "ARC":63.4,
        "HellaSwag":73.2,
        "MMLU":60.51,
        "TruthfulQA":66.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"76a91af140da0dcc1733a0bc575e51400ae50fcc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Pallas-0.5-frankenmerge",
        "Average":65.96,
        "ARC":61.77,
        "HellaSwag":80.36,
        "MMLU":67.62,
        "TruthfulQA":54.07,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":36.06,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b72731a305b62fd9fbcd7c1e99e18d6530600ca9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-v3.0-11B",
        "Average":65.95,
        "ARC":64.08,
        "HellaSwag":85.32,
        "MMLU":66.18,
        "TruthfulQA":48.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.6,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"236b393ae07c1d80004eeda47ee017a71a899853"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/mistral_28B_instruct_v0.1",
        "Average":65.95,
        "ARC":58.36,
        "HellaSwag":80.53,
        "MMLU":60.73,
        "TruthfulQA":64.17,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":28.18,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d87f393ea232749bf48131107131778c79ab3a74"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/dolphin-2.2.1-mistral-7b",
        "Average":65.95,
        "ARC":63.48,
        "HellaSwag":83.86,
        "MMLU":63.28,
        "TruthfulQA":53.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":177,
        "Available on the hub":false,
        "Model Sha":"001b48e9aebffb395c698af47b6b48364cc3cbe8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kaitchup\/Maixtchup-4x7b",
        "Average":65.95,
        "ARC":62.54,
        "HellaSwag":83.83,
        "MMLU":61.28,
        "TruthfulQA":56.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"56e8ed399a3198c7f02c30ac48361e690aad8d8f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-mixtral-8x7b-v15.4",
        "Average":65.94,
        "ARC":66.47,
        "HellaSwag":71.81,
        "MMLU":70.01,
        "TruthfulQA":55.46,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.74,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d7ab397a06644e7b2a2ebd14c25e332dc0d29997"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nextai-team\/Moe-4x7b-reason-code-qa",
        "Average":65.93,
        "ARC":62.54,
        "HellaSwag":83.87,
        "MMLU":61.2,
        "TruthfulQA":56.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"6a88e94af88e8ab9386cd9f3d3694a68b2428952"
    },
    {
        "T":"?",
        "Model":"Plaban81\/Moe-4x7b-math-reason-code",
        "Average":65.93,
        "ARC":62.54,
        "HellaSwag":83.87,
        "MMLU":61.2,
        "TruthfulQA":56.12,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":24.15,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"630daafebf8b8fd6f3959b2e924b49598e8ee2d3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Walmart-the-bag\/Misted-7B",
        "Average":65.93,
        "ARC":63.65,
        "HellaSwag":84.14,
        "MMLU":63.94,
        "TruthfulQA":52.0,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"31245dbdcd0ace447a4434ac5e393a90ac862a87"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jeiku\/Soulful_Bepis_7B",
        "Average":65.92,
        "ARC":63.82,
        "HellaSwag":80.69,
        "MMLU":62.53,
        "TruthfulQA":56.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":8.99,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"182bf0a4e15570da44678d589b2b703cd21b024d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/WizardLM-33B-V1.0-Uncensored",
        "Average":65.92,
        "ARC":63.82,
        "HellaSwag":83.86,
        "MMLU":59.18,
        "TruthfulQA":56.83,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":39,
        "Available on the hub":true,
        "Model Sha":"3eca9fdee0ce28d6a4a635a6f19d9a413caee3e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Stellaris-internlm2-20b-r512",
        "Average":65.92,
        "ARC":63.82,
        "HellaSwag":84.0,
        "MMLU":66.34,
        "TruthfulQA":49.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":19.29,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"237a0fc03af85eb4624ef5f367b6125ea0aaa83f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"amu\/zen_moe",
        "Average":65.91,
        "ARC":63.82,
        "HellaSwag":85.05,
        "MMLU":64.75,
        "TruthfulQA":50.03,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5e6e23c4da1c3b6049a42d755cdf74848efd454a"
    },
    {
        "T":"?",
        "Model":"ehartford\/WizardLM-33B-V1.0-Uncensored",
        "Average":65.91,
        "ARC":63.65,
        "HellaSwag":83.84,
        "MMLU":59.36,
        "TruthfulQA":56.8,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":39,
        "Available on the hub":true,
        "Model Sha":"3eca9fdee0ce28d6a4a635a6f19d9a413caee3e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/mistral_18B_instruct_v0.1",
        "Average":65.91,
        "ARC":56.91,
        "HellaSwag":81.36,
        "MMLU":60.52,
        "TruthfulQA":64.85,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":17.71,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1ebb0c9b000e460b78736afd2e40a5c875d241b0"
    },
    {
        "T":"?",
        "Model":"sonthenguyen\/OpenHermes-2.5-Mistral-7B-mt-bench-DPO-reversed_corrupted",
        "Average":65.91,
        "ARC":64.42,
        "HellaSwag":83.95,
        "MMLU":63.61,
        "TruthfulQA":51.65,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0b6fe472592fbb8f3086d85938bac591f8153e58"
    },
    {
        "T":"?",
        "Model":"aloobun\/slerp_bun_mistral_7b_v2",
        "Average":65.9,
        "ARC":65.61,
        "HellaSwag":85.28,
        "MMLU":64.61,
        "TruthfulQA":48.1,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"18a2ee1b7535ccb435b15b78eea285ce8042d21b"
    },
    {
        "T":"?",
        "Model":"runkai\/PascalHermes-2.5-Mistral-7B",
        "Average":65.88,
        "ARC":63.82,
        "HellaSwag":83.75,
        "MMLU":62.22,
        "TruthfulQA":53.72,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"17a4070a6e2515ef3e2dfb690d171e0a047aa3b8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-llama-65b-v8-bf16",
        "Average":65.87,
        "ARC":62.8,
        "HellaSwag":83.6,
        "MMLU":62.01,
        "TruthfulQA":55.09,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":65.07,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"445b77821fac8e6cfb77d0399fb827400b5bb71e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/mistral_28B_instruct_v0.2",
        "Average":65.87,
        "ARC":58.19,
        "HellaSwag":80.52,
        "MMLU":60.53,
        "TruthfulQA":64.25,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":28.18,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"dc872860320a3fee6c64c88c42c92341a38d25e1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NovoCode\/Mistral-NeuralDPO-v0.2",
        "Average":65.87,
        "ARC":67.06,
        "HellaSwag":85.01,
        "MMLU":62.68,
        "TruthfulQA":48.73,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"8c6e6989261d5223ca613a22660f4ba7df70995a"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/samantha-1.2-mistral-7b",
        "Average":65.87,
        "ARC":64.08,
        "HellaSwag":85.08,
        "MMLU":63.91,
        "TruthfulQA":50.4,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":35,
        "Available on the hub":false,
        "Model Sha":"5574a021f55a446a756dcbc776f1765aefc280a1"
    },
    {
        "T":"?",
        "Model":"Azure99\/blossom-v5-9b",
        "Average":65.87,
        "ARC":62.46,
        "HellaSwag":78.41,
        "MMLU":69.81,
        "TruthfulQA":52.78,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.83,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"aa2232946290c89581c39172fe048fd70c5b6e92"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Chupacabra-7B-v2.03-128k",
        "Average":65.85,
        "ARC":64.68,
        "HellaSwag":84.56,
        "MMLU":63.02,
        "TruthfulQA":51.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"22bb3c15b2770dfe91e239573b6c35b475a43cbe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/mistral_15B_instruct_v0.1",
        "Average":65.85,
        "ARC":58.45,
        "HellaSwag":81.71,
        "MMLU":59.82,
        "TruthfulQA":63.43,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":14.22,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5871f452765a0fd097fbb186c3a6328832ddcfa0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/Mistral-7B-SlimOrca",
        "Average":65.85,
        "ARC":62.54,
        "HellaSwag":83.86,
        "MMLU":62.77,
        "TruthfulQA":54.23,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":31,
        "Available on the hub":false,
        "Model Sha":"a9744d8cf9ce4230678a891bcf8bba7cbc0aaece"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Mixtral-4x7B-DPO-RPChat",
        "Average":65.85,
        "ARC":64.59,
        "HellaSwag":85.36,
        "MMLU":63.57,
        "TruthfulQA":49.87,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":24.15,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"406aeb5ce848dfefbca65d69022ce1de36f9fde4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/dolphin-2.2.1-mistral-7b",
        "Average":65.84,
        "ARC":63.31,
        "HellaSwag":83.76,
        "MMLU":63.17,
        "TruthfulQA":53.11,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":177,
        "Available on the hub":false,
        "Model Sha":"001b48e9aebffb395c698af47b6b48364cc3cbe8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/Mistral-7B-OpenOrca",
        "Average":65.84,
        "ARC":64.08,
        "HellaSwag":83.99,
        "MMLU":62.24,
        "TruthfulQA":53.05,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":632,
        "Available on the hub":false,
        "Model Sha":"7233ac83317946d05c474b71cc1379f49eb74c14"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-mistral-7b-dare-0.85",
        "Average":65.84,
        "ARC":63.57,
        "HellaSwag":84.82,
        "MMLU":64.29,
        "TruthfulQA":50.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b19e60f64b3be7f41658958658658bc12038c68f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yash21\/OpenMistral-MoE",
        "Average":65.83,
        "ARC":64.08,
        "HellaSwag":83.99,
        "MMLU":60.69,
        "TruthfulQA":54.57,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":24.15,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4c212c0361b002474b192010cdd49338e2db7d13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cognitivecomputations\/dolphin-2.2.1-mistral-7b",
        "Average":65.83,
        "ARC":63.23,
        "HellaSwag":83.8,
        "MMLU":63.16,
        "TruthfulQA":53.14,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":177,
        "Available on the hub":false,
        "Model Sha":"2022924c0bb13588308d429e0b7f51568c07629c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lodrick-the-lafted\/Platyboros-Instruct-7B",
        "Average":65.83,
        "ARC":57.76,
        "HellaSwag":82.59,
        "MMLU":62.05,
        "TruthfulQA":60.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"166c6ba6e9fb6fcb011d98c5cdbe68d17953d3d0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/llama-30b-instruct-2048-PL-lora",
        "Average":65.83,
        "ARC":63.65,
        "HellaSwag":84.66,
        "MMLU":61.69,
        "TruthfulQA":53.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1a076bce564f03bd47951eecab628c541fb1a6ad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Walmart-the-bag\/openchat-3.5-Infinity",
        "Average":65.83,
        "ARC":62.63,
        "HellaSwag":84.05,
        "MMLU":64.65,
        "TruthfulQA":51.99,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d117307b5b813186aa4707ff602f0fb056752d66"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andysalerno\/openchat-nectar-0.14",
        "Average":65.83,
        "ARC":65.61,
        "HellaSwag":83.02,
        "MMLU":64.58,
        "TruthfulQA":50.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6a3412e4ece04c794bef9d90e38a6dcb6ad07f70"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/Synatra-7B-v0.3-dpo",
        "Average":65.83,
        "ARC":62.8,
        "HellaSwag":82.58,
        "MMLU":61.46,
        "TruthfulQA":56.46,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.0,
        "Hub":19,
        "Available on the hub":false,
        "Model Sha":"405a4f1e6513cd1b8de5eb4e003bb49cc86d1f8a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xDAN-AI\/xDAN-L1-Thinking",
        "Average":65.83,
        "ARC":63.74,
        "HellaSwag":84.53,
        "MMLU":62.9,
        "TruthfulQA":52.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":0.0,
        "Hub":59,
        "Available on the hub":false,
        "Model Sha":"6f3383932b5003e05beda95e31c0a4c7c92ba700"
    },
    {
        "T":"?",
        "Model":"ibm\/merlinite-7b",
        "Average":65.81,
        "ARC":63.65,
        "HellaSwag":84.52,
        "MMLU":64.91,
        "TruthfulQA":50.15,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":88,
        "Available on the hub":false,
        "Model Sha":"ba52e4164e649c48b7b5d724fc8bc4020049fe28"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JCX-kcuf\/openchat_3.5-gpt-4-80k",
        "Average":65.8,
        "ARC":63.31,
        "HellaSwag":81.21,
        "MMLU":64.33,
        "TruthfulQA":54.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f54231e6c3e3a3c8164a10a5bbe9cd055a57ff50"
    },
    {
        "T":"?",
        "Model":"dddsaty\/SOLAR_Merge_Adapter_DPO_Orca",
        "Average":65.79,
        "ARC":63.91,
        "HellaSwag":84.58,
        "MMLU":63.18,
        "TruthfulQA":51.49,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0d1d423bab515ce5aee7e7029f86cfabfc26b4d9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"posicube\/Llama2-chat-AYB-13B",
        "Average":65.79,
        "ARC":63.4,
        "HellaSwag":84.79,
        "MMLU":59.34,
        "TruthfulQA":55.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"cc7ca1b8f906b9f62ace094540f4ff4124dd581a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Radu1999\/Mistral-Instruct-Ukrainian-SFT-DPO",
        "Average":65.79,
        "ARC":60.49,
        "HellaSwag":83.84,
        "MMLU":60.9,
        "TruthfulQA":57.91,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"317a3016054ccd9e90956a7ac587f004b9f64a45"
    },
    {
        "T":"?",
        "Model":"Inv\/Kazbek-7B",
        "Average":65.79,
        "ARC":65.1,
        "HellaSwag":85.2,
        "MMLU":63.41,
        "TruthfulQA":49.43,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":63,
        "Available on the hub":false,
        "Model Sha":"890d48a457b7cd7f9aadb23b615afec741792590"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"speechlessai\/speechless-mistral-7b-dare-0.85",
        "Average":65.78,
        "ARC":63.31,
        "HellaSwag":84.93,
        "MMLU":64.22,
        "TruthfulQA":50.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5eefd1b560cd65aec2f689880476f909b46d306c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dreamgen\/opus-v1.2-7b",
        "Average":65.78,
        "ARC":58.45,
        "HellaSwag":82.58,
        "MMLU":61.76,
        "TruthfulQA":60.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":7.24,
        "Hub":23,
        "Available on the hub":false,
        "Model Sha":"2caa564170ab98b40247e2812de1ab053115d3a7"
    },
    {
        "T":"?",
        "Model":"h2oai\/h2ogpt-research-oasst1-llama-65b",
        "Average":65.78,
        "ARC":64.76,
        "HellaSwag":85.94,
        "MMLU":63.57,
        "TruthfulQA":48.85,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"a6d8676aaa2ca2c25ea99180b538f0369dc70185"
    },
    {
        "T":"?",
        "Model":"Fredithefish\/OpenZephyrChat",
        "Average":65.77,
        "ARC":64.85,
        "HellaSwag":85.08,
        "MMLU":64.92,
        "TruthfulQA":48.24,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"146727eb2ebe09ea90552b0b22cb0abbfb830999"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"maywell\/PiVoT-10.7B-Mistral-v0.2",
        "Average":65.77,
        "ARC":63.31,
        "HellaSwag":81.68,
        "MMLU":59.86,
        "TruthfulQA":58.23,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":10.73,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"a496457d0743b6030ffbb96dad2dc6a62d143943"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dreamgen\/opus-v1.2-7b",
        "Average":65.77,
        "ARC":58.45,
        "HellaSwag":82.55,
        "MMLU":61.79,
        "TruthfulQA":60.27,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":7.24,
        "Hub":23,
        "Available on the hub":false,
        "Model Sha":"a8740f45ee9f633479f1079bfb1ad8ad65231a80"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Riiid\/sheep-duck-llama-2-13b",
        "Average":65.76,
        "ARC":63.14,
        "HellaSwag":84.52,
        "MMLU":59.89,
        "TruthfulQA":55.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"71edf22c49677d0239caf5f87d8139dd9cc79078"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/neural-chat-7b-v3-3-wizardmath-dare-me",
        "Average":65.75,
        "ARC":59.64,
        "HellaSwag":82.63,
        "MMLU":58.13,
        "TruthfulQA":62.6,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1d86275bacb3229e3de6069a98123c6252c7b471"
    },
    {
        "T":"?",
        "Model":"ENERGY-DRINK-LOVE\/SOLAR_merge2_dpo",
        "Average":65.75,
        "ARC":64.42,
        "HellaSwag":82.73,
        "MMLU":64.57,
        "TruthfulQA":51.28,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"be7ad2d338fe4d3867b598776c24a4344a6c0d8a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/llama-30b-instruct-2048-PL-lora",
        "Average":65.74,
        "ARC":63.31,
        "HellaSwag":84.66,
        "MMLU":61.66,
        "TruthfulQA":53.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1a076bce564f03bd47951eecab628c541fb1a6ad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FinancialSupport\/saiga-7b",
        "Average":65.73,
        "ARC":63.14,
        "HellaSwag":83.14,
        "MMLU":61.66,
        "TruthfulQA":54.99,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"08daa40fbe05366466f96c92deb775d1b9b04669"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"42MARU\/sitebunny-13b",
        "Average":65.72,
        "ARC":63.14,
        "HellaSwag":83.64,
        "MMLU":59.91,
        "TruthfulQA":56.21,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"67107327d09c2f9bf3e4b316d97767c97f5a0804"
    },
    {
        "T":"?",
        "Model":"hydra-project\/OpenHyperion-2.5-Mistral-7B",
        "Average":65.72,
        "ARC":64.25,
        "HellaSwag":84.86,
        "MMLU":63.86,
        "TruthfulQA":49.92,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"85a94bc7584beb08e8df09bad85f06b786f184c4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PulsarAI\/2x-LoRA-Assemble-13B",
        "Average":65.72,
        "ARC":63.65,
        "HellaSwag":83.47,
        "MMLU":59.82,
        "TruthfulQA":55.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1aca45d37eade21eb381aaefc9245b58ec3b7b26"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"oh-yeontaek\/llama-2-13B-LoRA-assemble",
        "Average":65.71,
        "ARC":63.57,
        "HellaSwag":83.51,
        "MMLU":59.82,
        "TruthfulQA":55.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"85bb49d333dba4a08b051418663d16853ce30cee"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"joey00072\/ToxicHermes-2.5-Mistral-7B",
        "Average":65.71,
        "ARC":64.59,
        "HellaSwag":83.75,
        "MMLU":63.67,
        "TruthfulQA":50.84,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"b8355885ec4e429f8cf1c7f0c324a696ee7a2893"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MLewd-ReMM-L2-Chat-20B",
        "Average":65.71,
        "ARC":62.46,
        "HellaSwag":85.62,
        "MMLU":59.13,
        "TruthfulQA":55.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"cda06630a1d8173541431e5ce8bc17dcfaa37e5e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JunchengXie\/Starling-LM-7B-alpha-gpt-4-80k",
        "Average":65.71,
        "ARC":62.97,
        "HellaSwag":81.28,
        "MMLU":64.22,
        "TruthfulQA":54.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e38bac0fb1d74c5abc65715c2b60c7b1509b64fb"
    },
    {
        "T":"\u2b55",
        "Model":"Ba2han\/HermesStar-OrcaWind-Synth-11B",
        "Average":65.7,
        "ARC":65.27,
        "HellaSwag":83.69,
        "MMLU":65.31,
        "TruthfulQA":48.55,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"61aefa2ac956ce0e8ce40aa2521bdb5634452766"
    },
    {
        "T":"\u2b55",
        "Model":"mncai\/Mistral-7B-OpenOrca-1k",
        "Average":65.7,
        "ARC":62.97,
        "HellaSwag":84.66,
        "MMLU":62.2,
        "TruthfulQA":52.96,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.11,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"ae9e37811a54ffe45f41a572c7e68363aa11b062"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rombodawg\/Everyone-Coder-4x7b-Base",
        "Average":65.7,
        "ARC":64.51,
        "HellaSwag":84.76,
        "MMLU":64.35,
        "TruthfulQA":49.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":24.15,
        "Hub":40,
        "Available on the hub":false,
        "Model Sha":"df11f29693b1cd4da9967f1c1832c4f4e0eb3303"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"maywell\/PiVoT-MoE",
        "Average":65.69,
        "ARC":63.91,
        "HellaSwag":83.52,
        "MMLU":60.71,
        "TruthfulQA":54.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":36.1,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"5d1159dd60ec2cc92dbc52508430e620b6adbdaa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rombodawg\/Everyone-Coder-4x7b-Base",
        "Average":65.69,
        "ARC":64.51,
        "HellaSwag":84.81,
        "MMLU":64.27,
        "TruthfulQA":49.16,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":24.15,
        "Hub":40,
        "Available on the hub":false,
        "Model Sha":"df11f29693b1cd4da9967f1c1832c4f4e0eb3303"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"KnutJaegersberg\/internlm-20b-llama",
        "Average":65.68,
        "ARC":61.35,
        "HellaSwag":82.08,
        "MMLU":61.59,
        "TruthfulQA":57.71,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":19.56,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0c4e862aeb22eaf2854ea06b6f8b1e3824591e3c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Mistral-11B-TestBench3",
        "Average":65.68,
        "ARC":62.03,
        "HellaSwag":83.92,
        "MMLU":63.11,
        "TruthfulQA":53.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":11.0,
        "Hub":15,
        "Available on the hub":false,
        "Model Sha":"7eb397ad2ec67400e31dc010f9b364a72d64d965"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nlpguy\/Hermes-low-tune",
        "Average":65.68,
        "ARC":63.99,
        "HellaSwag":83.75,
        "MMLU":63.6,
        "TruthfulQA":51.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"302b18f207e867b5bd918aa268bff0268b8a6f78"
    },
    {
        "T":"?",
        "Model":"jondurbin\/bagel-7b-v0.4",
        "Average":65.67,
        "ARC":63.57,
        "HellaSwag":82.67,
        "MMLU":62.25,
        "TruthfulQA":54.2,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"365a4a895d052d2eb4263be0c4e2ed75a08513b4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/WikiHow-Mistral-Instruct-7B",
        "Average":65.66,
        "ARC":60.92,
        "HellaSwag":80.99,
        "MMLU":58.57,
        "TruthfulQA":62.16,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"4ad83e84cf315977c49c96e91dc28f09f86987f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JunchengXie\/Mistral-7B-v0.1-gpt-4-40k",
        "Average":65.65,
        "ARC":63.31,
        "HellaSwag":81.5,
        "MMLU":62.9,
        "TruthfulQA":54.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c96652218ba869551915f5f6a502cfb91091ba20"
    },
    {
        "T":"?",
        "Model":"hydra-project\/ChatHercules-2.5-Mistral-7B",
        "Average":65.65,
        "ARC":65.1,
        "HellaSwag":84.61,
        "MMLU":65.35,
        "TruthfulQA":47.52,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"a50dd22ab08cb628642dcbd62edc25230c649bc4"
    },
    {
        "T":"?",
        "Model":"CalderaAI\/30B-Epsilon",
        "Average":65.64,
        "ARC":63.05,
        "HellaSwag":83.59,
        "MMLU":56.89,
        "TruthfulQA":59.03,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"6962638c2b0368ad496af6e20e46e3de97a7772b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Test-Raw-Solar-v1",
        "Average":65.64,
        "ARC":63.23,
        "HellaSwag":84.82,
        "MMLU":65.52,
        "TruthfulQA":48.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5289b48902b793c5fbff4b596c6ffc8b657639a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wandb\/mistral-7b-zephyr-sft",
        "Average":65.63,
        "ARC":62.29,
        "HellaSwag":84.88,
        "MMLU":62.29,
        "TruthfulQA":53.07,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7c09dbc23ce9d5e5281494cc8d62b9104f9cef05"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Q-bert\/Bumblebee-7B",
        "Average":65.63,
        "ARC":63.4,
        "HellaSwag":84.16,
        "MMLU":64.0,
        "TruthfulQA":50.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0c95c597b9c6c5563273126d1306fdd56bd31618"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/llama-30b-2048-instruct-PL-lora_unload",
        "Average":65.63,
        "ARC":63.82,
        "HellaSwag":84.7,
        "MMLU":61.49,
        "TruthfulQA":52.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b15f4310ea37fef99e4f16372a4b1f2342e27613"
    },
    {
        "T":"?",
        "Model":"Azure99\/blossom-v5-14b",
        "Average":65.63,
        "ARC":58.45,
        "HellaSwag":80.72,
        "MMLU":68.45,
        "TruthfulQA":54.89,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":14.17,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"c7bcffc0500cff73fdee957c3428c2ade1135dfc"
    },
    {
        "T":"\u2b55",
        "Model":"tianlinliu0121\/zephyr-7b-dpo-full-beta-0.2",
        "Average":65.62,
        "ARC":61.86,
        "HellaSwag":83.98,
        "MMLU":61.85,
        "TruthfulQA":54.78,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"727b63fc1ca6a592072159a7185c22f74cd38480"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Code-Mistral-7B",
        "Average":65.61,
        "ARC":63.57,
        "HellaSwag":83.71,
        "MMLU":63.38,
        "TruthfulQA":51.81,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"ed3b9ad583910423a7b82e27274681e3865206f1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-m2.0",
        "Average":65.6,
        "ARC":65.02,
        "HellaSwag":86.35,
        "MMLU":64.37,
        "TruthfulQA":46.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"fa081d52619b35d7016fb40ce855187d6a8e7e4c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-m2.0",
        "Average":65.6,
        "ARC":65.1,
        "HellaSwag":86.34,
        "MMLU":64.32,
        "TruthfulQA":46.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"fa081d52619b35d7016fb40ce855187d6a8e7e4c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"EmbeddedLLM\/Mistral-7B-Merge-14-v0.3-ft-step-15936",
        "Average":65.59,
        "ARC":62.54,
        "HellaSwag":82.14,
        "MMLU":62.58,
        "TruthfulQA":55.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2e53a73f2315a5ef111aa4a3a445a4a6682b031c"
    },
    {
        "T":"\u2b55",
        "Model":"tianlinliu0121\/zephyr-7b-dpo-full-beta-0.2",
        "Average":65.58,
        "ARC":61.77,
        "HellaSwag":84.04,
        "MMLU":61.79,
        "TruthfulQA":54.72,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"727b63fc1ca6a592072159a7185c22f74cd38480"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"posicube\/Llama2-chat-AYT-13B",
        "Average":65.58,
        "ARC":63.31,
        "HellaSwag":83.53,
        "MMLU":59.67,
        "TruthfulQA":55.8,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"dd12dced8076a959c03b8b5c4a4266f234d6639a"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_13-7B-slerp",
        "Average":65.58,
        "ARC":63.82,
        "HellaSwag":84.95,
        "MMLU":64.9,
        "TruthfulQA":48.62,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"26787521ec76c6fb81caffbba3d3fd75e9ce8e26"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mobidic\/solar-10b-platypus-lora",
        "Average":65.57,
        "ARC":62.2,
        "HellaSwag":84.16,
        "MMLU":63.23,
        "TruthfulQA":52.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c71c66d8e751b39f0105a123144cc8ebfd4871b8"
    },
    {
        "T":"?",
        "Model":"vicgalle\/Worldsim-Hermes-7B",
        "Average":65.54,
        "ARC":64.08,
        "HellaSwag":83.45,
        "MMLU":63.12,
        "TruthfulQA":51.52,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a08a049d7b0ada1a9422f1502d103233ba9a9854"
    },
    {
        "T":"\u2b55",
        "Model":"mergedlm\/zephyrnotus-11b-alpha",
        "Average":65.51,
        "ARC":61.35,
        "HellaSwag":82.8,
        "MMLU":60.67,
        "TruthfulQA":57.22,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"a6f74e800b6c77261a1d212bb3e6b2752cbedef9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"twodgirl\/Nimue-7B",
        "Average":65.5,
        "ARC":63.74,
        "HellaSwag":82.74,
        "MMLU":64.64,
        "TruthfulQA":50.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2d8ded1a612d3695200a6f57db70c32152afb935"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"posicube\/Llama-chat-AY-13B",
        "Average":65.5,
        "ARC":62.8,
        "HellaSwag":83.23,
        "MMLU":60.01,
        "TruthfulQA":55.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"66037b5ee553f7b878d796d2b2d5ada5734cc164"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JunchengXie\/Mistral-7B-v0.1-gpt-4-20k",
        "Average":65.5,
        "ARC":62.71,
        "HellaSwag":81.73,
        "MMLU":62.85,
        "TruthfulQA":54.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f64d57716913038d3928cef575cdc6c82ae7436f"
    },
    {
        "T":"?",
        "Model":"Xenon1\/Xenon-2",
        "Average":65.49,
        "ARC":57.51,
        "HellaSwag":83.28,
        "MMLU":60.25,
        "TruthfulQA":60.92,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"84b328258928f6e9f4b4fede000f58a4df8fabb5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ArianAskari\/SOLID-SFT-DPO-MixQV2-SOLIDChosen-SFTRejected-Zephyr-7b-beta",
        "Average":65.49,
        "ARC":60.75,
        "HellaSwag":83.68,
        "MMLU":59.42,
        "TruthfulQA":58.1,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"993476d6a1c2b8ee4894f9b63b449b6d42c6495f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AIGeekLabs\/radiantloom-mixtral-8x7b-fusion",
        "Average":65.48,
        "ARC":63.48,
        "HellaSwag":83.65,
        "MMLU":60.03,
        "TruthfulQA":54.76,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"93b3807b8fa38b9c95267117d25055bbd3eab29b"
    },
    {
        "T":"\u2b55",
        "Model":"bartowski\/internlm2-chat-20b-llama",
        "Average":65.47,
        "ARC":63.65,
        "HellaSwag":82.58,
        "MMLU":66.89,
        "TruthfulQA":48.74,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":19.86,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"af7d5521bf657a2323ad437feaa060969244afab"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"XuanXuanXuanXuan\/Mistral-7B-Instruct-v0.2-gpt-4-80k",
        "Average":65.45,
        "ARC":58.02,
        "HellaSwag":78.89,
        "MMLU":60.96,
        "TruthfulQA":63.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"45c1256737a33c5a35a2e87f5b7fd0b5a149d5e4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JunchengXie\/Mistral-7B-Instruct-v0.2-gpt-4-80k",
        "Average":65.45,
        "ARC":58.02,
        "HellaSwag":78.89,
        "MMLU":60.96,
        "TruthfulQA":63.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b5b0442077e2bdbfe29943a82db8a306fdf75af5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/mistral_18B_v0.1",
        "Average":65.44,
        "ARC":62.54,
        "HellaSwag":79.93,
        "MMLU":61.98,
        "TruthfulQA":57.32,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc",
        "#Params (B)":17.71,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d2b6e259165661001696a8d2198d559b0e448685"
    },
    {
        "T":"?",
        "Model":"Aeala\/Alpaca-elina-65b",
        "Average":65.44,
        "ARC":65.27,
        "HellaSwag":85.75,
        "MMLU":63.42,
        "TruthfulQA":47.32,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":65.02,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"51ce30a69b3c3363c8cfcd6395bf1df974ba2977"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cookinai\/titanbagel",
        "Average":65.42,
        "ARC":62.71,
        "HellaSwag":83.36,
        "MMLU":63.12,
        "TruthfulQA":52.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.11,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"dc2dcdfe71a5f9a059d98c8e573df3254ff5bdd0"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Deacon-20B",
        "Average":65.42,
        "ARC":60.75,
        "HellaSwag":81.74,
        "MMLU":60.7,
        "TruthfulQA":58.49,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":20.09,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"dabbb1675c4bfe6fed3fd8fecc7f2d887e697fa7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JCX-kcuf\/Mistral-7B-v0.1-gpt-4-80k",
        "Average":65.42,
        "ARC":62.8,
        "HellaSwag":81.05,
        "MMLU":63.21,
        "TruthfulQA":54.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dceff6501d72e838b02f13b86ccb622cf6e3d8d7"
    },
    {
        "T":"\u2b55",
        "Model":"athirdpath\/Iambe-20b-DARE-v2",
        "Average":65.41,
        "ARC":62.8,
        "HellaSwag":84.53,
        "MMLU":60.45,
        "TruthfulQA":53.85,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"02bd8edd30a5ddd1eede94c19a6ae160842a2f9f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-10.7B-v1.5",
        "Average":65.4,
        "ARC":65.02,
        "HellaSwag":84.07,
        "MMLU":65.09,
        "TruthfulQA":47.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.6,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"634a8454c84e415721e7cab1373e0fe8daf0e944"
    },
    {
        "T":"?",
        "Model":"Kabster\/BioMistral-Zephyr-Beta-SLERP",
        "Average":65.37,
        "ARC":62.12,
        "HellaSwag":84.13,
        "MMLU":60.63,
        "TruthfulQA":54.6,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b6f6be7fa65ed209721e55c6545cb332113a6bd5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"indischepartij\/OpenMia-Indo-Mistral-7b-v4",
        "Average":65.36,
        "ARC":64.16,
        "HellaSwag":82.84,
        "MMLU":61.08,
        "TruthfulQA":53.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4cded6e49d74571408be7acf13a8e0ad7f5bf79e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DangFutures\/BIG_DANG_BOT",
        "Average":65.36,
        "ARC":60.32,
        "HellaSwag":82.02,
        "MMLU":70.02,
        "TruthfulQA":49.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b97d945f17c9e41dbe1809210c8f818b1cecca7c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cgato\/Thespis-Krangled-7b-v2",
        "Average":65.35,
        "ARC":62.88,
        "HellaSwag":83.04,
        "MMLU":62.44,
        "TruthfulQA":53.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dc8cbcfe36ae94b19cd7e4c4c5afdf55b825865f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Radiantloom\/radintloom-mistral-7b-fusion-dpo",
        "Average":65.34,
        "ARC":63.14,
        "HellaSwag":83.68,
        "MMLU":63.42,
        "TruthfulQA":51.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"8f11b7ed191f06add8c7de1a830505289db0afde"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/robin-65b-v2-fp16",
        "Average":65.34,
        "ARC":61.95,
        "HellaSwag":84.6,
        "MMLU":62.51,
        "TruthfulQA":52.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"40edb31ba93045d673735361bc98f56125bbc77b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"EmbeddedLLM\/Mistral-7B-Merge-14-v0.3-ft-step-9984",
        "Average":65.34,
        "ARC":62.54,
        "HellaSwag":82.18,
        "MMLU":62.92,
        "TruthfulQA":53.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4bb10bcc0f7dfc5039658eb5e6b36c8555d94e66"
    },
    {
        "T":"?",
        "Model":"TheBloke\/gpt4-alpaca-lora-30b-HF",
        "Average":65.33,
        "ARC":64.85,
        "HellaSwag":85.72,
        "MMLU":58.51,
        "TruthfulQA":52.24,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"3c8007467a081dc72ae09b9d358416b056b38920"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rombodawg\/Leaderboard-killer-MoE_4x7b",
        "Average":65.32,
        "ARC":63.65,
        "HellaSwag":81.97,
        "MMLU":64.9,
        "TruthfulQA":50.75,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"391ad4593c4fdff7a90271954649a373b80d13d4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azure99\/blossom-v4-mistral-7b",
        "Average":65.31,
        "ARC":62.03,
        "HellaSwag":82.9,
        "MMLU":62.48,
        "TruthfulQA":53.84,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"43d6205b109754c02a4606beee64f42d151067f1"
    },
    {
        "T":"?",
        "Model":"adamo1139\/Yi-34B-200K-AEZAKMI-XLCTX-v3",
        "Average":65.31,
        "ARC":64.85,
        "HellaSwag":84.76,
        "MMLU":74.48,
        "TruthfulQA":37.14,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"93d5dc04ee4a3c62a6dae9c5c6d62ed999cd6d7b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"maldv\/SHRDFU-7b-overbaked-lora",
        "Average":65.3,
        "ARC":64.33,
        "HellaSwag":83.46,
        "MMLU":62.42,
        "TruthfulQA":50.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"48d8e195edab6945f4eecead0e65f3aa9de4c1f8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Stellaris-internlm2-20b-r128",
        "Average":65.3,
        "ARC":61.26,
        "HellaSwag":81.75,
        "MMLU":65.67,
        "TruthfulQA":52.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.0,
        "Hub":18,
        "Available on the hub":false,
        "Model Sha":"b63187a2a0489f0b6768efe4b8e28381c3bcf025"
    },
    {
        "T":"\u2b55",
        "Model":"osanseviero\/mistral-instruct-moe-experimental",
        "Average":65.29,
        "ARC":61.01,
        "HellaSwag":81.55,
        "MMLU":58.22,
        "TruthfulQA":60.4,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"e926f4f97f89c54806547df1b65cb1e6f0c6b26e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Llama-2-70B-chat-GPTQ",
        "Average":65.29,
        "ARC":62.63,
        "HellaSwag":84.81,
        "MMLU":62.74,
        "TruthfulQA":50.98,
        "Type":"fine-tuned",
        "Precision":"None",
        "Hub License":"other",
        "#Params (B)":9.1,
        "Hub":180,
        "Available on the hub":true,
        "Model Sha":"054fbf6f65e7ab7691ec07ec9ad366acf2dd90bf"
    },
    {
        "T":"?",
        "Model":"Azure99\/blossom-v5-mistral-7b",
        "Average":65.29,
        "ARC":62.63,
        "HellaSwag":84.26,
        "MMLU":62.45,
        "TruthfulQA":51.83,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"67260800a04ea5cc751aec4998c3a74ce5e40c33"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"maldv\/SHRDFU-7b-beta",
        "Average":65.29,
        "ARC":66.38,
        "HellaSwag":85.03,
        "MMLU":60.29,
        "TruthfulQA":49.45,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"069b89231682be68466942567f80c2913199aff5"
    },
    {
        "T":"?",
        "Model":"Locutusque\/OpenHercules-2.5-Mistral-7B",
        "Average":65.29,
        "ARC":64.25,
        "HellaSwag":84.84,
        "MMLU":64.21,
        "TruthfulQA":47.84,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"2f08ffbda0f39413f34934a526118fb3fbdd6c03"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Stellaris-internlm2-20b-r256",
        "Average":65.29,
        "ARC":61.09,
        "HellaSwag":82.22,
        "MMLU":66.01,
        "TruthfulQA":51.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.0,
        "Hub":18,
        "Available on the hub":false,
        "Model Sha":"d137609421ccbe34f0275a469e33dae3e931adf7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-falcon-180b-v12-preview0",
        "Average":65.28,
        "ARC":63.4,
        "HellaSwag":84.6,
        "MMLU":58.97,
        "TruthfulQA":54.14,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":178.64,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4f1aeb136860ee3216f23faec0c598014e5c40a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Faradaylab\/ARIA-70B-V2",
        "Average":65.27,
        "ARC":62.12,
        "HellaSwag":85.68,
        "MMLU":63.49,
        "TruthfulQA":49.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2bf026af438d522268533484a85a3e54178e7809"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-m-7b-3.1.2",
        "Average":65.26,
        "ARC":61.86,
        "HellaSwag":83.51,
        "MMLU":61.91,
        "TruthfulQA":53.75,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":44,
        "Available on the hub":false,
        "Model Sha":"e9a7f0271fa442d65bf6be87feeb3f4de2f5760e"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/Synatra-10.7B-v0.4",
        "Average":65.26,
        "ARC":64.93,
        "HellaSwag":82.47,
        "MMLU":62.5,
        "TruthfulQA":51.11,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":10.6,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"ae32ccb01cc971cfb36370876bf8981db243b2a3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"decapoda-research\/Adrastea-7b-v1.0-dpo",
        "Average":65.24,
        "ARC":63.31,
        "HellaSwag":82.3,
        "MMLU":62.26,
        "TruthfulQA":53.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"cf8ccdae24f5b008c2f29cacadd05dd58e95da54"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Novocoders\/Mistral-NeuralDPO-v0.6",
        "Average":65.24,
        "ARC":65.87,
        "HellaSwag":84.68,
        "MMLU":62.19,
        "TruthfulQA":48.22,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"fb556fe63e21cf60d85106bf3b5c19ad20ff1c18"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-XS-v1.1",
        "Average":65.24,
        "ARC":63.91,
        "HellaSwag":84.06,
        "MMLU":63.07,
        "TruthfulQA":49.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"e8850e534a3a9f602f72201b09c7ef8f879c1c0b"
    },
    {
        "T":"?",
        "Model":"gagan3012\/Multilingual-mistral",
        "Average":65.24,
        "ARC":62.29,
        "HellaSwag":81.76,
        "MMLU":61.38,
        "TruthfulQA":55.53,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"692fa323156e1d2a81e43adc0dd032700dde7a1a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Isaak-Carter\/JOSIE_Beta-3-7B-slerp",
        "Average":65.23,
        "ARC":63.4,
        "HellaSwag":84.56,
        "MMLU":64.17,
        "TruthfulQA":48.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"3754562d668ac4a9903df03628b9dfa52443e501"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rxavier\/Taurus-7B-1.0",
        "Average":65.23,
        "ARC":63.57,
        "HellaSwag":83.64,
        "MMLU":63.5,
        "TruthfulQA":50.21,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"76ed64a2a381a5bffca52d336e1481dce83e16fa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rxavier\/Taurus-1.0-Mistral-7B",
        "Average":65.23,
        "ARC":63.57,
        "HellaSwag":83.64,
        "MMLU":63.5,
        "TruthfulQA":50.21,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7576d3a7e138017e3da7dd8721c34684f9f8311f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xxyyy123\/Mistral-dpo-v1",
        "Average":65.23,
        "ARC":63.48,
        "HellaSwag":83.59,
        "MMLU":63.35,
        "TruthfulQA":50.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3c677a659bffbccbd8cf5ea75d198541ea2ec990"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/SynthIA-7B-v1.5",
        "Average":65.22,
        "ARC":62.71,
        "HellaSwag":83.37,
        "MMLU":63.48,
        "TruthfulQA":51.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"5a9912ef90a0efc1aaea327e5cf3e9554c8bd897"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LeroyDyer\/Mixtral_AI_Cyber_2.0",
        "Average":65.21,
        "ARC":60.75,
        "HellaSwag":82.5,
        "MMLU":60.03,
        "TruthfulQA":57.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"79e60738a576cfb9669ef673d00dbd35f016bdd2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Radiantloom\/radiantloom-mixtral-8x7b-fusion-dpo",
        "Average":65.21,
        "ARC":63.48,
        "HellaSwag":82.49,
        "MMLU":59.68,
        "TruthfulQA":55.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"08812f2f90df6c78b3a653208c25db0eee97714f"
    },
    {
        "T":"\u2b55",
        "Model":"kimwooglae\/AISquare-Instruct-SOLAR-10.7b-v0.5.32",
        "Average":65.21,
        "ARC":61.86,
        "HellaSwag":84.66,
        "MMLU":63.13,
        "TruthfulQA":51.19,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.6,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2d978ca8513d3863d945e59a3569f59773618dc3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"upstage\/llama-30b-instruct",
        "Average":65.21,
        "ARC":62.46,
        "HellaSwag":86.23,
        "MMLU":59.37,
        "TruthfulQA":52.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"fea4312379557e8a1e8073965f560798de369edd"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/Luban-Marcoroni-13B",
        "Average":65.21,
        "ARC":63.65,
        "HellaSwag":82.92,
        "MMLU":58.7,
        "TruthfulQA":55.55,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"bf152c36935acd67a9029c017f0c1ff2d7a92314"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/Luban-Marcoroni-13B-v3",
        "Average":65.21,
        "ARC":63.74,
        "HellaSwag":82.88,
        "MMLU":58.64,
        "TruthfulQA":55.56,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9b68680ed8351ef8ef6948169e69a888af40002e"
    },
    {
        "T":"?",
        "Model":"0-hero\/Matter-0.1-7B-boost",
        "Average":65.2,
        "ARC":62.63,
        "HellaSwag":81.51,
        "MMLU":61.97,
        "TruthfulQA":54.7,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ba56089eed1211f02e8d0ff47901e77b0cd48f83"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Bucharest-0.2",
        "Average":65.2,
        "ARC":64.59,
        "HellaSwag":84.87,
        "MMLU":66.03,
        "TruthfulQA":45.3,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"8cbf07c769e920054948dada4d4a1d4f914d32fa"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/Mini_Synatra_SFT",
        "Average":65.19,
        "ARC":62.46,
        "HellaSwag":83.44,
        "MMLU":61.2,
        "TruthfulQA":53.67,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":0.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"fc042f671dc0c94b21a6107eda75a6f9c8d44f2d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/gemma-7b-ultrachat-sft",
        "Average":65.19,
        "ARC":61.26,
        "HellaSwag":80.82,
        "MMLU":64.16,
        "TruthfulQA":54.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"c4cd06ead3275d15e021423af0b69e59b0e3ec00"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/travel-mistral-7B-16b-base",
        "Average":65.18,
        "ARC":61.43,
        "HellaSwag":83.51,
        "MMLU":62.55,
        "TruthfulQA":53.23,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ba3ff8d628f9c2039ce88a6f2c5c06ea35580230"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"N8Programs\/Thestral-v0.2",
        "Average":65.18,
        "ARC":62.71,
        "HellaSwag":82.49,
        "MMLU":62.73,
        "TruthfulQA":52.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"766194ad6cb1e9e991afd1477d2112706cb19453"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenLemur\/lemur-70b-v1",
        "Average":65.17,
        "ARC":64.33,
        "HellaSwag":85.72,
        "MMLU":65.85,
        "TruthfulQA":44.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub":29,
        "Available on the hub":true,
        "Model Sha":"74432ae16ef50207fe17fb88b2f1c1d32ef3b481"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/Luban-Marcoroni-13B-v2",
        "Average":65.16,
        "ARC":63.48,
        "HellaSwag":82.89,
        "MMLU":58.72,
        "TruthfulQA":55.56,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d7c704a08218dcc03963bc08e9113e281c056f53"
    },
    {
        "T":"?",
        "Model":"OpenBuddy\/openbuddy-mixtral-7bx8-v17.3-32k",
        "Average":65.15,
        "ARC":64.51,
        "HellaSwag":66.96,
        "MMLU":70.0,
        "TruthfulQA":59.14,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.74,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"05910dc7113c255ad115d36ca27e9d3f533d9181"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"922CA\/Silicon-Monika-7b",
        "Average":65.15,
        "ARC":63.14,
        "HellaSwag":82.64,
        "MMLU":62.67,
        "TruthfulQA":52.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6cc958abfbbd5f9d3f4221158e729663166d819c"
    },
    {
        "T":"\u2b55",
        "Model":"teknium\/OpenHermes-2-Mistral-7B",
        "Average":65.14,
        "ARC":63.05,
        "HellaSwag":83.81,
        "MMLU":63.46,
        "TruthfulQA":50.24,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":251,
        "Available on the hub":false,
        "Model Sha":"843a9bb94fac7d7bfc1b7c9f201efba295b6f5d6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chlee10\/T3Q-platypus-SOLAR-10.7B-v1.0",
        "Average":65.14,
        "ARC":62.54,
        "HellaSwag":84.15,
        "MMLU":61.95,
        "TruthfulQA":51.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.6,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9a088d12a72e8cefe9d42943e64faf08bc0eb5c3"
    },
    {
        "T":"?",
        "Model":"lmsys\/vicuna-33b-v1.3",
        "Average":65.12,
        "ARC":62.12,
        "HellaSwag":83.0,
        "MMLU":59.22,
        "TruthfulQA":56.16,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":215,
        "Available on the hub":true,
        "Model Sha":"ef8d6becf883fb3ce52e3706885f761819477ab4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/mistral-7b-alpaca-sft",
        "Average":65.12,
        "ARC":61.69,
        "HellaSwag":83.56,
        "MMLU":61.65,
        "TruthfulQA":53.59,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a23b6bcdff99735543644928f7fa085a8bab51bb"
    },
    {
        "T":"?",
        "Model":"sethuiyer\/Herculoid-2.0",
        "Average":65.11,
        "ARC":62.88,
        "HellaSwag":83.93,
        "MMLU":64.03,
        "TruthfulQA":49.61,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"fd39739fa6569e7020bba9cb49c2920bbdcb7aba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JCX-kcuf\/Mistral-7B-v0.1-gpt-4-60k",
        "Average":65.11,
        "ARC":62.88,
        "HellaSwag":80.78,
        "MMLU":62.87,
        "TruthfulQA":53.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2a9f61ae0dc9c9dd6f80efad6b001b72cf90157a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JunchengXie\/Mistral-7B-v0.1-gpt-4-60k",
        "Average":65.11,
        "ARC":62.88,
        "HellaSwag":80.78,
        "MMLU":62.87,
        "TruthfulQA":53.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c3a1e2f26584a0220b79b58485f22318f3e9e923"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ausboss\/llama-30b-supercot",
        "Average":65.11,
        "ARC":64.85,
        "HellaSwag":85.08,
        "MMLU":56.56,
        "TruthfulQA":53.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":123,
        "Available on the hub":true,
        "Model Sha":"dc9d81f454d286ea040c5cd45b058aecaa51c13e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LeroyDyer\/Mixtral_AI_Cyber_3.1_SFT",
        "Average":65.11,
        "ARC":61.86,
        "HellaSwag":81.32,
        "MMLU":64.51,
        "TruthfulQA":52.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"f5c5468cc9c8191e4ed89a30b6d7b98d2a0dfadc"
    },
    {
        "T":"?",
        "Model":"tourist800\/mistral_2X7b",
        "Average":65.1,
        "ARC":63.4,
        "HellaSwag":83.77,
        "MMLU":61.18,
        "TruthfulQA":52.08,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8722dce4c447d974545f55623788c351a15bc36c"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"tourist800\/Marcoro14-7B-slerp",
        "Average":65.1,
        "ARC":63.4,
        "HellaSwag":83.77,
        "MMLU":61.18,
        "TruthfulQA":52.08,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8722dce4c447d974545f55623788c351a15bc36c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cgato\/Thespis-CurtainCall-7b-v0.3",
        "Average":65.09,
        "ARC":64.25,
        "HellaSwag":82.93,
        "MMLU":62.24,
        "TruthfulQA":50.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"cc6a7116ab0b3651bbd03a15eb90f8fb5330e340"
    },
    {
        "T":"?",
        "Model":"nasiruddin15\/Mistral-grok-instract-2-7B-slerp",
        "Average":65.09,
        "ARC":62.8,
        "HellaSwag":83.03,
        "MMLU":61.04,
        "TruthfulQA":53.51,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7ce8d94268c52ed2c7046624d5052fee5bc1e247"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/ChatAYT-Lora-Assamble-Marcoroni",
        "Average":65.09,
        "ARC":62.46,
        "HellaSwag":83.05,
        "MMLU":58.72,
        "TruthfulQA":56.12,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"51c9b600023cd26c4eb3754b9a89c60dde959ccc"
    },
    {
        "T":"?",
        "Model":"Azure99\/blossom-v4-qwen1_5-14b",
        "Average":65.08,
        "ARC":57.34,
        "HellaSwag":79.84,
        "MMLU":67.92,
        "TruthfulQA":55.21,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":14.17,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b505d4e2311a709de56a214a33820f5a4ee0d3e5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xxyyy123\/1701221123_Ads_Mistral7B-slimorca_all-Lqv-r4b128",
        "Average":65.08,
        "ARC":62.88,
        "HellaSwag":83.99,
        "MMLU":62.89,
        "TruthfulQA":50.55,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2872cd97f88418d6b07082048b316ea5b996982d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FPHam\/Writing_Partner_Mistral_7B",
        "Average":65.07,
        "ARC":64.59,
        "HellaSwag":84.59,
        "MMLU":62.55,
        "TruthfulQA":48.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":24,
        "Available on the hub":false,
        "Model Sha":"d71b744e4d7432301d891409a05710bf2e4fa4c3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"simonveitner\/Math-OpenHermes-2.5-Mistral-7B",
        "Average":65.06,
        "ARC":63.05,
        "HellaSwag":83.07,
        "MMLU":63.21,
        "TruthfulQA":50.91,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"db052d375f389aa264bacac47aeb07538698122d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Bucharest-0.3",
        "Average":65.06,
        "ARC":63.99,
        "HellaSwag":84.46,
        "MMLU":65.61,
        "TruthfulQA":46.19,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"11ba37297a32f302c27c653b58546eca9812dd6e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/jackalope-7b",
        "Average":65.06,
        "ARC":63.4,
        "HellaSwag":83.29,
        "MMLU":63.5,
        "TruthfulQA":50.06,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":29,
        "Available on the hub":false,
        "Model Sha":"5ba23522319a51d0af23b336a6a83c72ae3780e7"
    },
    {
        "T":"?",
        "Model":"Sao10K\/Stheno-v2-Delta-fp16",
        "Average":65.05,
        "ARC":62.46,
        "HellaSwag":83.45,
        "MMLU":59.04,
        "TruthfulQA":55.25,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"3979769be8d92aa2dd0c7aebf385635863f16dd9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-V2-Delta-fp16",
        "Average":65.05,
        "ARC":62.46,
        "HellaSwag":83.45,
        "MMLU":59.04,
        "TruthfulQA":55.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"3979769be8d92aa2dd0c7aebf385635863f16dd9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/MetaMath-neural-chat-7b-v3-2-Ties",
        "Average":65.04,
        "ARC":63.48,
        "HellaSwag":82.34,
        "MMLU":62.25,
        "TruthfulQA":52.06,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2b0436588c205a6ecae5f32617d88b087b3cc644"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Chupacabra-7B-v2.03",
        "Average":65.03,
        "ARC":63.82,
        "HellaSwag":84.73,
        "MMLU":63.05,
        "TruthfulQA":48.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"73641ebe6ba450a83f6e80ed919fba48cc5f2837"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ai-business\/Luban-13B",
        "Average":65.03,
        "ARC":63.05,
        "HellaSwag":82.8,
        "MMLU":58.73,
        "TruthfulQA":55.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"01b0f2046083dd8d9d8f9e626d78d83eaa1d57dd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chlee10\/T3Q-Platypus-Mistral7B",
        "Average":65.03,
        "ARC":63.14,
        "HellaSwag":84.41,
        "MMLU":60.71,
        "TruthfulQA":51.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"266e15172e2c985fe4b1bf3c3a3030fef3b40cac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xxyyy123\/Mistral7B_adaptor_v1",
        "Average":65.03,
        "ARC":62.97,
        "HellaSwag":83.81,
        "MMLU":63.56,
        "TruthfulQA":49.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"23e800094570c22fbaa4279ef7e7f27315ac61af"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"csujeong\/Gemma-7B-Finetuning-JCS-Ko-Ins",
        "Average":65.02,
        "ARC":62.46,
        "HellaSwag":82.78,
        "MMLU":66.23,
        "TruthfulQA":48.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"54ded4775e2db352629004939236918da8abc9a4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_3.5",
        "Average":65.01,
        "ARC":63.91,
        "HellaSwag":84.79,
        "MMLU":64.94,
        "TruthfulQA":46.38,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1078,
        "Available on the hub":false,
        "Model Sha":"5b874a33a91d63023055e6cb2d5d86afe883b4ec"
    },
    {
        "T":"?",
        "Model":"Henk717\/airochronos-33B",
        "Average":65.0,
        "ARC":64.42,
        "HellaSwag":85.21,
        "MMLU":59.79,
        "TruthfulQA":50.59,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"06843c6693cc265dabb464c818a3d3713239721a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_3.5",
        "Average":65.0,
        "ARC":63.82,
        "HellaSwag":84.8,
        "MMLU":64.98,
        "TruthfulQA":46.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1078,
        "Available on the hub":false,
        "Model Sha":"5b874a33a91d63023055e6cb2d5d86afe883b4ec"
    },
    {
        "T":"?",
        "Model":"alignment-handbook\/zephyr-7b-dpo-qlora",
        "Average":64.99,
        "ARC":63.65,
        "HellaSwag":85.35,
        "MMLU":63.82,
        "TruthfulQA":47.14,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"b991e934e478e9b406d07840940e9a785a62f0ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-33b-v1.3",
        "Average":64.99,
        "ARC":61.6,
        "HellaSwag":83.06,
        "MMLU":59.21,
        "TruthfulQA":56.09,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":215,
        "Available on the hub":true,
        "Model Sha":"ef8d6becf883fb3ce52e3706885f761819477ab4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"decem\/Dionysus-Mistral-m3-v6",
        "Average":64.99,
        "ARC":63.14,
        "HellaSwag":84.51,
        "MMLU":62.82,
        "TruthfulQA":49.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"417618a86cd04bfcc48bd987043a4ef096e866cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abdulrahman-nuzha\/finetuned-Mistral-7B-Instruct-v0.2-5000-v2.0",
        "Average":64.99,
        "ARC":59.3,
        "HellaSwag":82.65,
        "MMLU":58.45,
        "TruthfulQA":59.54,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b32cd037f8c83d08da8a5e593d3cc29de090af1a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"tuantran1632001\/Psyfighter2-Orca2-13B-ties",
        "Average":64.98,
        "ARC":62.46,
        "HellaSwag":81.74,
        "MMLU":60.31,
        "TruthfulQA":55.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"b858fbc15734cc797f1c9e4acb239bfb6c390f08"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"tuantran1632001\/Psyfighter2-Orca2-ties",
        "Average":64.98,
        "ARC":62.46,
        "HellaSwag":81.74,
        "MMLU":60.31,
        "TruthfulQA":55.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"e4ab7df425cfa2b2687194837c3b7fba4be7fc74"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Liberated-Qwen1.5-14B",
        "Average":64.97,
        "ARC":57.94,
        "HellaSwag":80.65,
        "MMLU":68.83,
        "TruthfulQA":52.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":14.0,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"cc0fa5102bfee821bb5e49f082731ccb9d1fedf1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Zephyrus-L1-33B",
        "Average":64.97,
        "ARC":64.51,
        "HellaSwag":84.15,
        "MMLU":57.37,
        "TruthfulQA":53.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"679aae34440d576456b283070371b2a15dbb948b"
    },
    {
        "T":"?",
        "Model":"rizla\/rizla55b",
        "Average":64.97,
        "ARC":60.32,
        "HellaSwag":80.42,
        "MMLU":63.54,
        "TruthfulQA":55.59,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nd-4.0",
        "#Params (B)":55.29,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"814ea2c4ddaf2c1b6e4780ff061f899b684a8275"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Henk717\/airochronos-33B",
        "Average":64.96,
        "ARC":64.25,
        "HellaSwag":85.2,
        "MMLU":59.83,
        "TruthfulQA":50.56,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"06843c6693cc265dabb464c818a3d3713239721a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cookinai\/Blitz-v0.1",
        "Average":64.95,
        "ARC":55.2,
        "HellaSwag":82.5,
        "MMLU":61.33,
        "TruthfulQA":60.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8617ca42117e462cbe856f23807cb9e8c3fbae8a"
    },
    {
        "T":"?",
        "Model":"PulsarAI\/Einstein-v3-7B",
        "Average":64.95,
        "ARC":62.29,
        "HellaSwag":83.01,
        "MMLU":63.32,
        "TruthfulQA":51.18,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"632d22a529a04a16f0297320ba221ef4091a797c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Einstein-v3-7B",
        "Average":64.95,
        "ARC":62.29,
        "HellaSwag":83.01,
        "MMLU":63.32,
        "TruthfulQA":51.18,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"632d22a529a04a16f0297320ba221ef4091a797c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"berkeley-nest\/Starling-LM-7B-alpha",
        "Average":64.95,
        "ARC":63.82,
        "HellaSwag":84.9,
        "MMLU":64.67,
        "TruthfulQA":46.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":526,
        "Available on the hub":false,
        "Model Sha":"f721e85293598f2ef774e483ae95343e39811577"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xDAN-AI\/xDAN-L1Mix-DeepThinking-v2",
        "Average":64.94,
        "ARC":62.37,
        "HellaSwag":82.32,
        "MMLU":59.69,
        "TruthfulQA":55.38,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":0.0,
        "Hub":59,
        "Available on the hub":false,
        "Model Sha":"47ca647c3bb26b647b1f66c3672b890803de46c8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TeeZee\/GALAXY_v03_slimorca_1_epoch_50k",
        "Average":64.94,
        "ARC":62.71,
        "HellaSwag":84.58,
        "MMLU":65.17,
        "TruthfulQA":47.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":15.97,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f7936789f085412986be9657da573028d8416397"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigostral-7b-chat",
        "Average":64.93,
        "ARC":62.63,
        "HellaSwag":84.34,
        "MMLU":63.53,
        "TruthfulQA":49.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":25,
        "Available on the hub":false,
        "Model Sha":"969fbfc7a91f53c8562a2c48a3c24dd3745d5a97"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bn22\/DolphinMini-Mistral-7B",
        "Average":64.93,
        "ARC":61.18,
        "HellaSwag":84.25,
        "MMLU":61.94,
        "TruthfulQA":52.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"16ddf12ee58e71664f7e76551294ba54794c7903"
    },
    {
        "T":"?",
        "Model":"bavest\/fin-llama-33b-merged",
        "Average":64.92,
        "ARC":65.02,
        "HellaSwag":86.2,
        "MMLU":58.73,
        "TruthfulQA":49.75,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"gpl",
        "#Params (B)":32.32,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"17114520801da7b9599fe7a9fdf238915713a59b"
    },
    {
        "T":"?",
        "Model":"adonlee\/LLaMA_2_13B_SFT_v1",
        "Average":64.92,
        "ARC":64.51,
        "HellaSwag":83.38,
        "MMLU":58.6,
        "TruthfulQA":53.2,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"31421b19a3f5fe2eff4871c86d3a94d5723b6fd2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rishiraj\/smol-7b",
        "Average":64.92,
        "ARC":63.74,
        "HellaSwag":84.77,
        "MMLU":65.0,
        "TruthfulQA":46.17,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":22,
        "Available on the hub":false,
        "Model Sha":"d3e24684f38e0332cf4a6c70a37ee894e7a27fdc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Liberated-Qwen1.5-14B",
        "Average":64.92,
        "ARC":57.94,
        "HellaSwag":80.56,
        "MMLU":68.81,
        "TruthfulQA":52.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":14.0,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"cc0fa5102bfee821bb5e49f082731ccb9d1fedf1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BlueNipples\/SnowLotus-v2-10.7B",
        "Average":64.92,
        "ARC":64.76,
        "HellaSwag":85.28,
        "MMLU":64.1,
        "TruthfulQA":45.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"5027a7a14e7f224e2fbdd0268a3a4ae75439229f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jan-hq\/stealth-rag-v1.1",
        "Average":64.91,
        "ARC":62.12,
        "HellaSwag":83.83,
        "MMLU":64.06,
        "TruthfulQA":49.64,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0ad40db97e7329778c9a3781049f7e31c57df83f"
    },
    {
        "T":"?",
        "Model":"rombodawg\/EveryoneLLM-7b-Gemma-Base",
        "Average":64.91,
        "ARC":64.33,
        "HellaSwag":81.98,
        "MMLU":62.95,
        "TruthfulQA":50.38,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":8.54,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"89441e1aec14b21bf39ad51994310ad67f48ae97"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azazelle\/Sina-Loki-7b-Merge",
        "Average":64.91,
        "ARC":59.13,
        "HellaSwag":81.96,
        "MMLU":64.71,
        "TruthfulQA":53.84,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5977c691a13280715c15559f2d90cb3142f74881"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AI-B\/UTENA-7B-NSFW-V2",
        "Average":64.91,
        "ARC":63.31,
        "HellaSwag":84.54,
        "MMLU":63.97,
        "TruthfulQA":47.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"unlicense",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"2da9543e68e222ca627a22a131772155d5ef9078"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/openchat-3.5-1210-32k",
        "Average":64.91,
        "ARC":64.68,
        "HellaSwag":84.06,
        "MMLU":61.59,
        "TruthfulQA":49.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"48fde7a1a1d644f603a828839047ff695165b387"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Starling-LM-alpha-8x7B-MoE",
        "Average":64.91,
        "ARC":63.65,
        "HellaSwag":84.9,
        "MMLU":64.68,
        "TruthfulQA":46.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"61a66c526af1238690c815051c0f4ebe866ca588"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"UCLA-AGI\/zephyr-7b-sft-full-SPIN-iter0",
        "Average":64.9,
        "ARC":63.57,
        "HellaSwag":84.43,
        "MMLU":61.28,
        "TruthfulQA":50.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"d457f58ca73bd5540dc4e12b70315e4464ea138c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/neural-chat-7b-v3-1-Nebula-v2-7B",
        "Average":64.9,
        "ARC":61.77,
        "HellaSwag":80.21,
        "MMLU":59.07,
        "TruthfulQA":58.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0b98e4ca35764da09cabcaaebbdac1f827629219"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bartowski\/internlm2-math-20b-llama",
        "Average":64.9,
        "ARC":59.98,
        "HellaSwag":81.64,
        "MMLU":65.07,
        "TruthfulQA":52.9,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":19.86,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"fb39351b1b98849aa87f486fa3130d97c92cb0fa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/SynthIA-7B-v1.3",
        "Average":64.9,
        "ARC":62.12,
        "HellaSwag":83.45,
        "MMLU":62.65,
        "TruthfulQA":51.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":140,
        "Available on the hub":false,
        "Model Sha":"8e6d0b18be876e0ebfff47d6c4f33d776f189971"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"UCLA-AGI\/test0",
        "Average":64.89,
        "ARC":63.65,
        "HellaSwag":84.44,
        "MMLU":61.01,
        "TruthfulQA":50.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"e90506303f046ebe6da9d8b41489a7365b455a06"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"UCLA-AGI\/zephyr-7b-sft-full-SPIN-iter0",
        "Average":64.89,
        "ARC":63.65,
        "HellaSwag":84.44,
        "MMLU":61.01,
        "TruthfulQA":50.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"d457f58ca73bd5540dc4e12b70315e4464ea138c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/openchat-3.5-1210-32k-8x7b-MoE",
        "Average":64.89,
        "ARC":64.59,
        "HellaSwag":84.07,
        "MMLU":61.6,
        "TruthfulQA":49.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c24bf500da78e987197055e96dda0dcc496de9ed"
    },
    {
        "T":"?",
        "Model":"NeuralNovel\/Mini-Mixtral-v0.2",
        "Average":64.89,
        "ARC":61.26,
        "HellaSwag":84.12,
        "MMLU":63.83,
        "TruthfulQA":50.36,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"326146db4ced10445991d84ec144765daf99b154"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Danielbrdz\/Barcenas-10.7b",
        "Average":64.89,
        "ARC":64.16,
        "HellaSwag":83.6,
        "MMLU":65.22,
        "TruthfulQA":46.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a6ffe3b262cad3a2aee5fd36420f1b36933a7159"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-1.4",
        "Average":64.89,
        "ARC":64.42,
        "HellaSwag":85.13,
        "MMLU":59.53,
        "TruthfulQA":50.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"04e1e194247a95cc60ba3cd70d026bc94c1f1764"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"berkeley-nest\/Starling-LM-7B-alpha",
        "Average":64.89,
        "ARC":63.65,
        "HellaSwag":84.87,
        "MMLU":64.7,
        "TruthfulQA":46.32,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":526,
        "Available on the hub":false,
        "Model Sha":"76e60ca9807f55acd8eff3ec7ae022c5fbdf1e0e"
    },
    {
        "T":"?",
        "Model":"zhengchenphd\/ICE-GRT",
        "Average":64.88,
        "ARC":62.88,
        "HellaSwag":86.14,
        "MMLU":57.34,
        "TruthfulQA":53.17,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"51b7c0c3f8439d648190c140dea1e14cab40ac11"
    },
    {
        "T":"\u2b55",
        "Model":"Norquinal\/Mistral-7B-claude-instruct",
        "Average":64.88,
        "ARC":63.23,
        "HellaSwag":84.99,
        "MMLU":63.84,
        "TruthfulQA":47.47,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"faff0de73681ad1f0500169ae18d7a5ff424eb7f"
    },
    {
        "T":"?",
        "Model":"Inv\/Dykh-Tau-7B",
        "Average":64.86,
        "ARC":63.74,
        "HellaSwag":84.67,
        "MMLU":63.79,
        "TruthfulQA":47.25,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":63,
        "Available on the hub":false,
        "Model Sha":"f2036b05109e411e358253449b5a66f967d27ba4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"liuda1\/dm7b_sft_gpt88w_merge",
        "Average":64.86,
        "ARC":62.29,
        "HellaSwag":82.47,
        "MMLU":61.35,
        "TruthfulQA":53.33,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f4f76170f6fe63e832e32d32be1eb4a1da36f402"
    },
    {
        "T":"?",
        "Model":"abhishekchohan\/Yi-9B-Forest-DPO-v1.0",
        "Average":64.85,
        "ARC":59.81,
        "HellaSwag":78.6,
        "MMLU":70.02,
        "TruthfulQA":50.98,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.57,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"af6d3cc25c901619d118ebf616f7a5902413a4ea"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Qwen\/Qwen-14B",
        "Average":64.85,
        "ARC":58.28,
        "HellaSwag":83.99,
        "MMLU":67.7,
        "TruthfulQA":49.43,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":14.17,
        "Hub":195,
        "Available on the hub":false,
        "Model Sha":"5eda9482e32a8ea7ed2dc47178f3b491eb207939"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama-30b",
        "Average":64.83,
        "ARC":64.25,
        "HellaSwag":83.64,
        "MMLU":58.23,
        "TruthfulQA":53.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7f035eabd1d0e7b38ace395847a623f475d90da8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/30B-Lazarus-instruct-PL-lora_unload",
        "Average":64.82,
        "ARC":62.8,
        "HellaSwag":84.13,
        "MMLU":56.87,
        "TruthfulQA":55.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"eeb29b35ceb6dd5c532f1e4e1235f1cdd3f51f23"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ArianAskari\/SOLID-SFT-DPO-MixQV3-SOLIDChosen-SFTRejected-Zephyr-7b-beta",
        "Average":64.82,
        "ARC":59.56,
        "HellaSwag":82.53,
        "MMLU":59.6,
        "TruthfulQA":57.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d23df4c9e2bc46656e4d894475d57584181b3a24"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"VAGOsolutions\/SauerkrautLM-7b-HerO",
        "Average":64.81,
        "ARC":63.23,
        "HellaSwag":83.52,
        "MMLU":63.3,
        "TruthfulQA":49.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":29,
        "Available on the hub":false,
        "Model Sha":"0aeb810af28e2910a92b929c21b931a5c06073de"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-llama2-luban-orca-platypus-13b",
        "Average":64.8,
        "ARC":62.54,
        "HellaSwag":82.76,
        "MMLU":59.23,
        "TruthfulQA":54.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"908cfb670611875b52045c4bab81cff53f0279a7"
    },
    {
        "T":"?",
        "Model":"wenbopan\/Faro-Yi-9B-200K",
        "Average":64.79,
        "ARC":61.26,
        "HellaSwag":76.95,
        "MMLU":70.77,
        "TruthfulQA":50.17,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":8.83,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"8bfc9c6721ab008dcb8693ba4407678706a7c245"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/BrainDerp3",
        "Average":64.78,
        "ARC":60.92,
        "HellaSwag":82.1,
        "MMLU":58.91,
        "TruthfulQA":57.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"0b575b9245406cca92942ce2ababb5b868109bed"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Delcos\/NATE-7b",
        "Average":64.78,
        "ARC":60.92,
        "HellaSwag":82.1,
        "MMLU":58.91,
        "TruthfulQA":57.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":7.0,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"dd844a22b3b1ec4ad1757ce1ce184b8c765ae4c9"
    },
    {
        "T":"\u2b55",
        "Model":"dfurman\/Mistral-7B-Instruct-v0.2",
        "Average":64.77,
        "ARC":60.15,
        "HellaSwag":82.79,
        "MMLU":60.07,
        "TruthfulQA":56.06,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"322faff8bb0c72b772762de7635f5aea9864a24a"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Qwen\/Qwen1.5-14B",
        "Average":64.77,
        "ARC":56.57,
        "HellaSwag":81.08,
        "MMLU":69.36,
        "TruthfulQA":52.06,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":14.17,
        "Hub":26,
        "Available on the hub":false,
        "Model Sha":"39b74a78357df4d2296e838d87565967d663a67a"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Qwen\/Qwen2-beta-14B",
        "Average":64.77,
        "ARC":56.57,
        "HellaSwag":81.08,
        "MMLU":69.36,
        "TruthfulQA":52.06,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":14.17,
        "Hub":26,
        "Available on the hub":false,
        "Model Sha":"56eee702d3528c6c29c42640625b5631e5ae9aea"
    },
    {
        "T":"\u2b55",
        "Model":"xxyyy123\/test_merge_p_ov1_w0.66_w0.5_n1",
        "Average":64.76,
        "ARC":62.46,
        "HellaSwag":82.37,
        "MMLU":58.05,
        "TruthfulQA":56.18,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e2349e81b46839bc8bedfc2c944ab35c640a5b51"
    },
    {
        "T":"?",
        "Model":"TeeZee\/BigMaid-20B-v1.0",
        "Average":64.76,
        "ARC":61.35,
        "HellaSwag":85.26,
        "MMLU":57.15,
        "TruthfulQA":55.29,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":19.99,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"d37d99b4656190a23ec51baaad4d1bf6421e67c5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-1.8-L2-13B",
        "Average":64.76,
        "ARC":63.48,
        "HellaSwag":84.12,
        "MMLU":58.57,
        "TruthfulQA":52.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"fe054ab749a69375285df40913a88bd40f1e2bf6"
    },
    {
        "T":"?",
        "Model":"eldogbbhed\/NeuralKrishnaMathWizard-7B",
        "Average":64.75,
        "ARC":63.05,
        "HellaSwag":85.12,
        "MMLU":61.78,
        "TruthfulQA":49.06,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"01af35d4f26b45d8ae0e042303a4995194e54c68"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/MetaMath-NeuralHermes-2.5-Mistral-7B-Linear",
        "Average":64.75,
        "ARC":62.8,
        "HellaSwag":84.21,
        "MMLU":63.43,
        "TruthfulQA":48.57,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6aa0b89656b98f8f2212f6822ce665ac9517dbd7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JunchengXie\/zephyr-7b-beta-gpt-4-80k",
        "Average":64.75,
        "ARC":60.84,
        "HellaSwag":79.08,
        "MMLU":60.67,
        "TruthfulQA":58.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d4ca0d52d950a5af64434243038f318b3c359f1f"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"abhishekchohan\/mistral-7B-med-merge",
        "Average":64.74,
        "ARC":64.51,
        "HellaSwag":82.96,
        "MMLU":57.84,
        "TruthfulQA":53.65,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"120987e276831fe6916a86a80c0c39ac3aa5dfb7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/BrainDerp2",
        "Average":64.74,
        "ARC":60.92,
        "HellaSwag":81.94,
        "MMLU":58.9,
        "TruthfulQA":57.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"948ee7af94a8b092807df4becfc0a8c1cd042878"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"chargoddard\/internlm2-7b-llama",
        "Average":64.72,
        "ARC":60.49,
        "HellaSwag":80.99,
        "MMLU":63.16,
        "TruthfulQA":54.25,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.74,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"67517f8c49907cf4c1e515b356ce6907189dbdd4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-1.2",
        "Average":64.72,
        "ARC":64.42,
        "HellaSwag":84.93,
        "MMLU":60.35,
        "TruthfulQA":49.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"b3254a827fb1dfe0d4e428bf5ab1c3a2bac82d68"
    },
    {
        "T":"?",
        "Model":"ChaoticNeutrals\/Bepis_9B",
        "Average":64.7,
        "ARC":62.54,
        "HellaSwag":80.12,
        "MMLU":62.84,
        "TruthfulQA":53.3,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":8.99,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"68e796a8e1612119821f3187d62779ca472991a0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MLewd-ReMM-L2-Chat-20B-Inverted",
        "Average":64.69,
        "ARC":61.69,
        "HellaSwag":85.32,
        "MMLU":58.0,
        "TruthfulQA":53.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"b5b501b4d23ec7ab24b827f79e48b2c67e548ddb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"caisarl76\/mistral-guanaco1k-ep2",
        "Average":64.68,
        "ARC":60.07,
        "HellaSwag":82.76,
        "MMLU":61.5,
        "TruthfulQA":54.4,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.11,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"9c9f31f213b69da7797c2c0630c17cf8f785fc13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"caisarl76\/Mistral-7B-guanaco1k-ep2",
        "Average":64.68,
        "ARC":60.07,
        "HellaSwag":82.76,
        "MMLU":61.5,
        "TruthfulQA":54.4,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.11,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"9c9f31f213b69da7797c2c0630c17cf8f785fc13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AbacusResearch\/haLLAwa2",
        "Average":64.68,
        "ARC":63.31,
        "HellaSwag":84.51,
        "MMLU":63.52,
        "TruthfulQA":47.38,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2ab34884c6ccfb52e625fdb3a5fc4f69fbe226fb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/alpaca-lora-65B-HF",
        "Average":64.67,
        "ARC":64.85,
        "HellaSwag":85.59,
        "MMLU":63.11,
        "TruthfulQA":45.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"113b61b37a2862b950ada68620e57acafbcefe13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jeiku\/Garbage_9B",
        "Average":64.67,
        "ARC":63.65,
        "HellaSwag":82.97,
        "MMLU":64.18,
        "TruthfulQA":47.87,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c7fa202637f4139a9a8c75e40440c2d4859f1ab5"
    },
    {
        "T":"\u2b55",
        "Model":"akjindal53244\/Mistral-7B-v0.1-Open-Platypus",
        "Average":64.64,
        "ARC":62.37,
        "HellaSwag":85.08,
        "MMLU":63.79,
        "TruthfulQA":47.33,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"aa2c84e89c4c8a10e0569e45021b59e6d1c08bda"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NovoCode\/Novocode7b",
        "Average":64.64,
        "ARC":58.79,
        "HellaSwag":80.51,
        "MMLU":56.5,
        "TruthfulQA":62.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a4cf91cc879937c3a45ca0f10aecd335c3919063"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/BrainDerp",
        "Average":64.64,
        "ARC":60.75,
        "HellaSwag":82.1,
        "MMLU":58.81,
        "TruthfulQA":56.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ba21a7ed5458b3fa2b05ce6aab431acd1f857516"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"macadeliccc\/gemma-orchid-7b-dpo",
        "Average":64.63,
        "ARC":62.88,
        "HellaSwag":80.95,
        "MMLU":61.41,
        "TruthfulQA":53.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":8.54,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"de936b0952f5c78ba6817b3ff91d3ab85e2e90fd"
    },
    {
        "T":"?",
        "Model":"gagan3012\/Multirial",
        "Average":64.62,
        "ARC":63.23,
        "HellaSwag":79.57,
        "MMLU":61.01,
        "TruthfulQA":54.7,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0bf35a998ce26287916c9d1e0575d5f15e6ae0df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-33b-instruct",
        "Average":64.62,
        "ARC":63.05,
        "HellaSwag":85.0,
        "MMLU":58.32,
        "TruthfulQA":52.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":32.32,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"9c2b558b888e0ef8b4a72e0771db72a06a5c8474"
    },
    {
        "T":"?",
        "Model":"NeuralNovel\/Senzu-7B-v0.1-DPO",
        "Average":64.62,
        "ARC":66.72,
        "HellaSwag":84.34,
        "MMLU":62.12,
        "TruthfulQA":45.29,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"b3f8b6d9d500024ccbe2b2a19eb4850046e24851"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeontaek\/llama-2-13B-ensemble-v5",
        "Average":64.61,
        "ARC":62.63,
        "HellaSwag":83.06,
        "MMLU":59.49,
        "TruthfulQA":53.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"177a954f7362bf6d1b9e06dbf17e9afccd774f3e"
    },
    {
        "T":"?",
        "Model":"Weyaxi\/einstein-v2-test-model",
        "Average":64.61,
        "ARC":62.37,
        "HellaSwag":83.46,
        "MMLU":62.08,
        "TruthfulQA":50.52,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"2e31cc426945278f93a91b3a93dc5bf524fe0972"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Einstein-v2-7B",
        "Average":64.61,
        "ARC":62.37,
        "HellaSwag":83.46,
        "MMLU":62.08,
        "TruthfulQA":50.52,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"2e31cc426945278f93a91b3a93dc5bf524fe0972"
    },
    {
        "T":"\u2b55",
        "Model":"garage-bAInd\/Platypus-30B",
        "Average":64.61,
        "ARC":64.59,
        "HellaSwag":84.26,
        "MMLU":64.23,
        "TruthfulQA":45.35,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"c5d21054f8dd71099696bd7790df07ac54990f29"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NECOUDBFM\/Jellyfish",
        "Average":64.61,
        "ARC":63.31,
        "HellaSwag":83.19,
        "MMLU":58.6,
        "TruthfulQA":53.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"33e7aa13e855f0342d7e3173e78142bd5989c671"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/OpenOrca-Platypus2-13B",
        "Average":64.6,
        "ARC":62.8,
        "HellaSwag":83.15,
        "MMLU":59.39,
        "TruthfulQA":53.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":184,
        "Available on the hub":true,
        "Model Sha":"e7a40134f7eb687c6ab66d445dc7251257f8d391"
    },
    {
        "T":"?",
        "Model":"altomek\/CodeRosa-70B-AB1",
        "Average":64.6,
        "ARC":65.53,
        "HellaSwag":83.16,
        "MMLU":59.87,
        "TruthfulQA":49.85,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2ca29018ad4b6c7f92453b0a6a97ca9053e6415a"
    },
    {
        "T":"?",
        "Model":"kajdun\/viwaai-30b_v4",
        "Average":64.6,
        "ARC":63.48,
        "HellaSwag":84.19,
        "MMLU":57.47,
        "TruthfulQA":53.25,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":30.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9519ed20f06dd5a88e280ba6a8c5c9956213f10a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lilloukas\/Platypus-30B",
        "Average":64.59,
        "ARC":64.59,
        "HellaSwag":84.24,
        "MMLU":64.19,
        "TruthfulQA":45.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"979ad39b58a8e4a9419b7bc7a0dc8419f3912e71"
    },
    {
        "T":"\u2b55",
        "Model":"uukuguy\/speechless-llama2-13b",
        "Average":64.59,
        "ARC":62.2,
        "HellaSwag":81.88,
        "MMLU":58.65,
        "TruthfulQA":55.62,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"5341819accf229a625b163b5611aa973cf9f9718"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"s3nh\/Noromaid-Aeryth-7B",
        "Average":64.58,
        "ARC":56.74,
        "HellaSwag":78.62,
        "MMLU":57.29,
        "TruthfulQA":65.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"e87dbfaf98d6d9422f3a16b10c8005801b28b139"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"ldahee\/SLAL-0.1",
        "Average":64.57,
        "ARC":57.94,
        "HellaSwag":80.14,
        "MMLU":65.99,
        "TruthfulQA":54.22,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"315b4b492c861e9445712d8bc0d7b9245d7cdeac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/OpenOrca-Platypus2-13B",
        "Average":64.57,
        "ARC":62.88,
        "HellaSwag":83.15,
        "MMLU":59.27,
        "TruthfulQA":52.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":184,
        "Available on the hub":true,
        "Model Sha":"7e041f686d73bb991613c9b85aab737d218849c0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kaitchup\/Maixtchup-4x7b-QLoRA-SFT-UltraChat",
        "Average":64.56,
        "ARC":60.92,
        "HellaSwag":83.23,
        "MMLU":60.78,
        "TruthfulQA":53.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ee716b901ff5ee52fe20417c6a0a2f6aa28d3f38"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CausalLM\/14B-DPO-alpha",
        "Average":64.56,
        "ARC":58.11,
        "HellaSwag":79.38,
        "MMLU":66.62,
        "TruthfulQA":54.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":14.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"34bc2dd73ae5f8738e5bcaaa5591427675f7801f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beowolx\/MistralHermes-CodePro-7B-v1",
        "Average":64.56,
        "ARC":62.46,
        "HellaSwag":82.68,
        "MMLU":63.44,
        "TruthfulQA":49.67,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"a74a9fa5797b75262187fffa173948f1c03e2af4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Ba2han\/Cucumber-7b-10k",
        "Average":64.56,
        "ARC":60.41,
        "HellaSwag":83.75,
        "MMLU":63.1,
        "TruthfulQA":50.97,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"137a42ed37aa261f95b99ccd0f91952bc7656e1e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Zardos\/Kant-Test-0.1-Mistral-7B",
        "Average":64.55,
        "ARC":62.37,
        "HellaSwag":82.84,
        "MMLU":63.38,
        "TruthfulQA":49.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5989100fa82aaab0db2f8ed3e37a446126050ef9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-llama2-13b",
        "Average":64.55,
        "ARC":62.03,
        "HellaSwag":81.82,
        "MMLU":58.69,
        "TruthfulQA":55.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"c6362c4fc0dc03420e3c08454b2e7689e4e32d3a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Henk717\/chronoboros-33B",
        "Average":64.54,
        "ARC":63.91,
        "HellaSwag":85.0,
        "MMLU":59.44,
        "TruthfulQA":49.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"a4deca117c5fa48f2cdc49ed2e2596046201d688"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airoboros-33b-2.1",
        "Average":64.54,
        "ARC":63.65,
        "HellaSwag":84.97,
        "MMLU":57.37,
        "TruthfulQA":52.17,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"12ccd0e6c9ef12c7d3c2eab8266cd32c0b2f7683"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HiTZ\/alpaca-lora-65b-en-pt-es-ca",
        "Average":64.54,
        "ARC":65.02,
        "HellaSwag":84.88,
        "MMLU":62.19,
        "TruthfulQA":46.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":65.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"aa5bd88bd132925cf2dd5c44eceafdb5ed5e5be4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/Mistralpaca-7B",
        "Average":64.54,
        "ARC":62.03,
        "HellaSwag":83.44,
        "MMLU":59.5,
        "TruthfulQA":53.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"a118f6f3cb1121fb6ce916c24280874b4e2c09d1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"quantumaikr\/QuantumLM-70B-hf",
        "Average":64.53,
        "ARC":59.47,
        "HellaSwag":83.02,
        "MMLU":62.25,
        "TruthfulQA":53.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"e13dd23ae5e611e959b6c8d5bc47bf4fd37cd9d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-llama2-hermes-orca-platypus-13b",
        "Average":64.52,
        "ARC":60.92,
        "HellaSwag":83.5,
        "MMLU":59.39,
        "TruthfulQA":54.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f227ad33b16726b099e35e5dc47f4db1f22665a7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beowolx\/CodeNinja-1.0-OpenChat-7B",
        "Average":64.52,
        "ARC":63.48,
        "HellaSwag":83.65,
        "MMLU":63.77,
        "TruthfulQA":47.16,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":99,
        "Available on the hub":false,
        "Model Sha":"9934c04c767e6ae0f792712a060f02915391d4ec"
    },
    {
        "T":"?",
        "Model":"ShenaoZ\/0001_dpo_iter_2",
        "Average":64.52,
        "ARC":60.41,
        "HellaSwag":84.52,
        "MMLU":60.02,
        "TruthfulQA":53.11,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d9b4ce3898d7e95949fea0ebb846cc255e19df12"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"alnrg2arg\/test3_sft_4bit",
        "Average":64.51,
        "ARC":61.52,
        "HellaSwag":83.89,
        "MMLU":64.79,
        "TruthfulQA":47.83,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"45e1dcfb08c47a66c602aa5a3b37229ef69dcf41"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aari1995\/germeo-7b-laser",
        "Average":64.49,
        "ARC":60.75,
        "HellaSwag":82.81,
        "MMLU":60.57,
        "TruthfulQA":53.83,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"b7577f83a0af27e1a380efce4f993c25c33d8b33"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kevin009\/Llamafia",
        "Average":64.49,
        "ARC":66.13,
        "HellaSwag":82.08,
        "MMLU":61.81,
        "TruthfulQA":47.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0de1702faa89250ae329b3989c487fb0feb9e3f6"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/Synatra-7B-v0.3-RP",
        "Average":64.48,
        "ARC":62.2,
        "HellaSwag":82.29,
        "MMLU":60.8,
        "TruthfulQA":52.64,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":18,
        "Available on the hub":false,
        "Model Sha":"372f6e0ab2c20b93e0c42218f76a71a4f9bb282e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NovoCode\/Mistral-NeuralDPO",
        "Average":64.48,
        "ARC":66.04,
        "HellaSwag":84.69,
        "MMLU":63.92,
        "TruthfulQA":43.27,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"1b8ecfe53f090c6c7dffcb46a3fade8087ab0767"
    },
    {
        "T":"?",
        "Model":"INSAIT-Institute\/BgGPT-7B-Instruct-v0.2",
        "Average":64.47,
        "ARC":60.58,
        "HellaSwag":82.18,
        "MMLU":60.5,
        "TruthfulQA":54.63,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.29,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"e1aa1a3ff1bde9ed33fbfc83eb9a0391afc19424"
    },
    {
        "T":"\u2b55",
        "Model":"Mihaiii\/Metis-0.5",
        "Average":64.47,
        "ARC":62.63,
        "HellaSwag":83.77,
        "MMLU":62.16,
        "TruthfulQA":49.33,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"87df101c3909e6bc2b22e237d92f74118ab1909c"
    },
    {
        "T":"\u2b55",
        "Model":"kyujinpy\/SOLAR-Platypus-10.7B-v1",
        "Average":64.47,
        "ARC":61.69,
        "HellaSwag":84.23,
        "MMLU":60.37,
        "TruthfulQA":51.58,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e9314a1f1ca7f790491c177e7720fb14851ef603"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"amu\/r-zephyr-7b-beta-qlora",
        "Average":64.46,
        "ARC":63.05,
        "HellaSwag":85.38,
        "MMLU":63.1,
        "TruthfulQA":46.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"3948f437f08ebb9f0bc7da37cdead0cc3dd7a562"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mahiatlinux\/MasherAI-7B-v3",
        "Average":64.46,
        "ARC":63.99,
        "HellaSwag":82.19,
        "MMLU":64.04,
        "TruthfulQA":47.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d1395cb02d5de6ed10f8ed3dbc4a570fe426e651"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Emerhyst-20B",
        "Average":64.45,
        "ARC":61.69,
        "HellaSwag":84.98,
        "MMLU":56.98,
        "TruthfulQA":54.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub":38,
        "Available on the hub":true,
        "Model Sha":"e4c23af4f5dd88cb27d245e2bfc3b81db652632c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MLewd-L2-Chat-13B",
        "Average":64.45,
        "ARC":62.03,
        "HellaSwag":84.19,
        "MMLU":58.75,
        "TruthfulQA":52.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":29,
        "Available on the hub":true,
        "Model Sha":"6c66622a99c1bc73498aa6a15a59da825d875310"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SicariusSicariiStuff\/Tenebra_30B_Alpha01_FP16",
        "Average":64.45,
        "ARC":64.51,
        "HellaSwag":84.79,
        "MMLU":54.29,
        "TruthfulQA":54.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":32.53,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ad31f850f8c061d79a05aaa2419ec0f0baf62034"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/MetaMath-NeuralHermes-2.5-Mistral-7B-Ties",
        "Average":64.44,
        "ARC":62.46,
        "HellaSwag":82.89,
        "MMLU":62.25,
        "TruthfulQA":50.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b11bbd94238e1cc568c476844b1900c6e3facfa7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/manticore-30b-chat-pyg-alpha",
        "Average":64.4,
        "ARC":64.16,
        "HellaSwag":84.38,
        "MMLU":57.49,
        "TruthfulQA":51.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.53,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"0cff8e9718e57202171003d556d2e6630061879d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"juhwanlee\/gemma-7B-alpaca-case-0-2",
        "Average":64.4,
        "ARC":62.2,
        "HellaSwag":82.19,
        "MMLU":65.57,
        "TruthfulQA":47.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f1701204d496edcec40e97377dcf46da3060b100"
    },
    {
        "T":"?",
        "Model":"DreadPoor\/ToppyEvil-7B-slerp",
        "Average":64.4,
        "ARC":63.65,
        "HellaSwag":84.29,
        "MMLU":63.6,
        "TruthfulQA":46.06,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6c032a15a05cc8c639de2c36c761b35f1955a3af"
    },
    {
        "T":"?",
        "Model":"M4-ai\/Hercules-Qwen1.5-14B",
        "Average":64.4,
        "ARC":56.23,
        "HellaSwag":80.6,
        "MMLU":68.73,
        "TruthfulQA":52.03,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":14.17,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1467881f0621f2b3906461c7e656a7fbeddbfd45"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MXLewd-L2-20B",
        "Average":64.39,
        "ARC":63.23,
        "HellaSwag":85.33,
        "MMLU":57.36,
        "TruthfulQA":51.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"ac279478abd9ddb8d1f5adcc548be0287b963adf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SeaLLMs\/SeaLLM-7B-v2",
        "Average":64.38,
        "ARC":61.86,
        "HellaSwag":82.34,
        "MMLU":62.15,
        "TruthfulQA":51.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.38,
        "Hub":49,
        "Available on the hub":false,
        "Model Sha":"9fddeaa79d3862ac4c2e3eab647f11e8d88f2920"
    },
    {
        "T":"?",
        "Model":"andysalerno\/rainbowfish-v7",
        "Average":64.38,
        "ARC":61.95,
        "HellaSwag":82.52,
        "MMLU":63.26,
        "TruthfulQA":49.78,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"896039c526d6d5977fb7943743666b4dc2563b3e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"microsoft\/Orca-2-13b",
        "Average":64.37,
        "ARC":60.92,
        "HellaSwag":79.85,
        "MMLU":60.3,
        "TruthfulQA":56.42,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":642,
        "Available on the hub":true,
        "Model Sha":"2539ff53e6baa4cc603774ad5a2d646f4041ea4e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/dolphin-2.0-mistral-7b",
        "Average":64.37,
        "ARC":59.22,
        "HellaSwag":80.26,
        "MMLU":56.9,
        "TruthfulQA":61.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":129,
        "Available on the hub":false,
        "Model Sha":"c673387016c622fd0a707426953c03957398bc37"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azazelle\/Half-NSFW_Noromaid-7b",
        "Average":64.36,
        "ARC":62.8,
        "HellaSwag":84.82,
        "MMLU":63.76,
        "TruthfulQA":46.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"378e5fb671d593432ce6c7ddc19ac8e04a490df8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fhai50032\/xLakeChat",
        "Average":64.35,
        "ARC":62.54,
        "HellaSwag":82.66,
        "MMLU":59.19,
        "TruthfulQA":53.02,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9aa5bda433ab1619afb2fae5d00e8762e2669129"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.0-7B",
        "Average":64.34,
        "ARC":62.2,
        "HellaSwag":84.1,
        "MMLU":64.14,
        "TruthfulQA":46.94,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"9609a969ba6429b84e538d96afac55eb133a9983"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vmajor\/Orca2-13B-selfmerge-39B",
        "Average":64.34,
        "ARC":60.84,
        "HellaSwag":79.84,
        "MMLU":60.32,
        "TruthfulQA":56.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"ms-pl",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"7a9e6775716a3947d0e40842b5d61753bc0551ac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vmajor\/Orca2-13B-selfmerge-26B",
        "Average":64.34,
        "ARC":60.84,
        "HellaSwag":79.84,
        "MMLU":60.32,
        "TruthfulQA":56.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"ms-pl",
        "#Params (B)":13.02,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"46cdde5be7e3c48ada1bd3143ad593eecfb641e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SeaLLMs\/SeaLLM-7B-v2",
        "Average":64.34,
        "ARC":62.03,
        "HellaSwag":82.32,
        "MMLU":61.89,
        "TruthfulQA":51.11,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.38,
        "Hub":49,
        "Available on the hub":false,
        "Model Sha":"8bb693e108dc92efdd608767144bc0232721b18a"
    },
    {
        "T":"?",
        "Model":"LLMs\/WizardLM-30B-V1.0",
        "Average":64.34,
        "ARC":62.54,
        "HellaSwag":83.27,
        "MMLU":59.05,
        "TruthfulQA":52.49,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":32.32,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"75318440dba949804d6263d368e1f29a94ea7c5f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/WizardLM-30B-fp16",
        "Average":64.33,
        "ARC":62.54,
        "HellaSwag":83.28,
        "MMLU":59.03,
        "TruthfulQA":52.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"465f87a243969963f25ae6cf8f8d2de6c0898bbe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hedronstone\/OpenHermes-7B-Symbolic",
        "Average":64.33,
        "ARC":63.14,
        "HellaSwag":82.73,
        "MMLU":62.62,
        "TruthfulQA":48.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"23eb76553aa37cd48c1f2d8a314d78fd3ead53f6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hedronstone\/OpenHermes-7B-Reasoner",
        "Average":64.33,
        "ARC":63.14,
        "HellaSwag":82.73,
        "MMLU":62.62,
        "TruthfulQA":48.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d26f2defbf9f40a65dbb2ead08c79cd61096ed08"
    },
    {
        "T":"?",
        "Model":"fhai50032\/xLakeChat",
        "Average":64.32,
        "ARC":62.37,
        "HellaSwag":82.64,
        "MMLU":59.32,
        "TruthfulQA":52.96,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9aa5bda433ab1619afb2fae5d00e8762e2669129"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"microsoft\/Orca-2-13b",
        "Average":64.32,
        "ARC":60.67,
        "HellaSwag":79.81,
        "MMLU":60.37,
        "TruthfulQA":56.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":642,
        "Available on the hub":true,
        "Model Sha":"2539ff53e6baa4cc603774ad5a2d646f4041ea4e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/Orca-2-13b-f16",
        "Average":64.32,
        "ARC":60.67,
        "HellaSwag":79.81,
        "MMLU":60.37,
        "TruthfulQA":56.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b29c52ea0757c460e83592e55ea89e016cef3549"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Changgil\/K2S3-Mistral-7b-v1.0",
        "Average":64.31,
        "ARC":60.75,
        "HellaSwag":81.87,
        "MMLU":63.13,
        "TruthfulQA":51.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.37,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6a11ae478588e7633b8d45fbcbf31a542e259c81"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/hippogriff-30b-chat",
        "Average":64.3,
        "ARC":64.51,
        "HellaSwag":85.2,
        "MMLU":59.09,
        "TruthfulQA":48.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"64c10edf5312cd13704925b07413882d9e94c7a0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MLewd-Chat-v2-13B",
        "Average":64.29,
        "ARC":61.86,
        "HellaSwag":83.81,
        "MMLU":57.0,
        "TruthfulQA":54.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":18,
        "Available on the hub":true,
        "Model Sha":"f6181961a6a2f9ca534e1a8907b4a4459be6b6bd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Mistral-11B-TestBench7",
        "Average":64.29,
        "ARC":63.31,
        "HellaSwag":82.86,
        "MMLU":64.09,
        "TruthfulQA":46.91,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":11.0,
        "Hub":15,
        "Available on the hub":false,
        "Model Sha":"3d4d99f90ec582e0d532e11f6da419d6b962c536"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hiyouga\/Qwen-14B-Chat-LLaMAfied",
        "Average":64.29,
        "ARC":57.51,
        "HellaSwag":82.11,
        "MMLU":65.57,
        "TruthfulQA":51.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":14.17,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"29e92e74dca4a79aa8c2c451287ff97c4dccb323"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chanwit\/flux-base-optimized",
        "Average":64.29,
        "ARC":65.53,
        "HellaSwag":81.76,
        "MMLU":59.84,
        "TruthfulQA":50.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ce9f1f49559007d5b81249fd1ca3eb8be088fe43"
    },
    {
        "T":"?",
        "Model":"seungduk\/KoSOLAR-10.7B-v0.1",
        "Average":64.29,
        "ARC":62.03,
        "HellaSwag":84.54,
        "MMLU":65.56,
        "TruthfulQA":45.03,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.86,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"a4ddde9b0d06f340ff9c29777b4bfd883700c6cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NovoCode\/NeuralPaca-7b",
        "Average":64.29,
        "ARC":62.8,
        "HellaSwag":83.01,
        "MMLU":63.02,
        "TruthfulQA":48.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4e02c1c08c3ce16db8df2a07db559eaab46a3ac4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"juhwanlee\/experiment2-non-cause-v1",
        "Average":64.28,
        "ARC":61.52,
        "HellaSwag":83.71,
        "MMLU":63.79,
        "TruthfulQA":48.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c911ffa558630d04c97f5d39e1d2e5412cd4346f"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/Chat-AYB-Nova-13B",
        "Average":64.28,
        "ARC":62.97,
        "HellaSwag":84.28,
        "MMLU":58.58,
        "TruthfulQA":51.28,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"942af4d59533af09cf9ba13d1e369b8e871a0a4b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jikaixuan\/test_merged_model",
        "Average":64.27,
        "ARC":61.6,
        "HellaSwag":83.1,
        "MMLU":63.73,
        "TruthfulQA":48.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"611ec6f78292124008a276ce5c2723e53d31a1e2"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"internlm\/internlm-20b",
        "Average":64.27,
        "ARC":60.49,
        "HellaSwag":82.13,
        "MMLU":61.85,
        "TruthfulQA":52.61,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.0,
        "Hub":72,
        "Available on the hub":false,
        "Model Sha":"b8825fe3394608fe84f0f5eb6471454384fb83aa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"concedo\/Vicuzard-30B-Uncensored",
        "Average":64.27,
        "ARC":62.97,
        "HellaSwag":83.68,
        "MMLU":58.16,
        "TruthfulQA":52.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"e2329c05a6e59660ba3cbcc01adf30a78f852594"
    },
    {
        "T":"?",
        "Model":"Azazelle\/Mocha-Dare-7b-ex",
        "Average":64.27,
        "ARC":61.26,
        "HellaSwag":81.6,
        "MMLU":60.77,
        "TruthfulQA":53.44,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4cedadeb455e4507b4fc1a4ac14b0340ed43cd05"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"upstage\/SOLAR-10.7B-v1.0",
        "Average":64.27,
        "ARC":61.95,
        "HellaSwag":84.6,
        "MMLU":65.48,
        "TruthfulQA":45.04,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":216,
        "Available on the hub":true,
        "Model Sha":"6e2783822f35c376ea96852fe479faa6a8bf09cb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Uncensored-Frank-33B",
        "Average":64.26,
        "ARC":62.12,
        "HellaSwag":83.3,
        "MMLU":57.57,
        "TruthfulQA":54.03,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":32.32,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"1c1f4e9256ac2be145a9106863ee9f2e9d701e74"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dfurman\/falcon-40b-openassistant-peft",
        "Average":64.25,
        "ARC":62.63,
        "HellaSwag":85.59,
        "MMLU":57.77,
        "TruthfulQA":51.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":40.0,
        "Hub":39,
        "Available on the hub":false,
        "Model Sha":"3d5084b6fbcb9f9f36493d9fd1e3795b0b9860f0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/Mistral-7B-Instruct-Aya-101",
        "Average":64.25,
        "ARC":59.13,
        "HellaSwag":83.2,
        "MMLU":61.96,
        "TruthfulQA":52.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"7724e49d560d6b030e67aea0fe319020103929c0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeuralNovel\/Gecko-7B-v0.1-DPO",
        "Average":64.24,
        "ARC":56.74,
        "HellaSwag":82.38,
        "MMLU":60.42,
        "TruthfulQA":57.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"7a377ce18d900f287222895973dd866fd53930f7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Medilora\/medilora-mistral-7b",
        "Average":64.24,
        "ARC":61.69,
        "HellaSwag":83.13,
        "MMLU":62.22,
        "TruthfulQA":49.91,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"b6512d2a2202e685da461ff876a1ffb707034c97"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/OpenOrca-Platypus2-13B-QLoRA-0.80-epoch",
        "Average":64.24,
        "ARC":62.37,
        "HellaSwag":82.99,
        "MMLU":59.38,
        "TruthfulQA":52.2,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"39ae03b77b4f1d453b02468ce6bb4ddeb6526b77"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-TotSirocco-7b",
        "Average":64.24,
        "ARC":62.03,
        "HellaSwag":84.23,
        "MMLU":64.19,
        "TruthfulQA":46.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"824e3a4738818142374721306ce85b83770de24b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"upaya07\/Birbal-7B-V1",
        "Average":64.23,
        "ARC":62.88,
        "HellaSwag":84.88,
        "MMLU":63.71,
        "TruthfulQA":45.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"6623e1ec77f20f7c152e86e99b49e501d0133b13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chanwit\/flux-base-optimized",
        "Average":64.23,
        "ARC":65.44,
        "HellaSwag":81.74,
        "MMLU":59.74,
        "TruthfulQA":50.02,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ce9f1f49559007d5b81249fd1ca3eb8be088fe43"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Zardos\/Kant-Test-0.1-Mistral-7B",
        "Average":64.23,
        "ARC":61.77,
        "HellaSwag":82.89,
        "MMLU":62.86,
        "TruthfulQA":49.4,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5989100fa82aaab0db2f8ed3e37a446126050ef9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"huggyllama\/llama-65b",
        "Average":64.23,
        "ARC":63.48,
        "HellaSwag":86.09,
        "MMLU":63.93,
        "TruthfulQA":43.43,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.29,
        "Hub":64,
        "Available on the hub":true,
        "Model Sha":"49707c5313d34d1c5a846e29cf2a2a650c22c8ee"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"huggingface\/llama-65b",
        "Average":64.23,
        "ARC":63.48,
        "HellaSwag":86.09,
        "MMLU":63.93,
        "TruthfulQA":43.43,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.29,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4ae2e56610e8b9b9a78472708390668e9096b4f9"
    },
    {
        "T":"?",
        "Model":"grayhacker91\/gemma-7b-open-platypus-commercial",
        "Average":64.23,
        "ARC":62.8,
        "HellaSwag":81.65,
        "MMLU":58.94,
        "TruthfulQA":53.54,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"50a1fda9a90009f9de508f08bf1b192ef195667a"
    },
    {
        "T":"?",
        "Model":"nlpguy\/StockFuseChat",
        "Average":64.23,
        "ARC":63.14,
        "HellaSwag":84.26,
        "MMLU":63.95,
        "TruthfulQA":45.57,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4c9556c99e5e9df6696186d9e741a0765710c436"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/Chat-AYB-Platypus2-13B",
        "Average":64.22,
        "ARC":60.49,
        "HellaSwag":84.03,
        "MMLU":57.83,
        "TruthfulQA":54.52,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5a54eb9d5a66df4720ec52422f5627ccd94d5fd6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-XS-v1-3-yarn-128K",
        "Average":64.21,
        "ARC":61.6,
        "HellaSwag":82.96,
        "MMLU":62.1,
        "TruthfulQA":50.2,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"72d393d13f1bd26442e59993c57840b91ff6f6fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gaodrew\/gaodrew-llama-30b-instruct-2048-Open-Platypus-100steps",
        "Average":64.21,
        "ARC":61.52,
        "HellaSwag":84.06,
        "MMLU":60.23,
        "TruthfulQA":51.05,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1114ff08ed15ef417502da58f0237d2f6650c9ce"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/GenAI-Nova-13B",
        "Average":64.2,
        "ARC":62.29,
        "HellaSwag":83.27,
        "MMLU":59.47,
        "TruthfulQA":51.79,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0ce62a64ca53cd5feb18f523a96dd3be86e6513d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"yanolja\/KoSOLAR-10.7B-v0.2",
        "Average":64.19,
        "ARC":61.35,
        "HellaSwag":82.63,
        "MMLU":64.85,
        "TruthfulQA":47.94,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.8,
        "Hub":27,
        "Available on the hub":true,
        "Model Sha":"ca6148593cca082392faa5d1b0b72995b672eddb"
    },
    {
        "T":"?",
        "Model":"Azazelle\/Moko-DARE",
        "Average":64.19,
        "ARC":60.58,
        "HellaSwag":82.08,
        "MMLU":61.94,
        "TruthfulQA":52.17,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b95e6fa7ed89fdb901a0e1fba45c94f2154a0c2f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/HelpSteer-filtered-Solar-Instruct",
        "Average":64.19,
        "ARC":63.14,
        "HellaSwag":83.05,
        "MMLU":64.32,
        "TruthfulQA":46.23,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"52b80cc07c0c2a2bb54561a9c3d556231ca7344d"
    },
    {
        "T":"?",
        "Model":"FuseAI\/OpenChat-3.5-7B-Solar",
        "Average":64.19,
        "ARC":62.97,
        "HellaSwag":84.19,
        "MMLU":63.94,
        "TruthfulQA":45.65,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"dc892da6642efb20ea88c3804bf75c0e8759139f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/mistral-7b-jondurbin-truthy-dpo",
        "Average":64.18,
        "ARC":60.75,
        "HellaSwag":83.89,
        "MMLU":63.65,
        "TruthfulQA":48.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d6705a82098d5f01fb0effbba395c818ad9bf5b4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"unaidedelf87777\/wizard-mistral-v0.1",
        "Average":64.18,
        "ARC":61.77,
        "HellaSwag":83.51,
        "MMLU":63.99,
        "TruthfulQA":47.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b66724f8195e7b76289f8f3f72a98392557c46ad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/experiment2-cause",
        "Average":64.18,
        "ARC":61.26,
        "HellaSwag":83.4,
        "MMLU":63.91,
        "TruthfulQA":48.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4d6f888de5ba44d5ca8ef766c54e92103a0afe16"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ArianAskari\/SOLID_SFT-WoDPO-WoMixQ",
        "Average":64.17,
        "ARC":59.64,
        "HellaSwag":81.69,
        "MMLU":60.1,
        "TruthfulQA":55.25,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5a31a78c59bd70f66ffafb91f2a507286354fb72"
    },
    {
        "T":"?",
        "Model":"fhai50032\/SamChat",
        "Average":64.17,
        "ARC":62.2,
        "HellaSwag":81.88,
        "MMLU":59.7,
        "TruthfulQA":52.89,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a8b9d9019c12775ce126b49bb25ef63b7cb05a93"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Enoch\/llama-65b-hf",
        "Average":64.17,
        "ARC":63.31,
        "HellaSwag":86.09,
        "MMLU":63.84,
        "TruthfulQA":43.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"7a7b897ab10b3d82d1e7e6fbcd2159d70b4586cf"
    },
    {
        "T":"?",
        "Model":"FuseAI\/OpenChat-3.5-7B-Mixtral",
        "Average":64.17,
        "ARC":62.8,
        "HellaSwag":84.24,
        "MMLU":63.95,
        "TruthfulQA":45.68,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"566e2d1a010864875443e9b91d4b1c78b216b9d8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fhai50032\/SamChat",
        "Average":64.16,
        "ARC":62.03,
        "HellaSwag":81.95,
        "MMLU":59.78,
        "TruthfulQA":52.9,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a8b9d9019c12775ce126b49bb25ef63b7cb05a93"
    },
    {
        "T":"?",
        "Model":"Azazelle\/Mocha-SR-7b-ex",
        "Average":64.16,
        "ARC":59.81,
        "HellaSwag":81.24,
        "MMLU":60.87,
        "TruthfulQA":54.72,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"18919ffe9ea22d5cd29cb582043bbc806098b9d7"
    },
    {
        "T":"\u2b55",
        "Model":"wenge-research\/yayi-70b-llama2",
        "Average":64.16,
        "ARC":60.67,
        "HellaSwag":83.93,
        "MMLU":64.42,
        "TruthfulQA":47.63,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":70.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"2799b262292f78f7c3965a1410d0ad6211438603"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-llama2-hermes-orca-platypus-wizardlm-13b",
        "Average":64.16,
        "ARC":59.64,
        "HellaSwag":82.7,
        "MMLU":58.3,
        "TruthfulQA":56.0,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"4410d8a20871927e9fe981c01bc8314b451b2fcd"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/zephyr-alpha-Nebula-v2-7B",
        "Average":64.16,
        "ARC":58.62,
        "HellaSwag":83.05,
        "MMLU":56.68,
        "TruthfulQA":58.28,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e8f1fd1acceda7fb662340f5afe312a7ef030374"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"upaya07\/Birbal-7B-V1",
        "Average":64.14,
        "ARC":62.8,
        "HellaSwag":84.83,
        "MMLU":63.59,
        "TruthfulQA":45.34,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"6623e1ec77f20f7c152e86e99b49e501d0133b13"
    },
    {
        "T":"\u2b55",
        "Model":"uukuguy\/speechless-llama2-hermes-orca-platypus-wizardlm-13b",
        "Average":64.13,
        "ARC":59.56,
        "HellaSwag":82.6,
        "MMLU":58.35,
        "TruthfulQA":56.02,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"4410d8a20871927e9fe981c01bc8314b451b2fcd"
    },
    {
        "T":"?",
        "Model":"FuseAI\/FuseChat-7B-VaRM",
        "Average":64.13,
        "ARC":62.88,
        "HellaSwag":84.25,
        "MMLU":63.71,
        "TruthfulQA":45.67,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":64,
        "Available on the hub":false,
        "Model Sha":"5d1e28bac6efc675549060b3babb64945b27d25c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Unholy-v1-12L-13B",
        "Average":64.12,
        "ARC":63.57,
        "HellaSwag":83.75,
        "MMLU":58.08,
        "TruthfulQA":51.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":37,
        "Available on the hub":true,
        "Model Sha":"ee25c078f08b0812d82597afa3f5e877c19a5c83"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SuperAGI\/SAM",
        "Average":64.12,
        "ARC":59.39,
        "HellaSwag":82.31,
        "MMLU":62.15,
        "TruthfulQA":52.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":27,
        "Available on the hub":false,
        "Model Sha":"ce1fb6a278121df73eee5d7d39dc0d30b214a1b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-XS-v1.0",
        "Average":64.12,
        "ARC":61.43,
        "HellaSwag":83.82,
        "MMLU":64.1,
        "TruthfulQA":47.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"a581ab1793366ff2d5f3c966ff0e7b8b1149d775"
    },
    {
        "T":"?",
        "Model":"FuseAI\/FuseChat-7B-TA",
        "Average":64.11,
        "ARC":62.54,
        "HellaSwag":84.22,
        "MMLU":63.96,
        "TruthfulQA":45.74,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"9862833e1c59df274ff426fb09638faa2e0bc9f0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Mythical-Destroyer-V2-L2-13B",
        "Average":64.11,
        "ARC":59.3,
        "HellaSwag":82.66,
        "MMLU":57.39,
        "TruthfulQA":57.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"cbc8b2e4a3beafc311b9e61f8fa9f7526a77c360"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/2x-LoRA-Assemble-Nova-13B",
        "Average":64.1,
        "ARC":62.63,
        "HellaSwag":83.24,
        "MMLU":58.64,
        "TruthfulQA":51.88,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2a344b91b28ce4d0bd48b9b5a6cc87b71123eab5"
    },
    {
        "T":"\u2b55",
        "Model":"bhenrym14\/mistral-7b-platypus-fp16",
        "Average":64.1,
        "ARC":63.05,
        "HellaSwag":84.15,
        "MMLU":64.11,
        "TruthfulQA":45.07,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"d836a261afa0871d3734a7dfd1a28dc23c173ea7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Fewshot-Metamath-OrcaVicuna-Mistral",
        "Average":64.1,
        "ARC":59.64,
        "HellaSwag":81.82,
        "MMLU":61.69,
        "TruthfulQA":53.23,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"42ac13a68c242f7aa1ffb9385871fc3ae7d8415d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fhai50032\/SamCoder-TxC",
        "Average":64.09,
        "ARC":62.37,
        "HellaSwag":81.93,
        "MMLU":59.68,
        "TruthfulQA":52.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"17dcf2e98fa91317316b05f5ff27ae24b31139aa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-XS-v1-3-yarn-128K",
        "Average":64.08,
        "ARC":61.09,
        "HellaSwag":82.95,
        "MMLU":62.15,
        "TruthfulQA":50.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"0f5977a5d2fa791359dc92eb1574b6112e709cad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-TotSirocco-7b",
        "Average":64.08,
        "ARC":62.2,
        "HellaSwag":84.28,
        "MMLU":63.8,
        "TruthfulQA":46.04,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"824e3a4738818142374721306ce85b83770de24b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Frostwind-v2.1-m7",
        "Average":64.08,
        "ARC":61.77,
        "HellaSwag":83.77,
        "MMLU":63.83,
        "TruthfulQA":46.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a6d8e1ec723f87e969f0ece1a2c9223665a34927"
    },
    {
        "T":"?",
        "Model":"FuseAI\/FuseChat-7B-Slerp",
        "Average":64.08,
        "ARC":62.63,
        "HellaSwag":84.17,
        "MMLU":63.9,
        "TruthfulQA":45.62,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"19be316337d21c8ba0fb8a15e19257fc814e6a3c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SkunkworksAI\/Mistralic-7B-1",
        "Average":64.08,
        "ARC":60.84,
        "HellaSwag":82.29,
        "MMLU":60.8,
        "TruthfulQA":52.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":26,
        "Available on the hub":false,
        "Model Sha":"ebf138de4fb7a57f0d187ad0ab43abd6b35bfb62"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Venomia-m7",
        "Average":64.07,
        "ARC":63.14,
        "HellaSwag":84.0,
        "MMLU":60.06,
        "TruthfulQA":49.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"46d997c522776af0236b254bd4c5f071b39a06a0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/experiment2-cause-non",
        "Average":64.07,
        "ARC":61.09,
        "HellaSwag":83.72,
        "MMLU":64.13,
        "TruthfulQA":47.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3236726d4a5c4a3e18a8eedf35593bf4b1c14b8b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-code-mistral-7b-v1.0",
        "Average":64.06,
        "ARC":61.18,
        "HellaSwag":83.77,
        "MMLU":63.4,
        "TruthfulQA":47.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":15,
        "Available on the hub":false,
        "Model Sha":"43dea8e97d05f2e4358415b9a95a1b327c1f5804"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeverSleep\/Noromaid-7b-v0.2",
        "Average":64.06,
        "ARC":62.12,
        "HellaSwag":84.92,
        "MMLU":63.1,
        "TruthfulQA":46.09,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"bc35358ec19cf0335642228538a83bb306c0e074"
    },
    {
        "T":"?",
        "Model":"andysalerno\/rainbowfish-7B-v10",
        "Average":64.06,
        "ARC":61.18,
        "HellaSwag":82.33,
        "MMLU":63.26,
        "TruthfulQA":49.45,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"22a4cd7ecfdafb957ba2233b9c06fccd70663cfa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AetherResearch\/Cerebrum-1.0-7b",
        "Average":64.05,
        "ARC":61.6,
        "HellaSwag":84.56,
        "MMLU":63.56,
        "TruthfulQA":46.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":48,
        "Available on the hub":false,
        "Model Sha":"da3d8699055cd5f49626613af771473ae447e082"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fhai50032\/SamCoder-TxC",
        "Average":64.05,
        "ARC":62.12,
        "HellaSwag":81.85,
        "MMLU":59.83,
        "TruthfulQA":52.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"17dcf2e98fa91317316b05f5ff27ae24b31139aa"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.2-7B",
        "Average":64.04,
        "ARC":61.77,
        "HellaSwag":84.11,
        "MMLU":64.38,
        "TruthfulQA":45.92,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"f4d471d7a9447d0969a58d5b3146d50cfa3005b3"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/2x-LoRA-Assemble-Platypus2-13B",
        "Average":64.04,
        "ARC":60.58,
        "HellaSwag":82.56,
        "MMLU":58.25,
        "TruthfulQA":54.77,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f147bf8428c174d1dc0332da626d4b039690ceab"
    },
    {
        "T":"?",
        "Model":"sethuiyer\/CodeCalc-Mistral-7B",
        "Average":64.04,
        "ARC":61.95,
        "HellaSwag":83.64,
        "MMLU":62.78,
        "TruthfulQA":47.79,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e03e7b8e6ea737f565848caaf3467b75b646c878"
    },
    {
        "T":"?",
        "Model":"kaist-ai\/mistral-orpo-beta",
        "Average":64.04,
        "ARC":61.18,
        "HellaSwag":84.03,
        "MMLU":63.26,
        "TruthfulQA":47.69,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"0b457487cd34991f5de81a941a1b56f9673bf38b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-7B-v1.4",
        "Average":64.04,
        "ARC":60.41,
        "HellaSwag":82.87,
        "MMLU":60.98,
        "TruthfulQA":51.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"53a5249ee9e5b2327de81f09c26a4577dea9260b"
    },
    {
        "T":"?",
        "Model":"Gille\/StrangeMerges_48-7B-dare_ties",
        "Average":64.03,
        "ARC":60.92,
        "HellaSwag":80.13,
        "MMLU":49.51,
        "TruthfulQA":65.55,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d19b545a9dda748b0b6edf2dffdb0189d3510088"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeverSleep\/Noromaid-7b-v0.2",
        "Average":64.02,
        "ARC":62.03,
        "HellaSwag":84.97,
        "MMLU":62.99,
        "TruthfulQA":46.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"bc35358ec19cf0335642228538a83bb306c0e074"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Radu1999\/Mistral-Instruct-Ukrainian-SFT",
        "Average":64.02,
        "ARC":57.85,
        "HellaSwag":83.12,
        "MMLU":60.95,
        "TruthfulQA":54.14,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"26d1f9e8efdd4a471698cd404ac5d7415e8ac80e"
    },
    {
        "T":"?",
        "Model":"Walmart-the-bag\/Influxient-4x13B",
        "Average":64.01,
        "ARC":61.26,
        "HellaSwag":83.42,
        "MMLU":57.25,
        "TruthfulQA":54.1,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"a06acd48979617eb1af25ede71b937767889218b"
    },
    {
        "T":"?",
        "Model":"andysalerno\/rainbowfish-7B-v9",
        "Average":64.01,
        "ARC":61.77,
        "HellaSwag":82.43,
        "MMLU":63.0,
        "TruthfulQA":48.82,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c1b344f0efaacd2309d22dcbe4358a00bdd50f15"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"camel-ai\/CAMEL-33B-Combined-Data",
        "Average":64.0,
        "ARC":62.97,
        "HellaSwag":83.83,
        "MMLU":58.98,
        "TruthfulQA":50.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"62c74e7531625c1383bbbdc7c8346a996e9d1e21"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fionazhang\/mistral-experiment-6-merge",
        "Average":63.99,
        "ARC":63.82,
        "HellaSwag":84.25,
        "MMLU":62.91,
        "TruthfulQA":44.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2a6525f8b5c6d02ef78e716ccb37c6ef1bb1a26d"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/storytime-13b",
        "Average":63.99,
        "ARC":62.03,
        "HellaSwag":83.96,
        "MMLU":57.48,
        "TruthfulQA":52.5,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":19,
        "Available on the hub":true,
        "Model Sha":"233568319a636b6a7b02a4def2c51d08a3e0fbfc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SCE\/Mistral-7B-math-ia3-tuned",
        "Average":63.99,
        "ARC":57.25,
        "HellaSwag":80.79,
        "MMLU":59.83,
        "TruthfulQA":58.07,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"49e50484f55d4d588f57b9d61becfffd6d5eaffe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"freeCS-dot-org\/Zero-7B-test-3",
        "Average":63.97,
        "ARC":64.25,
        "HellaSwag":79.85,
        "MMLU":53.49,
        "TruthfulQA":58.3,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ddcd86b0ef66dd8b7d7b9418b88f3fbc1cfdc828"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/duplicitous-slurpbeast-13b",
        "Average":63.97,
        "ARC":62.12,
        "HellaSwag":83.92,
        "MMLU":57.53,
        "TruthfulQA":52.33,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"88dc61b7afebf2220ca42898e1286c59961ed440"
    },
    {
        "T":"?",
        "Model":"seb-c\/Psydestroyer-20B",
        "Average":63.97,
        "ARC":60.32,
        "HellaSwag":85.17,
        "MMLU":55.56,
        "TruthfulQA":54.83,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":19.99,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6a8e7636f7546c0aae531e2c3b76a0653ea6858d"
    },
    {
        "T":"?",
        "Model":"MRAIRR\/mini_7B_dare_v1",
        "Average":63.97,
        "ARC":61.77,
        "HellaSwag":79.91,
        "MMLU":59.55,
        "TruthfulQA":54.64,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"49c083289b20fbe14e7002fa71eed01127b2fbf5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dsvv-cair\/alpaca-cleaned-llama-30b-bf16",
        "Average":63.96,
        "ARC":61.77,
        "HellaSwag":85.06,
        "MMLU":57.52,
        "TruthfulQA":51.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"2424b6346e9e8fd749b9a6734f5d7125b5926daf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MetaIX\/GPT4-X-Alpasta-30b",
        "Average":63.96,
        "ARC":63.05,
        "HellaSwag":83.56,
        "MMLU":57.71,
        "TruthfulQA":51.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":64,
        "Available on the hub":true,
        "Model Sha":"1a0d1d72a40946463fb4a9780207da19bfecc38b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"garage-bAInd\/Stable-Platypus2-13B",
        "Average":63.96,
        "ARC":62.71,
        "HellaSwag":82.29,
        "MMLU":58.3,
        "TruthfulQA":52.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub":19,
        "Available on the hub":true,
        "Model Sha":"0e54aa49c24617e30a23a20c0c5da61419b9fe68"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MM-ReMM-L2-20B",
        "Average":63.95,
        "ARC":60.84,
        "HellaSwag":85.18,
        "MMLU":56.45,
        "TruthfulQA":53.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"37869800c15fb37d017ea83bb50fec6d6141f6ba"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Luban-Platypus2-13B-QLora-0.80-epoch",
        "Average":63.94,
        "ARC":60.24,
        "HellaSwag":82.22,
        "MMLU":58.03,
        "TruthfulQA":55.26,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"15a99bc147cf9b744cbab7a7c8c5f232cd0c8d10"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/OpenOrca-Platypus2-13B-GPTQ",
        "Average":63.93,
        "ARC":62.54,
        "HellaSwag":82.67,
        "MMLU":58.56,
        "TruthfulQA":51.93,
        "Type":"fine-tuned",
        "Precision":"None",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":2.03,
        "Hub":40,
        "Available on the hub":true,
        "Model Sha":"0fa9a56066656fbc94e3ec088bc900fd1d4d38e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ArianAskari\/SOLID-SFT-WoDPO-MixQV2-Zephyr-7b-beta",
        "Average":63.92,
        "ARC":59.73,
        "HellaSwag":81.72,
        "MMLU":60.47,
        "TruthfulQA":53.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ced2cbdeef8389e754a8f4895b70032580d54b99"
    },
    {
        "T":"?",
        "Model":"andysalerno\/rainbowfish-v6",
        "Average":63.91,
        "ARC":61.95,
        "HellaSwag":82.51,
        "MMLU":62.79,
        "TruthfulQA":48.37,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2b62fc1c6f1105c21ec96f958f0d16d2197517cc"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"yanolja\/KoSOLAR-10.7B-v0.3",
        "Average":63.9,
        "ARC":62.8,
        "HellaSwag":83.73,
        "MMLU":64.51,
        "TruthfulQA":44.57,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.8,
        "Hub":32,
        "Available on the hub":true,
        "Model Sha":"4b30efea87be24e22eac00fd45f72388e981576e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Steelskull\/Aethora-7b-v1",
        "Average":63.9,
        "ARC":59.47,
        "HellaSwag":79.32,
        "MMLU":61.95,
        "TruthfulQA":54.87,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"86864ce7a36843e4f9b5aee77064c1e639050809"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"IkariDev\/Athena-tmp",
        "Average":63.89,
        "ARC":59.22,
        "HellaSwag":82.13,
        "MMLU":58.87,
        "TruthfulQA":55.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a8d08541b0b1c1123d51867a594dce60c241ec34"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/PsyMedRP-v1-20B",
        "Average":63.89,
        "ARC":60.49,
        "HellaSwag":83.94,
        "MMLU":56.68,
        "TruthfulQA":54.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub":28,
        "Available on the hub":true,
        "Model Sha":"78188101b6331d9e61ef80f0971d715de100b44a"
    },
    {
        "T":"?",
        "Model":"Weyaxi\/Einstein-v5-v0.2-7B",
        "Average":63.88,
        "ARC":60.92,
        "HellaSwag":80.99,
        "MMLU":61.02,
        "TruthfulQA":52.59,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"1b5e6e0bbefff2b7bbc15d11c15fa1ac3696fabd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddyEA\/openbuddy-llama-30b-v7.1-bf16",
        "Average":63.87,
        "ARC":62.46,
        "HellaSwag":82.3,
        "MMLU":58.15,
        "TruthfulQA":52.57,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":32.35,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"85f7ad9d6ff016312262a47d45ffd07dee54aab0"
    },
    {
        "T":"?",
        "Model":"Evillain\/StarDust_20B_v0.2",
        "Average":63.87,
        "ARC":61.01,
        "HellaSwag":83.76,
        "MMLU":59.29,
        "TruthfulQA":51.43,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":19.99,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"af50ba3e261a87df9817cd9fcfb9911e03e14c07"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"andysalerno\/mistral-sft-v3",
        "Average":63.87,
        "ARC":61.35,
        "HellaSwag":82.23,
        "MMLU":63.4,
        "TruthfulQA":48.49,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"48beb1e9490732abc6f85d92579d407d85e2cf5d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"athirdpath\/NSFW_DPO_Noromaid-7b",
        "Average":63.86,
        "ARC":62.63,
        "HellaSwag":84.5,
        "MMLU":63.34,
        "TruthfulQA":44.99,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"51b4408a40736e18f69d932cb403811558428378"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddyEA\/openbuddy-llama-30b-v7.1-bf16",
        "Average":63.86,
        "ARC":62.37,
        "HellaSwag":82.29,
        "MMLU":58.18,
        "TruthfulQA":52.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.35,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"85f7ad9d6ff016312262a47d45ffd07dee54aab0"
    },
    {
        "T":"?",
        "Model":"karakuri-ai\/karakuri-lm-70b-chat-v0.1",
        "Average":63.85,
        "ARC":61.52,
        "HellaSwag":83.13,
        "MMLU":59.35,
        "TruthfulQA":51.39,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":69.2,
        "Hub":22,
        "Available on the hub":true,
        "Model Sha":"2646170ac2788259d258de8d16c563e36efce299"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TencentARC\/Mistral_Pro_8B_v0.1",
        "Average":63.85,
        "ARC":62.2,
        "HellaSwag":82.13,
        "MMLU":61.74,
        "TruthfulQA":49.32,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":62,
        "Available on the hub":false,
        "Model Sha":"acae0ffeb040f1ee654068403a0305263e932ee0"
    },
    {
        "T":"\u2b55",
        "Model":"Secbone\/llama-33B-instructed",
        "Average":63.85,
        "ARC":64.59,
        "HellaSwag":86.17,
        "MMLU":60.5,
        "TruthfulQA":44.12,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":32.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7c40caaea4fe3264fd469dac428b0f9450e574a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"itsliupeng\/llama_9b_long",
        "Average":63.84,
        "ARC":60.32,
        "HellaSwag":78.62,
        "MMLU":70.5,
        "TruthfulQA":45.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":9.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e927d0cf1a8ce240c2d4dd023c869a7c18bf9b1c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Inv\/Elbrus-7B",
        "Average":63.84,
        "ARC":63.99,
        "HellaSwag":83.96,
        "MMLU":63.06,
        "TruthfulQA":44.34,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":63,
        "Available on the hub":false,
        "Model Sha":"fbd9336495f86592d1c8532abd04be5da8895c57"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BarraHome\/zephyr-dpo-v2",
        "Average":63.84,
        "ARC":57.85,
        "HellaSwag":82.72,
        "MMLU":58.61,
        "TruthfulQA":56.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"8276bfec42e8fed1d8d67e8ee8b2e4fa594addb0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"juhwanlee\/experiment2-cause-v1",
        "Average":63.83,
        "ARC":61.01,
        "HellaSwag":83.38,
        "MMLU":63.75,
        "TruthfulQA":47.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f7e94b05758d08a981906a62942283349c561ae7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"clowman\/openchat-mistral-7b-reproduce",
        "Average":63.83,
        "ARC":57.25,
        "HellaSwag":80.72,
        "MMLU":61.54,
        "TruthfulQA":55.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"338660d3330af39bfadab520d1e925351d7d4924"
    },
    {
        "T":"?",
        "Model":"Radiantloom\/radintloom-mistral-7b-fusion",
        "Average":63.83,
        "ARC":62.03,
        "HellaSwag":82.26,
        "MMLU":63.82,
        "TruthfulQA":47.19,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"654f37927cbc789f0206dcc333201de1b30edd03"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"jae24\/openhermes_dpo_norobot_0201",
        "Average":63.82,
        "ARC":62.03,
        "HellaSwag":83.4,
        "MMLU":62.4,
        "TruthfulQA":47.44,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"db7b39141559ca4810371593d9caab4361704646"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/duplicitous-mammal-13b",
        "Average":63.81,
        "ARC":61.69,
        "HellaSwag":83.79,
        "MMLU":57.5,
        "TruthfulQA":52.27,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a05d0562b8da2ac2e76aa65984e8063249bc85c8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/OpenOrcaxOpenChat-Preview2-13B",
        "Average":63.81,
        "ARC":62.37,
        "HellaSwag":82.96,
        "MMLU":58.68,
        "TruthfulQA":51.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":89,
        "Available on the hub":true,
        "Model Sha":"26d1bc5c54c1f60a5de0b1ed4d0b16f285aee230"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_007_13b_v2",
        "Average":63.81,
        "ARC":61.95,
        "HellaSwag":82.48,
        "MMLU":57.32,
        "TruthfulQA":53.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"1c959d4b5d5b8683b051f07475bb5c1ab24c8bb0"
    },
    {
        "T":"?",
        "Model":"dball\/zephyr-7b-dpo-qlora-no-sft",
        "Average":63.8,
        "ARC":62.46,
        "HellaSwag":84.5,
        "MMLU":64.02,
        "TruthfulQA":44.25,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"571698d74a5b4445015cb9c4a3dbe655e96cfcab"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"itsliupeng\/llama_9b_long",
        "Average":63.8,
        "ARC":60.07,
        "HellaSwag":78.67,
        "MMLU":70.53,
        "TruthfulQA":45.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":9.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e927d0cf1a8ce240c2d4dd023c869a7c18bf9b1c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-code-mistral-7b-v1.0",
        "Average":63.8,
        "ARC":60.58,
        "HellaSwag":83.75,
        "MMLU":62.98,
        "TruthfulQA":47.9,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":15,
        "Available on the hub":false,
        "Model Sha":"753852b8cb52dc5f0411568e98c0cb445a7835dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeverSleep\/Noromaid-13b-v0.2",
        "Average":63.8,
        "ARC":60.92,
        "HellaSwag":84.04,
        "MMLU":57.67,
        "TruthfulQA":52.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"dad2d749b01cf10b65951dea6e130da8cc53e2c0"
    },
    {
        "T":"?",
        "Model":"INSAIT-Institute\/BgGPT-7B-Instruct-v0.1",
        "Average":63.79,
        "ARC":60.24,
        "HellaSwag":81.6,
        "MMLU":59.66,
        "TruthfulQA":53.68,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.29,
        "Hub":42,
        "Available on the hub":false,
        "Model Sha":"9c96e8cefc1079ef566cc46fc9b60b52dc36f583"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Enno-Ai\/ennodata-raw-pankajmathur-13b-peft",
        "Average":63.79,
        "ARC":61.95,
        "HellaSwag":82.21,
        "MMLU":57.44,
        "TruthfulQA":53.57,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"206553873db96a6730d36477837335dbbcc906fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NurtureAI\/Starling-LM-11B-alpha-v1",
        "Average":63.79,
        "ARC":62.2,
        "HellaSwag":83.24,
        "MMLU":64.03,
        "TruthfulQA":45.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":11.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"b2b3b9fc069a8b5d8be82f68f0f578a6f23e9e5f"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.5-DPO-7B-QLoRA",
        "Average":63.79,
        "ARC":61.26,
        "HellaSwag":84.52,
        "MMLU":63.63,
        "TruthfulQA":45.75,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"56e805fbebffaf25e61df5a3d68b75cb604a6e1c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CobraMamba\/mamba-gpt-7b-v1",
        "Average":63.79,
        "ARC":61.26,
        "HellaSwag":84.1,
        "MMLU":63.46,
        "TruthfulQA":46.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e64d658b397748e409d9633fd24fc5a6df429600"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"IkariDev\/Athena-v3",
        "Average":63.79,
        "ARC":61.69,
        "HellaSwag":84.34,
        "MMLU":57.87,
        "TruthfulQA":51.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"5e4024b6694bb13f1a81ce4277ac9141f0b226df"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Walter-SOLAR-11B",
        "Average":63.79,
        "ARC":60.41,
        "HellaSwag":84.86,
        "MMLU":64.99,
        "TruthfulQA":44.88,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"e7bbf8ba7572aced748c7fc7368dc024e2df7df0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/Gemma-10.2B-Coder",
        "Average":63.78,
        "ARC":58.7,
        "HellaSwag":82.04,
        "MMLU":61.96,
        "TruthfulQA":52.44,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.2,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"540d930fa1ea5b20e7cad85f309a0822f4bb05ed"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/neural-chat-7b-v3-1-dare-0.85",
        "Average":63.78,
        "ARC":61.95,
        "HellaSwag":83.84,
        "MMLU":64.43,
        "TruthfulQA":44.9,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"3c15d3e2a7790e45501e105daed5eb88b665ceef"
    },
    {
        "T":"?",
        "Model":"Isaak-Carter\/J.O.S.I.E.3-Beta8-slerp",
        "Average":63.78,
        "ARC":60.41,
        "HellaSwag":83.66,
        "MMLU":62.35,
        "TruthfulQA":48.69,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6d97107268cbc28317cba748ce281f11a6f50ce9"
    },
    {
        "T":"?",
        "Model":"IkariDev\/Athnete-13B",
        "Average":63.78,
        "ARC":62.12,
        "HellaSwag":84.36,
        "MMLU":57.58,
        "TruthfulQA":51.05,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"954188534f72de489fc9fdc628c8041b73f5cc90"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-zephyr-code-functionary-7b",
        "Average":63.77,
        "ARC":61.52,
        "HellaSwag":83.88,
        "MMLU":64.71,
        "TruthfulQA":44.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"4ba64c10fb56ade9ce2e0c5a097819b888c0a272"
    },
    {
        "T":"?",
        "Model":"Novocoders\/Mistral-NeuralDPO-v0.5",
        "Average":63.77,
        "ARC":65.44,
        "HellaSwag":84.66,
        "MMLU":62.56,
        "TruthfulQA":42.43,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e37831e09cff71bfa2659430bbfa1a210729ea5a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-falcon-180b-v12-preview0",
        "Average":63.76,
        "ARC":62.29,
        "HellaSwag":83.8,
        "MMLU":55.92,
        "TruthfulQA":53.05,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":178.64,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4f1aeb136860ee3216f23faec0c598014e5c40a6"
    },
    {
        "T":"?",
        "Model":"dball\/zephyr-7b-dpo-qlora",
        "Average":63.76,
        "ARC":63.82,
        "HellaSwag":84.92,
        "MMLU":62.28,
        "TruthfulQA":44.03,
        "Type":"Unknown",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8fef86af4ca1c140559450cace2fd1839f979020"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"proto-llm\/uniwiz-7B-v0.1",
        "Average":63.76,
        "ARC":61.77,
        "HellaSwag":84.16,
        "MMLU":64.16,
        "TruthfulQA":44.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5ad4b3b5b2648cf841b39fbe8254a1c1fee832f6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Walmart-the-bag\/MysticFusion-13B",
        "Average":63.76,
        "ARC":61.35,
        "HellaSwag":84.43,
        "MMLU":57.29,
        "TruthfulQA":51.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"02255943c6eff59ef6bd17e1a43a37ce3751ff5e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"royallab\/Pygmalion-2-13b-SuperCOT",
        "Average":63.74,
        "ARC":63.23,
        "HellaSwag":83.68,
        "MMLU":54.9,
        "TruthfulQA":53.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"763b3fd5afc3e7fb6c7c8768d40f06901c8d5913"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/samantha-mistral-7b",
        "Average":63.73,
        "ARC":63.4,
        "HellaSwag":84.1,
        "MMLU":61.36,
        "TruthfulQA":46.08,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":39,
        "Available on the hub":false,
        "Model Sha":"7f9e40543fdff8c3e58eca0390c8a631829c1206"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Enno-Ai\/ennodata-13b-8bit-raw-15epoch",
        "Average":63.73,
        "ARC":61.6,
        "HellaSwag":82.2,
        "MMLU":57.55,
        "TruthfulQA":53.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ee2ceaae9cb806bc30df84ba4d598fdf32e53b17"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"WizardLM\/WizardMath-7B-V1.1",
        "Average":63.73,
        "ARC":61.86,
        "HellaSwag":84.5,
        "MMLU":61.53,
        "TruthfulQA":47.04,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":67,
        "Available on the hub":false,
        "Model Sha":"366a19a3a8e64aea2fc77d648bec5738fb1f89ce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"IkariDev\/Athena-v4",
        "Average":63.73,
        "ARC":62.54,
        "HellaSwag":84.19,
        "MMLU":57.33,
        "TruthfulQA":50.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"dde640538a44a08f6f456a2b7634e31a5d7a1245"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"caisarl76\/Mistral-7B-OpenOrca-Guanaco-accu16",
        "Average":63.73,
        "ARC":59.73,
        "HellaSwag":83.08,
        "MMLU":61.29,
        "TruthfulQA":50.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.11,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e83b8c1887c45473961a4ff36ae202ada1ca3d42"
    },
    {
        "T":"?",
        "Model":"Minami-su\/Qwen1.5-7B-Chat_llamafy",
        "Average":63.72,
        "ARC":57.59,
        "HellaSwag":78.52,
        "MMLU":61.18,
        "TruthfulQA":57.59,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.1,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"19941b2df44ccea90a21c396a5fe19742f20e596"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/Vicuzard-30B-Uncensored-instruct-PL-lora_unload",
        "Average":63.72,
        "ARC":62.46,
        "HellaSwag":83.66,
        "MMLU":57.82,
        "TruthfulQA":50.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"652f03ac67b4293198d98b618e64285fb32a28e9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"juhwanlee\/llmdo-Mistral-7B-case-1",
        "Average":63.72,
        "ARC":62.12,
        "HellaSwag":83.6,
        "MMLU":63.46,
        "TruthfulQA":45.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d9fb9995a0e378f2ed955dc0a24690768e3e81ea"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"boomerchan\/magpie-13b",
        "Average":63.71,
        "ARC":63.31,
        "HellaSwag":84.25,
        "MMLU":58.15,
        "TruthfulQA":49.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"a58124cdc9f39ccd59d4290a8bdfda93ff3690dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-m2.0",
        "Average":63.71,
        "ARC":64.68,
        "HellaSwag":84.95,
        "MMLU":57.77,
        "TruthfulQA":47.44,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"57bd88e24d603dc4bbe4016ed0871db7c0e529d5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/Yarn-Mistral-7b-64k-Mistral-7B-Instruct-v0.1",
        "Average":63.7,
        "ARC":59.64,
        "HellaSwag":81.52,
        "MMLU":60.57,
        "TruthfulQA":53.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"aad44cdaf573542f8d4821072e8a33f798dfc714"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeverSleep\/Noromaid-13b-v0.3",
        "Average":63.7,
        "ARC":62.8,
        "HellaSwag":84.42,
        "MMLU":56.86,
        "TruthfulQA":50.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":22,
        "Available on the hub":true,
        "Model Sha":"1013d7e539e53c15e5285ed27902a713c8caad09"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"juhwanlee\/llmdo-Mistral-7B-case-6",
        "Average":63.7,
        "ARC":61.69,
        "HellaSwag":83.59,
        "MMLU":63.25,
        "TruthfulQA":46.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"de9932b4d92e3a579c933b51cbdc39fa6fb2bada"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/ReMM-Mistral-13B",
        "Average":63.69,
        "ARC":62.2,
        "HellaSwag":83.82,
        "MMLU":55.43,
        "TruthfulQA":53.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a5ef9385d9430a81778183d71b58eb2b869d6a7e"
    },
    {
        "T":"?",
        "Model":"Xenon1\/Xenon-1",
        "Average":63.69,
        "ARC":55.29,
        "HellaSwag":81.56,
        "MMLU":61.22,
        "TruthfulQA":56.68,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"70bd2d2359875ec03a3b1e82cc653334dfe2e721"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_3.5",
        "Average":63.68,
        "ARC":62.46,
        "HellaSwag":83.96,
        "MMLU":62.89,
        "TruthfulQA":45.43,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1078,
        "Available on the hub":false,
        "Model Sha":"5b874a33a91d63023055e6cb2d5d86afe883b4ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Mythical-Destroyer-L2-13B",
        "Average":63.68,
        "ARC":58.7,
        "HellaSwag":82.0,
        "MMLU":57.66,
        "TruthfulQA":56.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"7c87376b201b1c30c4e12c0b7bc2f28f017ce7bc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/zephyr-7b-gemma-v0.1",
        "Average":63.67,
        "ARC":58.45,
        "HellaSwag":83.48,
        "MMLU":60.68,
        "TruthfulQA":52.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":8.54,
        "Hub":106,
        "Available on the hub":false,
        "Model Sha":"19186e70e5679c47aaef473ae2fd56e20765088d"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/mistral-7b-platypus1k",
        "Average":63.66,
        "ARC":61.6,
        "HellaSwag":82.93,
        "MMLU":63.16,
        "TruthfulQA":46.96,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c34c4a249ecf0cc391beba142a1f9cb23154fcd1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Wizard-Vicuna-30B-Uncensored-fp16",
        "Average":63.65,
        "ARC":62.12,
        "HellaSwag":83.45,
        "MMLU":58.24,
        "TruthfulQA":50.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"c7b7cecb5a314fc66deebabcb67c230a3fbe84f7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/Wizard-Vicuna-30B-Uncensored",
        "Average":63.65,
        "ARC":62.12,
        "HellaSwag":83.45,
        "MMLU":58.24,
        "TruthfulQA":50.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":68,
        "Available on the hub":true,
        "Model Sha":"6374baef4cedd41f85c111b8eec3eb38ee24c4b9"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Nova-13B",
        "Average":63.65,
        "ARC":62.71,
        "HellaSwag":82.57,
        "MMLU":57.98,
        "TruthfulQA":51.34,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ae1145f9fa846ab8d39d8b7da888287ef917efb5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/robin-33B-v2-fp16",
        "Average":63.65,
        "ARC":62.37,
        "HellaSwag":83.63,
        "MMLU":54.71,
        "TruthfulQA":53.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"c0ed7d40c3e52379780638dac3bd1f69597b8e18"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"juhwanlee\/llmdo-Mistral-7B-case-5",
        "Average":63.65,
        "ARC":62.2,
        "HellaSwag":83.4,
        "MMLU":63.52,
        "TruthfulQA":45.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"abbb0d441f9aff4d4b4edd8969ad1d2139282b55"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"totally-not-an-llm\/PuddleJumper-13b",
        "Average":63.64,
        "ARC":58.7,
        "HellaSwag":81.18,
        "MMLU":58.25,
        "TruthfulQA":56.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"f3a8a475ff0c6ae37ac8ae0690980be11cac731a"
    },
    {
        "T":"?",
        "Model":"Locutusque\/OpenCerebrum-1.0-7b-DPO",
        "Average":63.63,
        "ARC":62.71,
        "HellaSwag":84.33,
        "MMLU":62.59,
        "TruthfulQA":44.91,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"c62cf904fb834bf99cd281cc32fb58048be411fd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/CollectiveCognition-v1.1-Mistral-7B-dare-0.85",
        "Average":63.63,
        "ARC":61.01,
        "HellaSwag":84.31,
        "MMLU":64.34,
        "TruthfulQA":44.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"7ecfa4c5b100565bf8cfdfa7442e9772d28a9a23"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Winterreise-m7",
        "Average":63.62,
        "ARC":61.26,
        "HellaSwag":83.84,
        "MMLU":63.85,
        "TruthfulQA":45.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"418129599bdd914f275a44ce9ce5a111c5917b3c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/gemma-7b-experiment",
        "Average":63.62,
        "ARC":61.09,
        "HellaSwag":82.47,
        "MMLU":66.03,
        "TruthfulQA":44.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"299334f1358f38b40480b232f94a637c5636e77b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"google\/gemma-7b",
        "Average":63.62,
        "ARC":61.09,
        "HellaSwag":82.47,
        "MMLU":66.03,
        "TruthfulQA":44.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7646584ed746494da9e1058b1be53d1be8b2ee73"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibndias\/NeuralHermes-MoE-2x7B",
        "Average":63.62,
        "ARC":62.12,
        "HellaSwag":84.21,
        "MMLU":64.56,
        "TruthfulQA":43.61,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"f8a3c8339ea38ce577e0c45aba859ac63b4c3cf3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xenon1\/Zenith-7B",
        "Average":63.62,
        "ARC":56.31,
        "HellaSwag":81.11,
        "MMLU":61.3,
        "TruthfulQA":55.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4d49920793f6c408ef75b1032e4ae66df1fae066"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"juhwanlee\/llmdo-Mistral-7B-case-7",
        "Average":63.61,
        "ARC":61.95,
        "HellaSwag":83.54,
        "MMLU":63.13,
        "TruthfulQA":45.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b53d30318037781487bb5be6b19ff95b6703bd7e"
    },
    {
        "T":"\u2b55",
        "Model":"Locutusque\/Orca-2-13b-SFT-v6",
        "Average":63.6,
        "ARC":60.41,
        "HellaSwag":80.46,
        "MMLU":59.51,
        "TruthfulQA":54.01,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"c31bf6f2d18f8fa4f6a25444ace549c4394b2b5a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/U-Amethyst-20B",
        "Average":63.6,
        "ARC":62.2,
        "HellaSwag":83.11,
        "MMLU":55.88,
        "TruthfulQA":53.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub":27,
        "Available on the hub":true,
        "Model Sha":"c0cbe0b3c88041bb6beef27dbe85146af8dddec9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aeala\/VicUnlocked-alpaca-30b",
        "Average":63.58,
        "ARC":61.86,
        "HellaSwag":83.79,
        "MMLU":57.64,
        "TruthfulQA":51.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"c63d117d1ec5794766dd6dc5e1469769df8aba1d"
    },
    {
        "T":"\u2b55",
        "Model":"CoruNethron\/neu-sai-it1",
        "Average":63.58,
        "ARC":61.26,
        "HellaSwag":81.39,
        "MMLU":60.17,
        "TruthfulQA":51.49,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c78cd605142d20c62c78b2c7456fe61d49d990a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeverSleep\/Noromaid-7b-v0.1.1",
        "Average":63.55,
        "ARC":62.2,
        "HellaSwag":84.28,
        "MMLU":63.44,
        "TruthfulQA":44.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"349a2eb5c61e3e13c2b39d15c7b94f5c31ab6bd5"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Yhyu13\/oasst-rlhf-2-llama-30b-7k-steps-hf",
        "Average":63.55,
        "ARC":61.35,
        "HellaSwag":83.8,
        "MMLU":57.89,
        "TruthfulQA":51.18,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"e04207847429af03c4780f5ac85c726536217981"
    },
    {
        "T":"?",
        "Model":"jondurbin\/airoboros-33b-gpt4",
        "Average":63.55,
        "ARC":63.74,
        "HellaSwag":84.87,
        "MMLU":58.54,
        "TruthfulQA":47.06,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"5b6bd680b1c008e52521dc8c663dbc87820da3d0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-m2.0",
        "Average":63.55,
        "ARC":63.4,
        "HellaSwag":85.19,
        "MMLU":57.46,
        "TruthfulQA":48.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"84a89dee5bf3447079f115a3ef4d58ef8f924798"
    },
    {
        "T":"?",
        "Model":"0-hero\/Matter-0.1-7B-DPO-preview",
        "Average":63.55,
        "ARC":62.71,
        "HellaSwag":82.99,
        "MMLU":62.7,
        "TruthfulQA":45.79,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"78040e4754051df49dd907cf1fd46a6b8a6cc30f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"diffnamehard\/Psyfighter2-Noromaid-ties-Capybara-13B",
        "Average":63.54,
        "ARC":62.29,
        "HellaSwag":83.87,
        "MMLU":56.59,
        "TruthfulQA":51.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"a7fa1f27d0a9123ce9dc415a5573b9e0525c69f8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/zephyr-beta-Nebula-v2-7B",
        "Average":63.54,
        "ARC":56.57,
        "HellaSwag":82.53,
        "MMLU":56.4,
        "TruthfulQA":58.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"226caedb50a12730232c1f8fe9c96b6dcf818ba7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CobraMamba\/mamba-gpt-7b-v2",
        "Average":63.54,
        "ARC":61.95,
        "HellaSwag":83.83,
        "MMLU":61.74,
        "TruthfulQA":46.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"6439444e2c0b61253d3e61ae04fe0436717acc2f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"diffnamehard\/Psyfighter2-Noromaid-ties-13B",
        "Average":63.54,
        "ARC":61.86,
        "HellaSwag":84.58,
        "MMLU":57.04,
        "TruthfulQA":50.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"191d13355682a875a24d2ebdd3322df55d6f9954"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fionazhang\/fine-tune-mistral-environment-merge",
        "Average":63.53,
        "ARC":62.63,
        "HellaSwag":83.66,
        "MMLU":63.88,
        "TruthfulQA":43.97,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"162b38e3aea3c55fef316ab7f42af3af3a440c07"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Doctor-Shotgun\/CalliopeDS-v2-L2-13B",
        "Average":63.53,
        "ARC":62.8,
        "HellaSwag":84.14,
        "MMLU":56.14,
        "TruthfulQA":51.06,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e63d24870c840d47e82b029e7f405baa10ad9ea4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Amethyst-13B-Mistral",
        "Average":63.53,
        "ARC":62.63,
        "HellaSwag":83.17,
        "MMLU":55.91,
        "TruthfulQA":52.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.0,
        "Hub":52,
        "Available on the hub":false,
        "Model Sha":"4328809e568f01e3f0a05764e3bb58e901310415"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Amethyst-13B",
        "Average":63.53,
        "ARC":62.63,
        "HellaSwag":83.17,
        "MMLU":55.91,
        "TruthfulQA":52.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"d4a85b1006f0b9439e64f0e7400533a7b867c24d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"adonlee\/LLaMA_2_13B_SFT_v0",
        "Average":63.53,
        "ARC":62.03,
        "HellaSwag":83.8,
        "MMLU":58.39,
        "TruthfulQA":49.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a6790d83337578f38d2bcd51038a779eaa8d0fac"
    },
    {
        "T":"?",
        "Model":"TeeZee\/GALAXY-XB-v.03",
        "Average":63.53,
        "ARC":61.77,
        "HellaSwag":83.59,
        "MMLU":64.55,
        "TruthfulQA":44.19,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":15.97,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9ffedaa035687282793e54395e141e8e5e47d068"
    },
    {
        "T":"\u2b55",
        "Model":"mncai\/Mistral-7B-openplatypus-1k",
        "Average":63.52,
        "ARC":60.15,
        "HellaSwag":84.25,
        "MMLU":59.84,
        "TruthfulQA":49.86,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.11,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"dad401175da3782475a122008720ddc3338e2632"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"juhwanlee\/llmdo-Mistral-7B-case-c-v1",
        "Average":63.52,
        "ARC":62.03,
        "HellaSwag":83.55,
        "MMLU":62.69,
        "TruthfulQA":45.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a11d4a6dcfbe2dda496aa2a33cd388e8056f95f2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"athirdpath\/Orca-2-13b-Alpaca-Uncensored",
        "Average":63.52,
        "ARC":61.09,
        "HellaSwag":79.27,
        "MMLU":60.13,
        "TruthfulQA":53.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"2fdbef532345da9eba9b9f4b8aaef6ea11b664fe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/LLaMA2-13B-Estopia",
        "Average":63.51,
        "ARC":62.29,
        "HellaSwag":82.51,
        "MMLU":55.12,
        "TruthfulQA":54.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"cfbf7f1372454aefb45d27504b11431828ad14f8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azure99\/blossom-v3-mistral-7b",
        "Average":63.51,
        "ARC":60.49,
        "HellaSwag":81.9,
        "MMLU":61.35,
        "TruthfulQA":50.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"ec6e84a662c801e248d3bb3a19529155de02bda0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/mistral-7B-alpaca-case-2-2",
        "Average":63.51,
        "ARC":63.48,
        "HellaSwag":83.27,
        "MMLU":62.11,
        "TruthfulQA":45.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3ba1eb007fbab1e7b7ff8b48159f7a847c3bb400"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kquant03\/Medusa-7B-bf16",
        "Average":63.5,
        "ARC":60.58,
        "HellaSwag":79.98,
        "MMLU":57.71,
        "TruthfulQA":55.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"dfe9982247761c6a54b76803483fe0d412e182f2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kquant03\/Hippolyta-7B-bf16",
        "Average":63.5,
        "ARC":60.58,
        "HellaSwag":79.98,
        "MMLU":57.71,
        "TruthfulQA":55.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"dfe9982247761c6a54b76803483fe0d412e182f2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-7B-v3.0",
        "Average":63.5,
        "ARC":62.46,
        "HellaSwag":83.79,
        "MMLU":63.9,
        "TruthfulQA":43.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":20,
        "Available on the hub":false,
        "Model Sha":"93c2e8b8055b42779f2b68059ebe38af6f2789c4"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Minirecord\/Mini_DPO_test02",
        "Average":63.5,
        "ARC":59.73,
        "HellaSwag":83.89,
        "MMLU":61.9,
        "TruthfulQA":48.47,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cd417467644c4178100083e342bad88a3f968be6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MLewd-v2.4-13B",
        "Average":63.49,
        "ARC":61.69,
        "HellaSwag":83.83,
        "MMLU":55.1,
        "TruthfulQA":53.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":36,
        "Available on the hub":true,
        "Model Sha":"6f6ec6024ee054020e49fd96f149919692848f0b"
    },
    {
        "T":"?",
        "Model":"Joseph717171\/Cerebrum-1.0-10.7B",
        "Average":63.47,
        "ARC":60.92,
        "HellaSwag":82.92,
        "MMLU":63.84,
        "TruthfulQA":46.2,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bbaf12db44e8def28f52c9d536cf95c50e1de081"
    },
    {
        "T":"\u2b55",
        "Model":"tiiuae\/falcon-40b-instruct",
        "Average":63.47,
        "ARC":61.6,
        "HellaSwag":84.31,
        "MMLU":55.45,
        "TruthfulQA":52.52,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":41.3,
        "Hub":1075,
        "Available on the hub":true,
        "Model Sha":"7475ff8cfc36ed9a962b658ae3c33391566a85a5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/mistral-7b-distilabel-truthy-dpo",
        "Average":63.47,
        "ARC":60.92,
        "HellaSwag":83.64,
        "MMLU":64.18,
        "TruthfulQA":45.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"885601cb9baf6c0b18b421e9e36c47692abb898c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fionazhang\/fine-tune-mistral-long-merge",
        "Average":63.46,
        "ARC":62.88,
        "HellaSwag":83.62,
        "MMLU":63.39,
        "TruthfulQA":43.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2675e1e670ebe54c733ed27fb32d8610644eefca"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/LLaMA2-13B-Estopia",
        "Average":63.45,
        "ARC":62.12,
        "HellaSwag":82.53,
        "MMLU":54.99,
        "TruthfulQA":54.18,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"cfbf7f1372454aefb45d27504b11431828ad14f8"
    },
    {
        "T":"\u2b55",
        "Model":"pankajmathur\/orca_mini_v3_13b",
        "Average":63.45,
        "ARC":63.14,
        "HellaSwag":82.35,
        "MMLU":56.52,
        "TruthfulQA":51.81,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":31,
        "Available on the hub":true,
        "Model Sha":"72eec98f68d240a71d3da8a266917b6e754ae831"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/orca_mini_v3_13b",
        "Average":63.45,
        "ARC":63.14,
        "HellaSwag":82.35,
        "MMLU":56.52,
        "TruthfulQA":51.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":26,
        "Available on the hub":true,
        "Model Sha":"99904e4119575f2c1606ca1e31d288f38a9f20b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-PersonalityEngine-30b",
        "Average":63.45,
        "ARC":63.48,
        "HellaSwag":84.37,
        "MMLU":58.99,
        "TruthfulQA":46.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"1990b46a2e2ac1f6282d961bce691ceceafed514"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MisterRid\/wendigo-14b-alpha4",
        "Average":63.44,
        "ARC":59.3,
        "HellaSwag":79.65,
        "MMLU":59.85,
        "TruthfulQA":54.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":14.22,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ea3ecf4418cf3655cf5093a8feb045b47b92c331"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aeala\/GPT4-x-AlpacaDente-30b",
        "Average":63.44,
        "ARC":62.12,
        "HellaSwag":82.78,
        "MMLU":56.19,
        "TruthfulQA":52.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"ee76c821f861f0ab0276f9f429dd06565f1f2051"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"huangyt\/Mistral-7B-v0.1-Open-Platypus_2.5w-r16-gate_up_down",
        "Average":63.44,
        "ARC":61.26,
        "HellaSwag":83.19,
        "MMLU":63.87,
        "TruthfulQA":45.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"77f7bf749a6c4561b5364b291152b54ba19a59fb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MayaPH\/GodziLLa-30B",
        "Average":63.44,
        "ARC":61.52,
        "HellaSwag":82.13,
        "MMLU":54.21,
        "TruthfulQA":55.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"aa9912a2ac60abeac28b4566731cd903dcc582ac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"0-hero\/Matter-0.1-Slim-7B-C-DPO",
        "Average":63.44,
        "ARC":63.48,
        "HellaSwag":83.12,
        "MMLU":60.63,
        "TruthfulQA":46.53,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"129e2aecb1f5033821c0fbe148bb8aa994565112"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MisterRid\/wendigo-14b-alpha3",
        "Average":63.43,
        "ARC":59.39,
        "HellaSwag":79.51,
        "MMLU":59.72,
        "TruthfulQA":55.12,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":14.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"01c9ec549ddc830eaa6639e7e89b6337c51586e3"
    },
    {
        "T":"?",
        "Model":"ddyuudd\/mistral_dmbr05_32_sig",
        "Average":63.43,
        "ARC":59.9,
        "HellaSwag":83.28,
        "MMLU":60.86,
        "TruthfulQA":49.69,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c09c32edf2cfc817d3aeb010e5a43a530ad5cd62"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-m2.0",
        "Average":63.42,
        "ARC":63.14,
        "HellaSwag":85.19,
        "MMLU":57.28,
        "TruthfulQA":48.07,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"96af3dc6c9f2248d964cf14cef6e5f2e5894583a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NickyNicky\/Mistral-7B-OpenOrca-oasst_top1_2023-08-25-v3",
        "Average":63.41,
        "ARC":60.58,
        "HellaSwag":83.34,
        "MMLU":61.53,
        "TruthfulQA":48.21,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"43abfcab8bf532a2601ed6e61e0c3614272b7df9"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/SpeechlessV1-Nova-13B",
        "Average":63.41,
        "ARC":61.77,
        "HellaSwag":82.68,
        "MMLU":57.75,
        "TruthfulQA":51.44,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"fbe6f0e32b5ecf9d75510d0b11a286466f46d79e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeontaek\/llama-2-13B-ensemble-v4",
        "Average":63.41,
        "ARC":62.97,
        "HellaSwag":82.38,
        "MMLU":56.48,
        "TruthfulQA":51.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2c8efc96563bfb80b4a9b9141b8c0bf64eb5056c"
    },
    {
        "T":"?",
        "Model":"OpenBuddy\/openbuddy-mixtral-7bx8-v17.1-32k",
        "Average":63.41,
        "ARC":65.53,
        "HellaSwag":75.95,
        "MMLU":70.02,
        "TruthfulQA":42.14,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.74,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"f19e2f221c1ec96f6f0d13566763fc774d78c1a1"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.5-SFT-7B-QLoRA",
        "Average":63.4,
        "ARC":60.75,
        "HellaSwag":84.24,
        "MMLU":63.66,
        "TruthfulQA":44.94,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"2609363766acf308877a71aba352e60d7c044b49"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mwitiderrick\/SwahiliInstruct-v0.1",
        "Average":63.4,
        "ARC":57.59,
        "HellaSwag":80.92,
        "MMLU":57.0,
        "TruthfulQA":58.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"06ae9044dac3c8f7cf67f4fd33986c5b79dbe69e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/carl-33b",
        "Average":63.39,
        "ARC":64.59,
        "HellaSwag":85.27,
        "MMLU":58.38,
        "TruthfulQA":45.32,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":32.32,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"5f80b372b493d901cab4490b4f23c71499023615"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/zephyr-7b-alpha-dare-0.85",
        "Average":63.39,
        "ARC":61.18,
        "HellaSwag":83.67,
        "MMLU":64.3,
        "TruthfulQA":44.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"afe35301593b4ce2e7b5d1696066724ef1f802eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kiddyz\/testllm-c2",
        "Average":63.39,
        "ARC":60.58,
        "HellaSwag":81.91,
        "MMLU":61.2,
        "TruthfulQA":49.87,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b87c798bc27522824451dfccf5eae50edbd4263b"
    },
    {
        "T":"?",
        "Model":"Locutusque\/Hyperion-3.0-Mistral-7B-DPO",
        "Average":63.37,
        "ARC":60.67,
        "HellaSwag":83.95,
        "MMLU":62.71,
        "TruthfulQA":46.17,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"31b3358c9ec5e4dd7b159241a622ea68c60b0500"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Obrolin\/Kesehatan-7B-v0.1",
        "Average":63.37,
        "ARC":60.32,
        "HellaSwag":82.54,
        "MMLU":59.94,
        "TruthfulQA":50.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a05db4c08e78668ac7249f41be98ffa866c6bf5a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-2.0",
        "Average":63.37,
        "ARC":63.82,
        "HellaSwag":85.65,
        "MMLU":58.44,
        "TruthfulQA":45.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"ddc598f492f5098a8e308f51a82834f98f29a4ce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Orca-2-13B-GPTQ",
        "Average":63.35,
        "ARC":59.81,
        "HellaSwag":79.12,
        "MMLU":59.35,
        "TruthfulQA":55.14,
        "Type":"fine-tuned",
        "Precision":"None",
        "Hub License":"other",
        "#Params (B)":2.03,
        "Hub":27,
        "Available on the hub":true,
        "Model Sha":"2fc627e11b197c7d563eeea9c4338c2adc8e2c93"
    },
    {
        "T":"\u2b55",
        "Model":"Deci\/DeciLM-7B-instruct",
        "Average":63.34,
        "ARC":61.01,
        "HellaSwag":82.37,
        "MMLU":60.24,
        "TruthfulQA":49.75,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.04,
        "Hub":94,
        "Available on the hub":false,
        "Model Sha":"24a66a701c10e5d70397f9bfc1624447327a0a08"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"royallab\/PsyOrca2-13b-DARE",
        "Average":63.34,
        "ARC":60.58,
        "HellaSwag":83.83,
        "MMLU":55.69,
        "TruthfulQA":53.27,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"4d3b1d7a4a5e243d3b8882abaa4b4a13d0ecbce4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"liminerity\/m.star.7b",
        "Average":63.33,
        "ARC":60.15,
        "HellaSwag":80.96,
        "MMLU":58.28,
        "TruthfulQA":53.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1de2c02db0939bd92748b207d8f56dc06105712a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/mistral-7b-tak-stack-dpo",
        "Average":63.32,
        "ARC":61.18,
        "HellaSwag":83.98,
        "MMLU":64.32,
        "TruthfulQA":43.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"43b9486705a45d6da632e36a8c33925d9f36bd7f"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Stable-Platypus2-13B-QLoRA-0.80-epoch",
        "Average":63.31,
        "ARC":62.29,
        "HellaSwag":82.46,
        "MMLU":57.09,
        "TruthfulQA":51.41,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0c15b8540335b3e21a976a5fc5c33b47927fea6c"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"internlm\/internlm2-7b",
        "Average":63.31,
        "ARC":58.02,
        "HellaSwag":81.24,
        "MMLU":65.24,
        "TruthfulQA":48.73,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":28,
        "Available on the hub":false,
        "Model Sha":"aac482e5fbfd5a85daa2a8e3aa3a1c5c97331d58"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-1.3",
        "Average":63.3,
        "ARC":63.82,
        "HellaSwag":85.09,
        "MMLU":58.94,
        "TruthfulQA":45.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"f94e5249d2b998933466d42e08fa9551e3238205"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/X-MythoChronos-13B",
        "Average":63.29,
        "ARC":59.73,
        "HellaSwag":83.39,
        "MMLU":56.5,
        "TruthfulQA":53.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"8d302741466512f0621a594fce6bf5b8125c8d4c"
    },
    {
        "T":"?",
        "Model":"giraffe176\/Open_Neural_Monarch_Maidv0.2",
        "Average":63.29,
        "ARC":63.31,
        "HellaSwag":82.6,
        "MMLU":64.21,
        "TruthfulQA":43.04,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"608e14c72451900c353ad4cde064485c4b5d2490"
    },
    {
        "T":"?",
        "Model":"TehVenom\/oasst-sft-6-llama-33b-xor-MERGED-16bit",
        "Average":63.29,
        "ARC":61.52,
        "HellaSwag":83.5,
        "MMLU":57.43,
        "TruthfulQA":50.7,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"62f92ddab8b37eaeda15cf5ecb5605141a0525eb"
    },
    {
        "T":"\u2b55",
        "Model":"adamo1139\/Mistral-7B-AEZAKMI-v1",
        "Average":63.29,
        "ARC":58.87,
        "HellaSwag":82.01,
        "MMLU":58.72,
        "TruthfulQA":53.54,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"fec4e695e5af743bb49d1976de83fa695be5f105"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/OrcaMini-Platypus2-13B-QLoRA-0.80-epoch",
        "Average":63.28,
        "ARC":60.84,
        "HellaSwag":82.56,
        "MMLU":56.42,
        "TruthfulQA":53.32,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1f81c0439f60d848e3cbc7f06fcd58b5161a8557"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"royallab\/PsyOrca2-13b-DARE",
        "Average":63.28,
        "ARC":60.32,
        "HellaSwag":83.85,
        "MMLU":55.62,
        "TruthfulQA":53.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"4d3b1d7a4a5e243d3b8882abaa4b4a13d0ecbce4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-mistral-dolphin-orca-platypus-samantha-7b-dare-0.85",
        "Average":63.28,
        "ARC":61.69,
        "HellaSwag":83.85,
        "MMLU":64.43,
        "TruthfulQA":43.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7a3def1c382793d2b12741896302c31a471b6d1d"
    },
    {
        "T":"?",
        "Model":"l3utterfly\/mistral-7b-v0.1-layla-v4",
        "Average":63.28,
        "ARC":62.29,
        "HellaSwag":83.36,
        "MMLU":64.32,
        "TruthfulQA":43.14,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":34,
        "Available on the hub":false,
        "Model Sha":"c98e0493e7651e6acb90cf58b5f06d0b15486bbf"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"chargoddard\/internlm2-base-20b-llama",
        "Average":63.27,
        "ARC":63.05,
        "HellaSwag":82.11,
        "MMLU":63.97,
        "TruthfulQA":43.97,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":19.86,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"ab228139d9da36acb908b8ff7732b456dfdd57e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"juhwanlee\/gemma-7B-alpaca-case-1-2",
        "Average":63.27,
        "ARC":62.03,
        "HellaSwag":81.3,
        "MMLU":62.95,
        "TruthfulQA":46.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7f94c120e461d0a99ec60d38b124bc8bda47d8cb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-2.0",
        "Average":63.27,
        "ARC":63.91,
        "HellaSwag":85.67,
        "MMLU":57.95,
        "TruthfulQA":45.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"a4e1b721add286900c5a6f529c3d7a3e0049b2e0"
    },
    {
        "T":"\u2b55",
        "Model":"vicgalle\/SOLAR-13B-Instruct-v1.0",
        "Average":63.26,
        "ARC":57.25,
        "HellaSwag":78.03,
        "MMLU":55.75,
        "TruthfulQA":61.99,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.48,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"9608d346324d603a67e7cb52a9ebe8cb1ed9e42f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"intervitens\/internlm2-base-20b-llama",
        "Average":63.25,
        "ARC":62.97,
        "HellaSwag":82.15,
        "MMLU":63.78,
        "TruthfulQA":44.11,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":19.86,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"507aadd23e803c4a2204d3d34c7008c6603a86db"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/Chronorctypus-Limarobormes-13b",
        "Average":63.25,
        "ARC":59.9,
        "HellaSwag":82.75,
        "MMLU":58.45,
        "TruthfulQA":51.9,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"75c1bf5f4b40cf61873ff6487ccd3efc4f684330"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/ReMM-v2.2-L2-13B",
        "Average":63.25,
        "ARC":61.26,
        "HellaSwag":84.16,
        "MMLU":56.22,
        "TruthfulQA":51.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"d55031fbcd41d749bc0c0ffbcd85636718d373b6"
    },
    {
        "T":"?",
        "Model":"Azazelle\/Moko-SAMPLE",
        "Average":63.24,
        "ARC":61.09,
        "HellaSwag":83.85,
        "MMLU":64.57,
        "TruthfulQA":43.45,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0ed942b34411970e688adb0ec6fe39c2b56e5e0e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/gemma-7b-alpaca-52k-v0.1",
        "Average":63.24,
        "ARC":60.15,
        "HellaSwag":81.97,
        "MMLU":64.14,
        "TruthfulQA":46.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"05bc24728baa4a680dd7aff3f15053e28671e801"
    },
    {
        "T":"\u2b55",
        "Model":"CalderaAI\/13B-Thorns-l2",
        "Average":63.23,
        "ARC":62.88,
        "HellaSwag":83.57,
        "MMLU":56.95,
        "TruthfulQA":49.52,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"adc5e7befcc3d0a26f46198fdda4a098a2742fe6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"liminerity\/Mistral-quiet-star",
        "Average":63.23,
        "ARC":61.18,
        "HellaSwag":84.59,
        "MMLU":62.03,
        "TruthfulQA":45.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"4756708f7d5dd7044353e2bfc6d971c9aec7c826"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CallComply\/openchat-3.5-0106-11b",
        "Average":63.22,
        "ARC":63.65,
        "HellaSwag":78.64,
        "MMLU":62.54,
        "TruthfulQA":48.07,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"0ea960b3343ec36e7f130d45d140fe192acf344b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/ReMM-v2-L2-13B",
        "Average":63.22,
        "ARC":61.95,
        "HellaSwag":84.0,
        "MMLU":56.14,
        "TruthfulQA":50.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"bc42c77f88482c37c72c85c66135e99972bbca1b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheSkullery\/Aurora_25e_Test",
        "Average":63.22,
        "ARC":59.64,
        "HellaSwag":84.29,
        "MMLU":61.7,
        "TruthfulQA":47.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"12145811d11dd6e7cd493cbe9a4add1e951bbc3b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-gemma-7b",
        "Average":63.22,
        "ARC":62.12,
        "HellaSwag":79.77,
        "MMLU":61.57,
        "TruthfulQA":49.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0366d26aaef3342cf11691fbe97f7266fc30644d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Newton-7B",
        "Average":63.21,
        "ARC":63.99,
        "HellaSwag":81.72,
        "MMLU":62.78,
        "TruthfulQA":44.36,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"cfa9b51541b423bafc1e87d942000d8ab052e065"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airoboros-33b-gpt4-1.3",
        "Average":63.21,
        "ARC":63.91,
        "HellaSwag":85.04,
        "MMLU":58.53,
        "TruthfulQA":45.36,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"f94e5249d2b998933466d42e08fa9551e3238205"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/very-test",
        "Average":63.2,
        "ARC":63.91,
        "HellaSwag":81.71,
        "MMLU":62.89,
        "TruthfulQA":44.28,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"198a56764da3541778771d7882b6facf3debb107"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/SynthIA-7B-v1.3-dare-0.85",
        "Average":63.19,
        "ARC":61.01,
        "HellaSwag":83.5,
        "MMLU":64.49,
        "TruthfulQA":43.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"91381d0ac625dcde542428ed6cb35177b4260923"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/OpenOrca-Platypus2-13B",
        "Average":63.19,
        "ARC":61.52,
        "HellaSwag":82.27,
        "MMLU":58.85,
        "TruthfulQA":50.11,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":184,
        "Available on the hub":true,
        "Model Sha":"03cce7b49c5b92ac5d491dbf91c3d21469c0c74b"
    },
    {
        "T":"?",
        "Model":"Locutusque\/Hercules-2.5-Mistral-7B",
        "Average":63.19,
        "ARC":62.03,
        "HellaSwag":83.79,
        "MMLU":63.49,
        "TruthfulQA":43.44,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"ff62c26031544deeffd9f06250e71fd05fb1169a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-zephyr-6x7b-lora",
        "Average":63.18,
        "ARC":61.01,
        "HellaSwag":82.8,
        "MMLU":60.09,
        "TruthfulQA":48.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ebf239f263dc1bfb7cf2030c96f0e967683e5946"
    },
    {
        "T":"\u2b55",
        "Model":"amazingvince\/where-llambo-7b",
        "Average":63.18,
        "ARC":58.45,
        "HellaSwag":82.06,
        "MMLU":62.61,
        "TruthfulQA":49.61,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"554d9c7bab7ea6deabef0266aef17aa98f758543"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TFLai\/Nova-13B-50-step",
        "Average":63.18,
        "ARC":61.6,
        "HellaSwag":82.31,
        "MMLU":57.27,
        "TruthfulQA":51.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1a827ccb7f00157b3cc9ce538d61a6ba8d5a65db"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azure99\/blossom-v3_1-mistral-7b",
        "Average":63.18,
        "ARC":60.49,
        "HellaSwag":81.71,
        "MMLU":61.0,
        "TruthfulQA":49.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d84e28c169a93933829e10f314f1e3e674df9843"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/vicuna-33b-coder",
        "Average":63.18,
        "ARC":60.67,
        "HellaSwag":83.3,
        "MMLU":56.92,
        "TruthfulQA":51.83,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":32.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"67f6e669d7a15c1104a1478057f3752a503e83c0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"prhegde\/merge-aanaphi-phi2-orage-3b",
        "Average":63.17,
        "ARC":63.57,
        "HellaSwag":77.42,
        "MMLU":58.21,
        "TruthfulQA":53.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6396a99299f440c7d7ec93786d7874a49accce7e"
    },
    {
        "T":"?",
        "Model":"theNovaAI\/Supernova-experimental",
        "Average":63.17,
        "ARC":63.05,
        "HellaSwag":83.66,
        "MMLU":56.59,
        "TruthfulQA":49.37,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e0b2524a7ac1e08c8c04e50d4461b89699d3603c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"decem\/Dionysus-Mistral-m3-v5",
        "Average":63.17,
        "ARC":59.56,
        "HellaSwag":80.99,
        "MMLU":61.18,
        "TruthfulQA":50.93,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7c6a76f284740abd1b262b950aa59d72c65d39e0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"l3utterfly\/mistral-7b-v0.1-layla-v3",
        "Average":63.16,
        "ARC":61.77,
        "HellaSwag":83.41,
        "MMLU":64.26,
        "TruthfulQA":43.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3fd903c921129317a5002b67df39995040acaebc"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"google\/gemma-7b",
        "Average":63.16,
        "ARC":61.09,
        "HellaSwag":82.2,
        "MMLU":64.56,
        "TruthfulQA":44.79,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1685d3c30e63630be55aa9a8730531732b3bd93c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BlueNipples\/TimeCrystal-l2-13B",
        "Average":63.16,
        "ARC":61.18,
        "HellaSwag":83.71,
        "MMLU":56.46,
        "TruthfulQA":51.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"f0076c437e766880841dc1768693dc745d093b8b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/vicuna-33b-coder",
        "Average":63.16,
        "ARC":60.41,
        "HellaSwag":83.27,
        "MMLU":57.17,
        "TruthfulQA":51.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":32.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"67f6e669d7a15c1104a1478057f3752a503e83c0"
    },
    {
        "T":"?",
        "Model":"TeeZee\/GALAXY-XB-v.01",
        "Average":63.16,
        "ARC":60.92,
        "HellaSwag":82.92,
        "MMLU":65.11,
        "TruthfulQA":43.67,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":17.71,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"65626e3a0f529ae6466984f947cf38b4ecf2f584"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Emerald-13B",
        "Average":63.15,
        "ARC":62.29,
        "HellaSwag":83.69,
        "MMLU":55.7,
        "TruthfulQA":50.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"f7696299463d8ec402a4e1eb001f3a447f1c5552"
    },
    {
        "T":"?",
        "Model":"l3utterfly\/mistral-7b-v0.1-layla-v1",
        "Average":63.15,
        "ARC":60.15,
        "HellaSwag":83.25,
        "MMLU":60.31,
        "TruthfulQA":48.9,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5f06add6aa1d51d78288dbdcbd1abfd5f0ed0c84"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/mistral-7b-slimorca-sft",
        "Average":63.15,
        "ARC":58.53,
        "HellaSwag":83.16,
        "MMLU":60.71,
        "TruthfulQA":50.18,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"45c7963f6177f7fa1e07987264817b50611650e9"
    },
    {
        "T":"\u2b55",
        "Model":"robinsmits\/Mistral-Instruct-7B-v0.2-ChatAlpaca",
        "Average":63.13,
        "ARC":56.74,
        "HellaSwag":80.82,
        "MMLU":59.1,
        "TruthfulQA":55.86,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"730fe06d2e388636cf59d56d3473239305796fc8"
    },
    {
        "T":"?",
        "Model":"TeeZee\/GALAXY-XB-v.02",
        "Average":63.13,
        "ARC":60.67,
        "HellaSwag":83.27,
        "MMLU":64.99,
        "TruthfulQA":43.6,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":16.84,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"66beef010dea755eacf6e9316be7169df361ca99"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/mistral-7b-openhermes-2.5-sft",
        "Average":63.13,
        "ARC":59.47,
        "HellaSwag":83.2,
        "MMLU":61.32,
        "TruthfulQA":48.52,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3fa4dfd0f915897f6ec559e6095cdcc064ec04df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"arlineka\/Brunhilde-13b",
        "Average":63.13,
        "ARC":60.49,
        "HellaSwag":83.49,
        "MMLU":56.18,
        "TruthfulQA":52.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ce50fccfb850fc07618c6d215823b754b42346ed"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KatyTheCutie\/EstopianMaid-13B",
        "Average":63.13,
        "ARC":60.49,
        "HellaSwag":83.49,
        "MMLU":56.18,
        "TruthfulQA":52.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.02,
        "Hub":31,
        "Available on the hub":true,
        "Model Sha":"5770d488c48c4c97ee53572dd8577aae584f9230"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"01-ai\/Yi-9B",
        "Average":63.13,
        "ARC":61.18,
        "HellaSwag":78.82,
        "MMLU":70.06,
        "TruthfulQA":42.45,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":8.83,
        "Hub":170,
        "Available on the hub":true,
        "Model Sha":"6868e9775d48a9b6a1ae0653ff8865a438691a9c"
    },
    {
        "T":"?",
        "Model":"OpenBuddy\/openbuddy-qwen1.5-14b-v20.1-32k",
        "Average":63.12,
        "ARC":56.91,
        "HellaSwag":74.57,
        "MMLU":66.72,
        "TruthfulQA":54.28,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":14.17,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c5449bf8b3ede7b0b5a21d493fd88f6e04158c6e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/Mistral-7B-OpenOrca-lora",
        "Average":63.12,
        "ARC":61.95,
        "HellaSwag":83.62,
        "MMLU":64.16,
        "TruthfulQA":42.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"605dc043063cb9589c06883d839122920ed1eca5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/experiment2-cause",
        "Average":63.11,
        "ARC":60.41,
        "HellaSwag":82.76,
        "MMLU":62.15,
        "TruthfulQA":47.13,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4d6f888de5ba44d5ca8ef766c54e92103a0afe16"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/experiment2-cause-qLoRa",
        "Average":63.11,
        "ARC":60.41,
        "HellaSwag":82.76,
        "MMLU":62.15,
        "TruthfulQA":47.13,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4d6f888de5ba44d5ca8ef766c54e92103a0afe16"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Gryphe\/MythoMix-L2-13b",
        "Average":63.11,
        "ARC":61.09,
        "HellaSwag":83.86,
        "MMLU":55.42,
        "TruthfulQA":52.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"eca790fb9394c9c61be27ef709080b3b92783a45"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/Mistral-7B-OpenOrca-lora-merged",
        "Average":63.11,
        "ARC":61.77,
        "HellaSwag":83.61,
        "MMLU":64.34,
        "TruthfulQA":42.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"8a8e4763c3edd0a8e5bb02e4bc865c69a658b428"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-zephyr-6x7b",
        "Average":63.11,
        "ARC":60.75,
        "HellaSwag":82.8,
        "MMLU":60.03,
        "TruthfulQA":48.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8d7ffe152c8dd278fbd8f29a80dfa13b024f3e52"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aeala\/Enterredaas-33b",
        "Average":63.1,
        "ARC":60.92,
        "HellaSwag":84.18,
        "MMLU":58.3,
        "TruthfulQA":49.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d72dc1f05eaf1beb6373fd53fd22eb90f293a5c4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeontaek\/llama-2-13B-ensemble-v1",
        "Average":63.1,
        "ARC":62.29,
        "HellaSwag":82.36,
        "MMLU":57.59,
        "TruthfulQA":50.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"95c37a1fcf3a2f8ef2f410550c2a8002e4fe24f9"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/OpenOrcaPlatypus2-Platypus2-13B-QLora-0.80-epoch",
        "Average":63.1,
        "ARC":59.81,
        "HellaSwag":82.69,
        "MMLU":56.96,
        "TruthfulQA":52.92,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5427ceec420f943a0b011a4d96f3efc292306933"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/airoboros-m-7b-3.1.2-dare-0.85",
        "Average":63.09,
        "ARC":61.09,
        "HellaSwag":83.57,
        "MMLU":64.05,
        "TruthfulQA":43.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"b5bc02f4e1008bd3a72046a93ac2f4dd4bef02da"
    },
    {
        "T":"?",
        "Model":"ddyuudd\/mistral_dmbr03_32_sig",
        "Average":63.08,
        "ARC":59.98,
        "HellaSwag":83.22,
        "MMLU":61.22,
        "TruthfulQA":47.9,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"860f9cde13943b70bbea7d54975148005efa1b0a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/zephyr-7b-gemma-v0.1",
        "Average":63.07,
        "ARC":57.94,
        "HellaSwag":82.91,
        "MMLU":58.98,
        "TruthfulQA":52.47,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":8.54,
        "Hub":106,
        "Available on the hub":false,
        "Model Sha":"19186e70e5679c47aaef473ae2fd56e20765088d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NurtureAI\/openchat_3.5-16k",
        "Average":63.07,
        "ARC":63.31,
        "HellaSwag":83.58,
        "MMLU":61.9,
        "TruthfulQA":43.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":32,
        "Available on the hub":false,
        "Model Sha":"e8d66e7fb2ebb918f468137ea5fa3dc13ddc69da"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Ensemble5-Platypus2-13B-QLora-0.80-epoch",
        "Average":63.06,
        "ARC":59.73,
        "HellaSwag":82.66,
        "MMLU":56.94,
        "TruthfulQA":52.92,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2af03c3287c60c4ba2fb6afa86c26cf722ab001d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abideen\/Mistral-v2-orpo",
        "Average":63.06,
        "ARC":60.92,
        "HellaSwag":83.45,
        "MMLU":63.66,
        "TruthfulQA":44.21,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e807af8144a42a9fcd61f99da1460229f48b8398"
    },
    {
        "T":"?",
        "Model":"Locutusque\/Hyperion-2.1-Mistral-7B",
        "Average":63.06,
        "ARC":59.9,
        "HellaSwag":83.3,
        "MMLU":61.46,
        "TruthfulQA":47.58,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"1f3258db979c9cfc73e9a8a0bbd69757366fc921"
    },
    {
        "T":"?",
        "Model":"Locutusque\/Hercules-2.0-Mistral-7B",
        "Average":63.05,
        "ARC":61.09,
        "HellaSwag":83.69,
        "MMLU":63.47,
        "TruthfulQA":43.97,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"3463e3123ea32116e5aca1a4498c1f8fb5109244"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"l3utterfly\/mistral-7b-v0.1-layla-v4-chatml",
        "Average":63.05,
        "ARC":62.03,
        "HellaSwag":83.4,
        "MMLU":63.74,
        "TruthfulQA":43.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"5642d735943dd13df17a89dfe52839a2f10ee607"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"arlineka\/Brunhilde-13b-v3",
        "Average":63.05,
        "ARC":60.15,
        "HellaSwag":84.02,
        "MMLU":55.03,
        "TruthfulQA":52.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7b54e7e30e7156586d908857910e4fa502c2fcf5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/LLaMA2-13B-Psyfighter2",
        "Average":63.04,
        "ARC":60.07,
        "HellaSwag":84.02,
        "MMLU":55.07,
        "TruthfulQA":53.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":24,
        "Available on the hub":true,
        "Model Sha":"cc51a4e64b0821feda101dc04737486b4ff60735"
    },
    {
        "T":"?",
        "Model":"openaccess-ai-collective\/grendel",
        "Average":63.04,
        "ARC":60.49,
        "HellaSwag":79.99,
        "MMLU":58.98,
        "TruthfulQA":52.68,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"9444ef27ab9cc263745f9b24ffd7e2da60d2283c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeontaek\/llama-2-13B-ensemble-v3",
        "Average":63.03,
        "ARC":62.37,
        "HellaSwag":82.29,
        "MMLU":57.67,
        "TruthfulQA":49.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"738f5b53fecaf2b51789c77c4c28fe5b77fbd7d8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeverSleep\/Noromaid-7B-0.4-DPO",
        "Average":63.02,
        "ARC":62.2,
        "HellaSwag":84.41,
        "MMLU":63.14,
        "TruthfulQA":42.34,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":20,
        "Available on the hub":false,
        "Model Sha":"47a417a2167064112038e71f2be30d7293eb485d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeverSleep\/Noromaid-7B-0.4-DPO",
        "Average":63.02,
        "ARC":62.29,
        "HellaSwag":84.32,
        "MMLU":63.2,
        "TruthfulQA":42.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":20,
        "Available on the hub":false,
        "Model Sha":"47a417a2167064112038e71f2be30d7293eb485d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"adamo1139\/Mistral-7B-AEZAKMI-v2",
        "Average":63.01,
        "ARC":58.11,
        "HellaSwag":82.53,
        "MMLU":59.89,
        "TruthfulQA":51.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5a1bbf8066d2ff0effdf6ba311f295a1a5b88c65"
    },
    {
        "T":"?",
        "Model":"sethuiyer\/Aika-7B",
        "Average":62.99,
        "ARC":65.36,
        "HellaSwag":81.49,
        "MMLU":53.91,
        "TruthfulQA":51.22,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"00589aa6b5081b35c38103071c3901d191d5ecf2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"arlineka\/Brunhilde-13b-v1",
        "Average":62.99,
        "ARC":61.09,
        "HellaSwag":83.58,
        "MMLU":55.32,
        "TruthfulQA":51.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e13977c7951d5d8cd77d301f75a7a3822c4800ee"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/MythoMix-Platypus2-13B-QLoRA-0.80-epoch",
        "Average":62.99,
        "ARC":60.32,
        "HellaSwag":83.72,
        "MMLU":55.74,
        "TruthfulQA":52.18,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3d91f63d82abd598d5b80d24d74feb6b00b7d80f"
    },
    {
        "T":"\u2b55",
        "Model":"yihan6324\/llama2-13b-instructmining-40k-sharegpt",
        "Average":62.99,
        "ARC":59.98,
        "HellaSwag":83.06,
        "MMLU":56.48,
        "TruthfulQA":52.44,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"fa7ee5a68c863e76a5e65ffc9e4f158ea894e23c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheSkullery\/Aurora_19e_Test",
        "Average":62.99,
        "ARC":59.3,
        "HellaSwag":83.74,
        "MMLU":61.45,
        "TruthfulQA":47.46,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"b85cfdd8abb892807059e2df97912abf3b7ee978"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"timdettmers\/guanaco-33b-merged",
        "Average":62.98,
        "ARC":62.46,
        "HellaSwag":84.48,
        "MMLU":53.78,
        "TruthfulQA":51.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":163,
        "Available on the hub":true,
        "Model Sha":"b2e78a916582935b6616d184b22ea5e9e1eb4c34"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/EnsembleV5-Nova-13B",
        "Average":62.98,
        "ARC":62.71,
        "HellaSwag":82.55,
        "MMLU":56.79,
        "TruthfulQA":49.86,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7ba38d309709d35149b4a18f94096875885035ae"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/EnsembleV5-Nova-13B",
        "Average":62.98,
        "ARC":62.71,
        "HellaSwag":82.55,
        "MMLU":56.79,
        "TruthfulQA":49.86,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3e25556187ba576082a85c270d2d4b4ea6ea9f6f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/LLaMA2-13B-Tiefighter",
        "Average":62.97,
        "ARC":59.9,
        "HellaSwag":84.0,
        "MMLU":54.98,
        "TruthfulQA":53.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":66,
        "Available on the hub":true,
        "Model Sha":"0d193a4562d6836724485cb7df6e58ca846bbfeb"
    },
    {
        "T":"\u2b55",
        "Model":"ericpolewski\/AIRIC-The-Mistral",
        "Average":62.97,
        "ARC":59.98,
        "HellaSwag":82.98,
        "MMLU":60.67,
        "TruthfulQA":48.24,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"b491a2e09079cfd8d388a5a65e2c44910b10aad4"
    },
    {
        "T":"\u2b55",
        "Model":"monology\/openinstruct-mistral-7b",
        "Average":62.95,
        "ARC":59.73,
        "HellaSwag":82.77,
        "MMLU":60.55,
        "TruthfulQA":48.76,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"54f379bf7676ffd09b48b0ff607b7ae6c0a6f688"
    },
    {
        "T":"?",
        "Model":"Locutusque\/lr-experiment1-7B",
        "Average":62.95,
        "ARC":60.75,
        "HellaSwag":83.73,
        "MMLU":63.25,
        "TruthfulQA":44.07,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"184813a8265802dccbbc1f8bb0fe72ae32a3475e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"circulus\/Llama-2-13b-orca-v1",
        "Average":62.95,
        "ARC":62.2,
        "HellaSwag":82.32,
        "MMLU":57.67,
        "TruthfulQA":49.6,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"e77ec90f432bdffa210a0e4310d117e5d1c662df"
    },
    {
        "T":"?",
        "Model":"Locutusque\/Hercules-3.0-Mistral-7B",
        "Average":62.95,
        "ARC":61.26,
        "HellaSwag":83.43,
        "MMLU":63.68,
        "TruthfulQA":43.42,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"e9ab8a23f6c641729762f352014c66650b033a71"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Gryphe\/MythoMax-L2-13b",
        "Average":62.95,
        "ARC":60.92,
        "HellaSwag":83.56,
        "MMLU":55.33,
        "TruthfulQA":51.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":98,
        "Available on the hub":true,
        "Model Sha":"faa4ef8c87dbb00d447904ceb048d49b6a463d07"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/ReMM-SLERP-L2-13B",
        "Average":62.95,
        "ARC":60.92,
        "HellaSwag":83.56,
        "MMLU":55.33,
        "TruthfulQA":51.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"27baccf242bc1dc34fc39661a40bbf867cbea8b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"The-Face-Of-Goonery\/Huginn-13b-v1.2",
        "Average":62.95,
        "ARC":60.92,
        "HellaSwag":83.56,
        "MMLU":55.33,
        "TruthfulQA":51.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"cb3562e7aae05a95fe61610b7b8f4957d3529ce7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JunchengXie\/Mistral-7B-v0.1-raw-80k",
        "Average":62.94,
        "ARC":61.52,
        "HellaSwag":83.57,
        "MMLU":63.67,
        "TruthfulQA":43.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"24fd76a0416c83b6f306db4f3795ed5c576095e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/mistral-7b-dolphin-sft",
        "Average":62.94,
        "ARC":57.25,
        "HellaSwag":83.01,
        "MMLU":62.59,
        "TruthfulQA":48.91,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7f378f4989df82fee8b4971263aadf9cd2de4bd4"
    },
    {
        "T":"?",
        "Model":"TeeZee\/DarkSapling-7B-v1.0",
        "Average":62.94,
        "ARC":61.6,
        "HellaSwag":82.59,
        "MMLU":62.46,
        "TruthfulQA":45.09,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"df6fad2ddb8af14baaffdc731553be7e70cd83e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/UndiMix-v4-13B",
        "Average":62.92,
        "ARC":61.95,
        "HellaSwag":83.88,
        "MMLU":56.9,
        "TruthfulQA":48.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"6dd97c74cfe1d22432d5c993814e230f333ba401"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-code-mistral-orca-7b-v1.0",
        "Average":62.92,
        "ARC":59.64,
        "HellaSwag":82.25,
        "MMLU":61.33,
        "TruthfulQA":48.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"f7db67fe6c82657b35d0ffcf8b7ff1568d979482"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NovoCode\/Mistral-NeuralDPO-v0.3",
        "Average":62.92,
        "ARC":61.6,
        "HellaSwag":83.15,
        "MMLU":61.6,
        "TruthfulQA":45.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"dba42d919d7c2f6ccc2e42a4e75d4225e2725d00"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"walebadr\/Mistral-7B-v0.1-DPO",
        "Average":62.91,
        "ARC":61.26,
        "HellaSwag":83.94,
        "MMLU":63.76,
        "TruthfulQA":42.68,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bab460c2c68fca377bcc778031d51340104e2dc1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"stabilityai\/StableBeluga-13B",
        "Average":62.91,
        "ARC":62.03,
        "HellaSwag":82.27,
        "MMLU":57.71,
        "TruthfulQA":49.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":93,
        "Available on the hub":true,
        "Model Sha":"1d6eef4cc2b73f39600a568803ad8183f2da4514"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"circulus\/Llama-2-13b-orca-v1",
        "Average":62.91,
        "ARC":62.03,
        "HellaSwag":82.27,
        "MMLU":57.71,
        "TruthfulQA":49.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"e77ec90f432bdffa210a0e4310d117e5d1c662df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/ReMM-v2.1-L2-13B",
        "Average":62.9,
        "ARC":61.43,
        "HellaSwag":83.92,
        "MMLU":55.95,
        "TruthfulQA":50.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"e6b5ac97f74355cb281a621261debe5720fb4da2"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"walebadr\/Mistral-7B-v0.1-DPO",
        "Average":62.89,
        "ARC":60.32,
        "HellaSwag":83.69,
        "MMLU":64.01,
        "TruthfulQA":43.53,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e1fa6fa7e272027d648c92873c06a42064b483ec"
    },
    {
        "T":"\u2b55",
        "Model":"mrm8488\/mistral-7b-ft-h4-no_robots_instructions",
        "Average":62.89,
        "ARC":60.92,
        "HellaSwag":83.24,
        "MMLU":63.74,
        "TruthfulQA":43.64,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"785446da9a53ceae48795069bf7ccaf46a91a5ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/WizardLM-30B-Uncensored",
        "Average":62.88,
        "ARC":60.24,
        "HellaSwag":82.93,
        "MMLU":56.8,
        "TruthfulQA":51.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":116,
        "Available on the hub":true,
        "Model Sha":"761783745fcb97831ad8035d3cbd5de484aca3ce"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/OpenOrca-Nebula-7B",
        "Average":62.87,
        "ARC":58.7,
        "HellaSwag":81.84,
        "MMLU":57.77,
        "TruthfulQA":53.18,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"dbfa63b0d89340ecad26cb64385e9dd588456819"
    },
    {
        "T":"?",
        "Model":"rizla\/rizla54",
        "Average":62.87,
        "ARC":58.19,
        "HellaSwag":78.74,
        "MMLU":61.29,
        "TruthfulQA":53.26,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-2.0",
        "#Params (B)":53.58,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ba0f54d38f2dc2be8d6a1035d55f848c6b1b6ab9"
    },
    {
        "T":"\u2b55",
        "Model":"The-Face-Of-Goonery\/huginnv1.2",
        "Average":62.87,
        "ARC":62.37,
        "HellaSwag":84.28,
        "MMLU":57.02,
        "TruthfulQA":47.81,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"aed4ddc951c657993939fa5b87a4088550569a3b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/OpenOrcaxOpenChat-Preview2-13B-GPTQ",
        "Average":62.87,
        "ARC":61.26,
        "HellaSwag":82.14,
        "MMLU":57.85,
        "TruthfulQA":50.22,
        "Type":"fine-tuned",
        "Precision":"None",
        "Hub License":"other",
        "#Params (B)":2.03,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"ec9eb4f471b5bb6a7e5e505369628586c0c72252"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-L2-13B",
        "Average":62.87,
        "ARC":61.01,
        "HellaSwag":83.95,
        "MMLU":56.33,
        "TruthfulQA":50.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"c4e7b771e30fdbfd6bd2e66a6928024bd5692bbd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gradientputri\/MegaMix-T1-13B",
        "Average":62.87,
        "ARC":61.35,
        "HellaSwag":83.44,
        "MMLU":58.49,
        "TruthfulQA":48.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"55d31300f8972b56320855bb40efb5e3d1e1a6fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sauce1337\/BerrySauce-L2-13b",
        "Average":62.87,
        "ARC":62.29,
        "HellaSwag":83.78,
        "MMLU":57.1,
        "TruthfulQA":48.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c8788874b78c84bc5593586d16fbd8ae7b5b2991"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"BAAI\/Aquila2-34B",
        "Average":62.86,
        "ARC":52.65,
        "HellaSwag":81.99,
        "MMLU":76.02,
        "TruthfulQA":40.8,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":33.08,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"356733caf6221e9dd898cde8ff189a98175526ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-2-13b-instruct",
        "Average":62.86,
        "ARC":61.18,
        "HellaSwag":83.25,
        "MMLU":55.92,
        "TruthfulQA":51.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"ac1f326ea75a28197c4b8e7c015071e8eef64485"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NovoCode\/Novocode7b-v2",
        "Average":62.85,
        "ARC":61.01,
        "HellaSwag":84.12,
        "MMLU":64.05,
        "TruthfulQA":42.21,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"45db1dd584c06c31e72f9744ebfb531a54898212"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Einstein-7B",
        "Average":62.84,
        "ARC":61.6,
        "HellaSwag":84.35,
        "MMLU":62.87,
        "TruthfulQA":42.55,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"36f6450a618d8e665097df2891f30e0dcbcf82ce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mervinpraison\/tamil-large-language-model-7b-v1.0",
        "Average":62.84,
        "ARC":60.15,
        "HellaSwag":82.21,
        "MMLU":63.9,
        "TruthfulQA":45.09,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"b07baafc06099b5835118213e79768a60f4a8973"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Locutusque\/Orca-2-13b-SFT_v5",
        "Average":62.83,
        "ARC":59.22,
        "HellaSwag":80.09,
        "MMLU":60.19,
        "TruthfulQA":51.84,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"3c1b86e1a4e89119e373198ff018838988cc74d0"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"dhanushreddy29\/BrokenKeyboardMerge",
        "Average":62.83,
        "ARC":59.73,
        "HellaSwag":81.25,
        "MMLU":58.36,
        "TruthfulQA":52.0,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"79693860dd86978c3b3de3fefe3b0664c9183e07"
    },
    {
        "T":"?",
        "Model":"vicgalleorg\/OpenHermes-Yi-9B",
        "Average":62.83,
        "ARC":60.67,
        "HellaSwag":78.73,
        "MMLU":69.67,
        "TruthfulQA":42.25,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":9.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"1e757a2c2c0f32983e6d360ee2ca62581a121ea7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"The-Face-Of-Goonery\/Huginn-13b-FP16",
        "Average":62.82,
        "ARC":60.58,
        "HellaSwag":82.53,
        "MMLU":53.71,
        "TruthfulQA":54.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"69615d9a8e1547f2407afd3380868a99f780e008"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ArianAskari\/SOLID-SFT-DPO-MixQV2-SOLIDRejected-SFTChosen-Zephyr-7b-beta",
        "Average":62.82,
        "ARC":58.96,
        "HellaSwag":79.82,
        "MMLU":60.14,
        "TruthfulQA":52.36,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f03058e7f15c0d1c542e32c88f7813a4dac7c33f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/a",
        "Average":62.82,
        "ARC":63.48,
        "HellaSwag":86.49,
        "MMLU":56.76,
        "TruthfulQA":44.55,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dc60deab5bfc4c39904c23e6a5fd545b38301d5f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NickyNicky\/Mistral-7B-OpenOrca-oasst_top1_2023-08-25-v2",
        "Average":62.82,
        "ARC":60.49,
        "HellaSwag":82.07,
        "MMLU":62.34,
        "TruthfulQA":46.38,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"f01f41dc7c987ad6668931159feaa4469f7dcf3f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BAAI\/Aquila2-34B",
        "Average":62.81,
        "ARC":52.47,
        "HellaSwag":81.9,
        "MMLU":76.03,
        "TruthfulQA":40.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":33.08,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"356733caf6221e9dd898cde8ff189a98175526ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"qblocks\/mistral_7b_HalfEpoch_DolphinCoder",
        "Average":62.81,
        "ARC":61.77,
        "HellaSwag":82.26,
        "MMLU":61.75,
        "TruthfulQA":45.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a9256ea648ecd6450d5ea4ebc2d07a1222ba8e9a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"grimpep\/L2-MythoMax22b-instruct-Falseblock",
        "Average":62.8,
        "ARC":60.49,
        "HellaSwag":82.06,
        "MMLU":52.9,
        "TruthfulQA":55.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":21.62,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"2573392c8dc7a468d1a02d538e4311c4aaa4c42f"
    },
    {
        "T":"?",
        "Model":"Locutusque\/Hercules-3.1-Mistral-7B",
        "Average":62.8,
        "ARC":61.18,
        "HellaSwag":83.55,
        "MMLU":63.65,
        "TruthfulQA":42.83,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"ba7176142c6d3e5b8735b79f68552f16634bbbe7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Tijmen2\/cosmosage_v2",
        "Average":62.79,
        "ARC":59.73,
        "HellaSwag":80.9,
        "MMLU":59.57,
        "TruthfulQA":50.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"c7e3ab1a424aabd7b3386050b8ef8045983c1fba"
    },
    {
        "T":"?",
        "Model":"mobiuslabsgmbh\/aanaphi2-v0.1",
        "Average":62.79,
        "ARC":63.91,
        "HellaSwag":77.97,
        "MMLU":57.73,
        "TruthfulQA":51.56,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":20,
        "Available on the hub":false,
        "Model Sha":"3ccc97066e70b9aa5f686083b7d406c312c490e2"
    },
    {
        "T":"\u2b55",
        "Model":"MexIvanov\/zephyr-python-ru",
        "Average":62.79,
        "ARC":56.14,
        "HellaSwag":82.03,
        "MMLU":60.18,
        "TruthfulQA":52.8,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"64a1984f1cba96880047c8f93a83fde9f5b1df35"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MLewdBoros-L2-13B",
        "Average":62.79,
        "ARC":62.54,
        "HellaSwag":83.9,
        "MMLU":56.57,
        "TruthfulQA":48.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"a3033ac5825662f1c66418d7543648dc76980185"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MexIvanov\/zephyr-python-ru-merged",
        "Average":62.78,
        "ARC":56.06,
        "HellaSwag":82.06,
        "MMLU":60.2,
        "TruthfulQA":52.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"046d180301dd6b764fc5def83f39c8b4aa62782f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishekchohan\/mistral-7B-forest-v0.1",
        "Average":62.78,
        "ARC":60.58,
        "HellaSwag":83.13,
        "MMLU":63.69,
        "TruthfulQA":43.7,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"3d07a56be8c1911d1eae3ff5dcaee134e400286c"
    },
    {
        "T":"?",
        "Model":"BioMistral\/BioMistral-7B-DARE",
        "Average":62.77,
        "ARC":58.28,
        "HellaSwag":79.87,
        "MMLU":57.34,
        "TruthfulQA":55.61,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"fc46b30e1cf0fe45280fd9b0a948fd9344b31112"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/koOpenChat-sft",
        "Average":62.77,
        "ARC":59.81,
        "HellaSwag":78.73,
        "MMLU":61.32,
        "TruthfulQA":51.24,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":0.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"47472b36e181694422564b130ee075ffa596537d"
    },
    {
        "T":"\u2b55",
        "Model":"mrm8488\/mistral-7b-ft-h4-no_robots_instructions",
        "Average":62.77,
        "ARC":60.92,
        "HellaSwag":83.17,
        "MMLU":63.37,
        "TruthfulQA":43.63,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"785446da9a53ceae48795069bf7ccaf46a91a5ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-1.1-L2-13B",
        "Average":62.77,
        "ARC":60.75,
        "HellaSwag":83.64,
        "MMLU":56.39,
        "TruthfulQA":50.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0f45a9f834dd216ce25ffa606b3b1ef2c99e7acd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-falcon-40b-v16.1-4k",
        "Average":62.77,
        "ARC":60.58,
        "HellaSwag":83.86,
        "MMLU":56.05,
        "TruthfulQA":50.57,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":41.35,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"4531abf8028eea1e94ad33697ff25cc53a6b10c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PulsarAI\/CollectiveCognition-v1.1-Nebula-7B",
        "Average":62.76,
        "ARC":58.11,
        "HellaSwag":82.39,
        "MMLU":57.03,
        "TruthfulQA":53.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c41d373a2d49b79236d6c4d0dfc4086e709c07eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Zangs3011\/mistral_7b_HalfEpoch_DolphinCoder",
        "Average":62.76,
        "ARC":61.69,
        "HellaSwag":82.38,
        "MMLU":61.44,
        "TruthfulQA":45.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"94f00b028f630b625759e3a7798f4c57ce753506"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/experiment2-cause-non",
        "Average":62.75,
        "ARC":60.32,
        "HellaSwag":82.92,
        "MMLU":62.3,
        "TruthfulQA":45.47,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3236726d4a5c4a3e18a8eedf35593bf4b1c14b8b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/experiment2-cause-non-qLoRa",
        "Average":62.75,
        "ARC":60.32,
        "HellaSwag":82.92,
        "MMLU":62.3,
        "TruthfulQA":45.47,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3236726d4a5c4a3e18a8eedf35593bf4b1c14b8b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-1.2-L2-13B",
        "Average":62.75,
        "ARC":60.75,
        "HellaSwag":83.67,
        "MMLU":56.27,
        "TruthfulQA":50.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e76f35fe771ef142d6629092bd4a93301fd6cd4a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"csujeong\/Mistral-7B-Finetuning-Insurance-16R",
        "Average":62.75,
        "ARC":60.84,
        "HellaSwag":83.44,
        "MMLU":63.61,
        "TruthfulQA":43.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0d0870a954b4741097e3400d52ba8f82ff553dc5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Doctor-Shotgun\/CalliopeDS-L2-13B",
        "Average":62.75,
        "ARC":60.49,
        "HellaSwag":83.38,
        "MMLU":55.8,
        "TruthfulQA":51.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":13.02,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"b373eda586a6527e62382eda5480204652a82499"
    },
    {
        "T":"?",
        "Model":"Locutusque\/hyperion-medium-preview",
        "Average":62.75,
        "ARC":60.67,
        "HellaSwag":83.67,
        "MMLU":63.73,
        "TruthfulQA":42.93,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"10ca1480890fc2f84c78941d81b3950efbb2c995"
    },
    {
        "T":"?",
        "Model":"Locutusque\/NeuralHyperion-Medium-Preview",
        "Average":62.75,
        "ARC":60.67,
        "HellaSwag":83.67,
        "MMLU":63.73,
        "TruthfulQA":42.93,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"a7c0648096a20b3c92b73628e0fb441f0968820b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/mistral_11B_instruct_v0.1",
        "Average":62.74,
        "ARC":53.75,
        "HellaSwag":74.64,
        "MMLU":58.93,
        "TruthfulQA":63.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5e4a8289201232e829ea1c0276d76ce1b003cc20"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Nous-Hermes-Platypus2-13B-QLoRA-0.80-epoch",
        "Average":62.74,
        "ARC":59.9,
        "HellaSwag":83.29,
        "MMLU":56.69,
        "TruthfulQA":51.08,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6e49d3d205e7f2e15c01ace0901da8931bbaab3b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"souvik0306\/mistral_7b_2epoch_norobots",
        "Average":62.74,
        "ARC":61.01,
        "HellaSwag":83.37,
        "MMLU":63.96,
        "TruthfulQA":42.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"624be22cfde6797a100230ec9dc1421f52eb0aa2"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2xOpenOrca-13B-IA3-v4",
        "Average":62.73,
        "ARC":61.43,
        "HellaSwag":81.84,
        "MMLU":59.02,
        "TruthfulQA":48.64,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3aa9abe9cb2e5c699f80935e04fbb351cdfbf21b"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2xOpenOrca-13B-IA3",
        "Average":62.73,
        "ARC":62.12,
        "HellaSwag":82.1,
        "MMLU":58.84,
        "TruthfulQA":47.88,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5ca46029dd22c007d4dc1706f6284a32be4546c2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeontaek\/Platypus2xOpenOrca-13B-IA3-ensemble",
        "Average":62.73,
        "ARC":62.12,
        "HellaSwag":82.28,
        "MMLU":59.07,
        "TruthfulQA":47.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"39d43c517b2847048111b971a600ce9998cdfddc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/mistral-7B-alpaca-1-epoch",
        "Average":62.72,
        "ARC":61.77,
        "HellaSwag":82.66,
        "MMLU":63.09,
        "TruthfulQA":43.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5da747d1460bf5637b82f9e2e1da0e49eb03ec8e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/LlongOrca-13B-16k",
        "Average":62.71,
        "ARC":62.46,
        "HellaSwag":82.75,
        "MMLU":55.54,
        "TruthfulQA":50.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"8ea1fb205553cadbc90069d80a7e58281b6281c3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Llamix2-MLewd-4x13B",
        "Average":62.71,
        "ARC":61.01,
        "HellaSwag":83.17,
        "MMLU":56.32,
        "TruthfulQA":50.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":38.5,
        "Hub":55,
        "Available on the hub":false,
        "Model Sha":"19961590ae95ccd9316b13c66098cd61b28a7d5a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gradientputri\/MegaMix-A1-13B",
        "Average":62.71,
        "ARC":61.6,
        "HellaSwag":83.49,
        "MMLU":58.26,
        "TruthfulQA":47.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"14e0756c210bcf420fbf825e6b8087ee5c716e7f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"indischepartij\/OpenMia-Indo-Mistral-7b",
        "Average":62.71,
        "ARC":59.64,
        "HellaSwag":83.18,
        "MMLU":62.75,
        "TruthfulQA":45.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6e58648fdfd147ede34d9e26ed70e4b8be302e58"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-AdventurousWinds-7b",
        "Average":62.71,
        "ARC":61.01,
        "HellaSwag":83.47,
        "MMLU":63.69,
        "TruthfulQA":42.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"ddc7e4fcbbb5c666a3fe1bbe4a47b4477151b699"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-Inverted-L2-13B",
        "Average":62.67,
        "ARC":59.3,
        "HellaSwag":82.9,
        "MMLU":56.45,
        "TruthfulQA":52.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"efaf592c95ae8e769e0d56d36ba4ed23e3bf4059"
    },
    {
        "T":"?",
        "Model":"ddyuudd\/mistral_nucleus09_32_sig",
        "Average":62.66,
        "ARC":59.73,
        "HellaSwag":83.14,
        "MMLU":61.42,
        "TruthfulQA":46.37,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"49774a1df696b8c8c539f615422518233d21675d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Ichsan2895\/Merak-7B-v5-PROTOTYPE1",
        "Average":62.66,
        "ARC":62.2,
        "HellaSwag":82.07,
        "MMLU":60.97,
        "TruthfulQA":45.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"abe6a0e87f3f90efddd5f8762188e0d59f60335b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ArianAskari\/SOLID-SFT-DPO-MixQV3-SOLIDRejected-SFTChosen-Zephyr-7b-beta",
        "Average":62.66,
        "ARC":59.3,
        "HellaSwag":81.34,
        "MMLU":60.23,
        "TruthfulQA":49.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6f3914fb205bfa5c37f14dd82f690319c210c876"
    },
    {
        "T":"?",
        "Model":"Locutusque\/Hyperion-3.0-Mixtral-3x7B",
        "Average":62.66,
        "ARC":60.67,
        "HellaSwag":83.28,
        "MMLU":63.22,
        "TruthfulQA":43.46,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":18.52,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"5d2ce88eac4a5081053d8400c0d99982147d4933"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Brouz\/Slerpeno",
        "Average":62.65,
        "ARC":61.69,
        "HellaSwag":84.1,
        "MMLU":56.77,
        "TruthfulQA":48.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":13.02,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"7ff32abd17851a769a031659e91e660f219be363"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airoboros-l2-13b-2.2.1",
        "Average":62.64,
        "ARC":60.92,
        "HellaSwag":83.77,
        "MMLU":56.47,
        "TruthfulQA":49.42,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"9b2dbc1f6f17a162228799df6e9449c903ddf04d"
    },
    {
        "T":"?",
        "Model":"Joseph717171\/Genstruct-10.7B",
        "Average":62.64,
        "ARC":60.84,
        "HellaSwag":82.81,
        "MMLU":60.27,
        "TruthfulQA":46.66,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"619648c528f000c38a6de60c69b689caa210bbc5"
    },
    {
        "T":"?",
        "Model":"Dans-DiscountModels\/Dans-07YahooAnswers-7b",
        "Average":62.64,
        "ARC":61.52,
        "HellaSwag":83.69,
        "MMLU":63.52,
        "TruthfulQA":41.84,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a9d5e333dd7752b689b97bc7e0cfbd530536a06e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/OpenRP-13B",
        "Average":62.63,
        "ARC":62.12,
        "HellaSwag":82.6,
        "MMLU":57.5,
        "TruthfulQA":48.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"d11815287c51ef51485fb003f8f72773cf6f19a4"
    },
    {
        "T":"?",
        "Model":"ddyuudd\/mistral_mbr_32_sig",
        "Average":62.62,
        "ARC":59.64,
        "HellaSwag":83.1,
        "MMLU":61.43,
        "TruthfulQA":46.31,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4dcd4403589a336c689164613576b83860f4602c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"indischepartij\/OpenMia-Indo-Mistral-7b-v2",
        "Average":62.62,
        "ARC":60.32,
        "HellaSwag":83.11,
        "MMLU":62.7,
        "TruthfulQA":44.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9e702a205749747a66aa94d4e4baed2824aac9d8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"garage-bAInd\/Camel-Platypus2-13B",
        "Average":62.62,
        "ARC":60.75,
        "HellaSwag":83.61,
        "MMLU":56.51,
        "TruthfulQA":49.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"0480a52799cb8e8de73bb41994df8b6b793937c7"
    },
    {
        "T":"?",
        "Model":"Eric111\/Yarn-Mistral-7b-128k-DPO",
        "Average":62.62,
        "ARC":60.84,
        "HellaSwag":82.99,
        "MMLU":63.09,
        "TruthfulQA":43.55,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4ad6b6614b4647e4c0cd5cc9aa38d71c944697a5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Alpacino30b",
        "Average":62.62,
        "ARC":62.71,
        "HellaSwag":85.04,
        "MMLU":58.48,
        "TruthfulQA":44.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":65,
        "Available on the hub":true,
        "Model Sha":"300bc5f3dc129a3d17adf059394e381eff7fbd55"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chlee10\/T3Q-Platypus-SOLAR",
        "Average":62.6,
        "ARC":61.86,
        "HellaSwag":84.18,
        "MMLU":53.72,
        "TruthfulQA":50.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.6,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c86f8fbea101541d5e93f055cd0ee5e9a897bf58"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/bubo-bubo-13b",
        "Average":62.6,
        "ARC":61.43,
        "HellaSwag":83.14,
        "MMLU":58.18,
        "TruthfulQA":47.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"1fe715732317ccd1c1cf295b97acd5765e209e01"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/SynthIA-v1.3-Nebula-v2-7B",
        "Average":62.59,
        "ARC":59.39,
        "HellaSwag":82.77,
        "MMLU":57.57,
        "TruthfulQA":50.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c6030620e9d4390d54ec221a18ff3e530f4dcd84"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"juhwanlee\/llmdo-Mistral-7B-case-c",
        "Average":62.58,
        "ARC":60.92,
        "HellaSwag":82.92,
        "MMLU":61.8,
        "TruthfulQA":44.69,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8d36b11a83dd1d4f69fbfedcbf13907ffba21756"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Hermes-Llama2-13b",
        "Average":62.58,
        "ARC":61.52,
        "HellaSwag":83.29,
        "MMLU":55.11,
        "TruthfulQA":50.38,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":12.85,
        "Hub":207,
        "Available on the hub":true,
        "Model Sha":"8f95aa9cd207db7b24179fc779c2b8973e71bee2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BlouseJury\/Mistral-7B-Discord-0.1",
        "Average":62.57,
        "ARC":60.24,
        "HellaSwag":83.13,
        "MMLU":62.82,
        "TruthfulQA":44.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"090a440c18ac262ecc045b798b72f99ba9a22c9c"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2xOpenOrca-13B-IA3-v3",
        "Average":62.57,
        "ARC":62.54,
        "HellaSwag":82.1,
        "MMLU":58.67,
        "TruthfulQA":46.96,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"17493c1f2e4620a44d7947edad0386d338e805ce"
    },
    {
        "T":"?",
        "Model":"OpenBuddy\/openbuddy-mistral2-7b-v20.2-32k",
        "Average":62.57,
        "ARC":56.91,
        "HellaSwag":79.45,
        "MMLU":60.73,
        "TruthfulQA":53.18,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.28,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bc0207c5bc5b6838c50ec8db77225f4f18cd7871"
    },
    {
        "T":"\u2b55",
        "Model":"FPHam\/Karen_TheEditor_V2_STRICT_Mistral_7B",
        "Average":62.57,
        "ARC":59.56,
        "HellaSwag":81.79,
        "MMLU":59.56,
        "TruthfulQA":49.36,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"0935960b2765aa23d7a63c49873361b09dd12f60"
    },
    {
        "T":"?",
        "Model":"Locutusque\/Hyperion-2.0-Mistral-7B",
        "Average":62.56,
        "ARC":61.09,
        "HellaSwag":83.5,
        "MMLU":63.68,
        "TruthfulQA":41.97,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"c12978bd7a8322533bfb8e077f32e8de89b2f63c"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/MythicalDestroyerV2-Platypus2-13B-QLora-0.80-epoch",
        "Average":62.55,
        "ARC":57.34,
        "HellaSwag":81.24,
        "MMLU":55.64,
        "TruthfulQA":55.98,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ada55b32fe8ed55b7691d997ad2e86f232c91aad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Locutusque\/Orca-2-13b-SFT-v4",
        "Average":62.54,
        "ARC":59.22,
        "HellaSwag":79.58,
        "MMLU":60.23,
        "TruthfulQA":51.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"f3491a4c169a5b8307383499b72ab9e2174c37da"
    },
    {
        "T":"\u2b55",
        "Model":"Locutusque\/Orca-2-13B-no_robots",
        "Average":62.54,
        "ARC":59.13,
        "HellaSwag":79.57,
        "MMLU":60.28,
        "TruthfulQA":51.17,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"6f32722f7d24501036698cbca9c7a3e2336f071f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sequelbox\/DiamondForce",
        "Average":62.52,
        "ARC":62.12,
        "HellaSwag":83.43,
        "MMLU":58.1,
        "TruthfulQA":46.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e92bbb8e6373408235e30cebcf4a71cc319b0ae3"
    },
    {
        "T":"?",
        "Model":"TheBloke\/VicUnlocked-30B-LoRA-HF",
        "Average":62.52,
        "ARC":59.73,
        "HellaSwag":84.02,
        "MMLU":57.81,
        "TruthfulQA":48.54,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"3259cb3c2a10cfb429fb51c4a76fffa049f4c44d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"meta-math\/MetaMath-Mistral-7B",
        "Average":62.52,
        "ARC":60.67,
        "HellaSwag":82.58,
        "MMLU":61.95,
        "TruthfulQA":44.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":73,
        "Available on the hub":false,
        "Model Sha":"016a7bb03bfcd953860357e1a16d5b333b887d26"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Nous-Hermes-13B-Code",
        "Average":62.52,
        "ARC":61.18,
        "HellaSwag":83.21,
        "MMLU":55.13,
        "TruthfulQA":50.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"5a45cb2a6442581ce32cc19c561c49cec1db4ebb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fzzhang\/mistralv1_gsm8k_merged_s",
        "Average":62.52,
        "ARC":62.03,
        "HellaSwag":83.95,
        "MMLU":61.66,
        "TruthfulQA":42.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d2c604a23f608864c60c8cd3de29ce9ff336e8e9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"The-Face-Of-Goonery\/Chronos-Beluga-v2-13bfp16",
        "Average":62.5,
        "ARC":60.75,
        "HellaSwag":81.94,
        "MMLU":54.08,
        "TruthfulQA":53.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"6d50e6681bc26c9bc0c8377c26c438e295ee0c2f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sr5434\/CodegebraGPT-10b",
        "Average":62.5,
        "ARC":59.81,
        "HellaSwag":83.42,
        "MMLU":60.2,
        "TruthfulQA":46.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"263f3e4c48d6fb001cd556010ee50a0b6918b8cb"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"awnr\/Mistral-7B-v0.1-half-naive-A",
        "Average":62.5,
        "ARC":60.32,
        "HellaSwag":83.22,
        "MMLU":64.16,
        "TruthfulQA":42.28,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"76e4d06445c9048988beaa9d44b294258796b98c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Hermes-Llama2-13b",
        "Average":62.49,
        "ARC":61.26,
        "HellaSwag":83.26,
        "MMLU":55.04,
        "TruthfulQA":50.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":12.85,
        "Hub":207,
        "Available on the hub":true,
        "Model Sha":"8f95aa9cd207db7b24179fc779c2b8973e71bee2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishek\/ccy0-2g7e-wqsa-0",
        "Average":62.49,
        "ARC":58.19,
        "HellaSwag":82.19,
        "MMLU":59.59,
        "TruthfulQA":49.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1cd1158f3104fa8ed8469e2b09d674b997e229b4"
    },
    {
        "T":"?",
        "Model":"PotatoOff\/Michel-13B",
        "Average":62.49,
        "ARC":61.26,
        "HellaSwag":83.21,
        "MMLU":55.05,
        "TruthfulQA":50.43,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":13.02,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"2d7bb01004f3bec6c4f4cfd27b9b896f5fa464a7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"222limin\/Liph-36-imatwarwithmyself",
        "Average":62.49,
        "ARC":62.37,
        "HellaSwag":77.16,
        "MMLU":58.14,
        "TruthfulQA":52.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0ba0be5ca330c67a3a248372b7513b3b94441352"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Monero\/WizardLM-Uncensored-SuperCOT-StoryTelling-30b",
        "Average":62.48,
        "ARC":59.64,
        "HellaSwag":79.9,
        "MMLU":54.42,
        "TruthfulQA":55.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":30,
        "Available on the hub":true,
        "Model Sha":"e58bafedf660477c206ad64f3118a571951bb28e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/zephyr-7b-beta-MultiLoRA-mmlu-merged",
        "Average":62.48,
        "ARC":57.94,
        "HellaSwag":81.43,
        "MMLU":58.57,
        "TruthfulQA":51.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a01728a56fbe14f6f348052a748c8c4c89bbc5f1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MSL7\/Liph.42-slerp",
        "Average":62.48,
        "ARC":62.54,
        "HellaSwag":77.12,
        "MMLU":58.2,
        "TruthfulQA":52.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ad4e7f913c15183cdcd7ea9e89b96a662e1eb55e"
    },
    {
        "T":"?",
        "Model":"liminerity\/Phigments12",
        "Average":62.46,
        "ARC":62.63,
        "HellaSwag":77.1,
        "MMLU":58.43,
        "TruthfulQA":51.71,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"387d840de390441faa8bdab7b44b0c65ec0abfa8"
    },
    {
        "T":"?",
        "Model":"TheBloke\/Wizard-Vicuna-30B-Uncensored-GPTQ",
        "Average":62.46,
        "ARC":61.09,
        "HellaSwag":82.4,
        "MMLU":56.46,
        "TruthfulQA":49.9,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":4.45,
        "Hub":339,
        "Available on the hub":true,
        "Model Sha":"56a82ece7a9309189561a590e8f4d2fe0d4be92b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/mistral-7b-openhermes-sft",
        "Average":62.46,
        "ARC":60.58,
        "HellaSwag":82.01,
        "MMLU":60.95,
        "TruthfulQA":46.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"911e144035ebbffe7fe41335cb0aca44c188fb58"
    },
    {
        "T":"?",
        "Model":"harshitv804\/MetaMath-Mistral-2x7B",
        "Average":62.46,
        "ARC":60.58,
        "HellaSwag":82.59,
        "MMLU":61.87,
        "TruthfulQA":44.8,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"193485a4016e12c1a3d3347801648fa4913dbd7c"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Qwen\/Qwen1.5-7B-Chat",
        "Average":62.45,
        "ARC":55.89,
        "HellaSwag":78.56,
        "MMLU":61.7,
        "TruthfulQA":53.65,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.72,
        "Hub":100,
        "Available on the hub":false,
        "Model Sha":"0addb6bfd79e59bce8f61ed60cdafd906c04d447"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BELLE-2\/BELLE-Llama2-13B-chat-0.4M",
        "Average":62.44,
        "ARC":60.67,
        "HellaSwag":82.31,
        "MMLU":55.94,
        "TruthfulQA":50.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":30,
        "Available on the hub":true,
        "Model Sha":"1776feacbf1052cff02eb3d7531a854555d3f6dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openbmb\/UltraLM-13b-v2.0",
        "Average":62.44,
        "ARC":62.63,
        "HellaSwag":81.49,
        "MMLU":56.17,
        "TruthfulQA":49.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"a452045c96ae62379a98ef0d85666616a66e78a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-thoughts-mistral-7b",
        "Average":62.42,
        "ARC":58.96,
        "HellaSwag":80.71,
        "MMLU":60.11,
        "TruthfulQA":49.91,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e4428eeadd912f5ad207c4c6f53b10b6ec537af9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"liminerity\/Liph.42",
        "Average":62.42,
        "ARC":62.29,
        "HellaSwag":77.12,
        "MMLU":58.2,
        "TruthfulQA":52.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b63ea4fb0fda33ac633f972094a5e23b05c92021"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"linlinlin\/zephy_SFT_Hermes",
        "Average":62.42,
        "ARC":60.32,
        "HellaSwag":83.37,
        "MMLU":63.81,
        "TruthfulQA":42.17,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d64495ffe34dbd40d5fe93639ca6f967d7c684cf"
    },
    {
        "T":"\u2b55",
        "Model":"Mihaiii\/Metis-0.1",
        "Average":62.42,
        "ARC":60.15,
        "HellaSwag":82.85,
        "MMLU":61.42,
        "TruthfulQA":45.24,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"ead51068b4208b37c37733109570b445d086551e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/OpenOrcaxOpenChat-Preview2-13B",
        "Average":62.41,
        "ARC":62.71,
        "HellaSwag":81.99,
        "MMLU":57.51,
        "TruthfulQA":47.45,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":89,
        "Available on the hub":true,
        "Model Sha":"26d1bc5c54c1f60a5de0b1ed4d0b16f285aee230"
    },
    {
        "T":"?",
        "Model":"Qwen\/Qwen1.5-7B-Chat",
        "Average":62.41,
        "ARC":55.89,
        "HellaSwag":78.56,
        "MMLU":61.65,
        "TruthfulQA":53.54,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.72,
        "Hub":100,
        "Available on the hub":false,
        "Model Sha":"0addb6bfd79e59bce8f61ed60cdafd906c04d447"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"alnrg2arg\/blockchainlabs_7B_merged_test2_4_prune",
        "Average":62.41,
        "ARC":60.58,
        "HellaSwag":77.74,
        "MMLU":52.27,
        "TruthfulQA":59.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"15b6cd5986ef27910202295530522cd433538a72"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"mistralai\/Mistral-7B-v0.1",
        "Average":62.4,
        "ARC":59.98,
        "HellaSwag":83.31,
        "MMLU":64.16,
        "TruthfulQA":42.15,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3048,
        "Available on the hub":false,
        "Model Sha":"e836d8f71b5812f9fee65618453dc537c66bd82a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JCX-kcuf\/Mistral-7B-v0.1-gpt-4-20k",
        "Average":62.4,
        "ARC":60.07,
        "HellaSwag":83.3,
        "MMLU":64.09,
        "TruthfulQA":42.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"31724e80517950b4b80e03754619be2b24b824af"
    },
    {
        "T":"?",
        "Model":"Cartinoe5930\/Llama2_init_Mistral",
        "Average":62.4,
        "ARC":60.07,
        "HellaSwag":83.3,
        "MMLU":64.09,
        "TruthfulQA":42.15,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e6d5223e089c417e29f56c5750a91e26e8fd5e01"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hamxea\/Mistral-7B-v0.1-activity-fine-tuned-v5",
        "Average":62.4,
        "ARC":60.07,
        "HellaSwag":83.3,
        "MMLU":64.09,
        "TruthfulQA":42.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ddeca14550068d75b10801ab1d261632b15f6264"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hamxea\/Mistral-7B-v0.1-activity-fine-tuned-v3",
        "Average":62.4,
        "ARC":60.07,
        "HellaSwag":83.3,
        "MMLU":64.09,
        "TruthfulQA":42.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ee975408108178dcd9b4f3bfbb5ed000357ce6b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hamxea\/Mistral-7B-v0.1-activity-fine-tuned-v2",
        "Average":62.4,
        "ARC":60.07,
        "HellaSwag":83.3,
        "MMLU":64.09,
        "TruthfulQA":42.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"72f64c7d384fde5d89736efa5a514cae84a2995f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sr5434\/CodegebraGPT-10b",
        "Average":62.4,
        "ARC":59.56,
        "HellaSwag":83.45,
        "MMLU":60.07,
        "TruthfulQA":46.53,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"15e64a7f77eba0367eedbaaacb3560351471093b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Reverb\/Mistral-7B-LoreWeaver",
        "Average":62.39,
        "ARC":59.98,
        "HellaSwag":83.29,
        "MMLU":64.12,
        "TruthfulQA":42.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"1e1796b7230cd5ba6146d748a90db15493465f22"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-mistral-moloras-7b",
        "Average":62.39,
        "ARC":59.98,
        "HellaSwag":83.29,
        "MMLU":64.12,
        "TruthfulQA":42.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"7ef22bee2557aab8a29331653965b3fca22c9a97"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/CausalLM-Platypus-14B",
        "Average":62.38,
        "ARC":56.91,
        "HellaSwag":80.06,
        "MMLU":64.98,
        "TruthfulQA":47.57,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":14.17,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"1659d3cdbb8bb8dba902ab2874f4fa886980fc70"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Dans-DiscountModels\/Mistral-7b-FFT-Test3",
        "Average":62.38,
        "ARC":60.41,
        "HellaSwag":82.31,
        "MMLU":62.45,
        "TruthfulQA":44.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ff6ab8204162794d7d74297d60acb741c2ef8e3a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Austism\/chronos-hermes-13b-v2",
        "Average":62.38,
        "ARC":60.32,
        "HellaSwag":83.21,
        "MMLU":55.05,
        "TruthfulQA":50.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"2f0e2cb734685a6ce0736a9f3e909a795d7592cc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sauce1337\/AppleSauce-L2-13b",
        "Average":62.37,
        "ARC":61.01,
        "HellaSwag":83.61,
        "MMLU":57.07,
        "TruthfulQA":47.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ba253c52eb85e24987c81e5d36b5a9a00e276ce7"
    },
    {
        "T":"?",
        "Model":"Locutusque\/Hyperion-1.5-Mistral-7B",
        "Average":62.37,
        "ARC":60.49,
        "HellaSwag":83.64,
        "MMLU":63.57,
        "TruthfulQA":41.78,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"ff35c0c0b6f925ac510a6692cc21e813457b1fbb"
    },
    {
        "T":"?",
        "Model":"InnerI\/InnerI-AI-sn6-7B-slerp",
        "Average":62.37,
        "ARC":58.36,
        "HellaSwag":77.58,
        "MMLU":58.82,
        "TruthfulQA":54.7,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0823e2608713b502626b28a267cf81b7a7cd7d5e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-Inverted-1.2-L2-13B",
        "Average":62.35,
        "ARC":59.39,
        "HellaSwag":83.01,
        "MMLU":55.77,
        "TruthfulQA":51.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8d2e9087093eef1c9173e167beb40b9d034a4655"
    },
    {
        "T":"?",
        "Model":"ddyuudd\/mistral_kmmbr_32_sig",
        "Average":62.35,
        "ARC":58.96,
        "HellaSwag":82.84,
        "MMLU":61.39,
        "TruthfulQA":46.2,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"257fbb05778a72079d3ef3b881335c24bc37c3f7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"InnerI\/A-I-0xtom-7B-slerp",
        "Average":62.34,
        "ARC":58.19,
        "HellaSwag":77.64,
        "MMLU":58.74,
        "TruthfulQA":54.78,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e7299eec852381a17aa9c0720322c1db065753f4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Medilora\/medilora-qwen-14b",
        "Average":62.34,
        "ARC":56.66,
        "HellaSwag":79.08,
        "MMLU":65.86,
        "TruthfulQA":47.75,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":14.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"0649cf49b7a879fe837567a346a3ebbbac77614a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CausalLM\/14B",
        "Average":62.34,
        "ARC":56.66,
        "HellaSwag":79.08,
        "MMLU":65.86,
        "TruthfulQA":47.75,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":14.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2576a37434e2e03804c841d36c669c8a34c729de"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2xOpenOrca-13B-IA3-v2",
        "Average":62.33,
        "ARC":62.29,
        "HellaSwag":82.09,
        "MMLU":57.91,
        "TruthfulQA":47.03,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6c8129720b3909afea42f3e38516f4f531063b17"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2xOpenOrca-13B-IA3-v2.1",
        "Average":62.33,
        "ARC":62.29,
        "HellaSwag":82.09,
        "MMLU":57.91,
        "TruthfulQA":47.03,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"31e1e3235515717a151915131bc970be188d964e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Gryphe\/MythoLogic-L2-13b",
        "Average":62.32,
        "ARC":61.01,
        "HellaSwag":83.93,
        "MMLU":55.7,
        "TruthfulQA":48.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"665948fc79acc2bcce3e9e7d2b0689ca43ae62d4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-mistral-hermes-code-7b",
        "Average":62.31,
        "ARC":59.39,
        "HellaSwag":78.59,
        "MMLU":59.95,
        "TruthfulQA":51.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"12afe40d27008de12bb786795229174f3d6ab8d3"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"KnutJaegersberg\/Qwen-14B-Llamafied",
        "Average":62.31,
        "ARC":55.2,
        "HellaSwag":82.31,
        "MMLU":66.11,
        "TruthfulQA":45.6,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":13.39,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"c53e0ea05664c66346627714f332a9b46cde8fd6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Dans-DiscountModels\/Mistral-7b-FFT-Test3",
        "Average":62.29,
        "ARC":60.24,
        "HellaSwag":82.36,
        "MMLU":62.2,
        "TruthfulQA":44.36,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ff6ab8204162794d7d74297d60acb741c2ef8e3a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"decem\/Dionysus-Mistral-n1-v1",
        "Average":62.27,
        "ARC":60.24,
        "HellaSwag":81.6,
        "MMLU":59.32,
        "TruthfulQA":47.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d60ffacb4671aa412dde58d6c58173296cb0d566"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-mistral-hermes-code-7b",
        "Average":62.27,
        "ARC":59.39,
        "HellaSwag":78.55,
        "MMLU":59.88,
        "TruthfulQA":51.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"12afe40d27008de12bb786795229174f3d6ab8d3"
    },
    {
        "T":"\u2b55",
        "Model":"kimwooglae\/AISquare-Instruct-SOLAR-10.7b-v0.5.31",
        "Average":62.27,
        "ARC":60.67,
        "HellaSwag":84.2,
        "MMLU":52.86,
        "TruthfulQA":51.35,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.6,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a8ef130719aa323afa1fec4ce4ebb9236a1d57a0"
    },
    {
        "T":"?",
        "Model":"jingyeom\/freeze_KoSoLAR-10.7B-v0.2_1.4_dedup",
        "Average":62.26,
        "ARC":58.45,
        "HellaSwag":81.26,
        "MMLU":64.83,
        "TruthfulQA":44.5,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.8,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f090bee9157ddc907f747408ec39098c8d676d22"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Walmart-the-bag\/Yi-6B-Infinity-Chat",
        "Average":62.26,
        "ARC":56.57,
        "HellaSwag":77.66,
        "MMLU":64.05,
        "TruthfulQA":50.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc",
        "#Params (B)":6.06,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"7a441a69e1ebd192fbf52b904589130c3875aacc"
    },
    {
        "T":"\u2b55",
        "Model":"totally-not-an-llm\/PuddleJumper-13b-V2",
        "Average":62.25,
        "ARC":57.0,
        "HellaSwag":81.06,
        "MMLU":58.3,
        "TruthfulQA":52.66,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"1fe9494e334a32ba73dc2926f58246450850c534"
    },
    {
        "T":"?",
        "Model":"liminerity\/phigment6-slerp",
        "Average":62.25,
        "ARC":62.63,
        "HellaSwag":77.25,
        "MMLU":58.65,
        "TruthfulQA":50.49,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"976d97de8cb3a7af72aa6ef9583d186f6911f919"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"EllieS\/zephyr-7b-dpo-lora-pubmedqa-ultrafeedback",
        "Average":62.25,
        "ARC":60.49,
        "HellaSwag":83.13,
        "MMLU":60.58,
        "TruthfulQA":44.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b688325e94904c69ab3815543d5ec51e1e869e8b"
    },
    {
        "T":"?",
        "Model":"Azure99\/blossom-v5-7b",
        "Average":62.25,
        "ARC":56.06,
        "HellaSwag":77.36,
        "MMLU":61.29,
        "TruthfulQA":54.29,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"adac4d4172343ca5fa56c788615cb1a98f03794d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jingyeom\/KoSoLAR-10.7B-v0.2_1.4_dedup",
        "Average":62.23,
        "ARC":60.07,
        "HellaSwag":82.18,
        "MMLU":61.3,
        "TruthfulQA":45.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.8,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0945d3f95080bc4adc06964ecbc8131d90456bd0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Severian\/ANIMA-Phi-Neptune-Mistral-7B-v3",
        "Average":62.22,
        "ARC":56.83,
        "HellaSwag":78.82,
        "MMLU":53.84,
        "TruthfulQA":59.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fd6fda131561917202905be1f4f3b0adc13efdb5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NovoCode\/Novocode7b-v3",
        "Average":62.22,
        "ARC":57.51,
        "HellaSwag":81.17,
        "MMLU":61.91,
        "TruthfulQA":48.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0b6cba6cc3071b54e70c91d1d9e5463f1aa9c942"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"namirocks\/tutor-model-13b-ep3",
        "Average":62.21,
        "ARC":57.34,
        "HellaSwag":81.51,
        "MMLU":57.02,
        "TruthfulQA":52.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"714f04010ca1c3d72bbeead4a14695576ad36a88"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/vicuna-class-tutor-13b-ep3",
        "Average":62.21,
        "ARC":57.34,
        "HellaSwag":81.51,
        "MMLU":57.02,
        "TruthfulQA":52.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2cf2424169d31299caff38cd7ac68e69974d6535"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/orca_mini_v3_13B-GPTQ",
        "Average":62.21,
        "ARC":61.95,
        "HellaSwag":81.56,
        "MMLU":56.1,
        "TruthfulQA":49.22,
        "Type":"fine-tuned",
        "Precision":"None",
        "Hub License":"other",
        "#Params (B)":2.03,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"7b7a2dcd946f393e26215268c4c7e0699be2bbd8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"maldv\/electric-mist-7b",
        "Average":62.2,
        "ARC":61.18,
        "HellaSwag":82.56,
        "MMLU":59.71,
        "TruthfulQA":45.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b224f450fa0db5e09ce96ee3b4c4bc9c2e614c84"
    },
    {
        "T":"?",
        "Model":"Locutusque\/Hyperion-3.0-Mistral-7B-alpha",
        "Average":62.2,
        "ARC":59.98,
        "HellaSwag":83.48,
        "MMLU":62.5,
        "TruthfulQA":42.82,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"ca8e69436624292143bda2c80be29d9d47becfb1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"frankenmerger\/delta-4b-orange",
        "Average":62.19,
        "ARC":58.87,
        "HellaSwag":76.59,
        "MMLU":56.5,
        "TruthfulQA":56.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":4.67,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b701c3329f7ecb6cafe7f38b27f59eea548a9c92"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"gmonsoon\/delta-4b-orange",
        "Average":62.19,
        "ARC":58.87,
        "HellaSwag":76.59,
        "MMLU":56.5,
        "TruthfulQA":56.82,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":4.67,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b701c3329f7ecb6cafe7f38b27f59eea548a9c92"
    },
    {
        "T":"?",
        "Model":"0-hero\/Matter-0.1-7B",
        "Average":62.19,
        "ARC":61.77,
        "HellaSwag":82.14,
        "MMLU":62.42,
        "TruthfulQA":42.44,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"035c8193ce71be90be7d90098669afb9164ec6cb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"augtoma\/qCammel-13",
        "Average":62.19,
        "ARC":60.84,
        "HellaSwag":83.66,
        "MMLU":56.73,
        "TruthfulQA":47.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"af473e64f6a4fa02a7e24ee7679eea9505eb179d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rhysjones\/phi-2-orange-v2",
        "Average":62.18,
        "ARC":61.86,
        "HellaSwag":76.32,
        "MMLU":55.72,
        "TruthfulQA":54.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":21,
        "Available on the hub":false,
        "Model Sha":"c18e2743c806a9730659e7c3c627b9b01d1ff8ce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aivince\/alpaca_mistral-7b-v0.2",
        "Average":62.17,
        "ARC":60.92,
        "HellaSwag":83.28,
        "MMLU":61.82,
        "TruthfulQA":42.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a7f91c5db12f3baf8d4e0279dde5a2183ddb070c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama2-13b-v1.2",
        "Average":62.17,
        "ARC":60.67,
        "HellaSwag":80.46,
        "MMLU":56.51,
        "TruthfulQA":51.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"97279d20a8c7e2d0576c9ff4b2e15a421c40d58a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/StableBeluga-13B-instruct-PL-lora_unload",
        "Average":62.17,
        "ARC":60.92,
        "HellaSwag":82.13,
        "MMLU":56.99,
        "TruthfulQA":48.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6e1a6e1f91f6ac97b643be1bd24be6096e2e7dd3"
    },
    {
        "T":"?",
        "Model":"chrischain\/SatoshiNv5",
        "Average":62.16,
        "ARC":60.49,
        "HellaSwag":82.94,
        "MMLU":63.42,
        "TruthfulQA":41.8,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"e4c3153b0bfdd9a927c2cad3fc44f4a93f6f0884"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Weyaxi\/Mistral-7B-v0.2-hf-duplicate",
        "Average":62.16,
        "ARC":60.49,
        "HellaSwag":82.94,
        "MMLU":63.42,
        "TruthfulQA":41.8,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"835d752a28c1d458d9fcc8f98beb878c4f35a06f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"unsloth\/mistral-7b-v0.2",
        "Average":62.16,
        "ARC":60.49,
        "HellaSwag":82.94,
        "MMLU":63.42,
        "TruthfulQA":41.8,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"8b2d7b48e924f9ae1ec3882ce01a7a3e78fa430e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andreaskoepf\/llama2-13b-megacode2_min100",
        "Average":62.16,
        "ARC":60.58,
        "HellaSwag":81.26,
        "MMLU":57.92,
        "TruthfulQA":48.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b38d1b53c358a0313c69bcceebe97628327ada82"
    },
    {
        "T":"\u2b55",
        "Model":"rombodawg\/LosslessMegaCoder-llama2-13b-mini",
        "Average":62.16,
        "ARC":60.58,
        "HellaSwag":81.26,
        "MMLU":57.92,
        "TruthfulQA":48.89,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"1f5609ffd40bc3af2dcbc5c88e9312d47a73c4b4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"manishiitg\/open-aditi-hi-v2",
        "Average":62.16,
        "ARC":59.39,
        "HellaSwag":82.01,
        "MMLU":61.41,
        "TruthfulQA":45.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"a142544973d7baa480ca71145ae297343ed84d38"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BlouseJury\/Mistral-7B-Discord-0.2",
        "Average":62.16,
        "ARC":60.58,
        "HellaSwag":82.49,
        "MMLU":62.82,
        "TruthfulQA":42.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"721a1203baea1e9b234e90f98aebdca0e556153f"
    },
    {
        "T":"\u2b55",
        "Model":"yulan-team\/YuLan-Chat-2-13b-fp16",
        "Average":62.15,
        "ARC":59.04,
        "HellaSwag":80.66,
        "MMLU":56.72,
        "TruthfulQA":52.18,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.95,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"2d439187efd6edd91a0c0146f08dff52d92aa7bc"
    },
    {
        "T":"?",
        "Model":"Corianas\/NearalMistral-2x7B",
        "Average":62.15,
        "ARC":57.42,
        "HellaSwag":77.67,
        "MMLU":56.46,
        "TruthfulQA":57.03,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d290558a090dfaeeca02e48e04ad0bf9ecdc39c8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sethuiyer\/Dr_Samantha_7b_mistral",
        "Average":62.14,
        "ARC":60.41,
        "HellaSwag":83.65,
        "MMLU":63.14,
        "TruthfulQA":41.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"e0201aa9423f082a4182cbf910d75ba438528ddb"
    },
    {
        "T":"\u2b55",
        "Model":"osanseviero\/mistral-instruct-slerp",
        "Average":62.14,
        "ARC":57.42,
        "HellaSwag":78.34,
        "MMLU":55.19,
        "TruthfulQA":57.61,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1994dd1daadcfd88c471531e6a264271d6e07b4d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishek\/autotrain-ixpiv-6kj1e",
        "Average":62.14,
        "ARC":61.69,
        "HellaSwag":82.54,
        "MMLU":58.61,
        "TruthfulQA":45.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6081141e37bb24ef90b1bb7464d53107e48c5fb4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gradientputri\/MegaMix-S1-13B",
        "Average":62.13,
        "ARC":62.46,
        "HellaSwag":83.65,
        "MMLU":57.88,
        "TruthfulQA":44.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"afca2c9488cf8738faec4db6721f6a4c755a5d81"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/mistral-7B-alpaca-case-3-2",
        "Average":62.12,
        "ARC":62.2,
        "HellaSwag":83.19,
        "MMLU":62.19,
        "TruthfulQA":40.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"27be9a3845418f925ade3c1cbb4f09ef4a2f5af1"
    },
    {
        "T":"?",
        "Model":"vilm\/Quyen-Plus-v0.1",
        "Average":62.07,
        "ARC":55.72,
        "HellaSwag":78.52,
        "MMLU":60.45,
        "TruthfulQA":53.6,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.72,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"b5bd5bf9c0c0976e18fa7341326a87090d8aa626"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"spmurrayzzz\/Mistral-Syndicate-7B",
        "Average":62.07,
        "ARC":60.84,
        "HellaSwag":82.91,
        "MMLU":60.83,
        "TruthfulQA":43.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d95d34db5d0aa50fd3b3594d1632c6ce69937243"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HenryJJ\/Instruct_Mistral-7B-v0.1_Dolly15K",
        "Average":62.07,
        "ARC":59.39,
        "HellaSwag":82.62,
        "MMLU":62.71,
        "TruthfulQA":43.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c1d04418a3f404a9500c8292ec912e2b00694f45"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Orca-Nova-13B",
        "Average":62.07,
        "ARC":62.37,
        "HellaSwag":82.47,
        "MMLU":57.44,
        "TruthfulQA":45.97,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5a6c3686749ecb76971a915403da8c07a98078a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/mistralai-case-2-0",
        "Average":62.06,
        "ARC":60.41,
        "HellaSwag":83.08,
        "MMLU":62.94,
        "TruthfulQA":41.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5721e98796e26536d9df830647cc46cc2b34c0a5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/mistralai-case-1-0",
        "Average":62.06,
        "ARC":60.41,
        "HellaSwag":83.08,
        "MMLU":62.94,
        "TruthfulQA":41.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c2ce0cb3094b8eb6b33cf08247d50a16204dd894"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/mistralai-case-0-0",
        "Average":62.06,
        "ARC":60.41,
        "HellaSwag":83.08,
        "MMLU":62.94,
        "TruthfulQA":41.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"15ba501a51a1404a440b2db715695efc9154027a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"acrastt\/kalomaze-stuff",
        "Average":62.06,
        "ARC":59.64,
        "HellaSwag":83.55,
        "MMLU":63.41,
        "TruthfulQA":41.64,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7752f615d76e515aa956335ba8d2705c2cbc297b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ZoidBB\/unraveled-7b-a1",
        "Average":62.06,
        "ARC":59.81,
        "HellaSwag":82.8,
        "MMLU":63.39,
        "TruthfulQA":42.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fac05775fa8121b58cda8031b7001323bd43983d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elinas\/chronos-33b",
        "Average":62.05,
        "ARC":62.2,
        "HellaSwag":83.48,
        "MMLU":55.87,
        "TruthfulQA":46.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":23,
        "Available on the hub":true,
        "Model Sha":"3c11f81d9180618f13777276b1eb0eb70ab99cf0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"manishiitg\/open-aditi-hi-v4",
        "Average":62.05,
        "ARC":60.15,
        "HellaSwag":81.84,
        "MMLU":61.32,
        "TruthfulQA":44.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b396464b51acb625c8bf1875fb8fb0ebe973e9a3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama2-13b",
        "Average":62.04,
        "ARC":59.13,
        "HellaSwag":81.99,
        "MMLU":55.49,
        "TruthfulQA":51.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"6e918dc8beb1e764def5938fdb8e3f64ba40a456"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Medusa-13b",
        "Average":62.04,
        "ARC":58.19,
        "HellaSwag":81.35,
        "MMLU":57.39,
        "TruthfulQA":51.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"be755c9eef8233ca59e0178db75de878f5859222"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/mythalion-13b",
        "Average":62.04,
        "ARC":61.26,
        "HellaSwag":83.81,
        "MMLU":56.53,
        "TruthfulQA":46.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":25,
        "Available on the hub":true,
        "Model Sha":"24916f62b8243a7e4646ea53eeb45d890cbd308f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"0-hero\/Matter-0.1-Slim-7B-C",
        "Average":62.01,
        "ARC":61.35,
        "HellaSwag":81.76,
        "MMLU":61.45,
        "TruthfulQA":43.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ce9d68b563a4ad68ac6349672ca3b2f9ca492957"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewqr2130\/mistral-se-inst-ppo",
        "Average":62.01,
        "ARC":56.31,
        "HellaSwag":79.49,
        "MMLU":60.91,
        "TruthfulQA":51.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f9d25d717f3972f80336fd15450329e2d8ee3ed4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/mistralai-case-0-1",
        "Average":62.01,
        "ARC":60.84,
        "HellaSwag":83.05,
        "MMLU":62.72,
        "TruthfulQA":41.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a0386ced06a6282843b950fb549a28dd96b9fd20"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zelus82\/Obelix-Phi2",
        "Average":62.0,
        "ARC":61.77,
        "HellaSwag":76.76,
        "MMLU":58.19,
        "TruthfulQA":51.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a9e4d388944139cf342c6118202e4b3440794770"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-qwen1.5-en-7b-dpo-v0.1",
        "Average":62.0,
        "ARC":54.35,
        "HellaSwag":76.04,
        "MMLU":61.21,
        "TruthfulQA":56.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.72,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"f4f3ed15b4d59f3e22a290085a51aa5f1ac39455"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"spmurrayzzz\/Mistral-Syndicate-7B",
        "Average":61.99,
        "ARC":60.84,
        "HellaSwag":82.88,
        "MMLU":60.52,
        "TruthfulQA":43.73,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d95d34db5d0aa50fd3b3594d1632c6ce69937243"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/SlimOrca-13B",
        "Average":61.99,
        "ARC":60.15,
        "HellaSwag":81.4,
        "MMLU":57.04,
        "TruthfulQA":49.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":12.85,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"75427e93dc99a5e1d8b9aefa106ad36fc750b744"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/Mistral-7B-Alpaca-52k-v0.1",
        "Average":61.99,
        "ARC":60.92,
        "HellaSwag":82.13,
        "MMLU":63.41,
        "TruthfulQA":41.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"6ea2490bdb8511490f21188e4a2368ea37557ebd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-13B-v1.2",
        "Average":61.99,
        "ARC":61.26,
        "HellaSwag":82.93,
        "MMLU":56.47,
        "TruthfulQA":47.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"60d4937ac3c4dcb84c40bbf7265c5cc7f5f3d4f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OptimalScale\/robin-65b-v2-delta",
        "Average":61.98,
        "ARC":60.75,
        "HellaSwag":81.62,
        "MMLU":60.82,
        "TruthfulQA":44.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":65.02,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"cde761c8c5e956a4d981d396f993f46971ea2cd4"
    },
    {
        "T":"?",
        "Model":"minlik\/chinese-alpaca-33b-merged",
        "Average":61.97,
        "ARC":59.3,
        "HellaSwag":78.43,
        "MMLU":57.69,
        "TruthfulQA":52.45,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.44,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"fc2535104c0b48afc42575f9fe10bbcbb7612ec3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-mistral-7b-v17.1-32k",
        "Average":61.96,
        "ARC":55.55,
        "HellaSwag":77.95,
        "MMLU":58.29,
        "TruthfulQA":56.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.28,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"43f9853350f222b3802d6df332d026d344626aee"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xwin-LM\/Xwin-LM-13B-V0.1",
        "Average":61.96,
        "ARC":62.54,
        "HellaSwag":82.8,
        "MMLU":56.53,
        "TruthfulQA":45.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":58,
        "Available on the hub":true,
        "Model Sha":"32938856dc3d713dcba706aded7c82791b6ff647"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/llama2-13b-megacode2-oasst",
        "Average":61.95,
        "ARC":60.67,
        "HellaSwag":81.93,
        "MMLU":57.38,
        "TruthfulQA":47.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"2c45ecf161da2ff2aa984900f2e4d2b7a7311ab8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kwchoi\/DPO_mistral_v01_7b_ultra_0130_1k",
        "Average":61.95,
        "ARC":57.17,
        "HellaSwag":79.16,
        "MMLU":55.85,
        "TruthfulQA":55.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"21b07f830456a4793db84060af6026597668bfd4"
    },
    {
        "T":"?",
        "Model":"Novocoders\/Mistral-NeuralDPO-v0.7",
        "Average":61.94,
        "ARC":65.87,
        "HellaSwag":84.4,
        "MMLU":57.6,
        "TruthfulQA":39.91,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3e86d80ad5268021c77f0b86b047df5467b174f8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/mistralai-case-1-1",
        "Average":61.94,
        "ARC":60.92,
        "HellaSwag":82.87,
        "MMLU":62.87,
        "TruthfulQA":41.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e7b1b13c3618dc97d3562984447af3772d3c76a0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/CreativityEngine",
        "Average":61.94,
        "ARC":59.3,
        "HellaSwag":82.42,
        "MMLU":53.55,
        "TruthfulQA":52.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7870cc50b82b5cbebfa9935b6d73a9d20170299a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cookinai\/Blitz-v0.2",
        "Average":61.93,
        "ARC":59.04,
        "HellaSwag":83.0,
        "MMLU":62.96,
        "TruthfulQA":42.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ce6df89ef2377c14f2fb5d9b7810b2f65b7fc997"
    },
    {
        "T":"\u2b55",
        "Model":"lu-vae\/llama2-13B-sharegpt4-orca-openplatypus-8w",
        "Average":61.91,
        "ARC":62.8,
        "HellaSwag":84.04,
        "MMLU":55.13,
        "TruthfulQA":45.66,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ad086aacf0176911133b6cccfb34364afce9de5a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/OpenAssistant-SFT-7-Llama-30B-HF",
        "Average":61.91,
        "ARC":60.58,
        "HellaSwag":82.17,
        "MMLU":57.93,
        "TruthfulQA":46.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"a7a2306b9a63de2c545f35b24735f4540baf5903"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sarahlintang\/mistral-indo-7b",
        "Average":61.9,
        "ARC":61.09,
        "HellaSwag":81.19,
        "MMLU":62.99,
        "TruthfulQA":42.34,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"eb5051623b2057c2af3d69247a649d4e8ec5b111"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"01-ai\/Yi-9B-200K",
        "Average":61.89,
        "ARC":58.02,
        "HellaSwag":78.58,
        "MMLU":70.34,
        "TruthfulQA":40.63,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":8.83,
        "Hub":65,
        "Available on the hub":true,
        "Model Sha":"f5ced3c13a454363282a9c463d30b6cf5b989893"
    },
    {
        "T":"?",
        "Model":"OpenBuddy\/openbuddy-mistral-7b-v17.1-32k",
        "Average":61.88,
        "ARC":55.38,
        "HellaSwag":78.0,
        "MMLU":58.08,
        "TruthfulQA":56.07,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.28,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"43f9853350f222b3802d6df332d026d344626aee"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/UndiMix-v1-13b",
        "Average":61.88,
        "ARC":59.47,
        "HellaSwag":82.45,
        "MMLU":55.83,
        "TruthfulQA":49.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"fd311f52648825d6988d2f945918468ceb32289f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/HelpSteer-filtered-7B",
        "Average":61.88,
        "ARC":59.56,
        "HellaSwag":83.32,
        "MMLU":63.52,
        "TruthfulQA":41.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0f14404caa1b4609bb2f50714df973223f443e40"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/mistralai-case-2-1",
        "Average":61.88,
        "ARC":60.92,
        "HellaSwag":82.54,
        "MMLU":62.54,
        "TruthfulQA":41.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8c1a2a752a63904e81061a7e65920d92fba01929"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/Samantha-1.11-13b",
        "Average":61.88,
        "ARC":60.84,
        "HellaSwag":82.99,
        "MMLU":55.96,
        "TruthfulQA":47.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"e355ead3a939f471fe2586201156fb972fad0f4b"
    },
    {
        "T":"?",
        "Model":"Locutusque\/OpenCerebrum-1.0-7b-SFT",
        "Average":61.87,
        "ARC":60.07,
        "HellaSwag":83.25,
        "MMLU":62.71,
        "TruthfulQA":41.45,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"05f8aa218b005048ad9aef2e72852b4ac376766a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beaugogh\/Llama2-13b-sharegpt4",
        "Average":61.87,
        "ARC":61.77,
        "HellaSwag":84.53,
        "MMLU":55.21,
        "TruthfulQA":45.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"294c40349bf0c5377f71d92e7539bf5de3176a74"
    },
    {
        "T":"?",
        "Model":"Locutusque\/NeuralHyperion-2.0-Mistral-7B",
        "Average":61.86,
        "ARC":57.76,
        "HellaSwag":82.29,
        "MMLU":61.9,
        "TruthfulQA":45.5,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"01cfc8a66cab065fba04130e64a89743c881aeca"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/llama-2-16b-nastychat",
        "Average":61.86,
        "ARC":57.42,
        "HellaSwag":80.59,
        "MMLU":55.99,
        "TruthfulQA":53.45,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":16.19,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"6fb7f82d486b3eee53d750f83cc7eae434349809"
    },
    {
        "T":"?",
        "Model":"ddyuudd\/mistral_dmbr20_32_sig",
        "Average":61.85,
        "ARC":58.7,
        "HellaSwag":82.54,
        "MMLU":61.41,
        "TruthfulQA":44.75,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2f6b2e47ddcde6ae6d7b690c2c2ff1d7be9d3e1b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aeala\/GPT4-x-AlpacaDente2-30b",
        "Average":61.85,
        "ARC":60.58,
        "HellaSwag":81.81,
        "MMLU":56.63,
        "TruthfulQA":48.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":29,
        "Available on the hub":true,
        "Model Sha":"9fe5a8dada738f44e7ee9293b2140ae0be021787"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Yarn-Mistral-7b-64k",
        "Average":61.81,
        "ARC":59.9,
        "HellaSwag":82.51,
        "MMLU":62.96,
        "TruthfulQA":41.86,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":45,
        "Available on the hub":false,
        "Model Sha":"0273c624561fcecc8e8f4030492a9307aa60f945"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zelus82\/Obelix-Phi2",
        "Average":61.8,
        "ARC":61.6,
        "HellaSwag":76.68,
        "MMLU":58.14,
        "TruthfulQA":50.79,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a9e4d388944139cf342c6118202e4b3440794770"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/nash-vicuna-13b-v1dot5-ep2-w-rag-w-simple",
        "Average":61.8,
        "ARC":59.13,
        "HellaSwag":80.64,
        "MMLU":56.12,
        "TruthfulQA":51.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"848ef91ab46a72260542283918a971347c6bfa93"
    },
    {
        "T":"?",
        "Model":"Test157t\/Kunocchini-1.2-7b-longtext",
        "Average":61.79,
        "ARC":59.9,
        "HellaSwag":82.51,
        "MMLU":63.05,
        "TruthfulQA":41.72,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"c1c34be434bd1819202cc88b2a4eea7d898ebdae"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Uncensored-Frank-13B",
        "Average":61.78,
        "ARC":61.6,
        "HellaSwag":82.62,
        "MMLU":54.55,
        "TruthfulQA":48.34,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"73a27445e5e5a72857626e551c70542ec607f60c"
    },
    {
        "T":"\u2b55",
        "Model":"vihangd\/smartyplats-7b-v2",
        "Average":61.78,
        "ARC":57.94,
        "HellaSwag":80.76,
        "MMLU":58.16,
        "TruthfulQA":50.26,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"99049eb184b9b3ef074043d6e626fe3db09f5a19"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WizardLM\/WizardLM-13B-V1.1",
        "Average":61.78,
        "ARC":60.24,
        "HellaSwag":81.39,
        "MMLU":50.92,
        "TruthfulQA":54.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":69,
        "Available on the hub":true,
        "Model Sha":"badd80f8a6f46fb15310fedf6d4db54959854897"
    },
    {
        "T":"\u2b55",
        "Model":"totally-not-an-llm\/EverythingLM-13b-V3-peft",
        "Average":61.77,
        "ARC":58.36,
        "HellaSwag":81.03,
        "MMLU":54.7,
        "TruthfulQA":52.98,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"7a2eed5038addcf4fa3b8dd358b45eb96134e749"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fzzhang\/mistralv1_gsm8k_merged",
        "Average":61.76,
        "ARC":61.35,
        "HellaSwag":83.11,
        "MMLU":63.04,
        "TruthfulQA":39.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b9cb1edd3a535cabc500ce9fb81d98bbfed0b047"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zelus82\/Obelix-Phi2-v0",
        "Average":61.76,
        "ARC":63.4,
        "HellaSwag":76.66,
        "MMLU":58.21,
        "TruthfulQA":48.78,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c647172f62c099d1d599da4d99fdb54e7febc77a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-AdventurousWinds-Mk2-7b",
        "Average":61.76,
        "ARC":58.19,
        "HellaSwag":83.48,
        "MMLU":61.8,
        "TruthfulQA":43.56,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"cfcc969a7e97275b2298253f1eabf4575e5a3768"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"ZySec-AI\/ZySec-7B",
        "Average":61.75,
        "ARC":57.51,
        "HellaSwag":79.73,
        "MMLU":58.65,
        "TruthfulQA":51.11,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":24,
        "Available on the hub":false,
        "Model Sha":"4736490de5d4dc374d8c7ee47fd9a1c587f539e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/mistral-7B-alpaca-case-0-2",
        "Average":61.75,
        "ARC":61.69,
        "HellaSwag":81.74,
        "MMLU":60.0,
        "TruthfulQA":43.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d8cdb077e67fe9de8fec3ce47b79dab8e1bacf95"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-orca-platypus-coig-lite-2k-0.6e-13b",
        "Average":61.74,
        "ARC":59.9,
        "HellaSwag":80.76,
        "MMLU":58.34,
        "TruthfulQA":47.97,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"65214c9923d55795ecd6e7f9e0fcee5ba5f26929"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/Mistral-7B-Holodeck-1",
        "Average":61.74,
        "ARC":60.24,
        "HellaSwag":82.53,
        "MMLU":62.67,
        "TruthfulQA":41.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"76057cc5c1923921162133c81ae7ca0e92755810"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Yarn-Mistral-7b-128k",
        "Average":61.74,
        "ARC":59.64,
        "HellaSwag":82.5,
        "MMLU":63.02,
        "TruthfulQA":41.78,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":554,
        "Available on the hub":false,
        "Model Sha":"d09f1f8ed437d61c1aff94c1beabee554843dcdd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ericpolewski\/ASTS-PFAF",
        "Average":61.73,
        "ARC":61.26,
        "HellaSwag":82.94,
        "MMLU":58.96,
        "TruthfulQA":43.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9c8f78a3ced78392dfcdf350628f5044e6b77122"
    },
    {
        "T":"?",
        "Model":"huggyllama\/llama-30b",
        "Average":61.72,
        "ARC":61.43,
        "HellaSwag":84.73,
        "MMLU":58.45,
        "TruthfulQA":42.27,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub":37,
        "Available on the hub":true,
        "Model Sha":"2b1edcdb3c7ced7bce6c1aa75c94545777c3118b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Qwen\/Qwen1.5-MoE-A2.7B",
        "Average":61.72,
        "ARC":54.86,
        "HellaSwag":79.39,
        "MMLU":62.54,
        "TruthfulQA":50.09,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":14.32,
        "Hub":124,
        "Available on the hub":false,
        "Model Sha":"e8c04bdfc419473cfb5e03385ee9b4e9dedca7eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/ReMM-L2-13B-PIPPA",
        "Average":61.72,
        "ARC":59.73,
        "HellaSwag":83.12,
        "MMLU":54.1,
        "TruthfulQA":49.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"79e711178c6881496ae1f5635b08bc193f370709"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/ReMM-L2-13B",
        "Average":61.72,
        "ARC":59.73,
        "HellaSwag":83.1,
        "MMLU":54.11,
        "TruthfulQA":49.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"c4710577003a23ca8e9040d16dfb8f3e9bc5d636"
    },
    {
        "T":"?",
        "Model":"ddyuudd\/mistral_dmbr10_32_sig",
        "Average":61.72,
        "ARC":58.62,
        "HellaSwag":82.57,
        "MMLU":61.35,
        "TruthfulQA":44.34,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d028c5bfc34a205d9cb215bbf66371765408283d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/OpenHermes-2.5-Code-290k-13B",
        "Average":61.71,
        "ARC":57.34,
        "HellaSwag":80.48,
        "MMLU":56.53,
        "TruthfulQA":52.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.02,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"5fe89b1eb555644dd8a658c74ea118620ba3fdc1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gaodrew\/gaodrew-gorgonzola-13b",
        "Average":61.7,
        "ARC":53.84,
        "HellaSwag":78.86,
        "MMLU":71.54,
        "TruthfulQA":42.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a53fbe358d4cb546916847d861ccfaf7c724a103"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-13b-v1.5",
        "Average":61.69,
        "ARC":57.0,
        "HellaSwag":81.23,
        "MMLU":56.87,
        "TruthfulQA":51.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":87,
        "Available on the hub":true,
        "Model Sha":"3deb0106f72a3a433f0c6ea0cb978bdf14bcd3a6"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2xOpenOrca-13B-LoRa",
        "Average":61.69,
        "ARC":60.75,
        "HellaSwag":82.09,
        "MMLU":58.77,
        "TruthfulQA":45.15,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8b2f5d65c03d415b7c43530def622e133e1ef014"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yhyu13\/llama-30B-hf-openassitant",
        "Average":61.68,
        "ARC":61.26,
        "HellaSwag":84.73,
        "MMLU":58.47,
        "TruthfulQA":42.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":32.32,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"fba493af11a73cf5a2ee7857dd7aecb98c659dc4"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"huggingface\/llama-30b",
        "Average":61.68,
        "ARC":61.26,
        "HellaSwag":84.73,
        "MMLU":58.47,
        "TruthfulQA":42.27,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"13c77caa472bfa79d4f3f0ec82cbdc9dd88e5d22"
    },
    {
        "T":"?",
        "Model":"Locutusque\/Hercules-1.0-Mistral-7B",
        "Average":61.67,
        "ARC":57.08,
        "HellaSwag":81.13,
        "MMLU":58.98,
        "TruthfulQA":49.47,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"28c4847fbd13acc613078092ddfa2995ba6cf470"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kwchoi\/DPO_mistral_v01_7b_ultra_0131_1k_1epoch",
        "Average":61.67,
        "ARC":55.97,
        "HellaSwag":76.78,
        "MMLU":55.97,
        "TruthfulQA":57.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d9049778b541c69946ec235b81985020e065fbff"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nvidia\/OpenMath-Mistral-7B-v0.1-hf",
        "Average":61.66,
        "ARC":59.39,
        "HellaSwag":81.78,
        "MMLU":59.34,
        "TruthfulQA":46.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"e378a80b22387a5a30ccbb9feaf3e9b0bc3cfc57"
    },
    {
        "T":"\u2b55",
        "Model":"Envoid\/Libra-19B",
        "Average":61.65,
        "ARC":60.58,
        "HellaSwag":82.04,
        "MMLU":55.57,
        "TruthfulQA":48.41,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.2,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"a4e1f8f62740d676c25eedb4f29f4e776dcc0c22"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"prithivida\/Asimov-7B-v1",
        "Average":61.65,
        "ARC":59.04,
        "HellaSwag":80.04,
        "MMLU":56.35,
        "TruthfulQA":51.15,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0b33ad0a6dde60156ee6008ff47f7cfa6cd27937"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/mistral-7b_open_platypus",
        "Average":61.64,
        "ARC":55.8,
        "HellaSwag":82.13,
        "MMLU":59.76,
        "TruthfulQA":48.87,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b9a60b9ad0fe06bd314ffe99d543f1df6ecd10da"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"crumb\/apricot-wildflower-20",
        "Average":61.64,
        "ARC":59.64,
        "HellaSwag":81.76,
        "MMLU":63.38,
        "TruthfulQA":41.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"27610b542c84b446c397dd92cc28d53c278b1ecb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-13b-v1.5",
        "Average":61.63,
        "ARC":57.08,
        "HellaSwag":81.24,
        "MMLU":56.67,
        "TruthfulQA":51.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":87,
        "Available on the hub":true,
        "Model Sha":"3deb0106f72a3a433f0c6ea0cb978bdf14bcd3a6"
    },
    {
        "T":"\u2b55",
        "Model":"pinkyponky\/Mistral-7B-Instruct-sft-tuned-v0.2",
        "Average":61.63,
        "ARC":58.02,
        "HellaSwag":79.26,
        "MMLU":58.78,
        "TruthfulQA":50.45,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"26b1b06ca6ee8db77d915e0ec685b3e999a226d0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dball\/zephyr-7b-sft-qlora",
        "Average":61.61,
        "ARC":59.73,
        "HellaSwag":82.49,
        "MMLU":61.9,
        "TruthfulQA":42.32,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ffcebbaaabb14ac25326c6385327f73785ec4a95"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/SthenoWriter-L2-13B",
        "Average":61.61,
        "ARC":62.29,
        "HellaSwag":83.28,
        "MMLU":56.14,
        "TruthfulQA":44.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a6d9e26ab765eb170cc0aa428ee5e25b08524657"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Uncensored-Jordan-13B",
        "Average":61.6,
        "ARC":57.42,
        "HellaSwag":82.7,
        "MMLU":55.75,
        "TruthfulQA":50.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":12.85,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"c56a396342133bbd75ab3f79622c85cb55be49a4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ValiantLabs\/ShiningValiantXS",
        "Average":61.59,
        "ARC":58.96,
        "HellaSwag":81.93,
        "MMLU":56.75,
        "TruthfulQA":48.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"8c1f86bd2e646408eed2ed3a2634b38ea4e5c599"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"umd-zhou-lab\/claude2-alpaca-13B",
        "Average":61.58,
        "ARC":61.18,
        "HellaSwag":84.21,
        "MMLU":55.93,
        "TruthfulQA":45.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"1d670244f2f70ab35219c9bbf83eef4f5dc28730"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Changgil\/K2S3-Mistral-7b-v1.1",
        "Average":61.58,
        "ARC":59.47,
        "HellaSwag":80.75,
        "MMLU":60.56,
        "TruthfulQA":45.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.37,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0f7e1ed84843f50791fa74315dfa0f975f300344"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Danielbrdz\/Barcenas-13b",
        "Average":61.58,
        "ARC":61.26,
        "HellaSwag":82.13,
        "MMLU":56.25,
        "TruthfulQA":46.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"fa988ba73f67ad0c8e7fa8f408106ea040070258"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CallComply\/Starling-LM-11B-alpha",
        "Average":61.57,
        "ARC":61.26,
        "HellaSwag":81.99,
        "MMLU":61.5,
        "TruthfulQA":41.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"db8cffdb7d63b88239c3b27b5afe1b433400e72f"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Athena-Platypus2-13B-QLora-0.80-epoch",
        "Average":61.57,
        "ARC":56.66,
        "HellaSwag":80.56,
        "MMLU":55.43,
        "TruthfulQA":53.62,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f7b6c11b4df16079dfdd1e8dd8c489a8835c7cc4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Borealis-10.7B-DPO",
        "Average":61.56,
        "ARC":57.94,
        "HellaSwag":81.21,
        "MMLU":60.74,
        "TruthfulQA":46.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.6,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"9d6e34fa51cd3c4745a044fbb2bca91b1c9a9f5a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-13b-3.0",
        "Average":61.54,
        "ARC":59.81,
        "HellaSwag":83.71,
        "MMLU":54.86,
        "TruthfulQA":47.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"2fcef275782b2c1061cf671d889aea652d13236c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/test-case-1",
        "Average":61.54,
        "ARC":57.17,
        "HellaSwag":79.47,
        "MMLU":56.41,
        "TruthfulQA":53.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"35203580388abb5beb595b57630258f415c4dd03"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"XuanXuanXuanXuan\/Llama-2-13b-hf-gpt-4-80k",
        "Average":61.53,
        "ARC":60.84,
        "HellaSwag":79.88,
        "MMLU":55.56,
        "TruthfulQA":49.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"31b6e9f91da1c9a95a9ec7a480de73641b1afaf4"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/Nebula-v2-7B",
        "Average":61.52,
        "ARC":58.7,
        "HellaSwag":83.06,
        "MMLU":57.61,
        "TruthfulQA":46.72,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d2a5611f7d7c37bfa2270d1823bceef01c0be383"
    },
    {
        "T":"\u2b55",
        "Model":"kyujinpy\/SOLAR-Platypus-10.7B-v2",
        "Average":61.51,
        "ARC":59.39,
        "HellaSwag":83.57,
        "MMLU":59.93,
        "TruthfulQA":43.15,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"39a8673aa6d98a994661200e87cbd4069b8b6aa8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/dromedary-65b-lora-HF",
        "Average":61.51,
        "ARC":61.6,
        "HellaSwag":82.53,
        "MMLU":63.08,
        "TruthfulQA":38.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub":19,
        "Available on the hub":true,
        "Model Sha":"3fa4546259d6bbd6b5d637484c325ab19181a73c"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/llama-2-13b-Beluga-QLoRA",
        "Average":61.51,
        "ARC":59.22,
        "HellaSwag":81.92,
        "MMLU":56.67,
        "TruthfulQA":48.23,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c0d3c0a5d4e9001ea933c6b71ca3adc99d1f71a2"
    },
    {
        "T":"\u2b55",
        "Model":"The-Face-Of-Goonery\/Huginn-13b-v4.5",
        "Average":61.49,
        "ARC":60.67,
        "HellaSwag":82.34,
        "MMLU":52.32,
        "TruthfulQA":50.62,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"f3be56d8bf71a8d3905974b1e5fcba7336b02159"
    },
    {
        "T":"\u2b55",
        "Model":"The-Face-Of-Goonery\/Huginn-13b-V4",
        "Average":61.49,
        "ARC":60.67,
        "HellaSwag":82.34,
        "MMLU":52.32,
        "TruthfulQA":50.62,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6186feee849e0c2b7e62d4cbdc4cdc48260ac684"
    },
    {
        "T":"\u2b55",
        "Model":"The-Face-Of-Goonery\/Huginn-v3-13b",
        "Average":61.49,
        "ARC":60.67,
        "HellaSwag":82.34,
        "MMLU":52.32,
        "TruthfulQA":50.62,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"6c2faf828c5380d28c51fcb4d3d0f1a420fb9a9a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Delcos\/Velara",
        "Average":61.48,
        "ARC":58.96,
        "HellaSwag":82.83,
        "MMLU":59.45,
        "TruthfulQA":44.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":11.39,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"0fad8e711563d3a5a4631500d6a1d6b87f10d396"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"tiiuae\/falcon-40b",
        "Average":61.48,
        "ARC":61.95,
        "HellaSwag":85.28,
        "MMLU":56.98,
        "TruthfulQA":41.72,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":41.3,
        "Hub":2269,
        "Available on the hub":false,
        "Model Sha":"c47b371b31a68349c233104050ac76680b8485db"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Zangs3011\/mistral_7b_2EPOCH_DolphinCoder",
        "Average":61.48,
        "ARC":60.75,
        "HellaSwag":81.15,
        "MMLU":59.37,
        "TruthfulQA":44.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"521da4841efa9eee3716294f08fd3326d271b574"
    },
    {
        "T":"?",
        "Model":"zhengchenphd\/Mistral-Plus-7B",
        "Average":61.47,
        "ARC":62.2,
        "HellaSwag":84.24,
        "MMLU":63.63,
        "TruthfulQA":35.8,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"d7d4ca78648dd7c8833c7029946956619d2d714d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wandb\/gemma-7b-zephyr-sft",
        "Average":61.46,
        "ARC":61.43,
        "HellaSwag":80.73,
        "MMLU":60.33,
        "TruthfulQA":43.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"51918d1d0284e398a08f3b74b642f940efc925be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"tcapelle\/gemma-7b-zephyr-sft",
        "Average":61.46,
        "ARC":61.43,
        "HellaSwag":80.73,
        "MMLU":60.33,
        "TruthfulQA":43.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"51918d1d0284e398a08f3b74b642f940efc925be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Expert68\/llama2_13b_instructed_version2",
        "Average":61.46,
        "ARC":60.07,
        "HellaSwag":84.05,
        "MMLU":55.61,
        "TruthfulQA":46.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ea321257d81e0f41c985f5155297b7fbd6ac375a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WhoTookMyAmogusNickname\/NewHope_HF_not_official",
        "Average":61.45,
        "ARC":61.09,
        "HellaSwag":84.03,
        "MMLU":55.73,
        "TruthfulQA":44.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f587f4a31de6818f4200d9cdc7f116ca8ba1cdc2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ramachaitanya22\/mistral-7B-finetune-health-fitness",
        "Average":61.45,
        "ARC":59.13,
        "HellaSwag":82.65,
        "MMLU":61.93,
        "TruthfulQA":42.07,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0453c4ebdbf0462a0ed55c9a026ca194323e5a11"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/TekniumAiroboros-Nebula-7B",
        "Average":61.44,
        "ARC":57.17,
        "HellaSwag":81.72,
        "MMLU":55.25,
        "TruthfulQA":51.64,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ef964d514cc25a600b0de78fc469d1acbec34591"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Qwen\/Qwen1.5-7B",
        "Average":61.44,
        "ARC":54.18,
        "HellaSwag":78.51,
        "MMLU":61.97,
        "TruthfulQA":51.08,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.72,
        "Hub":24,
        "Available on the hub":false,
        "Model Sha":"e52fa2ef47411cc8bc9f752d1d8d9072b37742e7"
    },
    {
        "T":"\u2b55",
        "Model":"Secbone\/llama-2-13B-instructed",
        "Average":61.43,
        "ARC":59.39,
        "HellaSwag":83.88,
        "MMLU":55.57,
        "TruthfulQA":46.89,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e676fbd9015beacfba5d71426beace7605200477"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gaodrew\/OpenOrca-Platypus2-13B-thera-1250",
        "Average":61.43,
        "ARC":59.22,
        "HellaSwag":81.02,
        "MMLU":57.04,
        "TruthfulQA":48.43,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b1c2ebcda387211732e87911e39edca503502a33"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"frankenmerger\/delta-4B-super",
        "Average":61.43,
        "ARC":58.62,
        "HellaSwag":76.29,
        "MMLU":59.06,
        "TruthfulQA":51.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":4.67,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"680f13a7d44182d799a826c52f3929590f5fd4d6"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"gmonsoon\/Delta-4B-Base",
        "Average":61.43,
        "ARC":58.62,
        "HellaSwag":76.29,
        "MMLU":59.06,
        "TruthfulQA":51.74,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":4.67,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b052176caad85b31111242ad67aa84a41efb3e13"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"tiiuae\/falcon-40b",
        "Average":61.42,
        "ARC":61.86,
        "HellaSwag":85.28,
        "MMLU":56.89,
        "TruthfulQA":41.65,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":41.3,
        "Hub":2269,
        "Available on the hub":true,
        "Model Sha":"4a70170c215b36a3cce4b4253f6d0612bb7d4146"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CallComply\/openchat-3.5-0106-128k",
        "Average":61.41,
        "ARC":64.25,
        "HellaSwag":77.31,
        "MMLU":57.58,
        "TruthfulQA":46.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"35cf427cc9af94533baeea8afa1428a0eff78f3f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ArtMindia\/artmindia3k",
        "Average":61.39,
        "ARC":59.98,
        "HellaSwag":82.99,
        "MMLU":60.99,
        "TruthfulQA":41.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a9c25b4b3253b15a9be09c3f1c7bfb96df71d984"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MisterRid\/wendigo-14b-alpha2",
        "Average":61.39,
        "ARC":56.66,
        "HellaSwag":77.19,
        "MMLU":58.0,
        "TruthfulQA":53.71,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":14.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"f8332eddcb7f8ab2b5195486d4b508c4628992f6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chickencaesar\/llama2-platypus-llama2-chat-13B-hf",
        "Average":61.38,
        "ARC":62.97,
        "HellaSwag":82.75,
        "MMLU":56.86,
        "TruthfulQA":42.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e35bb473156d74c8b5ad23a5e9df815891e8139a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SLAM-group\/NewHope",
        "Average":61.38,
        "ARC":60.92,
        "HellaSwag":84.0,
        "MMLU":55.72,
        "TruthfulQA":44.87,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"560ca6df8335d6d2998ac8f079218816a5742b02"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ResplendentAI\/Obscura_32k_7B",
        "Average":61.36,
        "ARC":55.29,
        "HellaSwag":78.0,
        "MMLU":49.13,
        "TruthfulQA":63.03,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"d4a17e943e5e358a9ff3a932bbc237814719590d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"garage-bAInd\/Platypus2-13B",
        "Average":61.35,
        "ARC":61.26,
        "HellaSwag":82.56,
        "MMLU":56.7,
        "TruthfulQA":44.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"b5e926e3d6c03e83c7983e87eb71098b5e80a62e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-13B",
        "Average":61.34,
        "ARC":59.98,
        "HellaSwag":81.86,
        "MMLU":56.11,
        "TruthfulQA":47.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"fbb23bc41438b016f1df1e9180c6c350a03557ea"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewqr2130\/alignment-handbook-zephyr-7b-sft-full-dpo-5e7-cont2",
        "Average":61.34,
        "ARC":60.32,
        "HellaSwag":82.88,
        "MMLU":59.79,
        "TruthfulQA":42.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a6b293b6a0fc4c3d0eed5dcf41055ef851e2b29c"
    },
    {
        "T":"?",
        "Model":"avinash31d\/phi-2-slerp",
        "Average":61.33,
        "ARC":62.54,
        "HellaSwag":76.04,
        "MMLU":57.6,
        "TruthfulQA":49.15,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"38ff8f0e47a438ea4eb5bc1f3abb592690a503fb"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/Nebula-7B",
        "Average":61.33,
        "ARC":59.3,
        "HellaSwag":83.46,
        "MMLU":57.0,
        "TruthfulQA":45.56,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"569f848698a468fb03d37033c67f3734bbaec127"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ConvexAI\/Luminex-72B-v0.1",
        "Average":61.32,
        "ARC":43.43,
        "HellaSwag":86.66,
        "MMLU":73.36,
        "TruthfulQA":41.85,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":72.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"73491b7bda948e0755f5a6cd655238e0ba73fad6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MisterRid\/wendigo-14b-alpha1",
        "Average":61.32,
        "ARC":56.48,
        "HellaSwag":77.2,
        "MMLU":57.83,
        "TruthfulQA":53.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":14.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0024ee75d8ed5d9373ff42df72c21f3217ba9d2e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"gmonsoon\/Delta-4B-notso-base",
        "Average":61.32,
        "ARC":57.59,
        "HellaSwag":76.1,
        "MMLU":57.26,
        "TruthfulQA":54.31,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":4.67,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6a9afe87c5cc0f8afed83635ac71f0ceff0a1421"
    },
    {
        "T":"?",
        "Model":"abacaj\/phi-2-super",
        "Average":61.31,
        "ARC":61.86,
        "HellaSwag":76.6,
        "MMLU":58.41,
        "TruthfulQA":48.37,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":80,
        "Available on the hub":false,
        "Model Sha":"2d6482f24447855e463fcefc40572505607e5693"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GritLM\/GritLM-7B",
        "Average":61.31,
        "ARC":58.11,
        "HellaSwag":80.97,
        "MMLU":60.29,
        "TruthfulQA":45.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":47,
        "Available on the hub":false,
        "Model Sha":"13f00a0e36500c80ce12870ea513846a066004af"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Severian\/ANIMA-Phi-Neptune-Mistral-7B-v4",
        "Average":61.31,
        "ARC":55.46,
        "HellaSwag":77.63,
        "MMLU":53.12,
        "TruthfulQA":59.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"artistic-2.0",
        "#Params (B)":7.0,
        "Hub":29,
        "Available on the hub":false,
        "Model Sha":"a8e18f970f7ca994740177d6c228adee9e17aba9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"0-hero\/Matter-0.1-Slim-7B-B",
        "Average":61.3,
        "ARC":60.75,
        "HellaSwag":81.55,
        "MMLU":61.01,
        "TruthfulQA":41.91,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"195d352943c0e71ddffb12eec30b479a07696d11"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Zangs3011\/mistral_7b_DolphinCoder",
        "Average":61.3,
        "ARC":59.73,
        "HellaSwag":81.64,
        "MMLU":59.87,
        "TruthfulQA":43.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"faac4b13e74395ea4b366156fd8bed15498c667c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"qblocks\/mistral_7b_DolphinCoder",
        "Average":61.3,
        "ARC":59.73,
        "HellaSwag":81.64,
        "MMLU":59.87,
        "TruthfulQA":43.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7c05d338e0210072e13eb82b023e7747d5354c6e"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/Dolphin-Nebula-7B",
        "Average":61.3,
        "ARC":55.2,
        "HellaSwag":78.57,
        "MMLU":53.44,
        "TruthfulQA":57.97,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c14b3545066e5ee5562c1724a037b41db95f1f0d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elinas\/chronos-13b-v2",
        "Average":61.29,
        "ARC":58.7,
        "HellaSwag":82.52,
        "MMLU":53.39,
        "TruthfulQA":50.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"e5d411138e72370c5613dfea0f66ded99f6e62f9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Cartinoe5930\/SOLAR-DUS-implement",
        "Average":61.28,
        "ARC":59.56,
        "HellaSwag":81.18,
        "MMLU":63.68,
        "TruthfulQA":40.72,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"bfce9b8f3e599767b3c3974b0a3cbbd1b7f2da6c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abdulrahman-nuzha\/finetuned-Mistral-5000-v1.0",
        "Average":61.28,
        "ARC":59.9,
        "HellaSwag":82.37,
        "MMLU":61.68,
        "TruthfulQA":41.17,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e8050c54d484d7e0a885b9d97a0781f0dd2e745e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Venomia-1.1-m7",
        "Average":61.27,
        "ARC":58.45,
        "HellaSwag":83.04,
        "MMLU":56.39,
        "TruthfulQA":47.21,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"00dd78ef6ee386c860f9136b9ef703a4c141e7f3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Fewshot-Metamath-OrcaVicuna-Mistral-10B",
        "Average":61.25,
        "ARC":56.4,
        "HellaSwag":78.12,
        "MMLU":59.52,
        "TruthfulQA":50.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"5fc7d7bb8ee87dab18d74bb1e25024eba3019a0b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aloobun\/bun_mistral_7b_v2",
        "Average":61.25,
        "ARC":59.9,
        "HellaSwag":82.65,
        "MMLU":61.77,
        "TruthfulQA":40.67,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"4b7c558e530a9e887ba38fc5f58caf7b41db608e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Mistral-11B-v0.1",
        "Average":61.24,
        "ARC":59.56,
        "HellaSwag":81.17,
        "MMLU":63.56,
        "TruthfulQA":40.67,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"e9698271ea1ab340bacfd5ebf0d77108a6f18a90"
    },
    {
        "T":"\u2b55",
        "Model":"lu-vae\/llama2-13b-sharegpt4-test",
        "Average":61.23,
        "ARC":58.02,
        "HellaSwag":82.65,
        "MMLU":55.99,
        "TruthfulQA":48.27,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2be36a2dab4ed0f97727a1508367f53d59950818"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/Darewin-7B-v2",
        "Average":61.23,
        "ARC":62.63,
        "HellaSwag":78.28,
        "MMLU":53.01,
        "TruthfulQA":50.99,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c0b08aff3f8cc55470b3e3861e45c86d543f2ac1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CorticalStack\/mistral-7b-metamathqa-sft",
        "Average":61.23,
        "ARC":58.45,
        "HellaSwag":80.44,
        "MMLU":61.28,
        "TruthfulQA":44.73,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c16588b550a4238a113c1b56f6e7e2825491236d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"IkariDev\/Athena-v1",
        "Average":61.22,
        "ARC":60.07,
        "HellaSwag":82.64,
        "MMLU":55.61,
        "TruthfulQA":46.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"8f96e561c8c795e383ca0faeb1696fa1e33e87de"
    },
    {
        "T":"?",
        "Model":"Replete-AI\/Phi-Elothir",
        "Average":61.22,
        "ARC":59.56,
        "HellaSwag":75.63,
        "MMLU":58.45,
        "TruthfulQA":51.23,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":5.14,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"147462ec7112778bc5f51b932b9f0691f93314c4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Severian\/ANIMA-Phi-Neptune-Mistral-7B",
        "Average":61.21,
        "ARC":55.97,
        "HellaSwag":76.22,
        "MMLU":52.89,
        "TruthfulQA":59.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"artistic-2.0",
        "#Params (B)":7.0,
        "Hub":29,
        "Available on the hub":false,
        "Model Sha":"e8e9a4804c842b84def9e9aaae38236d4754f277"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GritLM\/GritLM-7B",
        "Average":61.21,
        "ARC":58.11,
        "HellaSwag":80.91,
        "MMLU":60.02,
        "TruthfulQA":45.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":47,
        "Available on the hub":false,
        "Model Sha":"13f00a0e36500c80ce12870ea513846a066004af"
    },
    {
        "T":"\u2b55",
        "Model":"pinkyponky\/Mistral-7B-Instruct-Sft-Tuned-V0.2",
        "Average":61.21,
        "ARC":57.34,
        "HellaSwag":78.95,
        "MMLU":57.9,
        "TruthfulQA":50.66,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"826783eb0e7f2fc471ab9dfeea59acd112a6ecc3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chargoddard\/llama-polyglot-13b",
        "Average":61.21,
        "ARC":59.81,
        "HellaSwag":81.27,
        "MMLU":55.04,
        "TruthfulQA":48.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"7a08a96118aa86e0405a5f980d7e40dadf86e1be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CausalLM\/7B-DPO-alpha",
        "Average":61.21,
        "ARC":50.85,
        "HellaSwag":73.0,
        "MMLU":63.39,
        "TruthfulQA":57.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"gpl-3.0",
        "#Params (B)":7.0,
        "Hub":73,
        "Available on the hub":false,
        "Model Sha":"36501a519950fb80c2e7df77e12c9110dca580f4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mosaicml\/mpt-30b-chat",
        "Average":61.21,
        "ARC":58.7,
        "HellaSwag":82.54,
        "MMLU":51.16,
        "TruthfulQA":52.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":29.96,
        "Hub":177,
        "Available on the hub":true,
        "Model Sha":"54f33278a04aa4e612bca482b82f801ab658e890"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-OpenOrca_5w",
        "Average":61.2,
        "ARC":61.01,
        "HellaSwag":82.82,
        "MMLU":56.09,
        "TruthfulQA":44.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0ddd810c9150492d7318656acac44849651edbf2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"0-hero\/Matter-0.1-Slim-7B-preview",
        "Average":61.18,
        "ARC":59.98,
        "HellaSwag":80.66,
        "MMLU":61.53,
        "TruthfulQA":42.55,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"191ddd498835979ffc2b7bcb405f2f0d1cceed61"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-hermes-coig-lite-13b",
        "Average":61.17,
        "ARC":59.56,
        "HellaSwag":82.26,
        "MMLU":55.3,
        "TruthfulQA":47.56,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2ee11d9c7acaefb723796227e2ad099b165f0dd9"
    },
    {
        "T":"?",
        "Model":"Azure99\/blossom-v4-qwen1_5-7b",
        "Average":61.16,
        "ARC":54.44,
        "HellaSwag":76.11,
        "MMLU":60.43,
        "TruthfulQA":53.69,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.72,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"eb86db987cf43f8d3ca023e2ea3e467eb24fa61b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/mistral-7b-sft-beta",
        "Average":61.16,
        "ARC":57.42,
        "HellaSwag":82.23,
        "MMLU":61.42,
        "TruthfulQA":43.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":21,
        "Available on the hub":false,
        "Model Sha":"c985a04e76fb00d3c3f65214d0b02c5a751d2274"
    },
    {
        "T":"?",
        "Model":"TheBloke\/tulu-30B-fp16",
        "Average":61.16,
        "ARC":59.98,
        "HellaSwag":83.4,
        "MMLU":56.1,
        "TruthfulQA":45.14,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"37c3655676c37662f60c68dacfce3f0e861be846"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-hermes-coig-lite-13b",
        "Average":61.13,
        "ARC":59.47,
        "HellaSwag":82.28,
        "MMLU":55.18,
        "TruthfulQA":47.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2ee11d9c7acaefb723796227e2ad099b165f0dd9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"teknium\/OpenHermes-13B",
        "Average":61.13,
        "ARC":60.15,
        "HellaSwag":82.18,
        "MMLU":56.19,
        "TruthfulQA":45.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"f09d0fe655ad57cce9179b7b40ea6f81e07db18c"
    },
    {
        "T":"\u2b55",
        "Model":"hywu\/Camelidae-8x13B",
        "Average":61.12,
        "ARC":61.18,
        "HellaSwag":82.73,
        "MMLU":57.21,
        "TruthfulQA":43.37,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"857292e46549732062a27eb965f3c9869dc62794"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Cartinoe5930\/iDUS-8layers",
        "Average":61.12,
        "ARC":59.3,
        "HellaSwag":81.34,
        "MMLU":63.22,
        "TruthfulQA":40.62,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5291ad4b5e6388ff2a346cfd783d119b0e17e0df"
    },
    {
        "T":"?",
        "Model":"zyh3826\/llama2-13b-ft-openllm-leaderboard-v1",
        "Average":61.11,
        "ARC":59.64,
        "HellaSwag":83.14,
        "MMLU":60.93,
        "TruthfulQA":40.72,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"70404059013c74b0641ed69d293b3d1ad708cd1e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xDAN-AI\/xDAN_13b_l2_lora",
        "Average":61.11,
        "ARC":61.01,
        "HellaSwag":82.64,
        "MMLU":56.03,
        "TruthfulQA":44.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":13.0,
        "Hub":59,
        "Available on the hub":false,
        "Model Sha":"a8db938daa42016324291e38c4b45e34536ecbf4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"teknium\/OpenHermes-13B",
        "Average":61.1,
        "ARC":59.81,
        "HellaSwag":82.24,
        "MMLU":56.35,
        "TruthfulQA":46.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"f09d0fe655ad57cce9179b7b40ea6f81e07db18c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"G-reen\/EXPERIMENT-DPO-m7b2-2-merged",
        "Average":61.09,
        "ARC":59.47,
        "HellaSwag":82.47,
        "MMLU":62.31,
        "TruthfulQA":40.11,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":3.86,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"df1a7ecdbb41ee220410f243043346af2e8a491a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"tcapelle\/gemma-7b-zephyr-dpo",
        "Average":61.09,
        "ARC":60.84,
        "HellaSwag":80.44,
        "MMLU":60.6,
        "TruthfulQA":42.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":8.54,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"a3980aba73509cc3fa7553dd612478ac589255ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wandb\/gemma-7b-zephyr-dpo",
        "Average":61.09,
        "ARC":60.84,
        "HellaSwag":80.44,
        "MMLU":60.6,
        "TruthfulQA":42.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":8.54,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"a3980aba73509cc3fa7553dd612478ac589255ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pharaouk\/fusedyi",
        "Average":61.09,
        "ARC":55.03,
        "HellaSwag":76.6,
        "MMLU":63.43,
        "TruthfulQA":49.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.91,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5e3fdfa75a3bebd5d18d25e3bada1da27f200fd6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-13b-v1.5-16k",
        "Average":61.09,
        "ARC":56.74,
        "HellaSwag":80.37,
        "MMLU":55.28,
        "TruthfulQA":51.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":138,
        "Available on the hub":true,
        "Model Sha":"277697af19d4b267626ebc9f4e078d19a9a0fddf"
    },
    {
        "T":"?",
        "Model":"Joseph717171\/Mistral-12.25B-v0.2",
        "Average":61.07,
        "ARC":58.87,
        "HellaSwag":81.77,
        "MMLU":63.22,
        "TruthfulQA":40.44,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.48,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e9bfa6be73552731485ea1a3098888cc2bee5b28"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"0-hero\/Matter-0.1-Slim-7B-A",
        "Average":61.07,
        "ARC":60.49,
        "HellaSwag":81.33,
        "MMLU":60.68,
        "TruthfulQA":41.79,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"9259ec16319e314d0e189159302f0033cb01e964"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"0-hero\/Matter-0.1-Slim-7B",
        "Average":61.07,
        "ARC":60.49,
        "HellaSwag":81.33,
        "MMLU":60.68,
        "TruthfulQA":41.79,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9259ec16319e314d0e189159302f0033cb01e964"
    },
    {
        "T":"\u2b55",
        "Model":"martyn\/llama-megamerge-dare-13b",
        "Average":61.06,
        "ARC":60.58,
        "HellaSwag":83.0,
        "MMLU":54.91,
        "TruthfulQA":45.76,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5529ddb255dbdabdd179bdc911f141c3f0d2fb3f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_v3.1",
        "Average":61.06,
        "ARC":60.15,
        "HellaSwag":82.84,
        "MMLU":56.84,
        "TruthfulQA":44.38,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"cc708183e430234b8718c08d9f90474569eabeac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SciPhi\/SciPhi-Self-RAG-Mistral-7B-32k",
        "Average":61.06,
        "ARC":57.34,
        "HellaSwag":80.44,
        "MMLU":60.81,
        "TruthfulQA":45.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":79,
        "Available on the hub":false,
        "Model Sha":"640192e2ba5898f87c407a9f771fc270f7628dee"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BarraHome\/rezephyr-dpo",
        "Average":61.05,
        "ARC":57.59,
        "HellaSwag":81.75,
        "MMLU":60.55,
        "TruthfulQA":44.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2be4ee1d2a8e693bb68031f719d12642d9e47b13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/mistral-7b-grok",
        "Average":61.05,
        "ARC":58.7,
        "HellaSwag":81.88,
        "MMLU":61.55,
        "TruthfulQA":42.07,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":38,
        "Available on the hub":false,
        "Model Sha":"038a70da219335747827bc58464bc95dbdbdd623"
    },
    {
        "T":"?",
        "Model":"G-reen\/EXPERIMENT-DPO-m7b2-1-merged",
        "Average":61.03,
        "ARC":59.47,
        "HellaSwag":82.42,
        "MMLU":62.21,
        "TruthfulQA":40.01,
        "Type":"Unknown",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":3.86,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f6b3b809c43964a07393735a9216aa65563fb1e1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-mixtral-8x7b-v16.2-32k",
        "Average":61.02,
        "ARC":34.39,
        "HellaSwag":81.72,
        "MMLU":71.33,
        "TruthfulQA":56.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.74,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"eae1e422ac65e856c03a9da0a840114267d24b68"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-dolphin_5w",
        "Average":61.0,
        "ARC":60.67,
        "HellaSwag":82.69,
        "MMLU":56.23,
        "TruthfulQA":44.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0ec406128968b41a9b7a5f18c358f7638d696b56"
    },
    {
        "T":"?",
        "Model":"OpenBuddy\/openbuddy-mistral2-7b-v20.1-32k",
        "Average":61.0,
        "ARC":53.5,
        "HellaSwag":77.76,
        "MMLU":59.76,
        "TruthfulQA":52.97,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.28,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"242cedbcfa8d0c846a27ba89675b19335f6444ff"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/NyakuraV2.1-m7",
        "Average":60.99,
        "ARC":58.62,
        "HellaSwag":81.89,
        "MMLU":58.46,
        "TruthfulQA":45.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0a1cd69beed347cd80a290ce5b568c03264ec595"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/test-case-0",
        "Average":60.99,
        "ARC":57.51,
        "HellaSwag":79.64,
        "MMLU":58.02,
        "TruthfulQA":48.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"75bc9a822d2f1200cc41eb0fbe0f48326a0e8947"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/vicuna-13b-v1.5-PL-lora_unload",
        "Average":60.99,
        "ARC":56.91,
        "HellaSwag":81.22,
        "MMLU":56.06,
        "TruthfulQA":49.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"5c8aeb722e11d1c7258abd45f9f2840f57976c28"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MLewd-L2-13B",
        "Average":60.98,
        "ARC":58.28,
        "HellaSwag":82.32,
        "MMLU":54.67,
        "TruthfulQA":48.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"feb1fa71e0b24261d3ca428b4aed881dd31f166e"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/minotaur-llama2-13b-qlora",
        "Average":60.98,
        "ARC":60.07,
        "HellaSwag":82.42,
        "MMLU":55.87,
        "TruthfulQA":45.57,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"22c83f7d68e547fb0b59acfa01c60b108c59fe55"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kajdun\/iubaris-13b-v3",
        "Average":60.98,
        "ARC":59.13,
        "HellaSwag":81.78,
        "MMLU":54.42,
        "TruthfulQA":48.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"eafacff141e7714da7e58625cc779e07c0034263"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Limarp-Platypus2-13B-QLoRA-0.80-epoch",
        "Average":60.98,
        "ARC":60.49,
        "HellaSwag":82.76,
        "MMLU":56.52,
        "TruthfulQA":44.14,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0a8560232ff73ca3c3f8e217b4517fa6c4f55558"
    },
    {
        "T":"\u2b55",
        "Model":"euclaise\/Ferret-7B",
        "Average":60.97,
        "ARC":62.29,
        "HellaSwag":81.31,
        "MMLU":60.27,
        "TruthfulQA":40.01,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"b1ef5adff5ceb06d2d9808bccf5e06705f9e19dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/test-case-6",
        "Average":60.96,
        "ARC":57.34,
        "HellaSwag":78.86,
        "MMLU":58.21,
        "TruthfulQA":49.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"49d1b9c1430ebd5ca0562b0c7d5893c5277eb0f7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_v3.1",
        "Average":60.96,
        "ARC":59.81,
        "HellaSwag":82.8,
        "MMLU":56.76,
        "TruthfulQA":44.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"a95be7130d32da99bcd484f6f436b2dd49341110"
    },
    {
        "T":"\u2b55",
        "Model":"hfl\/chinese-alpaca-2-13b",
        "Average":60.95,
        "ARC":58.7,
        "HellaSwag":79.76,
        "MMLU":55.12,
        "TruthfulQA":50.22,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.97,
        "Hub":83,
        "Available on the hub":true,
        "Model Sha":"3b2e3895ff83c8892ab20fb8f98754d947879186"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-orca-platypus-coig-lite-4k-0.6e-13b",
        "Average":60.95,
        "ARC":58.79,
        "HellaSwag":79.93,
        "MMLU":56.77,
        "TruthfulQA":48.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6bf4cf6211489bdbea70585a4a5c0f39deefb4e5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ziqingyang\/chinese-alpaca-2-13b",
        "Average":60.94,
        "ARC":58.7,
        "HellaSwag":79.74,
        "MMLU":55.1,
        "TruthfulQA":50.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.97,
        "Hub":51,
        "Available on the hub":true,
        "Model Sha":"576094cbf4988baf88b3bb66678be1db70bd720a"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Airboros2.1-Platypus2-13B-QLora-0.80-epoch",
        "Average":60.94,
        "ARC":58.96,
        "HellaSwag":82.46,
        "MMLU":54.62,
        "TruthfulQA":47.71,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"45bd1e47218ba2e075e03f6407980eb839e67eb3"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"mosaicml\/mpt-30b-chat",
        "Average":60.94,
        "ARC":58.36,
        "HellaSwag":82.41,
        "MMLU":50.98,
        "TruthfulQA":52.0,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":29.96,
        "Hub":177,
        "Available on the hub":true,
        "Model Sha":"b7957743f18845ff8695f7919420adb769ec225e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"khoantap\/wizard-limarp",
        "Average":60.93,
        "ARC":58.62,
        "HellaSwag":81.87,
        "MMLU":54.96,
        "TruthfulQA":48.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":38,
        "Available on the hub":false,
        "Model Sha":"7301565c37edfe74296dbb280c69aab05e82d39a"
    },
    {
        "T":"\u2b55",
        "Model":"dfurman\/Llama-2-13B-Instruct-v0.2",
        "Average":60.93,
        "ARC":60.58,
        "HellaSwag":81.96,
        "MMLU":55.46,
        "TruthfulQA":45.71,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"ac4b0962df8430f0b31c76a3d97a61134114c87e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewqr2130\/alignment-handbook-zephyr-7b-sft-full-dpo-5e7-cont1",
        "Average":60.92,
        "ARC":60.24,
        "HellaSwag":82.28,
        "MMLU":60.61,
        "TruthfulQA":40.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a1ad23ee605793a7d2dd6a5030b293c68cc8f6f1"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/llama-2-13b-Guanaco-QLoRA",
        "Average":60.92,
        "ARC":61.09,
        "HellaSwag":82.99,
        "MMLU":55.47,
        "TruthfulQA":44.12,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"67e68284234538d3851d5c0c334383daffec57a2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/test-case-2",
        "Average":60.92,
        "ARC":58.53,
        "HellaSwag":79.4,
        "MMLU":56.14,
        "TruthfulQA":49.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c172974de60343295ebb355ebc204d3804786e9f"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/Synatra-RP-Orca-2-7b-v0.1",
        "Average":60.92,
        "ARC":57.68,
        "HellaSwag":77.37,
        "MMLU":56.1,
        "TruthfulQA":52.52,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"da80bc823c407c28c464cc0547a8ed9e0ca82f79"
    },
    {
        "T":"\u2b55",
        "Model":"euclaise\/Ferret_7B",
        "Average":60.91,
        "ARC":62.29,
        "HellaSwag":81.33,
        "MMLU":60.09,
        "TruthfulQA":39.94,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"c1e1e2743ffa7b9369aebac751b04f7e8740f80d"
    },
    {
        "T":"\u2b55",
        "Model":"euclaise\/Ferret-7B",
        "Average":60.91,
        "ARC":62.29,
        "HellaSwag":81.33,
        "MMLU":60.09,
        "TruthfulQA":39.94,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"e96b5245ef97999f143a2c9f9739e5cf52ec0d64"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/MistralInstructLongish",
        "Average":60.91,
        "ARC":60.75,
        "HellaSwag":81.86,
        "MMLU":60.49,
        "TruthfulQA":40.55,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"813c4707970cb5bf3e2a49f7f350af59e7032c24"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"speechlessai\/speechless-llama2-dolphin-orca-platypus-13b",
        "Average":60.91,
        "ARC":59.64,
        "HellaSwag":82.65,
        "MMLU":57.9,
        "TruthfulQA":43.44,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"fd23b7d052eb7c18ecd2acc1be77c66b7b8d6dad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-13b-v1.5-16k",
        "Average":60.89,
        "ARC":55.72,
        "HellaSwag":80.61,
        "MMLU":55.45,
        "TruthfulQA":51.79,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":138,
        "Available on the hub":true,
        "Model Sha":"277697af19d4b267626ebc9f4e078d19a9a0fddf"
    },
    {
        "T":"?",
        "Model":"NeuralNovel\/Senzu-7B-v0.1",
        "Average":60.89,
        "ARC":58.19,
        "HellaSwag":81.98,
        "MMLU":63.2,
        "TruthfulQA":40.2,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"571f2733df7ca66eee20b32674cedce1017e21f6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/based-30b",
        "Average":60.89,
        "ARC":63.91,
        "HellaSwag":85.67,
        "MMLU":58.28,
        "TruthfulQA":35.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":32.32,
        "Hub":38,
        "Available on the hub":true,
        "Model Sha":"5818a6344f48dc5a324589b57cb288a9d54c0b79"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"XuanXuanXuanXuan\/Mistral-7B-Instruct-v0.1-gpt-4-80k",
        "Average":60.89,
        "ARC":55.12,
        "HellaSwag":74.79,
        "MMLU":56.13,
        "TruthfulQA":57.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0b74a57c33242b7441a9b85fbcca5d477c3584bd"
    },
    {
        "T":"?",
        "Model":"BioMistral\/BioMistral-7B-TIES",
        "Average":60.88,
        "ARC":55.46,
        "HellaSwag":79.59,
        "MMLU":56.29,
        "TruthfulQA":52.2,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"143f0ab48dc2bd35cce0973826cd9d5d549c2fab"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/Synatra-11B-Testbench",
        "Average":60.88,
        "ARC":57.34,
        "HellaSwag":78.66,
        "MMLU":55.56,
        "TruthfulQA":51.97,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":11.0,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"9399ea6c2a1d955e31d6b4d68b2b86115aea0e59"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kingbri\/airolima-chronos-grad-l2-13B",
        "Average":60.88,
        "ARC":59.56,
        "HellaSwag":83.5,
        "MMLU":55.78,
        "TruthfulQA":44.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"d2ad57b2b50361485b2b04e59a989161599cb08b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_v3.2",
        "Average":60.87,
        "ARC":59.64,
        "HellaSwag":82.68,
        "MMLU":56.68,
        "TruthfulQA":44.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":35,
        "Available on the hub":true,
        "Model Sha":"65320bf6dbe0cb4682d45a9e55dbc876502f8b66"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-orca-platypus-coig-lite-4k-0.5e-13b",
        "Average":60.87,
        "ARC":58.02,
        "HellaSwag":80.15,
        "MMLU":57.26,
        "TruthfulQA":48.04,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"081d1da5cfa2f6ad43abdf4fb5e41f8ec5846224"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewqr2130\/alignment-handbook-zephyr-7b_ppo_5e7step_51",
        "Average":60.87,
        "ARC":59.73,
        "HellaSwag":82.52,
        "MMLU":59.76,
        "TruthfulQA":41.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"58adefcddc81bda70caea1bf7b3d377b5e585942"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ericpolewski\/TacoBeLLM",
        "Average":60.86,
        "ARC":58.53,
        "HellaSwag":81.9,
        "MMLU":56.97,
        "TruthfulQA":46.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":13.02,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"8ae631952a2421f9c7a12e048bc9d578dfc640f1"
    },
    {
        "T":"?",
        "Model":"TeeZee\/NEBULA-XB-v1.0",
        "Average":60.86,
        "ARC":56.66,
        "HellaSwag":81.78,
        "MMLU":60.98,
        "TruthfulQA":44.03,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":23.82,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1a41b53056d6e3fa55ed55d01d37b8ebefb453ae"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2-13B-LoRa",
        "Average":60.85,
        "ARC":60.67,
        "HellaSwag":82.5,
        "MMLU":56.34,
        "TruthfulQA":43.91,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1450c541cf9e378e81862fabeb234b8e0a2bdf5a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kingbri\/chronolima-airo-grad-l2-13B",
        "Average":60.85,
        "ARC":59.56,
        "HellaSwag":83.47,
        "MMLU":55.8,
        "TruthfulQA":44.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"9195bd6ea775daf347a275e190665e10bf1fb54b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_v3.2",
        "Average":60.85,
        "ARC":59.47,
        "HellaSwag":82.6,
        "MMLU":56.82,
        "TruthfulQA":44.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":35,
        "Available on the hub":true,
        "Model Sha":"bc771c901529dedbf04864d0b81452f62301f882"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"maywell\/Synatra-RP-Orca-2-7b-v0.1",
        "Average":60.85,
        "ARC":57.42,
        "HellaSwag":77.31,
        "MMLU":56.12,
        "TruthfulQA":52.55,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"da80bc823c407c28c464cc0547a8ed9e0ca82f79"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/PuddleJumper-Platypus2-13B-QLoRA-0.80-epoch",
        "Average":60.84,
        "ARC":54.52,
        "HellaSwag":79.36,
        "MMLU":55.15,
        "TruthfulQA":54.32,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"4b5aabc51907e4cba49f373c6dc09a2634f2fb8a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shareAI\/llama2-13b-Chinese-chat",
        "Average":60.83,
        "ARC":60.58,
        "HellaSwag":82.19,
        "MMLU":55.45,
        "TruthfulQA":45.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub":35,
        "Available on the hub":false,
        "Model Sha":"31103acf93479d5c3865fb9b51dcb38e10d8b801"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/test-case-3",
        "Average":60.83,
        "ARC":57.76,
        "HellaSwag":79.56,
        "MMLU":56.77,
        "TruthfulQA":49.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ec6e8c7d2a2f05c7459a5490e18dc99da01e73a1"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"amu\/spin-phi2",
        "Average":60.82,
        "ARC":63.57,
        "HellaSwag":75.57,
        "MMLU":57.93,
        "TruthfulQA":46.22,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"5040b8b4108f00030839472e5c97d7c5944904e7"
    },
    {
        "T":"?",
        "Model":"huseyinatahaninan\/phi-2-dpo",
        "Average":60.81,
        "ARC":63.05,
        "HellaSwag":76.36,
        "MMLU":58.46,
        "TruthfulQA":45.35,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e23c721e850052435d5b0c1c664432a11bbbd26e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/WizardLM-1.0-Uncensored-Llama2-13b",
        "Average":60.81,
        "ARC":55.8,
        "HellaSwag":80.41,
        "MMLU":55.59,
        "TruthfulQA":51.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":28,
        "Available on the hub":true,
        "Model Sha":"134cea14627fd875f6f277cad92f988024855478"
    },
    {
        "T":"\u2b55",
        "Model":"Aspik101\/Redmond-Puffin-13B-instruct-PL-lora_unload",
        "Average":60.81,
        "ARC":60.92,
        "HellaSwag":82.43,
        "MMLU":55.61,
        "TruthfulQA":44.26,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b933009635299bca32c694336aa2007d756a2dda"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wannaphong\/han-llm-7b-v3",
        "Average":60.8,
        "ARC":58.7,
        "HellaSwag":81.79,
        "MMLU":59.59,
        "TruthfulQA":43.12,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.27,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4cdb6b256a117955a52bd017f9e2d3bfef859da8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeontaek\/llama-2-13B-ensemble-v6",
        "Average":60.8,
        "ARC":52.22,
        "HellaSwag":80.95,
        "MMLU":57.38,
        "TruthfulQA":52.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8134f0296812055a7008c9e2f31f68d59f962908"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WizardLM\/WizardLM-13B-V1.2",
        "Average":60.79,
        "ARC":59.04,
        "HellaSwag":82.21,
        "MMLU":54.64,
        "TruthfulQA":47.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":133,
        "Available on the hub":true,
        "Model Sha":"6760d0c07ffdc2405295ed7a29437cf4dc414bac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Zangs3011\/mistral_7b_3Epoch_DolphinCoder",
        "Average":60.79,
        "ARC":59.22,
        "HellaSwag":82.32,
        "MMLU":57.91,
        "TruthfulQA":43.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9335fee0bbe38195226c63e3f4aa606bbc387e8d"
    },
    {
        "T":"?",
        "Model":"TheBloke\/WizardLM-13B-V1.1-GPTQ",
        "Average":60.78,
        "ARC":58.53,
        "HellaSwag":80.66,
        "MMLU":49.59,
        "TruthfulQA":54.35,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.03,
        "Hub":26,
        "Available on the hub":true,
        "Model Sha":"9df807ac64034bc6e7387326689d6e39656ce5e0"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/Samantha-Nebula-7B",
        "Average":60.76,
        "ARC":57.0,
        "HellaSwag":82.25,
        "MMLU":54.21,
        "TruthfulQA":49.58,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a7d4b8a1683e33dd3c60064d7dd9d5c35691323f"
    },
    {
        "T":"?",
        "Model":"Joseph717171\/Mistral-10.7B-v0.2",
        "Average":60.76,
        "ARC":58.28,
        "HellaSwag":80.92,
        "MMLU":63.44,
        "TruthfulQA":40.39,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"5e9d95dc097aa3d5e5ee63d696d1697590344747"
    },
    {
        "T":"?",
        "Model":"FredrikBL\/test-ties",
        "Average":60.76,
        "ARC":58.53,
        "HellaSwag":81.66,
        "MMLU":61.7,
        "TruthfulQA":41.14,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"99c5b76e67ee862e05e733933ccb5cb382380a94"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/Synatra-V0.1-7B-Instruct",
        "Average":60.74,
        "ARC":55.29,
        "HellaSwag":76.63,
        "MMLU":55.29,
        "TruthfulQA":55.76,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"7ee3416f31a3c7e8d5ab4295ac1b641075f36345"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"maywell\/Synatra-V0.1-7B",
        "Average":60.74,
        "ARC":55.29,
        "HellaSwag":76.63,
        "MMLU":55.29,
        "TruthfulQA":55.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"7ee3416f31a3c7e8d5ab4295ac1b641075f36345"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"alignment-handbook\/zephyr-7b-sft-qlora",
        "Average":60.74,
        "ARC":60.07,
        "HellaSwag":82.36,
        "MMLU":61.65,
        "TruthfulQA":38.88,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"156bec577ff12a65236cfc90860dcc61e96c6fd6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JosephusCheung\/Pwen-14B-Chat-20_30",
        "Average":60.74,
        "ARC":56.14,
        "HellaSwag":79.78,
        "MMLU":60.01,
        "TruthfulQA":47.02,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":14.0,
        "Hub":35,
        "Available on the hub":false,
        "Model Sha":"e878e1f1f7b533c32beb8e06ebcf0cfa23f3fe9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/T1B",
        "Average":60.74,
        "ARC":56.14,
        "HellaSwag":79.78,
        "MMLU":60.01,
        "TruthfulQA":47.02,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6f3664328e9f07a6578ccb0c5713b747cc0549d5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bhenrym14\/airophin-13b-pntk-16k-fp16",
        "Average":60.73,
        "ARC":61.43,
        "HellaSwag":82.81,
        "MMLU":55.44,
        "TruthfulQA":43.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"6b5418b69e8270df659eacb192f469e7c3af70b3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/WizardLM-1.0-Uncensored-Llama2-13b",
        "Average":60.72,
        "ARC":55.72,
        "HellaSwag":80.34,
        "MMLU":55.4,
        "TruthfulQA":51.44,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":28,
        "Available on the hub":true,
        "Model Sha":"134cea14627fd875f6f277cad92f988024855478"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewqr2130\/alignment-handbook-zephyr-7b_ppo_5e7step_102",
        "Average":60.71,
        "ARC":59.22,
        "HellaSwag":82.45,
        "MMLU":59.62,
        "TruthfulQA":41.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2624e0c1fb055d98bd6ee41670d46d534e01facc"
    },
    {
        "T":"\u2b55",
        "Model":"martyn\/llama2-megamerge-dare-13b-v2",
        "Average":60.71,
        "ARC":59.39,
        "HellaSwag":80.93,
        "MMLU":55.26,
        "TruthfulQA":47.27,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d0ff28a0cb4a70b15f55a416fbae6979f4ae5775"
    },
    {
        "T":"?",
        "Model":"wannaphong\/han-llm-7b-v2",
        "Average":60.71,
        "ARC":58.79,
        "HellaSwag":81.75,
        "MMLU":59.93,
        "TruthfulQA":42.38,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.27,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e192c06c348f148afe82df13baf56c253eede0e6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-thoughts-mistral-7b-v1.0",
        "Average":60.71,
        "ARC":58.53,
        "HellaSwag":81.25,
        "MMLU":54.95,
        "TruthfulQA":48.09,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e5aa2dc21065b86fced97bd09d8567bd384a273d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Radu1999\/Mister",
        "Average":60.7,
        "ARC":61.69,
        "HellaSwag":71.74,
        "MMLU":43.53,
        "TruthfulQA":65.85,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"36cc93cd7bc754a5b23b2d47760ee7ed814d84f3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/Qwen1.5-8x7b-v0.1",
        "Average":60.68,
        "ARC":51.62,
        "HellaSwag":75.71,
        "MMLU":59.61,
        "TruthfulQA":55.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":38.02,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"1cc77bff283c5e5fee805d5220dc7da2fbfc29f5"
    },
    {
        "T":"?",
        "Model":"ozayezerceli\/Threnystril-2x7B-moe",
        "Average":60.68,
        "ARC":52.82,
        "HellaSwag":73.36,
        "MMLU":63.83,
        "TruthfulQA":52.71,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fe2f1b6f3e3153ae1e3628644b8be77f17d3903e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Changgil\/K2S3-Mistral-7b-v1.3",
        "Average":60.67,
        "ARC":58.11,
        "HellaSwag":78.94,
        "MMLU":58.44,
        "TruthfulQA":47.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.37,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3825ea65280f33aad5dab2d8b51a0af776f8e4a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mikael110\/llama-2-13b-guanaco-fp16",
        "Average":60.67,
        "ARC":60.92,
        "HellaSwag":83.18,
        "MMLU":54.58,
        "TruthfulQA":44.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"feb7ef47ceca6aec9548264a39622b63fdcb853c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kno10\/ende-chat-0.0.4",
        "Average":60.65,
        "ARC":56.57,
        "HellaSwag":79.63,
        "MMLU":55.22,
        "TruthfulQA":51.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f2ce69509c3d0f461818aa81d90b569df423b207"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-qwen1.5-en-7b",
        "Average":60.64,
        "ARC":53.41,
        "HellaSwag":75.51,
        "MMLU":61.67,
        "TruthfulQA":51.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.72,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"173ccc71f9c2efca1c3494d94956fe100829e5ab"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/airoboros-2.1-llama-2-13B-QLoRa",
        "Average":60.64,
        "ARC":59.73,
        "HellaSwag":82.91,
        "MMLU":54.77,
        "TruthfulQA":45.14,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ebf991c8d34314caab6ccc6b078c681d20bac39a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CalderaAI\/13B-Legerdemain-L2",
        "Average":60.63,
        "ARC":61.26,
        "HellaSwag":83.26,
        "MMLU":56.0,
        "TruthfulQA":41.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"d6624ce1bcc6b50c86b86e879a8c9822218b84d2"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16",
        "Average":60.61,
        "ARC":60.41,
        "HellaSwag":82.58,
        "MMLU":55.86,
        "TruthfulQA":43.61,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"891be2d8f205baa04c8a92f6ab1225f0d0c3e5bd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"grimpep\/llama2-22b-wizard_vicuna",
        "Average":60.61,
        "ARC":58.96,
        "HellaSwag":82.01,
        "MMLU":54.55,
        "TruthfulQA":46.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":21.62,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"b1fe4450efe20d1330e2e4335deaa23076596070"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"grimpep\/llama2-22B-GPLATTY",
        "Average":60.61,
        "ARC":58.96,
        "HellaSwag":82.01,
        "MMLU":54.55,
        "TruthfulQA":46.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":21.62,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7707d4baf43a8654a77619af02bbf948e07829d6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bhenrym14\/airophin-13b-pntk-16k-fp16",
        "Average":60.61,
        "ARC":61.18,
        "HellaSwag":82.86,
        "MMLU":55.19,
        "TruthfulQA":43.2,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"6b5418b69e8270df659eacb192f469e7c3af70b3"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/llama-2-13b-QLoRA",
        "Average":60.6,
        "ARC":58.02,
        "HellaSwag":82.33,
        "MMLU":55.8,
        "TruthfulQA":46.23,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d1a41d83c6bcc14378ee4859d65ef77a261d39d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"amu\/spin-phi2-1.5",
        "Average":60.59,
        "ARC":63.65,
        "HellaSwag":75.79,
        "MMLU":56.52,
        "TruthfulQA":46.4,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5c9c6b9819b1a1631ac4d6db1e93b011a318756c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"amu\/spin-phi2-2",
        "Average":60.59,
        "ARC":63.65,
        "HellaSwag":75.79,
        "MMLU":56.52,
        "TruthfulQA":46.4,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5c9c6b9819b1a1631ac4d6db1e93b011a318756c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/llama2-13b-orca-8k-3319",
        "Average":60.59,
        "ARC":60.75,
        "HellaSwag":81.91,
        "MMLU":57.06,
        "TruthfulQA":42.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":112,
        "Available on the hub":true,
        "Model Sha":"160f58ec85ef25ad935eb583f14c7e8c7f7e7839"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Clover3-17B",
        "Average":60.57,
        "ARC":59.9,
        "HellaSwag":81.18,
        "MMLU":60.47,
        "TruthfulQA":40.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":16.84,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"428f6f58869426baae2c49442b207a15bc2da3cc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kno10\/ende-chat-0.0.4",
        "Average":60.56,
        "ARC":56.14,
        "HellaSwag":79.62,
        "MMLU":54.89,
        "TruthfulQA":51.61,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f2ce69509c3d0f461818aa81d90b569df423b207"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/WizardLM-13B-V1-1-SuperHOT-8K-fp16",
        "Average":60.55,
        "ARC":58.62,
        "HellaSwag":81.07,
        "MMLU":48.32,
        "TruthfulQA":54.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"83905656ca3e63877b8d9f3a74118da0c9bc6939"
    },
    {
        "T":"?",
        "Model":"vicgalle\/OpenHermes-Gemma-7B",
        "Average":60.55,
        "ARC":57.0,
        "HellaSwag":76.3,
        "MMLU":55.74,
        "TruthfulQA":53.14,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"289270b57a23e0db3113437094aba0e9c9deb0c1"
    },
    {
        "T":"?",
        "Model":"vince62s\/phi-2-psy",
        "Average":60.54,
        "ARC":60.84,
        "HellaSwag":75.52,
        "MMLU":57.57,
        "TruthfulQA":48.22,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":15,
        "Available on the hub":false,
        "Model Sha":"9a9a31679e597e92dd02af20e94e4cd7fb211112"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"duliadotio\/dulia-13b-8k-alpha",
        "Average":60.53,
        "ARC":60.67,
        "HellaSwag":82.0,
        "MMLU":56.87,
        "TruthfulQA":42.59,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c3bcafd7f6133a7e7c069f8765a99fe84989d926"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/athene-noctua-13b",
        "Average":60.52,
        "ARC":57.17,
        "HellaSwag":81.52,
        "MMLU":55.91,
        "TruthfulQA":47.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7b5e2639d2d9f0b94c7e6834e6082f7c10fc8e12"
    },
    {
        "T":"?",
        "Model":"azarafrooz\/mistral-7b-selfplay-v0",
        "Average":60.52,
        "ARC":54.69,
        "HellaSwag":75.69,
        "MMLU":55.4,
        "TruthfulQA":56.28,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f953e2d8749d9dec967dd05d6e649c7c25a9fb23"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/LewdEngine",
        "Average":60.51,
        "ARC":60.49,
        "HellaSwag":83.08,
        "MMLU":54.84,
        "TruthfulQA":43.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6e918ff9f563552af4ad66f4308f6d040e24af4b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Deci\/DeciLM-7B",
        "Average":60.5,
        "ARC":59.39,
        "HellaSwag":82.51,
        "MMLU":59.76,
        "TruthfulQA":40.33,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.04,
        "Hub":214,
        "Available on the hub":false,
        "Model Sha":"b943e32a12bc21df2b8b3c50525c6646acd442bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"janhq\/Mistral-7B-Instruct-v0.2-DARE",
        "Average":60.48,
        "ARC":61.95,
        "HellaSwag":75.62,
        "MMLU":49.99,
        "TruthfulQA":54.36,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"98731ddd2dd52fd1b2c69c4cb95bbb1ac03ce496"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-llama2-13b-v8.1-fp16",
        "Average":60.47,
        "ARC":55.97,
        "HellaSwag":79.79,
        "MMLU":54.95,
        "TruthfulQA":51.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.88,
        "Hub":60,
        "Available on the hub":true,
        "Model Sha":"b51c6b29abdf7c420cb5e5f4f309ff83179c7bb8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Changgil\/K2S3-Mistral-7b-v1.2",
        "Average":60.46,
        "ARC":57.51,
        "HellaSwag":79.61,
        "MMLU":58.04,
        "TruthfulQA":46.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.37,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"52d060cd9e93f176911c91ee232f582f253e7f8f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-OpenOrca_20w",
        "Average":60.46,
        "ARC":59.9,
        "HellaSwag":82.51,
        "MMLU":56.3,
        "TruthfulQA":43.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f01882672e89b164f76093cf3bd26cfc6ecf72ed"
    },
    {
        "T":"\u2b55",
        "Model":"mistralai\/Mistral-7B-Instruct-v0.1",
        "Average":60.45,
        "ARC":54.52,
        "HellaSwag":75.63,
        "MMLU":55.38,
        "TruthfulQA":56.28,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1383,
        "Available on the hub":false,
        "Model Sha":"7961f5aa9b736bf8e364b2e6f201190f97a27931"
    },
    {
        "T":"\u2b55",
        "Model":"Enno-Ai\/vigogne2-enno-13b-sft-lora-4bit",
        "Average":60.44,
        "ARC":62.03,
        "HellaSwag":82.65,
        "MMLU":54.11,
        "TruthfulQA":42.98,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2a1b03977395eee44742abda63a4787ea5371d06"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bhenrym14\/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16",
        "Average":60.44,
        "ARC":60.58,
        "HellaSwag":82.97,
        "MMLU":52.1,
        "TruthfulQA":46.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.53,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"24ebae726954e4c1f24a8b2cbe0ca863012a7338"
    },
    {
        "T":"\u2b55",
        "Model":"inswave\/AISquare-Instruct-llama2-koen-13b-v0.9.24",
        "Average":60.44,
        "ARC":55.63,
        "HellaSwag":81.35,
        "MMLU":51.76,
        "TruthfulQA":53.0,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.16,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0302553f7fe0a74925719b197b9c119aad0586c2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jefferylovely\/AiMaven-Orca2",
        "Average":60.43,
        "ARC":54.69,
        "HellaSwag":79.0,
        "MMLU":54.61,
        "TruthfulQA":53.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fbba65dad747e1461c2b024fe6cc690a3b20db24"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-Mix-L2-20B",
        "Average":60.43,
        "ARC":57.76,
        "HellaSwag":79.63,
        "MMLU":52.51,
        "TruthfulQA":51.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":20.63,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6f9dcdaae6ef9071effe63d2107abe8b9712345b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FlagAlpha\/Llama2-Chinese-13b-Chat",
        "Average":60.41,
        "ARC":55.97,
        "HellaSwag":82.05,
        "MMLU":54.74,
        "TruthfulQA":48.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":175,
        "Available on the hub":true,
        "Model Sha":"cb69cda10a72bc9736b1c10181ac41f28b69ff9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LLMs\/WizardLM-13B-V1.0",
        "Average":60.4,
        "ARC":57.25,
        "HellaSwag":80.88,
        "MMLU":52.92,
        "TruthfulQA":50.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"f802ea7c01e2da27b0f7091c70d3ecfd8fc042b9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chansung\/gpt4-alpaca-lora-13b-decapoda-1024",
        "Average":60.4,
        "ARC":59.39,
        "HellaSwag":81.87,
        "MMLU":47.75,
        "TruthfulQA":52.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"7aedafea409de07a997d70a84e30242c7b86877c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/wizardLM-13B-1.0-fp16",
        "Average":60.4,
        "ARC":57.25,
        "HellaSwag":80.88,
        "MMLU":52.9,
        "TruthfulQA":50.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"b79733805e98e668ff9a459975c259881b1b8014"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"steve-cse\/MelloGPT",
        "Average":60.39,
        "ARC":53.84,
        "HellaSwag":76.12,
        "MMLU":55.99,
        "TruthfulQA":55.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"aedecb296e2cdcb3da95a345a794ea26f071c419"
    },
    {
        "T":"?",
        "Model":"digitous\/13B-Chimera",
        "Average":60.39,
        "ARC":57.59,
        "HellaSwag":81.5,
        "MMLU":49.86,
        "TruthfulQA":52.59,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"85cfe8e6db2bee804873cfdb48955696cc5b0689"
    },
    {
        "T":"?",
        "Model":"amu\/spin-phi2",
        "Average":60.39,
        "ARC":63.14,
        "HellaSwag":75.56,
        "MMLU":57.08,
        "TruthfulQA":45.77,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"b206227dcf0c36eb30edcee377e5b0ccdd3668c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-1.3-L2-13B",
        "Average":60.39,
        "ARC":56.83,
        "HellaSwag":81.7,
        "MMLU":52.79,
        "TruthfulQA":50.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"45ba2f603769aa6b97639962f522b8d7398c2393"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2xOpenOrcaxGuanaco-13B-LoRa",
        "Average":60.36,
        "ARC":61.26,
        "HellaSwag":80.52,
        "MMLU":57.84,
        "TruthfulQA":41.82,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2f33c341f9308b3d851f3d04f2f078f86972b5a0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-13b-2.1",
        "Average":60.36,
        "ARC":59.47,
        "HellaSwag":82.47,
        "MMLU":54.83,
        "TruthfulQA":44.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"172e30e56e939f73d7d00a165c2d49cbd284481f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aboros98\/merlin1",
        "Average":60.36,
        "ARC":60.67,
        "HellaSwag":74.55,
        "MMLU":57.86,
        "TruthfulQA":48.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"eb6ead2cd06e254435c9ea39070bbcefc42e21a1"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/Platypus-Nebula-v2-7B",
        "Average":60.35,
        "ARC":55.38,
        "HellaSwag":83.02,
        "MMLU":56.07,
        "TruthfulQA":46.94,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2d95180bae03c0b268dff44a1f9806fc295adc09"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"duoqi\/Nanbeige-16B-Base-Llama",
        "Average":60.35,
        "ARC":56.48,
        "HellaSwag":78.97,
        "MMLU":63.34,
        "TruthfulQA":42.6,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":15.83,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"183d749c4556abc66f6fd0d821d1d193e80053c1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/test-case-5",
        "Average":60.33,
        "ARC":56.57,
        "HellaSwag":79.04,
        "MMLU":55.73,
        "TruthfulQA":50.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3ce1dc83a11f5875e876ca7efb43643901e0ae1f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Monero\/WizardLM-30B-Uncensored-Guanaco-SuperCOT-30b",
        "Average":60.31,
        "ARC":55.55,
        "HellaSwag":80.37,
        "MMLU":54.01,
        "TruthfulQA":51.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"f25d922536e602af035d476a287b68361fdda5de"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CausalLM\/72B-preview-canary-llamafied-qwen-llamafy-unbias-qkv",
        "Average":60.3,
        "ARC":53.07,
        "HellaSwag":63.13,
        "MMLU":67.39,
        "TruthfulQA":57.62,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":72.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dc66cf314292f3bfd5a2eed74018671effac6405"
    },
    {
        "T":"?",
        "Model":"liminerity\/Liph42",
        "Average":60.3,
        "ARC":62.03,
        "HellaSwag":75.87,
        "MMLU":57.37,
        "TruthfulQA":45.94,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1a760e1ed7ebe8b202f632e687fb129c87a02537"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/UltraLM-13B-fp16",
        "Average":60.3,
        "ARC":57.59,
        "HellaSwag":80.2,
        "MMLU":51.85,
        "TruthfulQA":51.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"734f5641f6c548474517d1536c46024517f120e0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/minotaur-13b-fixed",
        "Average":60.29,
        "ARC":59.04,
        "HellaSwag":81.66,
        "MMLU":50.1,
        "TruthfulQA":50.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"5dac6f7559dba1c6fb59fee18c3e713cc3c83db7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/Llama-2-13b-FINETUNE4",
        "Average":60.27,
        "ARC":58.7,
        "HellaSwag":81.93,
        "MMLU":57.21,
        "TruthfulQA":43.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"939d06081210fa943c60210a47583f43b60901ad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Redmond-Puffin-13B",
        "Average":60.27,
        "ARC":60.41,
        "HellaSwag":83.2,
        "MMLU":55.36,
        "TruthfulQA":42.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":12.85,
        "Hub":96,
        "Available on the hub":true,
        "Model Sha":"12af25fa7ea02c4fc636952ea8b9dc9cf48e35be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"manishiitg\/open-aditi-hi-v1",
        "Average":60.25,
        "ARC":58.79,
        "HellaSwag":81.38,
        "MMLU":58.51,
        "TruthfulQA":42.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"1f6cbcdf01831830ff0f25f6f0e84ec4e9337e72"
    },
    {
        "T":"\u2b55",
        "Model":"KoboldAI\/LLaMA2-13B-Holomax",
        "Average":60.25,
        "ARC":60.49,
        "HellaSwag":82.86,
        "MMLU":54.67,
        "TruthfulQA":42.97,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"2c4fddeb097636d6462b7628a8e053ad3ff4678c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lajonbot\/WizardLM-13B-V1.2-PL-lora_unload",
        "Average":60.24,
        "ARC":58.53,
        "HellaSwag":81.1,
        "MMLU":55.15,
        "TruthfulQA":46.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5f14e6f5ea67fd2840791c46b3e00846cbdb32cf"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2-13B-LoRa-v2",
        "Average":60.24,
        "ARC":59.47,
        "HellaSwag":82.41,
        "MMLU":57.15,
        "TruthfulQA":41.92,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e85fb36125224af68581b2e2583532f3314b8b29"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/pygmalion-2-13b",
        "Average":60.23,
        "ARC":60.32,
        "HellaSwag":82.37,
        "MMLU":56.02,
        "TruthfulQA":42.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":69,
        "Available on the hub":true,
        "Model Sha":"3cdc103995ccd5fc7fd2cb5f51f71b510466f5fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/airoboros-13B-HF",
        "Average":60.23,
        "ARC":58.28,
        "HellaSwag":81.05,
        "MMLU":50.03,
        "TruthfulQA":51.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"9219b61a0e8bc880e4cd0f8bebc48a97ee0950c7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-13b",
        "Average":60.23,
        "ARC":58.28,
        "HellaSwag":81.05,
        "MMLU":50.03,
        "TruthfulQA":51.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":93,
        "Available on the hub":true,
        "Model Sha":"44830f9e1559f318f5dad875bab40d1d1beddbfc"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Walter-Mistral-7B",
        "Average":60.22,
        "ARC":58.87,
        "HellaSwag":83.43,
        "MMLU":58.65,
        "TruthfulQA":39.93,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d7ccd4f0360c397765578521efaed394fe00dbf5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"alignment-handbook\/zephyr-7b-sft-full",
        "Average":60.22,
        "ARC":58.11,
        "HellaSwag":80.83,
        "MMLU":60.2,
        "TruthfulQA":41.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"92f9fac4529acacb2c33a35c46917393690c6311"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jjaaaww\/posi_13b",
        "Average":60.22,
        "ARC":59.64,
        "HellaSwag":82.52,
        "MMLU":56.56,
        "TruthfulQA":42.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ff4eeb0f876c41553c302020041a0e78a15f9aa7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"The-Face-Of-Goonery\/Huginn-V5-10.7B",
        "Average":60.21,
        "ARC":63.31,
        "HellaSwag":78.8,
        "MMLU":54.22,
        "TruthfulQA":44.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a3d7e197ba2a96ff73bbbb2b315d38f43a1e5508"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Liberated-Qwen1.5-7B",
        "Average":60.21,
        "ARC":52.05,
        "HellaSwag":76.59,
        "MMLU":61.25,
        "TruthfulQA":50.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"8619393688e7a490f4855ce108ca7358503cfe7e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CoolWP\/llama-2-13b-guanaco-fp16",
        "Average":60.21,
        "ARC":59.56,
        "HellaSwag":82.39,
        "MMLU":55.47,
        "TruthfulQA":43.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a60e8e39e4fbe271655e1c78eb1ceb2081518231"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mwitiderrick\/SwahiliInstruct-v0.2",
        "Average":60.2,
        "ARC":55.2,
        "HellaSwag":78.22,
        "MMLU":50.3,
        "TruthfulQA":57.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"1b822c08b1065d5843cc48bf3a841ac5cd9d3b40"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"splm\/zephyr-7b-sft-full-spin-peft-iter2",
        "Average":60.2,
        "ARC":58.02,
        "HellaSwag":80.77,
        "MMLU":60.22,
        "TruthfulQA":41.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a07fc6ae0f9729767e2cedb229a515e7d84bd87f"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2-13B-QLoRa",
        "Average":60.19,
        "ARC":57.51,
        "HellaSwag":82.55,
        "MMLU":57.34,
        "TruthfulQA":43.38,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e62a8fafce0d64ac03d465a4e915bc1f50776a08"
    },
    {
        "T":"?",
        "Model":"h2oai\/h2ogpt-research-oig-oasst1-512-30b",
        "Average":60.19,
        "ARC":58.96,
        "HellaSwag":82.61,
        "MMLU":50.74,
        "TruthfulQA":48.47,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"4215e83b9038c9e61d979cf5223b29f860bace9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dfurman\/llama-2-13b-guanaco-peft",
        "Average":60.19,
        "ARC":59.98,
        "HellaSwag":82.43,
        "MMLU":55.76,
        "TruthfulQA":42.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"10b58a7c31d5513fa56a9b8b38739253bf5cc0b4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"splm\/zephyr-7b-sft-full-spin-peft-iter0",
        "Average":60.19,
        "ARC":57.94,
        "HellaSwag":80.77,
        "MMLU":60.26,
        "TruthfulQA":41.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2995cd6e9b2780b8a14043fbc4241a81ba6d1feb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"splm\/zephyr-7b-sft-full-spin-peft-iter1",
        "Average":60.19,
        "ARC":57.94,
        "HellaSwag":80.78,
        "MMLU":60.23,
        "TruthfulQA":41.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bdc360638c254864af30b5c0e6ff9a7b19e51b46"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Redmond-Puffin-13B",
        "Average":60.18,
        "ARC":60.49,
        "HellaSwag":83.21,
        "MMLU":54.95,
        "TruthfulQA":42.08,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":12.85,
        "Hub":96,
        "Available on the hub":true,
        "Model Sha":"12af25fa7ea02c4fc636952ea8b9dc9cf48e35be"
    },
    {
        "T":"\u2b55",
        "Model":"pe-nlp\/llama-2-13b-platypus-vicuna-wizard",
        "Average":60.17,
        "ARC":61.26,
        "HellaSwag":82.31,
        "MMLU":55.21,
        "TruthfulQA":41.91,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"71aa919fc15fa9d9def9185791b15a3f76e7bd8d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-dolphin_20w",
        "Average":60.17,
        "ARC":59.56,
        "HellaSwag":82.55,
        "MMLU":55.89,
        "TruthfulQA":42.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c75073d7545a4d222f40dc519021c55a81850d75"
    },
    {
        "T":"?",
        "Model":"Technoculture\/Medchator-2x7b",
        "Average":60.16,
        "ARC":57.59,
        "HellaSwag":78.14,
        "MMLU":56.13,
        "TruthfulQA":48.77,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.07,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b25e7e678c6881b3599cd00387f0f622be6ae7e9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NovoCode\/Tiger-DPO",
        "Average":60.16,
        "ARC":48.21,
        "HellaSwag":81.82,
        "MMLU":59.85,
        "TruthfulQA":50.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7fc4622f783428dcbfba81a7aa8344c84b74a7b3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Hermes-13b",
        "Average":60.15,
        "ARC":56.57,
        "HellaSwag":82.11,
        "MMLU":50.44,
        "TruthfulQA":51.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"gpl",
        "#Params (B)":12.85,
        "Hub":380,
        "Available on the hub":true,
        "Model Sha":"24e8c03148ffd1f3e469744dfc24ad2ad82848f8"
    },
    {
        "T":"\u2b55",
        "Model":"NobodyExistsOnTheInternet\/GiftedConvo13bLoraNoEconsE4",
        "Average":60.15,
        "ARC":59.9,
        "HellaSwag":84.11,
        "MMLU":54.67,
        "TruthfulQA":41.94,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f3d421aadb29830345bf392f793ce3c33e7d68c5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CausalLM\/72B-preview-canary-llamafied-qwen-llamafy-unbias-qkv",
        "Average":60.15,
        "ARC":52.56,
        "HellaSwag":62.99,
        "MMLU":67.45,
        "TruthfulQA":57.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":72.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dc66cf314292f3bfd5a2eed74018671effac6405"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-Open-Platypus_2.5w",
        "Average":60.13,
        "ARC":59.56,
        "HellaSwag":82.46,
        "MMLU":56.06,
        "TruthfulQA":42.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"bc55678af8226e1323305f743a4882da31994e0c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BioMistral\/BioMistral-7B",
        "Average":60.13,
        "ARC":54.27,
        "HellaSwag":79.09,
        "MMLU":55.56,
        "TruthfulQA":51.61,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":315,
        "Available on the hub":false,
        "Model Sha":"e7298b35e5460a37b83fb0dc69eae03f46b49275"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"alignment-handbook\/zephyr-7b-sft-full",
        "Average":60.13,
        "ARC":57.68,
        "HellaSwag":80.82,
        "MMLU":60.31,
        "TruthfulQA":41.71,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"92f9fac4529acacb2c33a35c46917393690c6311"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Wizard-Vicuna-13B-Uncensored-HF",
        "Average":60.13,
        "ARC":58.96,
        "HellaSwag":81.95,
        "MMLU":47.92,
        "TruthfulQA":51.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":198,
        "Available on the hub":true,
        "Model Sha":"fff9ac7f0e2e7b340f2301f5f089d989fc03be67"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/Wizard-Vicuna-13B-Uncensored",
        "Average":60.13,
        "ARC":58.96,
        "HellaSwag":81.95,
        "MMLU":47.92,
        "TruthfulQA":51.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":214,
        "Available on the hub":true,
        "Model Sha":"95bfd1640a54e76b3e857c2462fd3a77eca0b275"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_v3.2_super",
        "Average":60.13,
        "ARC":59.81,
        "HellaSwag":82.5,
        "MMLU":55.9,
        "TruthfulQA":42.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"aab7ce4d48b31a295a0116b61569d8e87a09bb7a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"macadeliccc\/Mistral-7B-v0.2-OpenHermes",
        "Average":60.12,
        "ARC":55.8,
        "HellaSwag":81.61,
        "MMLU":60.0,
        "TruthfulQA":43.09,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"3ee2ade8cb70d99a9f616ff7affe31b059d9a1f6"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged",
        "Average":60.12,
        "ARC":59.13,
        "HellaSwag":82.13,
        "MMLU":54.98,
        "TruthfulQA":44.23,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"aad13bce3b243721e52e9cda479f1102dda99f12"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"anhnv125\/llama-op-v4",
        "Average":60.12,
        "ARC":61.52,
        "HellaSwag":79.21,
        "MMLU":57.01,
        "TruthfulQA":42.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6cd644049de2b944beaefcc6aa34965c00e08529"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bhenrym14\/airophin-v2-13b-PI-8k-fp16",
        "Average":60.11,
        "ARC":60.58,
        "HellaSwag":82.96,
        "MMLU":56.75,
        "TruthfulQA":40.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"26b7edfd282af223d86d5e539451357bb114247b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/manticore-13b",
        "Average":60.08,
        "ARC":58.7,
        "HellaSwag":81.63,
        "MMLU":50.84,
        "TruthfulQA":49.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":104,
        "Available on the hub":true,
        "Model Sha":"aed786b0200251c9962ac200c50f7e367f264b46"
    },
    {
        "T":"\u2b55",
        "Model":"abacusai\/Giraffe-13b-32k-v3",
        "Average":60.08,
        "ARC":59.04,
        "HellaSwag":79.59,
        "MMLU":55.01,
        "TruthfulQA":46.68,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"bbc483fc0a3b88740fd6e04a7fd0c7d98b85cd1d"
    },
    {
        "T":"\u2b55",
        "Model":"The-Face-Of-Goonery\/Huginn-22b-Prototype",
        "Average":60.07,
        "ARC":57.68,
        "HellaSwag":80.69,
        "MMLU":49.81,
        "TruthfulQA":52.11,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"29222b05794abb862ad0aaaf3020696c9f599810"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jphme\/Llama-2-13b-chat-german",
        "Average":60.07,
        "ARC":57.85,
        "HellaSwag":81.66,
        "MMLU":54.45,
        "TruthfulQA":46.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":39,
        "Available on the hub":true,
        "Model Sha":"d72667bd92fd6f76835466d302563d213e0b1ee1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"grimpep\/llama2-28B-Airo03",
        "Average":60.06,
        "ARC":58.45,
        "HellaSwag":81.39,
        "MMLU":53.31,
        "TruthfulQA":47.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":28.26,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"28edc75ddcb1b6e83f28d5d1076f065c05a4942a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SicariusSicariiStuff\/Tinybra_13B",
        "Average":60.06,
        "ARC":55.72,
        "HellaSwag":80.99,
        "MMLU":54.37,
        "TruthfulQA":49.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"fa81ddf8b87ec339b2519044c4271bc59c4b65aa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Kimiko-v2-13B-fp16",
        "Average":60.04,
        "ARC":61.01,
        "HellaSwag":83.32,
        "MMLU":55.17,
        "TruthfulQA":40.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"0fed305667508e50330e71a2d43e9cee5ea73783"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FPHam\/Free_Sydney_13b_HF",
        "Average":60.04,
        "ARC":59.39,
        "HellaSwag":81.4,
        "MMLU":53.73,
        "TruthfulQA":45.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"5474ecbccd1f2a2cda9f77a157993f55c97377ed"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"scb10x\/typhoon-7b",
        "Average":60.04,
        "ARC":58.53,
        "HellaSwag":81.55,
        "MMLU":59.54,
        "TruthfulQA":40.52,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.27,
        "Hub":72,
        "Available on the hub":false,
        "Model Sha":"35fb2f9cee5dbac35109effc816ca206962dad43"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"oh-yeontaek\/llama-2-7B-LoRA-assemble",
        "Average":60.02,
        "ARC":57.34,
        "HellaSwag":78.81,
        "MMLU":50.75,
        "TruthfulQA":53.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"72e866a96a2e9afc6527c8d757c69088c3a069c8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-13b-v1.3",
        "Average":60.01,
        "ARC":54.61,
        "HellaSwag":80.41,
        "MMLU":52.88,
        "TruthfulQA":52.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":149,
        "Available on the hub":true,
        "Model Sha":"7900eeb715a49affee9e6390f824e62eea3f3fb1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/CodeEngine",
        "Average":60.0,
        "ARC":58.36,
        "HellaSwag":82.27,
        "MMLU":54.18,
        "TruthfulQA":45.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f57879831c39f2dcb656cb2c9e9ce5878e92bb44"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishek\/zephyr-beta-math",
        "Average":60.0,
        "ARC":56.66,
        "HellaSwag":81.26,
        "MMLU":57.24,
        "TruthfulQA":44.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"dd3d070a104d8b36ba98d14a485d88fa95aaab63"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/llama2-13b-math1.1",
        "Average":59.99,
        "ARC":57.25,
        "HellaSwag":80.74,
        "MMLU":53.56,
        "TruthfulQA":48.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3c4d83d3525e54a493ff510443fdcca44bf63b59"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"allenai\/digital-socrates-13b",
        "Average":59.99,
        "ARC":58.36,
        "HellaSwag":80.14,
        "MMLU":57.01,
        "TruthfulQA":44.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"c738ee4bb61e67eebb9d196c440dcb2d99e5f906"
    },
    {
        "T":"?",
        "Model":"pansophic\/new_model_test2",
        "Average":59.99,
        "ARC":62.03,
        "HellaSwag":75.36,
        "MMLU":56.03,
        "TruthfulQA":46.54,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f8b3ddd61dcf89f6ee6c5cac4185ff6c00f767a0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/llama-13b-FINETUNE3",
        "Average":59.98,
        "ARC":59.3,
        "HellaSwag":81.53,
        "MMLU":57.46,
        "TruthfulQA":41.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"bacd035db122dafaf86bf52bb9ca8c613070cc58"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HyperbeeAI\/Tulpar-7b-v1",
        "Average":59.96,
        "ARC":57.0,
        "HellaSwag":79.69,
        "MMLU":51.33,
        "TruthfulQA":51.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"719d8e1eb4a820f01e0a92ef6220d041964bb472"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CalderaAI\/13B-BlueMethod",
        "Average":59.95,
        "ARC":59.64,
        "HellaSwag":82.07,
        "MMLU":50.34,
        "TruthfulQA":47.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"315aa0924dd42840b8cced581c9db1240f9bae1d"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"meta-llama\/Llama-2-13b-chat-hf",
        "Average":59.93,
        "ARC":59.04,
        "HellaSwag":81.94,
        "MMLU":54.64,
        "TruthfulQA":44.12,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":520,
        "Available on the hub":false,
        "Model Sha":"f848cf15ab9a51ae5735ab28120a9a0773eeb541"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wannaphong\/han-llm-7b-v1",
        "Average":59.93,
        "ARC":58.19,
        "HellaSwag":81.58,
        "MMLU":58.99,
        "TruthfulQA":40.97,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.27,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f09bd92495b986d7b79b3e6ac373a7eddcb715d5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NewstaR\/Morningstar-13b-hf",
        "Average":59.93,
        "ARC":59.04,
        "HellaSwag":81.93,
        "MMLU":54.63,
        "TruthfulQA":44.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2605b5b3b0ecba906ac26d39aab40f33c2ec81c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"deepse\/CodeUp-Llama-2-13b-chat-hf",
        "Average":59.93,
        "ARC":59.04,
        "HellaSwag":81.93,
        "MMLU":54.63,
        "TruthfulQA":44.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail++",
        "#Params (B)":12.85,
        "Hub":26,
        "Available on the hub":true,
        "Model Sha":"d4af0b233a5b6a214e96582e103396e99dcf5f95"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xriminact\/TarsDolly",
        "Average":59.93,
        "ARC":59.3,
        "HellaSwag":81.85,
        "MMLU":56.26,
        "TruthfulQA":42.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e8dab99d2af077e63200a07a60bafe3247d8eb09"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"grimpep\/MythoMax-L2-33b",
        "Average":59.92,
        "ARC":57.25,
        "HellaSwag":79.12,
        "MMLU":50.85,
        "TruthfulQA":52.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"db6e214fdb9afc91e9ba234940efffc516d3c3f2"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardMath-13B-V1.0",
        "Average":59.92,
        "ARC":60.24,
        "HellaSwag":81.91,
        "MMLU":54.99,
        "TruthfulQA":42.55,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"209316bea6eab73d8b18fca2a730b1dff3dcf999"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aboros98\/merlin1.3",
        "Average":59.9,
        "ARC":59.98,
        "HellaSwag":75.19,
        "MMLU":57.66,
        "TruthfulQA":46.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4fa78a0c685d8ddeb3977aca081e0da3135a9717"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardMath-13B-V1.0",
        "Average":59.89,
        "ARC":60.07,
        "HellaSwag":82.01,
        "MMLU":54.8,
        "TruthfulQA":42.7,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"209316bea6eab73d8b18fca2a730b1dff3dcf999"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xriminact\/TarsChattyBasev0.1",
        "Average":59.89,
        "ARC":59.98,
        "HellaSwag":82.41,
        "MMLU":55.75,
        "TruthfulQA":41.41,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"472d986efb1336e3290518f9aad54db5faa510d5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HyperbeeAI\/Tulpar-7b-v0",
        "Average":59.89,
        "ARC":56.31,
        "HellaSwag":79.01,
        "MMLU":52.55,
        "TruthfulQA":51.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"d7c2bc52a3ae13571357f51273ae948caf84400e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ab24g21\/LaterLlamaV2",
        "Average":59.89,
        "ARC":59.04,
        "HellaSwag":81.82,
        "MMLU":54.53,
        "TruthfulQA":44.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"66f0995de46d9407f1aa6baa185c6d03e7542ca1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xxyyy123\/test_qkvo_adptor",
        "Average":59.88,
        "ARC":55.38,
        "HellaSwag":78.99,
        "MMLU":51.64,
        "TruthfulQA":53.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"eeb9a04e95c03dd03f0d664e34c56099cabbc402"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"xxyyy123\/mc_data_30k_from_platpus_orca_7b_10k_v1_lora_qkvo_rank14_v2",
        "Average":59.87,
        "ARC":57.17,
        "HellaSwag":79.57,
        "MMLU":50.24,
        "TruthfulQA":52.51,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9c4a7444d6fb12931e50f111053e016531fe60b7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abacusai\/Fewshot-Metamath-Mistral",
        "Average":59.86,
        "ARC":57.76,
        "HellaSwag":80.59,
        "MMLU":58.05,
        "TruthfulQA":43.04,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":80,
        "Available on the hub":false,
        "Model Sha":"5c6d79d66a84efd6b6e879c2fe4f2e4a21df3a1e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pansophic\/m2",
        "Average":59.86,
        "ARC":61.26,
        "HellaSwag":75.28,
        "MMLU":54.73,
        "TruthfulQA":48.17,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":27,
        "Available on the hub":false,
        "Model Sha":"1be3a323f2d735eb6aad1905c5bfb2bec4475d6f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_v2_w",
        "Average":59.86,
        "ARC":57.34,
        "HellaSwag":81.23,
        "MMLU":50.17,
        "TruthfulQA":50.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":29,
        "Available on the hub":true,
        "Model Sha":"0eb53946b8fac30606dc72541f2fc073cb6a0e12"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"frankenmerger\/delta-4B-scientific",
        "Average":59.86,
        "ARC":59.39,
        "HellaSwag":74.1,
        "MMLU":57.56,
        "TruthfulQA":48.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":4.67,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ec54bb8cac88216c172e941c3adeeb8e1992f1f2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/llama2-13b-math1.1",
        "Average":59.86,
        "ARC":56.83,
        "HellaSwag":80.69,
        "MMLU":53.43,
        "TruthfulQA":48.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3c4d83d3525e54a493ff510443fdcca44bf63b59"
    },
    {
        "T":"\u2b55",
        "Model":"pankajmathur\/orca_mini_v3_7b",
        "Average":59.86,
        "ARC":56.91,
        "HellaSwag":79.64,
        "MMLU":52.37,
        "TruthfulQA":50.51,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":27,
        "Available on the hub":true,
        "Model Sha":"f9849ea6bf0f6ebb78dca1cea1c7a3ef8f7d715c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/orca_mini_v3_7b",
        "Average":59.86,
        "ARC":56.91,
        "HellaSwag":79.64,
        "MMLU":52.37,
        "TruthfulQA":50.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":27,
        "Available on the hub":true,
        "Model Sha":"a1583d2f02041fb37df28eeae4da644d8dff33eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ab24g21\/llama-2-new",
        "Average":59.85,
        "ARC":58.7,
        "HellaSwag":81.54,
        "MMLU":54.59,
        "TruthfulQA":44.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"663384b17e7156a858e2f8fdfd7e3ea4bdce105f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-mistral-7b-v13-base",
        "Average":59.85,
        "ARC":52.9,
        "HellaSwag":76.12,
        "MMLU":57.54,
        "TruthfulQA":52.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"8ff18d61b1c8295ecd73153b8e0b63934187a50e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/Metharme-13b-Merged",
        "Average":59.84,
        "ARC":59.9,
        "HellaSwag":81.12,
        "MMLU":47.18,
        "TruthfulQA":51.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"90c02cc338afcdd890a948af06432674743363ad"
    },
    {
        "T":"\u2b55",
        "Model":"jingyeom\/SOLAR_KO_1.3_deup",
        "Average":59.84,
        "ARC":55.97,
        "HellaSwag":79.97,
        "MMLU":55.88,
        "TruthfulQA":47.55,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3218e4304fe55ec950347c96018f14f60baca25d"
    },
    {
        "T":"?",
        "Model":"nisten\/BigCodeLlama-92b",
        "Average":59.84,
        "ARC":54.78,
        "HellaSwag":77.84,
        "MMLU":55.4,
        "TruthfulQA":51.34,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":92.08,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"77640c1d7006d83f26c29a3c2454a4639277a106"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xxyyy123\/10k_v1_lora_qkvo_rank14_v3",
        "Average":59.84,
        "ARC":55.97,
        "HellaSwag":79.22,
        "MMLU":50.71,
        "TruthfulQA":53.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3267b35a0215d937884a6228fdbb91f2fa23d935"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/llama2-13b-orca-v2-8k-3166",
        "Average":59.83,
        "ARC":56.91,
        "HellaSwag":80.2,
        "MMLU":55.5,
        "TruthfulQA":46.7,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":13.0,
        "Hub":112,
        "Available on the hub":false,
        "Model Sha":"386700af58cc125fc843a0fe031ae969b267dbba"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"maywell\/PiVoT-0.1-Evil-a",
        "Average":59.82,
        "ARC":59.64,
        "HellaSwag":81.48,
        "MMLU":58.94,
        "TruthfulQA":39.23,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.24,
        "Hub":38,
        "Available on the hub":false,
        "Model Sha":"b6e20287ba4156f06b4288d4003acc677040527f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KeyonZeng\/philion-2",
        "Average":59.82,
        "ARC":61.6,
        "HellaSwag":75.06,
        "MMLU":58.12,
        "TruthfulQA":44.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"90f0c46c25ed0bc5bf1cbec18405e2793b7a3d58"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeen214\/llama2_7b_merge_orcafamily",
        "Average":59.81,
        "ARC":56.91,
        "HellaSwag":81.17,
        "MMLU":51.49,
        "TruthfulQA":49.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.74,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"fb65f697de632f2f3fef57fc3cd12fb5e4913a89"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Llamix2-Xwin-MoE-4x13B",
        "Average":59.81,
        "ARC":60.41,
        "HellaSwag":82.96,
        "MMLU":56.24,
        "TruthfulQA":39.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.0,
        "Hub":55,
        "Available on the hub":false,
        "Model Sha":"220833f87c233684e8a4b0e03126ffcdffce5229"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Severian\/ANIMA-Phi-Neptune-Mistral-LoRa",
        "Average":59.81,
        "ARC":53.07,
        "HellaSwag":74.66,
        "MMLU":52.13,
        "TruthfulQA":59.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"artistic-2.0",
        "#Params (B)":0.0,
        "Hub":29,
        "Available on the hub":false,
        "Model Sha":"feef1ab8eeb7ba21685b93e074141136d95174bf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/wizard-mega-13b",
        "Average":59.81,
        "ARC":57.34,
        "HellaSwag":81.09,
        "MMLU":50.59,
        "TruthfulQA":50.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":103,
        "Available on the hub":true,
        "Model Sha":"76e90314541be6cfa2b55208831c99f1351c1a33"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-13b-gpt4-1.4",
        "Average":59.81,
        "ARC":59.64,
        "HellaSwag":83.22,
        "MMLU":47.56,
        "TruthfulQA":48.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":18,
        "Available on the hub":true,
        "Model Sha":"d0d2687ed2b4a63a644ed6c5b3f6401844718659"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-13b-gpt4-1.4-fp16",
        "Average":59.81,
        "ARC":59.64,
        "HellaSwag":83.22,
        "MMLU":47.56,
        "TruthfulQA":48.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"037e369be06a8a0eef87f2cddfd3469670483f29"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"adamo1139\/Yi-6B-200K-AEZAKMI-v2-rawrr1-DPO",
        "Average":59.81,
        "ARC":52.47,
        "HellaSwag":77.04,
        "MMLU":62.57,
        "TruthfulQA":47.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.06,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9271df80f5221362cb5ffd71f463f8f8d08c31dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Monero\/Manticore-13b-Chat-Pyg-Guanaco",
        "Average":59.81,
        "ARC":56.83,
        "HellaSwag":82.3,
        "MMLU":47.81,
        "TruthfulQA":52.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"de665d6002f1f2ef460408e8fa5bc1e0b7bb99b1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hamxea\/Llama-2-13b-chat-hf-activity-fine-tuned-v4",
        "Average":59.8,
        "ARC":59.22,
        "HellaSwag":81.67,
        "MMLU":54.51,
        "TruthfulQA":43.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3536f3ba1dd84a732958ea563f2a70ecdbb03bcd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/Guanaco-13B-Uncensored",
        "Average":59.79,
        "ARC":59.56,
        "HellaSwag":82.7,
        "MMLU":53.65,
        "TruthfulQA":43.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"cf315234979f5924ad73399bcdcdf51b05a1fc98"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/samantha-mistral-instruct-7b",
        "Average":59.79,
        "ARC":53.5,
        "HellaSwag":75.14,
        "MMLU":51.72,
        "TruthfulQA":58.81,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"3a33eea0858d411617c472c3c0ae39f17d2b3f5d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Severian\/ANIMA-Phi-Neptune-Mistral-7B-v1",
        "Average":59.78,
        "ARC":52.9,
        "HellaSwag":74.68,
        "MMLU":52.18,
        "TruthfulQA":59.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"35dd5fee8563b61c41743e88be6c557f409c1c10"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"codellama\/CodeLlama-70b-Instruct-hf",
        "Average":59.78,
        "ARC":55.03,
        "HellaSwag":77.24,
        "MMLU":56.4,
        "TruthfulQA":50.44,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":189,
        "Available on the hub":true,
        "Model Sha":"6b762a8d3c16e4397aaa4f4627ebfda5db098831"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-Open_Platypus_and_ccp_2.6w-3_epoch",
        "Average":59.78,
        "ARC":58.62,
        "HellaSwag":82.56,
        "MMLU":55.84,
        "TruthfulQA":42.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"001a5f96daea57b5f256c2df270b35653b439f6f"
    },
    {
        "T":"\u2b55",
        "Model":"qblocks\/mistral_7b_norobots",
        "Average":59.78,
        "ARC":58.96,
        "HellaSwag":80.57,
        "MMLU":57.66,
        "TruthfulQA":41.91,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"36dde2c5b08140d612042d1ae047dd7551b7e15b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"microsoft\/Orca-2-7b",
        "Average":59.78,
        "ARC":54.1,
        "HellaSwag":76.19,
        "MMLU":56.37,
        "TruthfulQA":52.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":200,
        "Available on the hub":true,
        "Model Sha":"60e31e6bdcf582ad103b807cb74b73ee1d2c4b17"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/llama2-13b-math1.2",
        "Average":59.77,
        "ARC":56.91,
        "HellaSwag":80.71,
        "MMLU":53.21,
        "TruthfulQA":48.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b05b4c22893e950e8e33acb67087a9acc8f0ab97"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WebraftAI\/synapsellm-7b-mistral-v0.5-preview",
        "Average":59.77,
        "ARC":52.73,
        "HellaSwag":76.51,
        "MMLU":54.67,
        "TruthfulQA":55.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d1b4d9a4657d145ce7cda431ed46076c1518af55"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vankhoa\/test_phi2",
        "Average":59.77,
        "ARC":61.18,
        "HellaSwag":75.14,
        "MMLU":58.3,
        "TruthfulQA":44.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"be1f3e718cf8386d6ce637b9fb2eb37c2deeea09"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/platypus-2-22b-relora",
        "Average":59.76,
        "ARC":57.68,
        "HellaSwag":82.44,
        "MMLU":55.33,
        "TruthfulQA":43.61,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"15bca3e9b25cc2f280fec21686ef3bc445217503"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/platypus2-22b-relora",
        "Average":59.76,
        "ARC":57.68,
        "HellaSwag":82.44,
        "MMLU":55.33,
        "TruthfulQA":43.61,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"15bca3e9b25cc2f280fec21686ef3bc445217503"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/llama2-13b-math1.2",
        "Average":59.76,
        "ARC":57.08,
        "HellaSwag":80.61,
        "MMLU":53.05,
        "TruthfulQA":48.3,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b05b4c22893e950e8e33acb67087a9acc8f0ab97"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zarafusionex-1.2-l2-7b",
        "Average":59.76,
        "ARC":56.66,
        "HellaSwag":79.16,
        "MMLU":51.94,
        "TruthfulQA":51.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"68ca01427848528ab21263fd06720a081b09d063"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewqr2130\/7B_ppo_phiRM_2GPU_3e-7step_4000",
        "Average":59.76,
        "ARC":57.25,
        "HellaSwag":80.24,
        "MMLU":60.06,
        "TruthfulQA":41.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cebca6863a25b48f3a03f5ea65fdbcefcb934314"
    },
    {
        "T":"?",
        "Model":"robinsmits\/Qwen1.5-7B-Dutch-Chat-Sft-Bf16",
        "Average":59.76,
        "ARC":54.27,
        "HellaSwag":75.53,
        "MMLU":61.98,
        "TruthfulQA":47.26,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.72,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a0ecf9ea6e6a1a2b778ddfb6772c40a79ff84921"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"field2437\/phi-2-platypus-Commercial-lora",
        "Average":59.76,
        "ARC":60.41,
        "HellaSwag":75.12,
        "MMLU":58.03,
        "TruthfulQA":45.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"30ab76e8ec296e5e49a3a4a6933783964e269b40"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"field2437\/phi-2-test",
        "Average":59.76,
        "ARC":60.41,
        "HellaSwag":75.12,
        "MMLU":58.03,
        "TruthfulQA":45.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3e927bee6cff8a275a4a6aefa31e3f29a697ad5e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Gryphe\/MythoBoros-13b",
        "Average":59.75,
        "ARC":58.19,
        "HellaSwag":81.75,
        "MMLU":50.13,
        "TruthfulQA":48.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"67695d15e6610bc8055fbcde82f298e48ad2d374"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JunchengXie\/Mistral-7B-Instruct-v0.1-gpt-4-80k-base_lora",
        "Average":59.74,
        "ARC":53.67,
        "HellaSwag":73.58,
        "MMLU":54.89,
        "TruthfulQA":56.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6b896c4b31159fd551634e5d0234d7109870f33e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CalderaAI\/13B-Ouroboros",
        "Average":59.74,
        "ARC":57.42,
        "HellaSwag":82.11,
        "MMLU":51.43,
        "TruthfulQA":47.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"97981254d4b0ac0d1472376f602c004670070fdd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CallComply\/zephyr-7b-beta-128k",
        "Average":59.74,
        "ARC":58.28,
        "HellaSwag":81.0,
        "MMLU":53.57,
        "TruthfulQA":46.1,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"fc4c02de7b878edf07999d35efa91b62b6bfa35c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/llama2-13b-orca-v2-8k-3166",
        "Average":59.73,
        "ARC":56.48,
        "HellaSwag":80.27,
        "MMLU":55.42,
        "TruthfulQA":46.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":13.0,
        "Hub":112,
        "Available on the hub":false,
        "Model Sha":"386700af58cc125fc843a0fe031ae969b267dbba"
    },
    {
        "T":"?",
        "Model":"amu\/dpo-phi2",
        "Average":59.73,
        "ARC":61.69,
        "HellaSwag":75.13,
        "MMLU":58.1,
        "TruthfulQA":43.99,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"46d19a6f4e37644a426b0a6917959cf4bb388ef1"
    },
    {
        "T":"\u2b55",
        "Model":"heegyu\/LIMA2-13b-hf",
        "Average":59.72,
        "ARC":60.24,
        "HellaSwag":83.69,
        "MMLU":53.17,
        "TruthfulQA":41.81,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ed3535921eb24e0737f9a6cda70b1a3fd71532cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/13B-HyperMantis",
        "Average":59.71,
        "ARC":58.53,
        "HellaSwag":82.2,
        "MMLU":50.61,
        "TruthfulQA":47.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":27,
        "Available on the hub":true,
        "Model Sha":"aa828ef92c363a5577ffd7d29e678277b9d2eb3c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lole25\/phi-2-sft-lora-ultrachat",
        "Average":59.71,
        "ARC":61.26,
        "HellaSwag":74.86,
        "MMLU":57.26,
        "TruthfulQA":45.46,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"09f410606332b5d29075d7031420291e257de570"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Gryphe\/MythoLogic-13b",
        "Average":59.71,
        "ARC":58.45,
        "HellaSwag":81.56,
        "MMLU":49.36,
        "TruthfulQA":49.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"d89d925ad1eeaee465c4de3e5c74240a5a40b585"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NekoPunchBBB\/Llama-2-13b-hf_Open-Platypus",
        "Average":59.71,
        "ARC":58.87,
        "HellaSwag":82.14,
        "MMLU":54.98,
        "TruthfulQA":42.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c318a24121bd69509f395e17a9636093213ece21"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/llama-2-13b-hf-platypus",
        "Average":59.71,
        "ARC":58.87,
        "HellaSwag":82.14,
        "MMLU":54.98,
        "TruthfulQA":42.84,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"39e07f6213a64d79cf31e9c0773dea6224f7f021"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aboros98\/merlin1.5",
        "Average":59.7,
        "ARC":59.56,
        "HellaSwag":74.63,
        "MMLU":56.59,
        "TruthfulQA":48.03,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dca1ff5cadc7aec0caffe9dae6252af2ce9c0716"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zararp-1.1-l2-7b",
        "Average":59.7,
        "ARC":56.48,
        "HellaSwag":78.85,
        "MMLU":51.49,
        "TruthfulQA":51.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"31fa6527a3285d5fd320219d7c2dadde07b83718"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"iGenius-AI-Team\/LLAMA-13B-test-finetuning",
        "Average":59.7,
        "ARC":58.02,
        "HellaSwag":82.36,
        "MMLU":54.27,
        "TruthfulQA":44.14,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"5bd0eb026b12c59fd198f307c0c17188af69744c"
    },
    {
        "T":"?",
        "Model":"huseyinatahaninan\/phi-2-instruction",
        "Average":59.7,
        "ARC":61.35,
        "HellaSwag":74.73,
        "MMLU":57.77,
        "TruthfulQA":44.96,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"120e8a957f9889b744ae4d5fcf871f57f6bb4264"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"microsoft\/phi-2",
        "Average":59.7,
        "ARC":61.09,
        "HellaSwag":75.11,
        "MMLU":58.11,
        "TruthfulQA":44.47,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":3016,
        "Available on the hub":false,
        "Model Sha":"d3186761bf5c4409f7679359284066c25ab668ee"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-mixtral-8x7b-v16.1-32k",
        "Average":59.68,
        "ARC":29.1,
        "HellaSwag":82.27,
        "MMLU":71.37,
        "TruthfulQA":55.97,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.74,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"51086693792910d6bc89398200c5eca8b6930f6f"
    },
    {
        "T":"?",
        "Model":"huseyinatahaninan\/phi-2-instruction",
        "Average":59.67,
        "ARC":61.09,
        "HellaSwag":74.68,
        "MMLU":57.81,
        "TruthfulQA":45.1,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"120e8a957f9889b744ae4d5fcf871f57f6bb4264"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"feeltheAGI\/mistral-maths7B",
        "Average":59.66,
        "ARC":52.05,
        "HellaSwag":74.77,
        "MMLU":54.54,
        "TruthfulQA":57.3,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"3b0be352fd19f65f76221336594902b4b00b642c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NekoPunchBBB\/Llama-2-13b-hf_Open-Platypus-QLoRA-multigpu",
        "Average":59.66,
        "ARC":57.51,
        "HellaSwag":82.49,
        "MMLU":54.83,
        "TruthfulQA":43.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f65029ea8f030731ace568e40bab33a7097a13de"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/webMistral-7B",
        "Average":59.66,
        "ARC":59.04,
        "HellaSwag":80.89,
        "MMLU":59.0,
        "TruthfulQA":39.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0b221c617df3d2f883cfd925f646ebd93de23037"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elyza\/ELYZA-japanese-Llama-2-13b-instruct",
        "Average":59.65,
        "ARC":58.36,
        "HellaSwag":82.2,
        "MMLU":55.65,
        "TruthfulQA":42.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":35,
        "Available on the hub":true,
        "Model Sha":"ed15089024f3ecad9a8c4ce1db302cc01aa9f4ee"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"timpal0l\/Mistral-7B-v0.1-flashback-v2",
        "Average":59.64,
        "ARC":57.17,
        "HellaSwag":80.74,
        "MMLU":59.98,
        "TruthfulQA":40.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"2711647da9d8da18d746406d60ad8d806b7f1fd7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WebraftAI\/synapsellm-7b-mistral-v0.3-preview",
        "Average":59.63,
        "ARC":53.84,
        "HellaSwag":74.86,
        "MMLU":54.81,
        "TruthfulQA":55.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4e509275c5e51bee6e82c2c15082a6cc50d87b5b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Airoboros-L2-13B-2.1-GPTQ",
        "Average":59.63,
        "ARC":58.96,
        "HellaSwag":81.72,
        "MMLU":53.16,
        "TruthfulQA":44.68,
        "Type":"fine-tuned",
        "Precision":"None",
        "Hub License":"llama2",
        "#Params (B)":2.03,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"d90d96e40b9359cb5c35e6b6c8f0eb24896e827b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fionazhang\/mistral-experiment-6",
        "Average":59.63,
        "ARC":55.8,
        "HellaSwag":81.45,
        "MMLU":55.57,
        "TruthfulQA":45.69,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"df18562607b2ba0fc296da17c398b9d3451c6a89"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o_gate_up_down",
        "Average":59.62,
        "ARC":59.22,
        "HellaSwag":81.52,
        "MMLU":54.94,
        "TruthfulQA":42.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a759c4fae8dc5fcd264bf58b89b9fd13d06784ae"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"l3utterfly\/mistral-7b-v0.1-layla-v2",
        "Average":59.62,
        "ARC":56.31,
        "HellaSwag":79.76,
        "MMLU":50.81,
        "TruthfulQA":51.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1e0dc1ba4a198773c2d47d0c8142aef1649f8c33"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/platypus2-22b-relora",
        "Average":59.61,
        "ARC":57.51,
        "HellaSwag":82.36,
        "MMLU":54.94,
        "TruthfulQA":43.62,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"15bca3e9b25cc2f280fec21686ef3bc445217503"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_v2",
        "Average":59.61,
        "ARC":57.17,
        "HellaSwag":81.14,
        "MMLU":50.58,
        "TruthfulQA":49.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"bd2a0968964c0f2dfae8f5a8950b43e35142f830"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2-13B-IA3",
        "Average":59.6,
        "ARC":61.09,
        "HellaSwag":82.65,
        "MMLU":56.32,
        "TruthfulQA":38.35,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b738c64d536df02f5c137a94bc7a32a4c486012b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"circulus\/Llama-2-7b-orca-v1",
        "Average":59.59,
        "ARC":56.31,
        "HellaSwag":79.14,
        "MMLU":52.71,
        "TruthfulQA":50.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"e501f231277671710384ba0397da2c4486865958"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"stabilityai\/StableBeluga-7B",
        "Average":59.59,
        "ARC":56.31,
        "HellaSwag":79.14,
        "MMLU":52.71,
        "TruthfulQA":50.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":99,
        "Available on the hub":true,
        "Model Sha":"329adcfc39f48dce183eb0b155b732dbe03c6304"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"budecosystem\/genz-13b-v2",
        "Average":59.59,
        "ARC":55.97,
        "HellaSwag":79.98,
        "MMLU":54.3,
        "TruthfulQA":48.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"98e0e2086df11b9f80e1571110540a657e52c2e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/gpt4-x-vicuna-13B-HF",
        "Average":59.58,
        "ARC":53.41,
        "HellaSwag":80.12,
        "MMLU":51.22,
        "TruthfulQA":53.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":13.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"a247577c882940e0c6b040fe8239d760c0d10d40"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airoboros-c34b-2.2.1",
        "Average":59.58,
        "ARC":54.69,
        "HellaSwag":76.84,
        "MMLU":55.43,
        "TruthfulQA":51.36,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"79d9761af231fecbfaf6066d6d405a0f8c04f4ba"
    },
    {
        "T":"\u2b55",
        "Model":"NobodyExistsOnTheInternet\/GiftedConvo13bLoraNoEcons",
        "Average":59.57,
        "ARC":59.39,
        "HellaSwag":83.19,
        "MMLU":55.15,
        "TruthfulQA":40.56,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9d7031e7d956dd2d25c61d85f594d115ce65b172"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/phi-2-logical-sft",
        "Average":59.57,
        "ARC":61.35,
        "HellaSwag":75.14,
        "MMLU":57.4,
        "TruthfulQA":44.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"6efbdcdfc50d1b9387de01e58c3746f8a1677a61"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"marcel\/phi-2-openhermes-30k",
        "Average":59.57,
        "ARC":61.01,
        "HellaSwag":74.72,
        "MMLU":57.17,
        "TruthfulQA":45.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e09a1fa39a807edf8b3f644d81cd2c91984dfd10"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zarafusionex-1.1-l2-7b",
        "Average":59.56,
        "ARC":56.14,
        "HellaSwag":79.34,
        "MMLU":52.1,
        "TruthfulQA":50.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"3268ff5291934a14f3f5e7013bbb408f33adb542"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lajonbot\/tableBeluga-7B-instruct-pl-lora_unload",
        "Average":59.56,
        "ARC":56.23,
        "HellaSwag":79.12,
        "MMLU":52.7,
        "TruthfulQA":50.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"eeb22ca9481a5ed7e131a329324494f234300a45"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-13b-gpt4",
        "Average":59.55,
        "ARC":59.39,
        "HellaSwag":83.29,
        "MMLU":47.89,
        "TruthfulQA":47.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"c0eef6e6f63d4b11953539308717cea0079b44f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/LongQLoRA-Vicuna-13b-8k",
        "Average":59.55,
        "ARC":56.4,
        "HellaSwag":81.05,
        "MMLU":53.68,
        "TruthfulQA":47.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"470c90e30f9e49e948e066373c3ea6878ee5f171"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"01-ai\/Yi-6B",
        "Average":59.55,
        "ARC":55.55,
        "HellaSwag":76.57,
        "MMLU":64.11,
        "TruthfulQA":41.96,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.06,
        "Hub":355,
        "Available on the hub":true,
        "Model Sha":"e00f7cbde45745a22625ac85c6ad5d5b9f27098d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Changgil\/K2S3-Mistral-7bx2-48layers_v1.2",
        "Average":59.54,
        "ARC":56.31,
        "HellaSwag":77.83,
        "MMLU":57.91,
        "TruthfulQA":46.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.86,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ff242b7f1bcebcc1e0f913b934536e66045d8b4b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gaodrew\/gaodrew-gorgonzola-13b",
        "Average":59.54,
        "ARC":50.94,
        "HellaSwag":77.65,
        "MMLU":68.93,
        "TruthfulQA":40.63,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a53fbe358d4cb546916847d861ccfaf7c724a103"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-13b-gpt4-1.1",
        "Average":59.53,
        "ARC":59.04,
        "HellaSwag":83.05,
        "MMLU":49.41,
        "TruthfulQA":46.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"19c7060adcb34d42e742fe51dd36b8657ac069b7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zararp-l2-7b",
        "Average":59.53,
        "ARC":56.31,
        "HellaSwag":79.19,
        "MMLU":51.36,
        "TruthfulQA":51.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6032c5106970f98d59925959fbd330ae4b1d1a7e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/gpt4-alpaca-lora-13B-HF",
        "Average":59.52,
        "ARC":59.56,
        "HellaSwag":82.09,
        "MMLU":47.48,
        "TruthfulQA":48.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"49678a2dd15fb4e1f1b99616ccc1ffd269912833"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Python-Code-13B",
        "Average":59.51,
        "ARC":58.79,
        "HellaSwag":81.66,
        "MMLU":54.78,
        "TruthfulQA":42.83,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"981454b6a2275f787592589609df7f2bf558706d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pansophic\/m16",
        "Average":59.51,
        "ARC":59.81,
        "HellaSwag":74.82,
        "MMLU":56.31,
        "TruthfulQA":47.11,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":27,
        "Available on the hub":false,
        "Model Sha":"61b76c29f02a6b27f17b3e73ce50c218dfc6b7ff"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zarablendex-vq-l2-7b",
        "Average":59.51,
        "ARC":56.06,
        "HellaSwag":79.37,
        "MMLU":51.45,
        "TruthfulQA":51.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"0c47d39ac609c39b521b8ca3612f88b391ecd34e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/manticore-13b-chat-pyg",
        "Average":59.5,
        "ARC":58.53,
        "HellaSwag":81.96,
        "MMLU":48.76,
        "TruthfulQA":48.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":23,
        "Available on the hub":true,
        "Model Sha":"f9ef65a3cf50e3c09ccb443f99225148e08517aa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NovoCode\/Phi-2-DPO",
        "Average":59.5,
        "ARC":60.75,
        "HellaSwag":75.03,
        "MMLU":57.75,
        "TruthfulQA":44.46,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"f6df7b39876d53893e4f8dcdf50939225b38d08c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lajonbot\/Llama-2-13b-hf-instruct-pl-lora_unload",
        "Average":59.48,
        "ARC":59.47,
        "HellaSwag":82.16,
        "MMLU":54.83,
        "TruthfulQA":41.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"4ef2c736641c2983996c4662bf481782a9de5055"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NobodyExistsOnTheInternet\/PuffedLIMA13bQLORA",
        "Average":59.47,
        "ARC":59.9,
        "HellaSwag":84.39,
        "MMLU":53.68,
        "TruthfulQA":39.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7da6d235d625e16c850ccd0b947dee40071b1f89"
    },
    {
        "T":"?",
        "Model":"TheBloke\/WizardLM-13B-V1-1-SuperHOT-8K-GPTQ",
        "Average":59.46,
        "ARC":57.0,
        "HellaSwag":80.32,
        "MMLU":47.08,
        "TruthfulQA":53.46,
        "Type":"Unknown",
        "Precision":"None",
        "Hub License":"other",
        "#Params (B)":2.03,
        "Hub":44,
        "Available on the hub":true,
        "Model Sha":"085eb5cd394f30d72bf5efcf83a580e87264b3e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hamxea\/StableBeluga-7B-activity-fine-tuned-v2",
        "Average":59.46,
        "ARC":56.23,
        "HellaSwag":79.06,
        "MMLU":52.54,
        "TruthfulQA":50.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"97b647167ef3e6a043ff2c7a87ff1da117f32027"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xxyyy123\/10k_v1_lora_qkvo_rank28_v2",
        "Average":59.46,
        "ARC":55.38,
        "HellaSwag":79.21,
        "MMLU":50.5,
        "TruthfulQA":52.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"70e38a7424544193f0ad6a93ae26a5bfd15e4e90"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-13b-gpt4-1.4.1",
        "Average":59.45,
        "ARC":59.13,
        "HellaSwag":82.78,
        "MMLU":55.62,
        "TruthfulQA":40.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"35ff51ebe5668269dfd33a9ed94412d88f1f4b65"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16",
        "Average":59.43,
        "ARC":59.98,
        "HellaSwag":82.43,
        "MMLU":55.41,
        "TruthfulQA":39.9,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6a0a2b6672c7b36c714a66c4a836e0b50c6cb5e6"
    },
    {
        "T":"?",
        "Model":"l3utterfly\/phi-2-layla-v1",
        "Average":59.42,
        "ARC":60.84,
        "HellaSwag":75.0,
        "MMLU":57.85,
        "TruthfulQA":44.01,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"ee7cc7a033d7ed83df82037a4dca85c19976d8bd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NobodyExistsOnTheInternet\/PuffedConvo13bLoraE4",
        "Average":59.42,
        "ARC":59.81,
        "HellaSwag":84.39,
        "MMLU":53.62,
        "TruthfulQA":39.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"40e4fce0c25bd23f6011b424748ee2b5374b98d5"
    },
    {
        "T":"\u2b55",
        "Model":"yihan6324\/llama2-7b-instructmining-40k-sharegpt",
        "Average":59.42,
        "ARC":55.12,
        "HellaSwag":78.96,
        "MMLU":50.43,
        "TruthfulQA":53.18,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"82c010369dfe714dec04110904d99eca9955db37"
    },
    {
        "T":"?",
        "Model":"robinsmits\/Qwen1.5-7B-Dutch-Chat",
        "Average":59.42,
        "ARC":53.92,
        "HellaSwag":76.03,
        "MMLU":62.38,
        "TruthfulQA":45.34,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.72,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"e8981c1824d88d909cc56edc83fe70d79cb5c3b9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"01-ai\/Yi-6B",
        "Average":59.42,
        "ARC":55.55,
        "HellaSwag":76.42,
        "MMLU":63.85,
        "TruthfulQA":41.86,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":6.06,
        "Hub":355,
        "Available on the hub":false,
        "Model Sha":"d8029c814d8faa68e1aef2e488f668a3af5d1a8a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elyza\/ELYZA-japanese-Llama-2-13b-fast-instruct",
        "Average":59.42,
        "ARC":57.51,
        "HellaSwag":81.82,
        "MMLU":54.52,
        "TruthfulQA":43.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.92,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"2a0b52cd72a30d26ef0391c171b64900106a90a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-Open_Platypus_and_ccp_2.6w",
        "Average":59.41,
        "ARC":58.96,
        "HellaSwag":82.51,
        "MMLU":56.12,
        "TruthfulQA":40.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2929bfa1049db46df94f5710755178d18a981665"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aeala\/GPT4-x-Alpasta-13b",
        "Average":59.39,
        "ARC":58.53,
        "HellaSwag":79.92,
        "MMLU":46.03,
        "TruthfulQA":53.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"50af05b015446110a2dc52a1b4b341142c98e62b"
    },
    {
        "T":"\u2b55",
        "Model":"NobodyExistsOnTheInternet\/PuffedConvo13bLoraE4",
        "Average":59.39,
        "ARC":59.64,
        "HellaSwag":84.37,
        "MMLU":53.72,
        "TruthfulQA":39.82,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"40e4fce0c25bd23f6011b424748ee2b5374b98d5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"codemateai\/CodeMate-v0.1",
        "Average":59.38,
        "ARC":55.55,
        "HellaSwag":78.03,
        "MMLU":55.31,
        "TruthfulQA":48.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"01015a269bdf5283e6749e9a4f5ff7ccfb216e57"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"amu\/orpo-lora-phi2",
        "Average":59.38,
        "ARC":60.32,
        "HellaSwag":74.58,
        "MMLU":58.12,
        "TruthfulQA":44.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"646be9d724c5c041121426babe71c02b12d8ba31"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Chat-Stheno-L2-13B",
        "Average":59.38,
        "ARC":58.45,
        "HellaSwag":80.96,
        "MMLU":54.8,
        "TruthfulQA":43.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"20419fdd5b4bdcbbf075223c33b396958c48a6cf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aboros98\/merlin1.4",
        "Average":59.37,
        "ARC":59.3,
        "HellaSwag":74.5,
        "MMLU":56.34,
        "TruthfulQA":47.36,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"59f8d7e6aefd0305e7f54a9a405e0ff5f7c6bb0e"
    },
    {
        "T":"\u2b55",
        "Model":"StudentLLM\/Alpagasus-2-13b-QLoRA-merged",
        "Average":59.37,
        "ARC":60.84,
        "HellaSwag":82.43,
        "MMLU":55.55,
        "TruthfulQA":38.65,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"e324e828c8d68aa8510f50dfab133388a44fd821"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/cinematika-7b-v0.1",
        "Average":59.37,
        "ARC":59.98,
        "HellaSwag":81.14,
        "MMLU":51.87,
        "TruthfulQA":44.47,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"6df1846af7de7ab8e2201ad87071ed661e3b0de2"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Qwen\/Qwen-7B",
        "Average":59.37,
        "ARC":51.37,
        "HellaSwag":78.47,
        "MMLU":59.84,
        "TruthfulQA":47.79,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.72,
        "Hub":337,
        "Available on the hub":false,
        "Model Sha":"c9bdb955021a80ae26fa6978891996dbe4951d8d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/orca_mini_v2_13b",
        "Average":59.36,
        "ARC":55.12,
        "HellaSwag":79.69,
        "MMLU":50.07,
        "TruthfulQA":52.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub":31,
        "Available on the hub":true,
        "Model Sha":"1058709314f7ca090937d0a2b7b37b0b3a8f12a3"
    },
    {
        "T":"\u2b55",
        "Model":"Envoid\/Yousei-22B",
        "Average":59.36,
        "ARC":55.89,
        "HellaSwag":78.55,
        "MMLU":52.31,
        "TruthfulQA":50.68,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"ae8f93963266d31000433f1a52d43435e1473e2b"
    },
    {
        "T":"?",
        "Model":"prince-canuma\/Damysus-2.7B-Chat",
        "Average":59.35,
        "ARC":59.81,
        "HellaSwag":74.52,
        "MMLU":56.33,
        "TruthfulQA":46.74,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"d805640fae5928607626d5c89b66a9aaf98da752"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"StudentLLM\/Alpagasus-2-13b-QLoRA-merged",
        "Average":59.34,
        "ARC":61.09,
        "HellaSwag":82.46,
        "MMLU":55.27,
        "TruthfulQA":38.53,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"dacbafa40716a2d87e593240cc5c1dc883b5066a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama-13b",
        "Average":59.34,
        "ARC":58.96,
        "HellaSwag":79.71,
        "MMLU":49.1,
        "TruthfulQA":49.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"dd326f89ce885844d714d9ab33603e0d17f56cc5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/orca_mini_v2_13b",
        "Average":59.33,
        "ARC":55.29,
        "HellaSwag":79.62,
        "MMLU":49.83,
        "TruthfulQA":52.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub":31,
        "Available on the hub":true,
        "Model Sha":"1058709314f7ca090937d0a2b7b37b0b3a8f12a3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TIGER-Lab\/TIGERScore-13B",
        "Average":59.32,
        "ARC":59.04,
        "HellaSwag":82.79,
        "MMLU":55.07,
        "TruthfulQA":40.38,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":13.02,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"4a71ce15f9af6fd25b0cde1612e56a7ee589c3e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r16-q_k_v_o",
        "Average":59.31,
        "ARC":58.7,
        "HellaSwag":81.66,
        "MMLU":53.87,
        "TruthfulQA":43.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"33fd8a46a711ab8c45698dae9601678dfd7b3d33"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/mistral-7B-alpaca-case-1-2",
        "Average":59.31,
        "ARC":57.34,
        "HellaSwag":79.31,
        "MMLU":56.02,
        "TruthfulQA":44.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8df8e9f61a0d5ca5a5d5e7418dc2f9eb348f916b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zarafusionix-l2-7b",
        "Average":59.3,
        "ARC":55.55,
        "HellaSwag":79.4,
        "MMLU":51.21,
        "TruthfulQA":51.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"13d0e2498a4b5f53f6dc2464f20e093b07a4bd4b"
    },
    {
        "T":"\u2b55",
        "Model":"yihan6324\/llama2-7b-instructmining-60k-sharegpt",
        "Average":59.3,
        "ARC":54.44,
        "HellaSwag":78.59,
        "MMLU":51.26,
        "TruthfulQA":52.9,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"eadea344993864d17a92bfed14876002278f0e6e"
    },
    {
        "T":"\u2b55",
        "Model":"yihan6324\/llama-2-7b-instructmining-60k-sharegpt",
        "Average":59.3,
        "ARC":54.44,
        "HellaSwag":78.59,
        "MMLU":51.26,
        "TruthfulQA":52.9,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"eadea344993864d17a92bfed14876002278f0e6e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"layoric\/llama-2-13b-code-alpaca",
        "Average":59.3,
        "ARC":60.84,
        "HellaSwag":82.14,
        "MMLU":55.93,
        "TruthfulQA":38.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"aa1d543fe3391fe9f0e6143ef785fffe9c871225"
    },
    {
        "T":"\u2b55",
        "Model":"The-Face-Of-Goonery\/Huginn-19b-prototype",
        "Average":59.28,
        "ARC":59.22,
        "HellaSwag":81.03,
        "MMLU":55.73,
        "TruthfulQA":41.15,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":19.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"d2c8cc15c57da217ff29ebaaae4bc4f57d6b21b0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-13b-instruct",
        "Average":59.28,
        "ARC":57.94,
        "HellaSwag":81.32,
        "MMLU":47.62,
        "TruthfulQA":50.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":12.85,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"a13e08a36c355d64fae59f28162e5fa542a8d235"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lajonbot\/vicuna-13b-v1.3-PL-lora_unload",
        "Average":59.27,
        "ARC":54.86,
        "HellaSwag":80.41,
        "MMLU":52.2,
        "TruthfulQA":49.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5582369752583b02df3cba4bd2a733d12265cddb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lvkaokao\/llama2-7b-hf-chat-lora-v3",
        "Average":59.27,
        "ARC":57.25,
        "HellaSwag":78.62,
        "MMLU":50.57,
        "TruthfulQA":50.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"79047f667253c878ad3143b016e3dcb3df707572"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/Llama-2-13b-FINETUNE4_TEST3",
        "Average":59.26,
        "ARC":59.04,
        "HellaSwag":81.65,
        "MMLU":56.37,
        "TruthfulQA":39.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e81b5d4550224711929fdea4effdd990cc0c7404"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/dolphin-llama-13b",
        "Average":59.26,
        "ARC":55.55,
        "HellaSwag":77.11,
        "MMLU":52.16,
        "TruthfulQA":52.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":55,
        "Available on the hub":true,
        "Model Sha":"b6d16c3e1cffef5e914863f41fd96152dafddd6f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Locutusque\/UltraQwen-7B",
        "Average":59.25,
        "ARC":51.71,
        "HellaSwag":77.93,
        "MMLU":59.16,
        "TruthfulQA":48.2,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.72,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"dc90ee7e2014b4fc862fa84868373982bb106fbd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama-13b-v1.2",
        "Average":59.25,
        "ARC":56.74,
        "HellaSwag":80.34,
        "MMLU":48.9,
        "TruthfulQA":51.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"c0a56d9f5a15bea07493191b5a6295f6797a9b2c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Kimiko-13B-fp16",
        "Average":59.24,
        "ARC":59.22,
        "HellaSwag":82.35,
        "MMLU":55.85,
        "TruthfulQA":39.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"27868769e2d6b1af46337f0997c71b0577952a3d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"IDEA-CCNL\/Ziya2-13B-Base",
        "Average":59.24,
        "ARC":54.01,
        "HellaSwag":78.9,
        "MMLU":61.32,
        "TruthfulQA":42.74,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.89,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"91c5a0b534aaae12e59a092459e52814fb42bd88"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_addto15k_4.5w-r16-gate_up_down",
        "Average":59.24,
        "ARC":58.53,
        "HellaSwag":82.27,
        "MMLU":55.9,
        "TruthfulQA":40.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"fdc145fe1b47cdda483535c018e35a5ab249a552"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HenryJJ\/Instruct_Yi-6B_Dolly15K",
        "Average":59.24,
        "ARC":54.86,
        "HellaSwag":75.87,
        "MMLU":63.37,
        "TruthfulQA":42.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.06,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2c0644cf206bdc94f5e6db2aca63129af0fa4a45"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/Llama-2-13b-FINETUNE4_TEST2",
        "Average":59.24,
        "ARC":58.45,
        "HellaSwag":81.7,
        "MMLU":56.61,
        "TruthfulQA":40.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e312c4c59cab9d130c33288c92aad7c0cb5331d5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pillowtalks-ai\/delta13b",
        "Average":59.22,
        "ARC":52.73,
        "HellaSwag":80.13,
        "MMLU":51.94,
        "TruthfulQA":52.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"83fa0860990df1db35550f973ba4306449e35412"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kevinpro\/Vicuna-13B-CoT",
        "Average":59.22,
        "ARC":52.73,
        "HellaSwag":80.13,
        "MMLU":51.94,
        "TruthfulQA":52.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"346e3c46959cf9f1e03feffa761afe020c0fb6a8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/vicuna-13B-1.1-HF",
        "Average":59.22,
        "ARC":52.73,
        "HellaSwag":80.13,
        "MMLU":51.94,
        "TruthfulQA":52.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":95,
        "Available on the hub":true,
        "Model Sha":"8c71dbe9221e83d2ec72e4dc08beccfc78b563c0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eachadea\/vicuna-13b-1.1",
        "Average":59.22,
        "ARC":52.73,
        "HellaSwag":80.13,
        "MMLU":51.94,
        "TruthfulQA":52.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":134,
        "Available on the hub":true,
        "Model Sha":"bfcc6ca66694310be6c85ba0638597f4256c4143"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-13b-delta-v1.1",
        "Average":59.21,
        "ARC":52.73,
        "HellaSwag":80.14,
        "MMLU":51.9,
        "TruthfulQA":52.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":401,
        "Available on the hub":true,
        "Model Sha":"ffed4c7cf1b9814812078efbe29ec3f610ea39e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Vicuna-13B-CoT-fp16",
        "Average":59.21,
        "ARC":52.73,
        "HellaSwag":80.14,
        "MMLU":51.9,
        "TruthfulQA":52.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"fe74a0ece9089828b301bd0f067ae5f257516179"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-13b-v1.1",
        "Average":59.21,
        "ARC":52.73,
        "HellaSwag":80.14,
        "MMLU":51.9,
        "TruthfulQA":52.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":95,
        "Available on the hub":true,
        "Model Sha":"8c71dbe9221e83d2ec72e4dc08beccfc78b563c0"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"gordicaleksa\/YugoGPT",
        "Average":59.21,
        "ARC":58.11,
        "HellaSwag":81.45,
        "MMLU":60.68,
        "TruthfulQA":36.6,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":23,
        "Available on the hub":false,
        "Model Sha":"254ee66aebc46b483b1a3b4c2bfafb1d523dc18e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pansophic\/m17",
        "Average":59.2,
        "ARC":59.64,
        "HellaSwag":74.41,
        "MMLU":56.12,
        "TruthfulQA":46.62,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":27,
        "Available on the hub":false,
        "Model Sha":"af805fe99130a741b4d688f9e048b6f69362522f"
    },
    {
        "T":"\u2b55",
        "Model":"FPHam\/Sydney_Overthinker_13b_HF",
        "Average":59.2,
        "ARC":58.96,
        "HellaSwag":80.85,
        "MMLU":51.28,
        "TruthfulQA":45.7,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"c4d2617fb452a55ac3a39c64128a98874595adb1"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"hon9kon9ize\/CantoneseLLM-6B-preview202402",
        "Average":59.19,
        "ARC":55.63,
        "HellaSwag":75.8,
        "MMLU":63.07,
        "TruthfulQA":42.26,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":6.06,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"71474831ebfa33d02692e22f2ed7267d534f9e06"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Community-LM\/llava-v1.5-13b-hf",
        "Average":59.18,
        "ARC":56.14,
        "HellaSwag":80.36,
        "MMLU":56.89,
        "TruthfulQA":43.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.0,
        "Hub":781,
        "Available on the hub":false,
        "Model Sha":"abeec47322b80d4ca95cc59b1c9e7694840cf83b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aboros98\/form1",
        "Average":59.18,
        "ARC":58.79,
        "HellaSwag":75.25,
        "MMLU":56.83,
        "TruthfulQA":45.85,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"7dceb14eec7636fd7da57fad984333f4e7c07c60"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aboros98\/groot2",
        "Average":59.18,
        "ARC":59.04,
        "HellaSwag":73.88,
        "MMLU":56.38,
        "TruthfulQA":47.41,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"b357f5929c8d1919525ed021de639f3059b14d93"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xxyyy123\/20k_v1_lora_qkvo_rank14_v2",
        "Average":59.18,
        "ARC":55.38,
        "HellaSwag":79.1,
        "MMLU":50.65,
        "TruthfulQA":51.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fb849cfd4fd7c856e032d0576e3685ee54e68200"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/guanaco-13B-HF",
        "Average":59.18,
        "ARC":57.85,
        "HellaSwag":83.84,
        "MMLU":48.28,
        "TruthfulQA":46.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"bd59c700815124df616a17f5b49a0bc51590b231"
    },
    {
        "T":"?",
        "Model":"TheBloke\/vicuna-13b-v1.3.0-GPTQ",
        "Average":59.17,
        "ARC":54.35,
        "HellaSwag":79.47,
        "MMLU":51.97,
        "TruthfulQA":50.88,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.03,
        "Hub":19,
        "Available on the hub":true,
        "Model Sha":"6ef1f8d8638ea2d6681a8e3da73be57c501d847b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ericpolewski\/Palworld-SME-13b",
        "Average":59.17,
        "ARC":55.55,
        "HellaSwag":80.81,
        "MMLU":53.64,
        "TruthfulQA":46.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4b3b88c21071c77cffb23ddb5508c86f735fe229"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"edor\/Stable-Platypus2-mini-7B",
        "Average":59.16,
        "ARC":54.86,
        "HellaSwag":78.95,
        "MMLU":51.78,
        "TruthfulQA":51.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a595cdcbee7562e5ff13ff720245a8c5cf26ffdf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"totally-not-an-llm\/EverythingLM-13b-V2-16k",
        "Average":59.16,
        "ARC":58.7,
        "HellaSwag":80.88,
        "MMLU":49.69,
        "TruthfulQA":47.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":30,
        "Available on the hub":true,
        "Model Sha":"943f932ae1ae462389e6d2db5273158530749fff"
    },
    {
        "T":"?",
        "Model":"FelixChao\/Gemma-10.2B",
        "Average":59.15,
        "ARC":58.36,
        "HellaSwag":80.35,
        "MMLU":58.44,
        "TruthfulQA":39.46,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.2,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"28236d28ce6b57ec4217c561ab5b196ccdac7aeb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zaraxe-l2-7b",
        "Average":59.15,
        "ARC":57.17,
        "HellaSwag":79.34,
        "MMLU":51.0,
        "TruthfulQA":49.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0875bf202aedeef7a58d7382fd6f55f5bca12968"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beaugogh\/Llama2-7b-openorca-mc-v2",
        "Average":59.15,
        "ARC":55.55,
        "HellaSwag":81.26,
        "MMLU":48.3,
        "TruthfulQA":51.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1e74a9cca843cdeb8591d4e4f4320dc1870adf1b"
    },
    {
        "T":"\u2b55",
        "Model":"beaugogh\/Llama2-7b-openorca-mc-v2-dpo",
        "Average":59.15,
        "ARC":54.78,
        "HellaSwag":81.48,
        "MMLU":47.2,
        "TruthfulQA":53.13,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"734a6f0c69e1e53b988c107926bc17cb0536f851"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Nous-Hermes-13B-SuperHOT-8K-fp16",
        "Average":59.15,
        "ARC":55.29,
        "HellaSwag":81.87,
        "MMLU":48.23,
        "TruthfulQA":51.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"b407c1ece029ad5693d38e6e0931e9482962ed15"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_attn_only",
        "Average":59.13,
        "ARC":60.75,
        "HellaSwag":82.09,
        "MMLU":55.52,
        "TruthfulQA":38.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"0ecc726751e2e07255ac4cab41040bbf24321042"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CausalLM\/7B",
        "Average":59.12,
        "ARC":50.0,
        "HellaSwag":74.58,
        "MMLU":61.79,
        "TruthfulQA":50.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"gpl-3.0",
        "#Params (B)":7.0,
        "Hub":73,
        "Available on the hub":false,
        "Model Sha":"3f4f76e2d94308ea6b0edc3de83f18c213a8fde5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e4",
        "Average":59.11,
        "ARC":60.07,
        "HellaSwag":82.45,
        "MMLU":55.37,
        "TruthfulQA":38.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"dbe93078c2e3b8744ca4fc6fbba9fa1f43dd6dcd"
    },
    {
        "T":"?",
        "Model":"quantumaikr\/QuantumLM",
        "Average":59.1,
        "ARC":55.8,
        "HellaSwag":79.74,
        "MMLU":54.17,
        "TruthfulQA":46.71,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"9058130b416355b37f5f78777748aa56d98a4da0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NekoPunchBBB\/Llama-2-13b-hf_Open-Platypus-8bit-att",
        "Average":59.1,
        "ARC":57.51,
        "HellaSwag":82.14,
        "MMLU":54.56,
        "TruthfulQA":42.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"83a8e51d0a72dcfbe5de13dc7ee10dc20e91602e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"augmxnt\/shisa-gamma-7b-v1",
        "Average":59.1,
        "ARC":53.16,
        "HellaSwag":77.3,
        "MMLU":55.23,
        "TruthfulQA":50.73,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"49bf4a58453d191845668b8ff17e4b8f0e9ccae6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pansophic\/m3",
        "Average":59.1,
        "ARC":60.41,
        "HellaSwag":74.49,
        "MMLU":56.51,
        "TruthfulQA":44.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":27,
        "Available on the hub":false,
        "Model Sha":"c55ddfa2a2e72141f5cf6ddefb5596d79efcfd72"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/Llama-2-13b-FINETUNE4_compare8k2",
        "Average":59.1,
        "ARC":58.28,
        "HellaSwag":81.39,
        "MMLU":56.87,
        "TruthfulQA":39.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"fe1b604097aad9408ce63fa7ffc9c320cdd06e4f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/test-help-steer-filtered-orig",
        "Average":59.09,
        "ARC":57.59,
        "HellaSwag":80.42,
        "MMLU":57.24,
        "TruthfulQA":41.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bda6d45ddb3ef73df4d198d95416c66872429927"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-13b-gpt4-1.2",
        "Average":59.09,
        "ARC":58.36,
        "HellaSwag":81.61,
        "MMLU":48.84,
        "TruthfulQA":47.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"482bd38b65e73fde13f5d03fed2bee7acda8fadd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_compare15k_4.5w-r16-gate_up_down",
        "Average":59.09,
        "ARC":58.36,
        "HellaSwag":82.33,
        "MMLU":56.14,
        "TruthfulQA":39.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d824054153586d58139b7c3527ba211f33a81382"
    },
    {
        "T":"?",
        "Model":"prince-canuma\/Damysus-2.7B-Chat",
        "Average":59.07,
        "ARC":59.13,
        "HellaSwag":74.36,
        "MMLU":56.34,
        "TruthfulQA":46.45,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"d805640fae5928607626d5c89b66a9aaf98da752"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/robin-13B-v2-fp16",
        "Average":59.07,
        "ARC":56.48,
        "HellaSwag":80.37,
        "MMLU":48.79,
        "TruthfulQA":50.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"f4dd8fc4440ed84fcf3ff1122f2b7f6024cca29d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"itsliupeng\/llama2_7b_mmlu",
        "Average":59.07,
        "ARC":56.14,
        "HellaSwag":79.13,
        "MMLU":60.04,
        "TruthfulQA":40.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"553178f8d5d69eb1dfa5b9503d2ce0c1e481e5b1"
    },
    {
        "T":"\u2b55",
        "Model":"llm-agents\/tora-13b-v1.0",
        "Average":59.06,
        "ARC":58.96,
        "HellaSwag":82.31,
        "MMLU":54.73,
        "TruthfulQA":40.25,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"0636c1f582c979a5a292cc5f3dc293800b1494e2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Code-13B",
        "Average":59.06,
        "ARC":57.34,
        "HellaSwag":83.28,
        "MMLU":53.17,
        "TruthfulQA":42.46,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":12.85,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"91f5a6d5cdf93aeb86dd8965e195d51522957fc6"
    },
    {
        "T":"\u2b55",
        "Model":"qblocks\/zephyr_7b_norobots",
        "Average":59.06,
        "ARC":56.48,
        "HellaSwag":79.64,
        "MMLU":55.52,
        "TruthfulQA":44.6,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"312485e3c11a5cace45ad04dcf87a89df6e69571"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r8-q_k_v_o",
        "Average":59.06,
        "ARC":57.25,
        "HellaSwag":81.73,
        "MMLU":55.72,
        "TruthfulQA":41.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"209da26cff560ab34064f277190ab63f8c970b93"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TFLai\/llama-2-13b-4bit-alpaca-gpt4",
        "Average":59.06,
        "ARC":57.68,
        "HellaSwag":81.05,
        "MMLU":51.82,
        "TruthfulQA":45.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"ccf1ad19b07196fa3fab67261b7a0f9bcf28638f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r32_a16",
        "Average":59.05,
        "ARC":59.9,
        "HellaSwag":82.33,
        "MMLU":55.67,
        "TruthfulQA":38.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"5cae6af3eb89c28c8cd90322685dd4d0235d9946"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama2-13b-chat",
        "Average":59.05,
        "ARC":57.51,
        "HellaSwag":77.94,
        "MMLU":52.56,
        "TruthfulQA":48.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.97,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"9497e3bd12e19e1300bc7b1980fbe232420134b9"
    },
    {
        "T":"?",
        "Model":"OpenBuddy\/openbuddy-gemma-7b-v18.1-4k",
        "Average":59.05,
        "ARC":54.86,
        "HellaSwag":75.68,
        "MMLU":55.56,
        "TruthfulQA":50.08,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":8.54,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"be808f073ed32bd95263cd08084b096774aef74e"
    },
    {
        "T":"\u2b55",
        "Model":"yihan6324\/llama2-7b-instructmining-orca-40k",
        "Average":59.04,
        "ARC":56.74,
        "HellaSwag":80.24,
        "MMLU":48.16,
        "TruthfulQA":51.03,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"dff99c57ca08066fdc085f6db9dfbe8e790f489c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/oasst-llama-13b-2-epochs",
        "Average":59.04,
        "ARC":57.94,
        "HellaSwag":82.4,
        "MMLU":48.56,
        "TruthfulQA":47.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"0e3796192f7edf43968541b9454ea35da4a2b1c5"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged",
        "Average":59.04,
        "ARC":58.96,
        "HellaSwag":81.94,
        "MMLU":55.0,
        "TruthfulQA":40.26,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"30edbe648df2661dd779cd19ef613e6914dcc8e0"
    },
    {
        "T":"\u2b55",
        "Model":"Voicelab\/trurl-2-13b-academic",
        "Average":59.04,
        "ARC":57.94,
        "HellaSwag":79.55,
        "MMLU":55.2,
        "TruthfulQA":43.46,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"2e95049edf02368bbd4b4f6ffb50bc8821e919bb"
    },
    {
        "T":"\u2b55",
        "Model":"Open-Orca\/LlongOrca-7B-16k",
        "Average":59.03,
        "ARC":57.51,
        "HellaSwag":79.44,
        "MMLU":49.35,
        "TruthfulQA":49.84,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":33,
        "Available on the hub":true,
        "Model Sha":"1370c7c595e6c8394e6332bc535ae25e21def85b"
    },
    {
        "T":"?",
        "Model":"TheBloke\/orca_mini_v3_7B-GPTQ",
        "Average":59.03,
        "ARC":54.52,
        "HellaSwag":78.53,
        "MMLU":51.85,
        "TruthfulQA":51.2,
        "Type":"Unknown",
        "Precision":"None",
        "Hub License":"llama2",
        "#Params (B)":1.13,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"4f06a6151128861d5bb256275620f7eadcab3238"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ValiantLabs\/Esper-70b",
        "Average":59.02,
        "ARC":56.48,
        "HellaSwag":77.72,
        "MMLU":55.91,
        "TruthfulQA":45.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":68.98,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"13c5bb97ed6c5faaaa2e2a57fbb60aaff61a0f4c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aboros98\/merlin1.2",
        "Average":59.02,
        "ARC":59.22,
        "HellaSwag":74.19,
        "MMLU":56.45,
        "TruthfulQA":46.24,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"187ce2e4779483483ddc210ff225720db34cf789"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"llm-agents\/tora-13b-v1.0",
        "Average":59.02,
        "ARC":58.96,
        "HellaSwag":82.31,
        "MMLU":54.59,
        "TruthfulQA":40.22,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"0636c1f582c979a5a292cc5f3dc293800b1494e2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r2_a16",
        "Average":59.01,
        "ARC":59.73,
        "HellaSwag":82.38,
        "MMLU":55.27,
        "TruthfulQA":38.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"f470726821c72a58bb400e2dc3a5571f8f650a79"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/Nous-Hermes-13b-pl-lora_unload",
        "Average":59.01,
        "ARC":57.08,
        "HellaSwag":81.49,
        "MMLU":49.17,
        "TruthfulQA":48.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d0ef3991a11c4dc2ea2f832d4082c89c3c5e810c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r128_a16",
        "Average":59.01,
        "ARC":59.9,
        "HellaSwag":82.31,
        "MMLU":55.59,
        "TruthfulQA":38.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"6e8f872757721020c2ae983b6e186fe36105ef2d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/Samantha-1.11-CodeLlama-34b",
        "Average":59.0,
        "ARC":56.57,
        "HellaSwag":75.47,
        "MMLU":53.51,
        "TruthfulQA":50.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":39,
        "Available on the hub":true,
        "Model Sha":"3fd110de9282e52f56f999bf1da1a76425f00e29"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nkpz\/llama2-22b-chat-wizard-uncensored",
        "Average":59.0,
        "ARC":56.23,
        "HellaSwag":80.39,
        "MMLU":53.62,
        "TruthfulQA":45.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":21.83,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"90cffebc8f530161505b84740ff6c8f646299d6c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JunchengXie\/Llama-2-13b-chat-hf-gpt-4-80k-base_lora",
        "Average":59.0,
        "ARC":55.38,
        "HellaSwag":75.69,
        "MMLU":53.99,
        "TruthfulQA":50.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"75febccaa3daa15c1df629b1a1405f173f98e284"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-13b-chat",
        "Average":58.99,
        "ARC":58.62,
        "HellaSwag":80.85,
        "MMLU":47.76,
        "TruthfulQA":48.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"27002e974774c3599e6a4d731dd44e68b9e41f92"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Python-Code-33B",
        "Average":58.98,
        "ARC":56.31,
        "HellaSwag":81.01,
        "MMLU":54.22,
        "TruthfulQA":44.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":32.32,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"cf9a561b57145748455fd3e193d2b0e4ae0a0fce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r32_a4",
        "Average":58.98,
        "ARC":59.81,
        "HellaSwag":82.42,
        "MMLU":55.56,
        "TruthfulQA":38.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"d4d0d9ed2124d79d9e5cbda8ebf45528c4f1e32d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r2_a4",
        "Average":58.98,
        "ARC":59.98,
        "HellaSwag":82.37,
        "MMLU":55.42,
        "TruthfulQA":38.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"7253a71f780be10eb6c3590bf484cfe0975c3a4c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WebraftAI\/synapsellm-7b-mistral-v0.4-preview2",
        "Average":58.98,
        "ARC":52.99,
        "HellaSwag":74.54,
        "MMLU":54.6,
        "TruthfulQA":53.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"59e4ad04a24b656401fab0e8f20de387aaa95512"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r8_a4",
        "Average":58.97,
        "ARC":59.9,
        "HellaSwag":82.47,
        "MMLU":55.47,
        "TruthfulQA":38.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"8793eb75fb25d1cbbcd2811cbbe8f571291f2bdd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Capybara-7B",
        "Average":58.97,
        "ARC":55.29,
        "HellaSwag":80.73,
        "MMLU":48.72,
        "TruthfulQA":51.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":6.61,
        "Hub":26,
        "Available on the hub":true,
        "Model Sha":"42dfc6f7d735670e2f3e30b0919708a81f9a0df9"
    },
    {
        "T":"\u2b55",
        "Model":"beaugogh\/Llama2-7b-openorca-mc-v1",
        "Average":58.97,
        "ARC":55.63,
        "HellaSwag":80.17,
        "MMLU":48.44,
        "TruthfulQA":51.62,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2c4096fa2129665fb127f1c2a1302f30565a5265"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OptimalScale\/robin-13b-v2-delta",
        "Average":58.96,
        "ARC":56.57,
        "HellaSwag":80.35,
        "MMLU":48.39,
        "TruthfulQA":50.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"54c56605e22c731fc1d51273f7e18fc019b20436"
    },
    {
        "T":"\u2b55",
        "Model":"teknium\/Mistral-Trismegistus-7B",
        "Average":58.96,
        "ARC":54.1,
        "HellaSwag":77.91,
        "MMLU":54.49,
        "TruthfulQA":49.36,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":176,
        "Available on the hub":false,
        "Model Sha":"0a5752d096ebab21759dbe203f6b7c7f6092faf2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o",
        "Average":58.96,
        "ARC":57.68,
        "HellaSwag":81.91,
        "MMLU":54.95,
        "TruthfulQA":41.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f76f93dad8408523e69c59abbb96ce6b1b9b9f69"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Capybara-7B",
        "Average":58.96,
        "ARC":55.2,
        "HellaSwag":80.76,
        "MMLU":48.8,
        "TruthfulQA":51.07,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":6.61,
        "Hub":26,
        "Available on the hub":true,
        "Model Sha":"42dfc6f7d735670e2f3e30b0919708a81f9a0df9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r128_a4",
        "Average":58.96,
        "ARC":59.9,
        "HellaSwag":82.43,
        "MMLU":55.44,
        "TruthfulQA":38.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"029666001d8c6be175e42206969a593a4a3f4cb5"
    },
    {
        "T":"?",
        "Model":"l3utterfly\/phi-2-layla-v1-chatml",
        "Average":58.96,
        "ARC":60.41,
        "HellaSwag":74.58,
        "MMLU":56.62,
        "TruthfulQA":44.21,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"d65e3ff599f4b83cbf372ecf5665138dc923ddc0"
    },
    {
        "T":"\u2b55",
        "Model":"pe-nlp\/llama-2-13b-vicuna-wizard",
        "Average":58.93,
        "ARC":57.76,
        "HellaSwag":82.16,
        "MMLU":54.68,
        "TruthfulQA":41.11,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b51bf8c4e132308751cc8b9d9c1131539f79f07f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r8_a16",
        "Average":58.93,
        "ARC":59.73,
        "HellaSwag":82.3,
        "MMLU":55.73,
        "TruthfulQA":37.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"afa40d22d578e631c90017ae0cc67734d6f0b5d8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"internlm\/internlm-20b-chat",
        "Average":58.93,
        "ARC":55.38,
        "HellaSwag":78.58,
        "MMLU":58.53,
        "TruthfulQA":43.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.0,
        "Hub":135,
        "Available on the hub":false,
        "Model Sha":"79946225fa7a215e0ebcf4440a9cce88e475deaa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-mixtral-7bx8-v16.3-32k",
        "Average":58.91,
        "ARC":26.45,
        "HellaSwag":80.83,
        "MMLU":71.99,
        "TruthfulQA":56.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":46.74,
        "Hub":18,
        "Available on the hub":false,
        "Model Sha":"133279baf54f2b8fe414203318272e7d3619ace4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r16-gate_up_down",
        "Average":58.91,
        "ARC":58.7,
        "HellaSwag":81.89,
        "MMLU":56.08,
        "TruthfulQA":38.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4c3a4cb54c0487666bd58589b50f90c22de80969"
    },
    {
        "T":"?",
        "Model":"chargoddard\/llama2-22b",
        "Average":58.9,
        "ARC":58.53,
        "HellaSwag":82.55,
        "MMLU":54.68,
        "TruthfulQA":39.84,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":21.62,
        "Hub":35,
        "Available on the hub":true,
        "Model Sha":"2bece0787009b4b584f49d0e0d1b49ecf4a52da9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"01-ai\/Yi-6B-200K",
        "Average":58.89,
        "ARC":53.58,
        "HellaSwag":75.58,
        "MMLU":64.65,
        "TruthfulQA":41.74,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.06,
        "Hub":166,
        "Available on the hub":true,
        "Model Sha":"6cb672ed8441c35d043dd3cda448466daa3b38b1"
    },
    {
        "T":"?",
        "Model":"01-ai\/Yi-6B-200K",
        "Average":58.88,
        "ARC":53.75,
        "HellaSwag":75.57,
        "MMLU":64.65,
        "TruthfulQA":41.56,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":6.06,
        "Hub":166,
        "Available on the hub":true,
        "Model Sha":"6cb672ed8441c35d043dd3cda448466daa3b38b1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gywy\/llama2-13b-chinese-v1",
        "Average":58.86,
        "ARC":59.81,
        "HellaSwag":75.72,
        "MMLU":54.18,
        "TruthfulQA":45.72,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"c65de036628bb9024a74a85df3cd80aa6ccaf15c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE1_17w-r16",
        "Average":58.86,
        "ARC":57.25,
        "HellaSwag":82.27,
        "MMLU":56.16,
        "TruthfulQA":39.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5da5c92f3cf85a62c1be90a0bb2ae8dffce64a7d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-mistral-7b-v13.1",
        "Average":58.85,
        "ARC":52.56,
        "HellaSwag":75.73,
        "MMLU":56.68,
        "TruthfulQA":50.44,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":19,
        "Available on the hub":false,
        "Model Sha":"b64386bde3d7850a01df763f5c777c74888d34fc"
    },
    {
        "T":"?",
        "Model":"frank098\/Wizard-Vicuna-13B-juniper",
        "Average":58.83,
        "ARC":55.89,
        "HellaSwag":79.75,
        "MMLU":44.99,
        "TruthfulQA":54.72,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"24f58beb9ed4cf635fc962853ed71d0f4b1909ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"IGeniusDev\/llama13B-quant8-testv1-openorca-customdataset",
        "Average":58.81,
        "ARC":60.49,
        "HellaSwag":82.97,
        "MMLU":54.44,
        "TruthfulQA":37.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f364d000bedac80e72aa103c08b77aee1b61b7da"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-huangyt_Fintune_1_17w-gate_up_down_proj",
        "Average":58.81,
        "ARC":57.17,
        "HellaSwag":82.26,
        "MMLU":55.89,
        "TruthfulQA":39.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c1a5ad1b5e490ed860eeb1b449a02e14da10717f"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/llama-2-13b-chat-platypus",
        "Average":58.8,
        "ARC":53.84,
        "HellaSwag":80.67,
        "MMLU":54.44,
        "TruthfulQA":46.23,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"828aa1020fc7d394fe8ee2c596e3211df7656eac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eachadea\/vicuna-13b",
        "Average":58.79,
        "ARC":51.71,
        "HellaSwag":79.94,
        "MMLU":50.84,
        "TruthfulQA":52.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":95,
        "Available on the hub":true,
        "Model Sha":"ac4218770a58baaaaf25201076fe082abb6ffd13"
    },
    {
        "T":"\u2b55",
        "Model":"yihan6324\/llama2-7b-instructmining-orca-90k",
        "Average":58.79,
        "ARC":54.44,
        "HellaSwag":80.45,
        "MMLU":50.89,
        "TruthfulQA":49.36,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"ea5a9f32aa59c6ff0578ebeb93cc9b8db0350212"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"genaicore3434\/MistralLite-summ-sft-e1",
        "Average":58.78,
        "ARC":59.56,
        "HellaSwag":81.42,
        "MMLU":52.34,
        "TruthfulQA":41.79,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f8d5d904ff6bd07e59d6fcf484dc71986f856825"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aboros98\/motans1",
        "Average":58.77,
        "ARC":58.62,
        "HellaSwag":73.42,
        "MMLU":56.94,
        "TruthfulQA":46.1,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8e66c7f8c142a1eb4786469a68a414cdb2d2b26a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chargoddard\/llama2-22b-blocktriangular",
        "Average":58.77,
        "ARC":58.53,
        "HellaSwag":82.59,
        "MMLU":54.64,
        "TruthfulQA":39.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":21.62,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"40a51343ae776b5cb39f2b4343ae8f9b676ffd58"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/deacon-13b",
        "Average":58.77,
        "ARC":57.85,
        "HellaSwag":82.63,
        "MMLU":55.25,
        "TruthfulQA":39.33,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"6c3a002f6c9e8a481a7375d91856d603bf6dd040"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/mcq-vicuna-13b-v1.5",
        "Average":58.76,
        "ARC":56.66,
        "HellaSwag":81.09,
        "MMLU":53.3,
        "TruthfulQA":43.99,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f769a92cfeffe8ee07beee8814ce7eca7cd62805"
    },
    {
        "T":"?",
        "Model":"Yhyu13\/chimera-inst-chat-13b-hf",
        "Average":58.76,
        "ARC":55.38,
        "HellaSwag":78.93,
        "MMLU":50.6,
        "TruthfulQA":50.12,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"a6943d2d30d0af904b3321559157d589e60f9e0f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5",
        "Average":58.74,
        "ARC":59.22,
        "HellaSwag":82.41,
        "MMLU":55.67,
        "TruthfulQA":37.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"e3ba7c482d57dfe65e52a27b21d75a1da59230f5"
    },
    {
        "T":"\u2b55",
        "Model":"deepseek-ai\/deepseek-llm-7b-chat",
        "Average":58.73,
        "ARC":55.8,
        "HellaSwag":79.38,
        "MMLU":51.75,
        "TruthfulQA":47.98,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":6.49,
        "Hub":61,
        "Available on the hub":true,
        "Model Sha":"afbda8b347ec881666061fa67447046fc5164ec8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WebraftAI\/synapsellm-7b-mistral-v0.5-preview2",
        "Average":58.72,
        "ARC":52.22,
        "HellaSwag":75.54,
        "MMLU":51.64,
        "TruthfulQA":55.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b6378fa3b7d39f946d3ce1e0b854622c2866cf7e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Llama2-13B-no_robots-alpaca-lora",
        "Average":58.72,
        "ARC":58.87,
        "HellaSwag":82.43,
        "MMLU":53.11,
        "TruthfulQA":40.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"581aba329e607533c299746bb9eb4154a7aab139"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE1_17w-r4",
        "Average":58.71,
        "ARC":56.74,
        "HellaSwag":82.27,
        "MMLU":56.18,
        "TruthfulQA":39.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7e0046627fabb0f23ace4b71f279d459ec4a0ff1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/mcq-vicuna-13b-v1.5",
        "Average":58.71,
        "ARC":56.23,
        "HellaSwag":81.15,
        "MMLU":53.38,
        "TruthfulQA":44.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f769a92cfeffe8ee07beee8814ce7eca7cd62805"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/ypotryll-22b-epoch2-qlora",
        "Average":58.7,
        "ARC":59.22,
        "HellaSwag":80.66,
        "MMLU":54.52,
        "TruthfulQA":40.42,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":22.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"26fdd8fa420d72ed835c7d17086f0441db0985d4"
    },
    {
        "T":"\u2b55",
        "Model":"BramVanroy\/Llama-2-13b-chat-dutch",
        "Average":58.7,
        "ARC":59.3,
        "HellaSwag":81.45,
        "MMLU":55.82,
        "TruthfulQA":38.23,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"428508a0cf288c0f5b7891c9b2f758ddf4d62c26"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"totally-not-an-llm\/EverythingLM-13b-16k",
        "Average":58.7,
        "ARC":56.57,
        "HellaSwag":80.58,
        "MMLU":50.18,
        "TruthfulQA":47.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":28,
        "Available on the hub":true,
        "Model Sha":"8456a856a8b115b05e76a7d0d945853b10ac71e2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"deepseek-ai\/deepseek-llm-7b-chat",
        "Average":58.7,
        "ARC":55.72,
        "HellaSwag":79.38,
        "MMLU":51.77,
        "TruthfulQA":47.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.49,
        "Hub":61,
        "Available on the hub":true,
        "Model Sha":"afbda8b347ec881666061fa67447046fc5164ec8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/mcq-hal-vicuna-13b-v1.5",
        "Average":58.7,
        "ARC":56.06,
        "HellaSwag":80.7,
        "MMLU":52.9,
        "TruthfulQA":45.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"bb3029bce8347b09c2fd6908475b195bcabe53e3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/minotaur-13b",
        "Average":58.69,
        "ARC":56.4,
        "HellaSwag":79.13,
        "MMLU":49.61,
        "TruthfulQA":49.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"b5ae4519d4c8f4559a0aa80b6efe2008413ece01"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"IGeniusDev\/llama13B-quant8-testv1-openorca-customdataset",
        "Average":58.69,
        "ARC":60.15,
        "HellaSwag":82.99,
        "MMLU":54.33,
        "TruthfulQA":37.27,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f364d000bedac80e72aa103c08b77aee1b61b7da"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chargoddard\/llama2-22b-blocktriangular",
        "Average":58.68,
        "ARC":58.28,
        "HellaSwag":82.69,
        "MMLU":54.53,
        "TruthfulQA":39.23,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":21.62,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"7adbaa5b8e122bb93bf510d8655ec4132d7b4a8a"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Platypus2-13B-QLoRA-0.80-epoch",
        "Average":58.68,
        "ARC":57.76,
        "HellaSwag":81.63,
        "MMLU":55.63,
        "TruthfulQA":39.7,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"114eb8efd2de1c9eae85d92de490b95c854dfae9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r2_a64",
        "Average":58.66,
        "ARC":60.07,
        "HellaSwag":82.0,
        "MMLU":55.18,
        "TruthfulQA":37.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"5249d8dde98eccf4671d89a8e1fd7504edb1464e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"meta-llama\/Llama-2-13b-hf",
        "Average":58.66,
        "ARC":59.39,
        "HellaSwag":82.13,
        "MMLU":55.77,
        "TruthfulQA":37.38,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":342,
        "Available on the hub":false,
        "Model Sha":"7da18fb10421c3ae2a1eb92815bad75e84816e35"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-huangyt_FINETUNE2_3w-gate_up_down_proj",
        "Average":58.65,
        "ARC":57.42,
        "HellaSwag":82.42,
        "MMLU":55.57,
        "TruthfulQA":39.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"469c6674ad2190b639d6f5ce6bfecc1463825dfb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Changgil\/k2s3_test_24001",
        "Average":58.64,
        "ARC":55.72,
        "HellaSwag":80.69,
        "MMLU":54.6,
        "TruthfulQA":43.57,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"cceec03919d9a8e47dd98e4b2468503d52d37ef9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/mcq-hal-vicuna-13b-v1.5",
        "Average":58.64,
        "ARC":55.97,
        "HellaSwag":80.72,
        "MMLU":52.85,
        "TruthfulQA":45.03,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"bb3029bce8347b09c2fd6908475b195bcabe53e3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jilp00\/OpenHermes-Symbolic-Mistral-7B",
        "Average":58.64,
        "ARC":54.78,
        "HellaSwag":72.56,
        "MMLU":61.96,
        "TruthfulQA":45.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d231c07fea44298a7fa33f84a0179fb1d683a94d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jilp00\/OpenHermes-Symbolic-Mistral-7B",
        "Average":58.64,
        "ARC":54.86,
        "HellaSwag":72.55,
        "MMLU":61.8,
        "TruthfulQA":45.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d231c07fea44298a7fa33f84a0179fb1d683a94d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-mistral-7b-v13",
        "Average":58.64,
        "ARC":52.3,
        "HellaSwag":75.09,
        "MMLU":56.34,
        "TruthfulQA":50.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"e6c4cc00e1bb2aa2082c2b8fd93c949aa36ce300"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JosephusCheung\/Pwen-7B-Chat-20_30",
        "Average":58.63,
        "ARC":51.45,
        "HellaSwag":73.99,
        "MMLU":62.08,
        "TruthfulQA":47.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":7.0,
        "Hub":35,
        "Available on the hub":false,
        "Model Sha":"e6c38a7d2f4ba7b867fff421c08c02ba1908224e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/T2A",
        "Average":58.63,
        "ARC":51.45,
        "HellaSwag":73.99,
        "MMLU":62.08,
        "TruthfulQA":47.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c30e3b053299c7ecf250af143a816ef8a9a45c22"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NewstaR\/Starlight-13B",
        "Average":58.63,
        "ARC":59.3,
        "HellaSwag":82.15,
        "MMLU":55.67,
        "TruthfulQA":37.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"cb9fced568b1abd881133c642c427aaa488f00cc"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TheBloke\/Llama-2-13B-fp16",
        "Average":58.63,
        "ARC":59.3,
        "HellaSwag":82.15,
        "MMLU":55.67,
        "TruthfulQA":37.39,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":46,
        "Available on the hub":true,
        "Model Sha":"b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TaylorAI\/Flash-Llama-13B",
        "Average":58.63,
        "ARC":59.3,
        "HellaSwag":82.15,
        "MMLU":55.67,
        "TruthfulQA":37.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"81b40096471a8980e3e1a8998f358bd363033783"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r128_a256",
        "Average":58.61,
        "ARC":59.73,
        "HellaSwag":82.08,
        "MMLU":54.81,
        "TruthfulQA":37.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"3cc31d64036d6abf160c13adf4645a8980280c7f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Changgil\/k2s3_test_24001",
        "Average":58.61,
        "ARC":55.8,
        "HellaSwag":80.59,
        "MMLU":54.42,
        "TruthfulQA":43.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"60f5918773275ff16e43a945a24dd4ad8ddfeacf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o_gate_up_down",
        "Average":58.61,
        "ARC":57.25,
        "HellaSwag":81.49,
        "MMLU":55.9,
        "TruthfulQA":39.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a12fb5937e6904977e8123b0d5ef21283b6895d4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r8-gate_up_down",
        "Average":58.61,
        "ARC":57.17,
        "HellaSwag":82.15,
        "MMLU":54.88,
        "TruthfulQA":40.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"86adab5c098c9338e098a8e5b0188b0aa39b2478"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"maldv\/SHRDFU-7b-delta",
        "Average":58.61,
        "ARC":54.18,
        "HellaSwag":77.55,
        "MMLU":55.95,
        "TruthfulQA":46.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bff4000568782b3d5480cbb072c529861887c622"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-MysteryModel-13b",
        "Average":58.6,
        "ARC":57.0,
        "HellaSwag":80.35,
        "MMLU":52.06,
        "TruthfulQA":45.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c38a9df20162455b53eb35d38a9b67fb824559e8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"codellama\/CodeLlama-70b-hf",
        "Average":58.6,
        "ARC":56.74,
        "HellaSwag":78.21,
        "MMLU":59.67,
        "TruthfulQA":39.79,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":290,
        "Available on the hub":true,
        "Model Sha":"4570a4edc524fb9f20f605b417bb43828fa5997a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ericpolewski\/AIRIC-The-Intern",
        "Average":58.6,
        "ARC":52.73,
        "HellaSwag":77.07,
        "MMLU":51.92,
        "TruthfulQA":52.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":20.09,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0b8e768b81fd4718dbd9155becf2c8592704e7e4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"g-ronimo\/phi-2-OpenHermes-2.5-v2",
        "Average":58.58,
        "ARC":58.45,
        "HellaSwag":74.57,
        "MMLU":56.43,
        "TruthfulQA":44.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"246f56314bb9aada8d50267bc0764c07bdcd8b86"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Medusa-1.1-L2-7B",
        "Average":58.58,
        "ARC":56.48,
        "HellaSwag":78.57,
        "MMLU":51.56,
        "TruthfulQA":47.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"df23c3d22bc546dbce0267415e94bdb482446c06"
    },
    {
        "T":"?",
        "Model":"shareAI\/bimoGPT-llama2-13b",
        "Average":58.57,
        "ARC":58.79,
        "HellaSwag":82.08,
        "MMLU":55.6,
        "TruthfulQA":37.82,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":13.0,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"c29b67965ea55da3e2ac678eef7ffdf36f8ef5ab"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wahaha1987\/llama_13b_sharegpt94k_fastchat",
        "Average":58.57,
        "ARC":53.75,
        "HellaSwag":79.47,
        "MMLU":51.5,
        "TruthfulQA":49.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"388bc2f82a1ee8b963c7f94f9c7b6743f7214306"
    },
    {
        "T":"?",
        "Model":"TheBloke\/manticore-13b-chat-pyg-GPTQ",
        "Average":58.56,
        "ARC":57.85,
        "HellaSwag":81.07,
        "MMLU":47.56,
        "TruthfulQA":47.77,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.03,
        "Hub":32,
        "Available on the hub":true,
        "Model Sha":"923f27245d13058c9c1b3ab0eab6c6c93ffc162e"
    },
    {
        "T":"\u2b55",
        "Model":"Yehoon\/yehoon_llama2",
        "Average":58.55,
        "ARC":54.78,
        "HellaSwag":78.98,
        "MMLU":51.29,
        "TruthfulQA":49.17,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"443cb81ce988ea6c0b1e20132c170463d559367e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o",
        "Average":58.55,
        "ARC":59.3,
        "HellaSwag":81.2,
        "MMLU":55.58,
        "TruthfulQA":38.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"71224344025dbfada6821c6a89cade1d8358dad1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r8_a64",
        "Average":58.54,
        "ARC":59.56,
        "HellaSwag":82.18,
        "MMLU":55.32,
        "TruthfulQA":37.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"337b96d91b5323b1e4dc0775bccb08f5ae6928ea"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r128_a64",
        "Average":58.51,
        "ARC":59.04,
        "HellaSwag":82.27,
        "MMLU":55.54,
        "TruthfulQA":37.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"6446e661b63b07af1c57b623de637ca1c6fcb7b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BramVanroy\/llama2-13b-ft-mc4_nl_cleaned_tiny",
        "Average":58.51,
        "ARC":59.3,
        "HellaSwag":82.04,
        "MMLU":54.67,
        "TruthfulQA":38.03,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.02,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"b23fe7d174653b87dc08507d9b83504a8dddbc45"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"g-ronimo\/phi-2-OpenHermes-2.5",
        "Average":58.51,
        "ARC":59.81,
        "HellaSwag":74.85,
        "MMLU":55.51,
        "TruthfulQA":43.86,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"ee382f2c6f1006d6854a1b3cc26cbaa28eeab2cb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JosephusCheung\/Qwen-LLaMAfied-7B-Chat",
        "Average":58.5,
        "ARC":50.94,
        "HellaSwag":83.47,
        "MMLU":53.52,
        "TruthfulQA":46.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":7.1,
        "Hub":77,
        "Available on the hub":true,
        "Model Sha":"4d70cf0047a7a5cd2c864bc2606e81f0830e4405"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-llama2-13b-v11.1-bf16",
        "Average":58.5,
        "ARC":51.62,
        "HellaSwag":76.23,
        "MMLU":56.45,
        "TruthfulQA":49.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.88,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"fdbd9cc550b58aed9bee58649255191c88011829"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"adamo1139\/Yi-6B-200K-AEZAKMI-v2",
        "Average":58.5,
        "ARC":52.99,
        "HellaSwag":71.2,
        "MMLU":63.0,
        "TruthfulQA":46.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.06,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"0c4dd0e7119bbef9fa5b28b5a581b60822cebaf5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"totally-not-an-llm\/EverythingLM-13b-V3-16k",
        "Average":58.49,
        "ARC":58.19,
        "HellaSwag":80.12,
        "MMLU":50.48,
        "TruthfulQA":45.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"1de9244bfadb947f80872727f76790cbc76e7142"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mncai\/Llama2-7B-guanaco-dolphin-500",
        "Average":58.49,
        "ARC":56.74,
        "HellaSwag":81.62,
        "MMLU":48.68,
        "TruthfulQA":46.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"afe00170f084f773e401ba7d738d692533cca6b4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_8192",
        "Average":58.49,
        "ARC":59.56,
        "HellaSwag":81.44,
        "MMLU":46.26,
        "TruthfulQA":46.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":220,
        "Available on the hub":true,
        "Model Sha":"f661da5af278fbda8a43b19ff0250e4efc103e3e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-huangyt_Fintune_1_17w-q_k_v_o_proj",
        "Average":58.49,
        "ARC":59.73,
        "HellaSwag":81.06,
        "MMLU":54.53,
        "TruthfulQA":38.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"aeeded8db9eea97e2e6a2e19a006ce1acd110a82"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/llama-13b-pretrained-sft-do2",
        "Average":58.49,
        "ARC":58.96,
        "HellaSwag":80.32,
        "MMLU":47.25,
        "TruthfulQA":47.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"6cb016f5bfcbc24ee08312b52f08ef5e8f860871"
    },
    {
        "T":"\u2b55",
        "Model":"kz919\/mistral-7b-sft-open-orca-flan-50k",
        "Average":58.48,
        "ARC":58.79,
        "HellaSwag":81.92,
        "MMLU":55.72,
        "TruthfulQA":37.49,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"54129b5d7a3824af7d457e007742750029cb3904"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r32_a64",
        "Average":58.48,
        "ARC":58.96,
        "HellaSwag":82.31,
        "MMLU":55.23,
        "TruthfulQA":37.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"a2f587f367c78e478713c2eedeb99b2d343ad9f1"
    },
    {
        "T":"?",
        "Model":"lamhieu\/ghost-7b-v0.9.0",
        "Average":58.47,
        "ARC":53.07,
        "HellaSwag":77.93,
        "MMLU":55.09,
        "TruthfulQA":47.79,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"78441c9cec230d2dc76a746854078fa776a019c6"
    },
    {
        "T":"?",
        "Model":"abacusai\/bigyi-15b",
        "Average":58.47,
        "ARC":56.06,
        "HellaSwag":75.9,
        "MMLU":64.6,
        "TruthfulQA":37.33,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":15.06,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"5ec2656b39515e3a903adfb05e5022b0f4eb5e2a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-alpaca-test",
        "Average":58.47,
        "ARC":60.07,
        "HellaSwag":81.29,
        "MMLU":55.59,
        "TruthfulQA":36.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e30656d62d21fbdde207c8f7b3b6ba89cac85785"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r32_a256",
        "Average":58.47,
        "ARC":60.15,
        "HellaSwag":81.98,
        "MMLU":54.99,
        "TruthfulQA":36.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"f3192de7e37e861a697c0741dd4e641724b6fdbc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-llama2-13b-v11.1-bf16",
        "Average":58.46,
        "ARC":51.79,
        "HellaSwag":76.23,
        "MMLU":56.13,
        "TruthfulQA":49.7,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.88,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"76fb7d00836eb2f1d9c9605d8881d73b782cf324"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE2_TEST_2.2w",
        "Average":58.46,
        "ARC":56.23,
        "HellaSwag":82.7,
        "MMLU":55.35,
        "TruthfulQA":39.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3be177b35f1b44d147751ab38ca6d8a008eb6b7f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o",
        "Average":58.46,
        "ARC":56.23,
        "HellaSwag":81.98,
        "MMLU":55.87,
        "TruthfulQA":39.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"cc3c5e5a874cf4ff4f94ea919e819f8a914c8acb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xwin-LM\/Xwin-LM-7B-V0.1",
        "Average":58.46,
        "ARC":56.57,
        "HellaSwag":79.4,
        "MMLU":49.98,
        "TruthfulQA":47.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":75,
        "Available on the hub":true,
        "Model Sha":"470e680120a7249d6e8a875345015ddba1711100"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/vicuna-class-tutor-7b-ep3",
        "Average":58.46,
        "ARC":52.13,
        "HellaSwag":78.07,
        "MMLU":51.32,
        "TruthfulQA":52.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"724cf8becd6dbb0b67070c34711ef6d60ad5f216"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"project-baize\/baize-v2-13b",
        "Average":58.45,
        "ARC":56.91,
        "HellaSwag":79.29,
        "MMLU":49.72,
        "TruthfulQA":47.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"a3c4bbccca8b650700a49a225582c17bb49b446b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"vishesht27\/22-Neuro_Model",
        "Average":58.42,
        "ARC":49.15,
        "HellaSwag":62.31,
        "MMLU":62.01,
        "TruthfulQA":60.23,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"807caffa6064420c088fadb9f2d34012da6b3236"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-13b-gpt4-m2.0",
        "Average":58.42,
        "ARC":59.22,
        "HellaSwag":81.02,
        "MMLU":53.73,
        "TruthfulQA":39.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":23,
        "Available on the hub":true,
        "Model Sha":"a852b77f7d0777092c76898bc83f8e657ca2af3e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o_gate_up_down",
        "Average":58.42,
        "ARC":57.76,
        "HellaSwag":80.78,
        "MMLU":54.32,
        "TruthfulQA":40.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ebe1b75fa315a9b55f686368070a0bcd0245ee39"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"RatanRohith\/SRBOSGPT-7B-slerp",
        "Average":58.4,
        "ARC":49.15,
        "HellaSwag":62.28,
        "MMLU":61.95,
        "TruthfulQA":60.23,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"037f68c68da7ff3f981534f6deec3c85e86a9a86"
    },
    {
        "T":"?",
        "Model":"OpenBuddy\/openbuddy-mistral-7b-v19.1-4k",
        "Average":58.38,
        "ARC":53.41,
        "HellaSwag":74.58,
        "MMLU":57.29,
        "TruthfulQA":48.25,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.28,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"df3762aafe547c0c87cfbf7e527c891885e51084"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2xOpenOrca-13B-LoRa-v2",
        "Average":58.36,
        "ARC":58.62,
        "HellaSwag":81.17,
        "MMLU":50.23,
        "TruthfulQA":43.43,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"568ac6a5f1a9f5eb6bc09efb2188740d771ed0e9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Covasna-0.1",
        "Average":58.35,
        "ARC":48.81,
        "HellaSwag":70.07,
        "MMLU":61.9,
        "TruthfulQA":52.64,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":41.6,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"4f14f0584f79adc679d119f09bc69430d42babc1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ShadowFall09\/FANNO",
        "Average":58.34,
        "ARC":55.46,
        "HellaSwag":79.29,
        "MMLU":46.58,
        "TruthfulQA":52.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"204ab769ed98e2b0a8aa8288cfcd2791a91ebc2e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-huangyt_FINETUNE2_3w",
        "Average":58.34,
        "ARC":58.62,
        "HellaSwag":82.32,
        "MMLU":54.25,
        "TruthfulQA":38.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"08bc7112a775dd4223d441355f3d619694013789"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xzuyn\/Alpacino-SuperCOT-13B",
        "Average":58.34,
        "ARC":58.36,
        "HellaSwag":81.69,
        "MMLU":47.89,
        "TruthfulQA":45.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"3a82b04684fe99d59556421c3f96a187049a3cec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o",
        "Average":58.34,
        "ARC":59.04,
        "HellaSwag":81.15,
        "MMLU":53.0,
        "TruthfulQA":40.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ac40ecf48cf5f7168e8c3929632c654bc834c3d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r4-gate_up_down",
        "Average":58.33,
        "ARC":55.38,
        "HellaSwag":81.92,
        "MMLU":55.28,
        "TruthfulQA":40.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2ca747d779feaa99c475b8015c9b4a50aea41cd2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/llama2-13b-fintune2-4E",
        "Average":58.32,
        "ARC":55.89,
        "HellaSwag":80.95,
        "MMLU":53.73,
        "TruthfulQA":42.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"645ede9d6ec60d8fa051bc7ad32ab5f7bfdc066d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"minghaowu\/phi-2-OpenHermes-2.5",
        "Average":58.32,
        "ARC":56.48,
        "HellaSwag":73.88,
        "MMLU":54.8,
        "TruthfulQA":48.1,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"37ea7cc2653b7f5a6c53e95dca49f968a13a6d21"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-zephyr-7b-v14.1",
        "Average":58.3,
        "ARC":52.13,
        "HellaSwag":75.02,
        "MMLU":56.21,
        "TruthfulQA":49.84,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":49,
        "Available on the hub":false,
        "Model Sha":"208b6fb841239a36fb0ea675179a231e0ad9d287"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ankhamun\/xxxI-Ixxx",
        "Average":58.29,
        "ARC":54.18,
        "HellaSwag":72.55,
        "MMLU":52.02,
        "TruthfulQA":54.42,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4063a7f7f22b9f6f22cfaf518e85743bdce4dc11"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Biomimicry-AI\/ANIMA-Nectar-v2",
        "Average":58.28,
        "ARC":53.24,
        "HellaSwag":76.63,
        "MMLU":54.21,
        "TruthfulQA":49.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"304e41b614d1ac9debccfa266887640b508c9823"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o",
        "Average":58.28,
        "ARC":56.06,
        "HellaSwag":81.89,
        "MMLU":55.04,
        "TruthfulQA":40.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f907fffbb08698040325b3f2e47200a1b48b3ed9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r8-q_k_v_o_gate_up_down",
        "Average":58.26,
        "ARC":55.72,
        "HellaSwag":81.55,
        "MMLU":53.9,
        "TruthfulQA":41.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"eb934db4644738a74143b381445213979c8858ed"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-13b-gpt4-2.0",
        "Average":58.26,
        "ARC":59.04,
        "HellaSwag":82.82,
        "MMLU":54.71,
        "TruthfulQA":36.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"ec556571acc6783fea4414e4ca72d291c563b6dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r16-gate_up_down",
        "Average":58.26,
        "ARC":55.8,
        "HellaSwag":82.1,
        "MMLU":55.33,
        "TruthfulQA":39.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"86f255afabc8986c73376cafd98628a068649022"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o_gate_up_down",
        "Average":58.26,
        "ARC":57.94,
        "HellaSwag":81.19,
        "MMLU":53.43,
        "TruthfulQA":40.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"15f1b122d60631091419cb8e668a28737b92a0e0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aiplanet\/effi-13b",
        "Average":58.26,
        "ARC":53.33,
        "HellaSwag":81.22,
        "MMLU":53.57,
        "TruthfulQA":44.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"1b4b4c72dd41ddc1a80f2db6c85170e50a91ed7a"
    },
    {
        "T":"\u2b55",
        "Model":"clibrain\/Llama-2-13b-ft-instruct-es",
        "Average":58.25,
        "ARC":59.39,
        "HellaSwag":81.51,
        "MMLU":54.31,
        "TruthfulQA":37.81,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"772b53f64f484fa0d651d453bcefc35a0f52f251"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-huangyt_Fintune_1_17w",
        "Average":58.24,
        "ARC":59.47,
        "HellaSwag":81.0,
        "MMLU":54.31,
        "TruthfulQA":38.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"aa5b161b39900c5e80d5bb39d098f6333ad964f7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HenryJJ\/Instruct_Yi-6B_Dolly_CodeAlpaca",
        "Average":58.23,
        "ARC":53.16,
        "HellaSwag":75.3,
        "MMLU":63.06,
        "TruthfulQA":41.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.06,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"97c31498b579cf4808195dd21a858a258d40b2dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/llama2-13b-FINETUNE3_TEST2",
        "Average":58.23,
        "ARC":54.69,
        "HellaSwag":81.48,
        "MMLU":56.8,
        "TruthfulQA":39.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9e6431061bd13852a7435f5fe7a6eb0bbd148e14"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-2-7b-instruct",
        "Average":58.22,
        "ARC":56.23,
        "HellaSwag":79.97,
        "MMLU":47.17,
        "TruthfulQA":49.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"8f4dd9c870f748322989168af5c109e16b01c63d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r8_a256",
        "Average":58.21,
        "ARC":59.81,
        "HellaSwag":81.79,
        "MMLU":53.22,
        "TruthfulQA":38.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"c7fedbbc9bad7326196826cd290f9928e6e7342e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-huangyt_FINETUNE2_3w-q_k_v_o_proj",
        "Average":58.21,
        "ARC":58.53,
        "HellaSwag":82.47,
        "MMLU":53.9,
        "TruthfulQA":37.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d74752b931bfddaa063a292e7ea85dfb1d7a4998"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"formulae\/Dorflan",
        "Average":58.19,
        "ARC":54.44,
        "HellaSwag":75.78,
        "MMLU":51.36,
        "TruthfulQA":51.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"5d8e7e5764ace89e6ccd1deece33b0e8a4b4587b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elyza\/ELYZA-japanese-Llama-2-13b",
        "Average":58.17,
        "ARC":57.0,
        "HellaSwag":80.89,
        "MMLU":54.38,
        "TruthfulQA":40.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":18,
        "Available on the hub":true,
        "Model Sha":"24be61d31af8ac3e8c57d924c749ca3cf5f681ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Telugu-LLM-Labs\/Indic-gemma-7b-finetuned-sft-Navarasa-2.0",
        "Average":58.17,
        "ARC":54.61,
        "HellaSwag":74.35,
        "MMLU":54.15,
        "TruthfulQA":49.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":8.54,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"84d251f088d2954561a4348883ba28f6f3265182"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r8-gate_up_down",
        "Average":58.17,
        "ARC":57.25,
        "HellaSwag":81.79,
        "MMLU":53.96,
        "TruthfulQA":39.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8a75b17d4b60f820159bb0100f26f438727bb199"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o_gate_up_down",
        "Average":58.16,
        "ARC":55.97,
        "HellaSwag":81.53,
        "MMLU":54.42,
        "TruthfulQA":40.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"905fc0b26dcb9e1fc5be99e73596e0884f9b71df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-2-7b-chat",
        "Average":58.13,
        "ARC":55.63,
        "HellaSwag":78.71,
        "MMLU":50.98,
        "TruthfulQA":47.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"7a1b76feabe3e0ed007ea83ee93f7644156d3b23"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aiplanet\/effi-13b",
        "Average":58.13,
        "ARC":52.9,
        "HellaSwag":81.19,
        "MMLU":53.47,
        "TruthfulQA":44.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"37e746d687b38f5199f5eb0c7ea22647d1d88fb9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"haonan-li\/bactrian-x-llama-13b-merged",
        "Average":58.13,
        "ARC":56.4,
        "HellaSwag":79.33,
        "MMLU":48.4,
        "TruthfulQA":48.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"cc5ee2231066c147423f89e9df40f7364c3275a5"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16",
        "Average":58.12,
        "ARC":59.04,
        "HellaSwag":82.33,
        "MMLU":55.36,
        "TruthfulQA":35.75,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a3ed7416156963f49bf4dc056188e006c0c214d2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HWERI\/Llama2-7b-sharegpt4",
        "Average":58.12,
        "ARC":55.72,
        "HellaSwag":80.94,
        "MMLU":47.47,
        "TruthfulQA":48.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"8ecaba5dd0e9929f5858cfe9f5f8cd8ba285c9e5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beaugogh\/Llama2-7b-sharegpt4",
        "Average":58.12,
        "ARC":55.72,
        "HellaSwag":80.94,
        "MMLU":47.47,
        "TruthfulQA":48.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"922d1d963ad1b042c30b774a818d9f6180c28075"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-13b-gpt4-1.3",
        "Average":58.09,
        "ARC":58.53,
        "HellaSwag":81.6,
        "MMLU":46.96,
        "TruthfulQA":45.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"32a474742c2a235ca12c96afaea57dcb6b46ef56"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airoboros-c34b-2.1",
        "Average":58.09,
        "ARC":54.69,
        "HellaSwag":76.45,
        "MMLU":55.08,
        "TruthfulQA":46.15,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"2caa8ce3aab012bf34c7c531827f6befc7cc1c98"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/wizard-vicuna-13B-HF",
        "Average":58.09,
        "ARC":54.69,
        "HellaSwag":79.18,
        "MMLU":48.88,
        "TruthfulQA":49.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":48,
        "Available on the hub":true,
        "Model Sha":"12dc8aacb474522ae2a83c18cb0fdf0907987f8f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"junelee\/wizard-vicuna-13b",
        "Average":58.09,
        "ARC":54.69,
        "HellaSwag":79.18,
        "MMLU":48.88,
        "TruthfulQA":49.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":69,
        "Available on the hub":true,
        "Model Sha":"419dc5acc391de54a60d0b041e94e767d1ef2032"
    },
    {
        "T":"?",
        "Model":"yanolja\/EEVE-Korean-Instruct-2.8B-v1.0",
        "Average":58.09,
        "ARC":58.28,
        "HellaSwag":72.42,
        "MMLU":53.35,
        "TruthfulQA":48.32,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.82,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"ba710d9bbd03ec302064e6f19141364f7e01eb00"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/spicyboros-7b-2.2",
        "Average":58.09,
        "ARC":56.57,
        "HellaSwag":80.09,
        "MMLU":48.47,
        "TruthfulQA":47.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":18,
        "Available on the hub":true,
        "Model Sha":"fdf075081555f3ed84c037e8dd3fe85c3b3609d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/OpenOrca-Preview1-13B",
        "Average":58.08,
        "ARC":54.95,
        "HellaSwag":78.19,
        "MMLU":50.12,
        "TruthfulQA":49.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub":140,
        "Available on the hub":true,
        "Model Sha":"d120381b03051b60a7c77ec3fb1be6c3c1546466"
    },
    {
        "T":"?",
        "Model":"TheBloke\/h2ogpt-oasst1-512-30B-HF",
        "Average":58.07,
        "ARC":57.34,
        "HellaSwag":81.37,
        "MMLU":48.09,
        "TruthfulQA":45.46,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"3dc93836e4b08b7b2ee43e69c1e590a36fd24687"
    },
    {
        "T":"\u2b55",
        "Model":"abacusai\/Giraffe-beta-13b-32k",
        "Average":58.06,
        "ARC":55.63,
        "HellaSwag":80.42,
        "MMLU":53.61,
        "TruthfulQA":42.58,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"259f3fe9ebbff7532498f44286f253d56699da6f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o_gate_up_down",
        "Average":58.04,
        "ARC":56.31,
        "HellaSwag":81.43,
        "MMLU":55.3,
        "TruthfulQA":39.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0d8d502e4e5ef89592dd0d3bc7223eaf7f77f78b"
    },
    {
        "T":"?",
        "Model":"TheBloke\/Llama-2-13B-GPTQ",
        "Average":58.03,
        "ARC":59.13,
        "HellaSwag":81.48,
        "MMLU":54.45,
        "TruthfulQA":37.07,
        "Type":"Unknown",
        "Precision":"None",
        "Hub License":"llama2",
        "#Params (B)":2.03,
        "Hub":94,
        "Available on the hub":true,
        "Model Sha":"b7db471d1789802a3a8e3b93cdd66a9f046f17c3"
    },
    {
        "T":"?",
        "Model":"sethuiyer\/OpenDolphinHermes_Llama2_7B",
        "Average":58.03,
        "ARC":55.03,
        "HellaSwag":78.74,
        "MMLU":52.25,
        "TruthfulQA":46.1,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"3b6713b4ab2e2ea79535802f126287dd9d7036ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-MultiLoRA-sharegpt-mmlu-drop-ffn-1.0general",
        "Average":58.03,
        "ARC":53.33,
        "HellaSwag":77.41,
        "MMLU":51.04,
        "TruthfulQA":50.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ddaea6ff7adaa39cf175bc15732c4004389c6815"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hyunseoki\/ko-en-llama2-13b",
        "Average":58.01,
        "ARC":58.19,
        "HellaSwag":81.89,
        "MMLU":52.02,
        "TruthfulQA":39.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":25,
        "Available on the hub":true,
        "Model Sha":"2768cf6f955b65868ccbb20658e2cc444b2f3be9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"camel-ai\/CAMEL-13B-Combined-Data",
        "Average":58.01,
        "ARC":55.63,
        "HellaSwag":79.25,
        "MMLU":49.74,
        "TruthfulQA":47.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"6d98f2801f13d89de7978ee9f348a52ea46a24ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Danielbrdz\/Barcenas-Orca-2-7b",
        "Average":58.01,
        "ARC":55.2,
        "HellaSwag":77.08,
        "MMLU":56.02,
        "TruthfulQA":43.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"dd4cc9f2be4fb8acb30b5bc79ad759ae2906300c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-7b-v1.5",
        "Average":58.0,
        "ARC":53.24,
        "HellaSwag":77.39,
        "MMLU":51.04,
        "TruthfulQA":50.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":50,
        "Available on the hub":true,
        "Model Sha":"de56c35b1763eaae20f4d60efd64af0a9091ebe5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TencentARC\/LLaMA-Pro-8B-Instruct",
        "Average":57.99,
        "ARC":52.99,
        "HellaSwag":76.98,
        "MMLU":52.58,
        "TruthfulQA":49.43,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":8.36,
        "Hub":57,
        "Available on the hub":true,
        "Model Sha":"209760d8bffdc49afa18afdb038b0cf921b19fe4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Suprit\/Zhongjing-LLaMA-base",
        "Average":57.99,
        "ARC":55.12,
        "HellaSwag":79.72,
        "MMLU":48.23,
        "TruthfulQA":48.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.89,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1b53d10b830b864d88032ae467016f8a1d7ba239"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-7b-v1.5-16k",
        "Average":57.98,
        "ARC":54.69,
        "HellaSwag":77.32,
        "MMLU":49.51,
        "TruthfulQA":50.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":44,
        "Available on the hub":true,
        "Model Sha":"9a93d7d11fac7f3f9074510b80092b53bc1a5bec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o",
        "Average":57.98,
        "ARC":54.78,
        "HellaSwag":81.4,
        "MMLU":54.73,
        "TruthfulQA":41.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8702b433008a62e9f8bf15e70ba15fa7100e991c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-7B-v1.2",
        "Average":57.97,
        "ARC":54.35,
        "HellaSwag":79.29,
        "MMLU":49.33,
        "TruthfulQA":48.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"85ea4f4818478084eedd01e958ac5cc7cf64b3bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down-test1",
        "Average":57.96,
        "ARC":55.8,
        "HellaSwag":82.27,
        "MMLU":55.63,
        "TruthfulQA":38.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"48b8ceeb62e5ca897f284bbc0923201689af7c89"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e6",
        "Average":57.96,
        "ARC":58.87,
        "HellaSwag":81.9,
        "MMLU":55.03,
        "TruthfulQA":36.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"701562febb55ca9660f3c4d7be2249f3dbd5f0b3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-7b-v1.5",
        "Average":57.95,
        "ARC":53.24,
        "HellaSwag":77.39,
        "MMLU":50.82,
        "TruthfulQA":50.33,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":50,
        "Available on the hub":true,
        "Model Sha":"de56c35b1763eaae20f4d60efd64af0a9091ebe5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NovoCode\/Tiger-7B-v0.1-LaserRMT-Math-5-10-15-Neural-DPO",
        "Average":57.95,
        "ARC":39.42,
        "HellaSwag":82.58,
        "MMLU":61.63,
        "TruthfulQA":48.16,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"65089a9263dc7c51a787deb6392955d78621f72f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r4-gate_up_down",
        "Average":57.94,
        "ARC":55.8,
        "HellaSwag":81.74,
        "MMLU":55.09,
        "TruthfulQA":39.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"aefc3a122cb054b070a212d1127600775aded4be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down",
        "Average":57.93,
        "ARC":55.03,
        "HellaSwag":81.97,
        "MMLU":56.64,
        "TruthfulQA":38.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"555486843f613276b6edb480f6d37b9203daa226"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/Mistral-7B-attention-100000",
        "Average":57.92,
        "ARC":52.99,
        "HellaSwag":78.54,
        "MMLU":54.79,
        "TruthfulQA":45.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"be7a7aa124108276788139ee1c7269553dd4f9d4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ausboss\/llama-13b-supercot",
        "Average":57.92,
        "ARC":56.06,
        "HellaSwag":81.71,
        "MMLU":45.36,
        "TruthfulQA":48.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"f6953fa162b487a3d4c6bdc7b7951e09576c2ae5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/Llama-2-13b-longlora-32k-ft",
        "Average":57.91,
        "ARC":59.47,
        "HellaSwag":82.61,
        "MMLU":52.13,
        "TruthfulQA":37.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"6d17c854025b0bd54ce572ac803f1bb052875dbf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/tulu-13B-fp16",
        "Average":57.9,
        "ARC":53.92,
        "HellaSwag":80.66,
        "MMLU":53.19,
        "TruthfulQA":43.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"532aeb363b0ceee155b3cf9479ef635b797cee7c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xriminact\/TarsMeta",
        "Average":57.9,
        "ARC":52.9,
        "HellaSwag":78.2,
        "MMLU":52.63,
        "TruthfulQA":47.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"458bf65b37f4f89ea92a72ee3df6e0048324252f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"codellama\/CodeLlama-70b-Python-hf",
        "Average":57.89,
        "ARC":55.12,
        "HellaSwag":78.48,
        "MMLU":56.17,
        "TruthfulQA":41.78,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub":101,
        "Available on the hub":true,
        "Model Sha":"79467981bab591dd6860707ed517d1186fbcfc1e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Technoculture\/Medorca-2x7b",
        "Average":57.87,
        "ARC":54.1,
        "HellaSwag":76.04,
        "MMLU":53.3,
        "TruthfulQA":48.04,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.07,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"597c5b2e36b7b5375f0c05c05acc2699ec2a26cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/Llama-2-13b-FINETUNE4_TEST",
        "Average":57.87,
        "ARC":54.78,
        "HellaSwag":81.52,
        "MMLU":56.03,
        "TruthfulQA":39.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0ed198a814192b06e60715112d2a4b6bfd630806"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-code-mistral-7b-v2.0",
        "Average":57.86,
        "ARC":52.47,
        "HellaSwag":75.61,
        "MMLU":51.31,
        "TruthfulQA":52.05,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"8371b49e786758da62de015daa006c0e58b7ce82"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nisten\/bigdoc-c34b-instruct-tf32",
        "Average":57.86,
        "ARC":54.44,
        "HellaSwag":76.91,
        "MMLU":55.62,
        "TruthfulQA":44.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-3.0",
        "#Params (B)":34.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a20a49f517dbc82705e1c67f78ef47f794777f91"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r8-gate_up_down",
        "Average":57.85,
        "ARC":54.35,
        "HellaSwag":82.13,
        "MMLU":55.33,
        "TruthfulQA":39.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1646a2b77ddeaf0f848c96ed68726556c7539729"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Hermes-llama-2-7b",
        "Average":57.85,
        "ARC":55.12,
        "HellaSwag":78.94,
        "MMLU":48.34,
        "TruthfulQA":49.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":6.74,
        "Hub":44,
        "Available on the hub":true,
        "Model Sha":"60e58acecdc1552e1b1752a38d1d91d942d1c3f0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"genaicore3434\/MistralLite-summ-sft-e1",
        "Average":57.85,
        "ARC":57.59,
        "HellaSwag":80.66,
        "MMLU":52.28,
        "TruthfulQA":40.85,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f8d5d904ff6bd07e59d6fcf484dc71986f856825"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WebraftAI\/synapsellm-7b-mistral-v0.4-preview3",
        "Average":57.85,
        "ARC":51.28,
        "HellaSwag":74.83,
        "MMLU":52.93,
        "TruthfulQA":52.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"67c0d0fe71c620f0be410a06f58b928f89218639"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elyza\/ELYZA-japanese-Llama-2-13b-fast",
        "Average":57.83,
        "ARC":55.89,
        "HellaSwag":80.73,
        "MMLU":54.4,
        "TruthfulQA":40.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.92,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"f2d798d1a7dc6c254575b7a4fe24f4c76652e6d8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jlevin\/guanaco-13b-llama-2",
        "Average":57.83,
        "ARC":55.38,
        "HellaSwag":81.87,
        "MMLU":47.19,
        "TruthfulQA":46.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f1649a9de898859684b15ef8bf5652a8f86ddcfc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r4-q_k_v_o_gate_up_down",
        "Average":57.82,
        "ARC":55.89,
        "HellaSwag":81.38,
        "MMLU":53.77,
        "TruthfulQA":40.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a8b15badead658df6ec5b884b813962b9fd29cfb"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged",
        "Average":57.82,
        "ARC":58.45,
        "HellaSwag":81.97,
        "MMLU":55.02,
        "TruthfulQA":35.85,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5a89844b1aea3f0573e696143ec66727df4b5d79"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/Cinder-Phi-2-V1-F16-gguf",
        "Average":57.82,
        "ARC":58.28,
        "HellaSwag":74.04,
        "MMLU":54.46,
        "TruthfulQA":44.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"3d0fc5758f6b55757a669f7f05a9b19af452e045"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mncai\/Llama2-7B-guanaco-1k",
        "Average":57.82,
        "ARC":55.12,
        "HellaSwag":80.53,
        "MMLU":47.93,
        "TruthfulQA":47.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5f3194b779897bbc4c4218a9dddc44a9b5faea15"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-code-mistral-7b-v2.0",
        "Average":57.81,
        "ARC":52.3,
        "HellaSwag":75.61,
        "MMLU":51.28,
        "TruthfulQA":52.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"8371b49e786758da62de015daa006c0e58b7ce82"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r4-gate_up_down",
        "Average":57.79,
        "ARC":56.4,
        "HellaSwag":81.93,
        "MMLU":53.63,
        "TruthfulQA":39.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"dd61a482fa2f71efe6f22aae6949355ca4b06ccc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"codellama\/CodeLlama-34b-Instruct-hf",
        "Average":57.79,
        "ARC":54.27,
        "HellaSwag":76.92,
        "MMLU":55.54,
        "TruthfulQA":44.44,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub":104,
        "Available on the hub":true,
        "Model Sha":"bf5e5060fa30f33149efe84bbcc682001a00ab94"
    },
    {
        "T":"\u2b55",
        "Model":"Azure99\/blossom-v2-llama2-7b",
        "Average":57.79,
        "ARC":54.1,
        "HellaSwag":78.57,
        "MMLU":51.66,
        "TruthfulQA":46.84,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8c71cdb481ce6bbda3b2042e5526a232ab23825c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kaist-ai\/prometheus-13b-v1.0",
        "Average":57.78,
        "ARC":53.24,
        "HellaSwag":80.75,
        "MMLU":51.49,
        "TruthfulQA":45.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":95,
        "Available on the hub":true,
        "Model Sha":"9088377314f91af4b48940e09a0c76d0878f5020"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lamhieu\/ghost-7b-v0.9.1",
        "Average":57.78,
        "ARC":55.38,
        "HellaSwag":77.03,
        "MMLU":54.78,
        "TruthfulQA":43.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"afc39cbb0a3451d31442ff3d4a7a2752e3b0b67b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-7b-v1.5-16k",
        "Average":57.78,
        "ARC":54.18,
        "HellaSwag":77.31,
        "MMLU":49.3,
        "TruthfulQA":50.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":44,
        "Available on the hub":true,
        "Model Sha":"9a93d7d11fac7f3f9074510b80092b53bc1a5bec"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"chargoddard\/internlm2-base-7b-llama",
        "Average":57.77,
        "ARC":54.35,
        "HellaSwag":79.47,
        "MMLU":54.05,
        "TruthfulQA":43.23,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.74,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"db8227e5bf55053f8efde4630f1d170c84dd1808"
    },
    {
        "T":"?",
        "Model":"msy127\/mnsim-dpo-peftmerged-2-eos",
        "Average":57.77,
        "ARC":55.63,
        "HellaSwag":77.82,
        "MMLU":51.25,
        "TruthfulQA":46.37,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.16,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"10d1299a6a062f59490588367102cd398c05e2fe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/llama-13b-pretrained",
        "Average":57.77,
        "ARC":56.31,
        "HellaSwag":79.32,
        "MMLU":47.03,
        "TruthfulQA":48.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c28cc0cf5a1a1bf4de96b23d06b02129dca85eb9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jphme\/em_german_leo_mistral",
        "Average":57.77,
        "ARC":52.82,
        "HellaSwag":78.03,
        "MMLU":50.03,
        "TruthfulQA":50.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":61,
        "Available on the hub":false,
        "Model Sha":"aa63a32154923034fb89b1408d3d7ffa994d3327"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nkpz\/llama2-22b-daydreamer-v3",
        "Average":57.76,
        "ARC":56.06,
        "HellaSwag":80.07,
        "MMLU":52.49,
        "TruthfulQA":42.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":21.62,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"e6c74222958328e50712aa00294dc818c24075b2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"prithivida\/Asimov-7B-v2",
        "Average":57.76,
        "ARC":54.27,
        "HellaSwag":78.72,
        "MMLU":52.59,
        "TruthfulQA":45.44,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0aeea2284ac78cac081bee88e5a98a19bb987227"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r4-q_k_v_o",
        "Average":57.75,
        "ARC":58.36,
        "HellaSwag":81.1,
        "MMLU":54.53,
        "TruthfulQA":37.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5cbcd9c0a6b9a19f0d099e653cde18e11bf95303"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"ZySec-AI\/ZySec-7B-v2",
        "Average":57.74,
        "ARC":53.07,
        "HellaSwag":76.3,
        "MMLU":54.55,
        "TruthfulQA":47.05,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9554702bbe26b1d1515e75ccb0b3549096622440"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"aihub-app\/ZySec-8B-v2",
        "Average":57.74,
        "ARC":53.07,
        "HellaSwag":76.3,
        "MMLU":54.55,
        "TruthfulQA":47.05,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.0,
        "Hub":24,
        "Available on the hub":false,
        "Model Sha":"9554702bbe26b1d1515e75ccb0b3549096622440"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/llama-13b-pretrained-dropout",
        "Average":57.73,
        "ARC":56.4,
        "HellaSwag":79.34,
        "MMLU":46.59,
        "TruthfulQA":48.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"045c84727d495bfb4b612a2482ce0d807c067b46"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-13b-2.1",
        "Average":57.72,
        "ARC":55.12,
        "HellaSwag":80.24,
        "MMLU":50.89,
        "TruthfulQA":44.62,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"172e30e56e939f73d7d00a165c2d49cbd284481f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LeoLM\/leo-hessianai-13b",
        "Average":57.72,
        "ARC":57.25,
        "HellaSwag":81.94,
        "MMLU":53.65,
        "TruthfulQA":38.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":27,
        "Available on the hub":true,
        "Model Sha":"a947965cb07ca12a38ff981fe65b618d7dea28d3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/Mistral-7B-length-100000",
        "Average":57.68,
        "ARC":51.71,
        "HellaSwag":78.32,
        "MMLU":55.75,
        "TruthfulQA":44.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"69a99c77648c137a5f898e6dba0a22724c0da825"
    },
    {
        "T":"\u2b55",
        "Model":"LLMs\/Stable-Vicuna-13B",
        "Average":57.68,
        "ARC":53.41,
        "HellaSwag":78.57,
        "MMLU":50.37,
        "TruthfulQA":48.36,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":12.85,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"51f3d9eaa71de287c96195abd0ff954839857b19"
    },
    {
        "T":"?",
        "Model":"davzoku\/frankencria-llama2-12.5b-v1.3-m.2",
        "Average":57.67,
        "ARC":55.03,
        "HellaSwag":79.17,
        "MMLU":46.16,
        "TruthfulQA":50.31,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.4,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b51db8ade612c1fe6979f4075310077b777e787c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"64bits\/LexPodLM-13B",
        "Average":57.67,
        "ARC":57.76,
        "HellaSwag":81.04,
        "MMLU":48.38,
        "TruthfulQA":43.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"3553d84037addc97678f99a3464be4c866a0c268"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lizhuang144\/llama_mirror_13b_v1.0",
        "Average":57.67,
        "ARC":57.59,
        "HellaSwag":80.53,
        "MMLU":48.0,
        "TruthfulQA":44.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"379cb8f080110f3418155029f534f67a79e25db4"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"TheBloke\/stable-vicuna-13B-HF",
        "Average":57.63,
        "ARC":53.33,
        "HellaSwag":78.5,
        "MMLU":50.29,
        "TruthfulQA":48.38,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub":96,
        "Available on the hub":true,
        "Model Sha":"2b099b2be0dafb2606ae9808c0f6183fe4bff7bc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"itsliupeng\/llama2_7b_zh",
        "Average":57.62,
        "ARC":52.05,
        "HellaSwag":74.88,
        "MMLU":60.69,
        "TruthfulQA":42.86,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":5.8,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"410711781d2e24226c0d62959e4990d1de851c3c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/kuchiki-l2-7b",
        "Average":57.6,
        "ARC":54.35,
        "HellaSwag":78.44,
        "MMLU":47.74,
        "TruthfulQA":49.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"745c34e70aa92056e8cd79c1d16e8fcfe1797645"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-deepseek-10b-v17.1-4k",
        "Average":57.6,
        "ARC":54.35,
        "HellaSwag":76.93,
        "MMLU":53.17,
        "TruthfulQA":45.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":10.55,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3d1f7c5136dbf13607d591c66e21e268a6c0c29e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zarablend-1.1-l2-7b",
        "Average":57.58,
        "ARC":54.86,
        "HellaSwag":78.58,
        "MMLU":47.89,
        "TruthfulQA":49.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"e46bfa43829cbea7608192a6d07bcc147387fdb7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Abhaykoul\/vortex2",
        "Average":57.58,
        "ARC":50.68,
        "HellaSwag":76.72,
        "MMLU":47.11,
        "TruthfulQA":55.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7521183a05815492532f1da972657f06912e406c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/kuchiki-1.1-l2-7b",
        "Average":57.57,
        "ARC":54.18,
        "HellaSwag":78.0,
        "MMLU":48.14,
        "TruthfulQA":49.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"10fe70fec0df5c4dcbdfd2e9ec74830c41b3cfd2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zaraxls-l2-7b",
        "Average":57.57,
        "ARC":54.44,
        "HellaSwag":78.94,
        "MMLU":50.39,
        "TruthfulQA":46.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"cc1dad50689b3ebcc1c9c67f275da6b4bb63e2ce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Llama-2-13B-GPTQ",
        "Average":57.56,
        "ARC":57.25,
        "HellaSwag":81.64,
        "MMLU":54.81,
        "TruthfulQA":36.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":2.03,
        "Hub":94,
        "Available on the hub":true,
        "Model Sha":"b7db471d1789802a3a8e3b93cdd66a9f046f17c3"
    },
    {
        "T":"\u2b55",
        "Model":"Kiddyz\/testlm-3",
        "Average":57.56,
        "ARC":53.58,
        "HellaSwag":78.48,
        "MMLU":51.77,
        "TruthfulQA":46.42,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6ba288ac39fc4145144e360a8f2641d6f5a6a33a"
    },
    {
        "T":"\u2b55",
        "Model":"hfl\/chinese-alpaca-2-13b-16k",
        "Average":57.56,
        "ARC":55.03,
        "HellaSwag":77.41,
        "MMLU":51.28,
        "TruthfulQA":46.5,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.97,
        "Hub":28,
        "Available on the hub":true,
        "Model Sha":"ba4536aed022c49bda60e1b56a0dbefc2ea6a30a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/FINETUNE3_TEST4",
        "Average":57.55,
        "ARC":55.63,
        "HellaSwag":81.31,
        "MMLU":52.13,
        "TruthfulQA":41.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5195e87bb34317c5aaf201faa476aae78ecc9f1b"
    },
    {
        "T":"?",
        "Model":"pansophic\/rocket-3B",
        "Average":57.55,
        "ARC":50.6,
        "HellaSwag":76.69,
        "MMLU":47.1,
        "TruthfulQA":55.82,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":2.8,
        "Hub":67,
        "Available on the hub":false,
        "Model Sha":"ddf1caac5a50ff0984f08c9e195eaf952e3b0ca8"
    },
    {
        "T":"?",
        "Model":"amazon\/MistralLite",
        "Average":57.55,
        "ARC":59.56,
        "HellaSwag":81.84,
        "MMLU":50.93,
        "TruthfulQA":37.87,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":417,
        "Available on the hub":false,
        "Model Sha":"23486089ab7ba741b34adc69ab7555885f8abe71"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-7B",
        "Average":57.53,
        "ARC":56.14,
        "HellaSwag":78.6,
        "MMLU":50.35,
        "TruthfulQA":45.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"4f9e95665d95b4c692910190ff77257216e476f1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zarablend-l2-7b",
        "Average":57.51,
        "ARC":54.44,
        "HellaSwag":78.62,
        "MMLU":47.61,
        "TruthfulQA":49.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"8b14e71ae3f52c409a25e1ac98dd05e0bb91eaff"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"maldv\/electric-sheep-7b-alpha",
        "Average":57.5,
        "ARC":54.86,
        "HellaSwag":76.43,
        "MMLU":50.45,
        "TruthfulQA":48.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"995365226f183101014687b9dc33ea0cd8dc1285"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mosaicml\/mpt-30b-instruct",
        "Average":57.49,
        "ARC":58.45,
        "HellaSwag":84.31,
        "MMLU":49.15,
        "TruthfulQA":38.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":29.96,
        "Hub":92,
        "Available on the hub":true,
        "Model Sha":"2abf1163dd8c9b11f07d805c06e6ec90a1f2037e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-PileOfSets-Mk1-llama-13b-merged",
        "Average":57.48,
        "ARC":58.79,
        "HellaSwag":81.79,
        "MMLU":48.12,
        "TruthfulQA":41.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a7e5484df8aceae7800ae9301a3954cf74b527e9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vonjack\/Qwen-LLaMAfied-HFTok-7B-Chat",
        "Average":57.48,
        "ARC":50.51,
        "HellaSwag":83.65,
        "MMLU":51.53,
        "TruthfulQA":44.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.1,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"b8d5c09c83b1ef23668cb9209dbc43c0df2de8ae"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-codellama2-34b-v11.1-bf16",
        "Average":57.48,
        "ARC":50.0,
        "HellaSwag":71.19,
        "MMLU":55.71,
        "TruthfulQA":53.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":33.53,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"1b361b3634bf59913b47c9dad1b138e99833472b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openBuddy\/openbuddy-llama2-34b-v11.1-bf16",
        "Average":57.48,
        "ARC":50.0,
        "HellaSwag":71.19,
        "MMLU":55.71,
        "TruthfulQA":53.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":33.53,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"21ac0d26c0097e5ac5b4a757493574b156da7731"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"oPenBuddy\/openbuddy-llama2-34b-v11.1-bf16",
        "Average":57.48,
        "ARC":50.0,
        "HellaSwag":71.19,
        "MMLU":55.71,
        "TruthfulQA":53.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":33.53,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"21ac0d26c0097e5ac5b4a757493574b156da7731"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-34b-v2.0",
        "Average":57.47,
        "ARC":54.35,
        "HellaSwag":75.65,
        "MMLU":54.67,
        "TruthfulQA":45.21,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"cb81174d72dbe06f8db1c406ef97981532de6f09"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"s3nh\/poorx32124",
        "Average":57.47,
        "ARC":53.16,
        "HellaSwag":73.58,
        "MMLU":52.88,
        "TruthfulQA":50.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6062703b527da6fa7ede85ba17a5fba20524c042"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pleisto\/yuren-13b-chatml",
        "Average":57.44,
        "ARC":53.07,
        "HellaSwag":78.03,
        "MMLU":56.34,
        "TruthfulQA":42.32,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.88,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"d9479c8c554ef335b5fd5b9a2e328de03c35d50e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"l3utterfly\/llama2-7b-layla",
        "Average":57.43,
        "ARC":54.18,
        "HellaSwag":79.34,
        "MMLU":49.7,
        "TruthfulQA":46.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"733016abcd2abee63eb45ed63d2bba14b91da217"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Abhaykoul\/HelpingAI-3B",
        "Average":57.42,
        "ARC":50.6,
        "HellaSwag":76.64,
        "MMLU":46.82,
        "TruthfulQA":55.62,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"945820ba0c6e153601f477d29eb70661f2af58b1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KeyonZeng\/lion-gemma-2b",
        "Average":57.41,
        "ARC":51.11,
        "HellaSwag":73.47,
        "MMLU":57.15,
        "TruthfulQA":47.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"017d3769a05bcc683b76836d9deb156d8509b02c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/Cinder-Phi-2-STEM-2.94B-Test",
        "Average":57.41,
        "ARC":57.08,
        "HellaSwag":72.21,
        "MMLU":53.87,
        "TruthfulQA":46.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.94,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"ba849c8beeea4d6bffa6db6a590451d911df89ab"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"EleutherAI\/llemma_34b",
        "Average":57.4,
        "ARC":55.29,
        "HellaSwag":75.08,
        "MMLU":58.93,
        "TruthfulQA":40.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":74,
        "Available on the hub":true,
        "Model Sha":"08634a81f7bc7343f94d1c82fae461ad9b03e233"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DopeorNope\/LaOT",
        "Average":57.4,
        "ARC":55.63,
        "HellaSwag":78.96,
        "MMLU":50.3,
        "TruthfulQA":44.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"df3a2c77a63a370405c7711b323e7ffa550cdd9e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lajonbot\/vicuna-7b-v1.5-PL-lora_unload",
        "Average":57.4,
        "ARC":53.5,
        "HellaSwag":76.74,
        "MMLU":49.69,
        "TruthfulQA":49.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"92bf763ce7ae0bfe155bfd60190eed64582e5080"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"heegyu\/LIMA-13b-hf",
        "Average":57.39,
        "ARC":57.42,
        "HellaSwag":81.68,
        "MMLU":48.72,
        "TruthfulQA":41.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"98faa74a9b41cbd9033904cd58420705936849eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/MultiLoRA-mmlu",
        "Average":57.38,
        "ARC":52.39,
        "HellaSwag":77.21,
        "MMLU":49.73,
        "TruthfulQA":50.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4797aeaa428a56712db1ab611bc11f02019c4a2c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"frank098\/WizardLM_13B_juniper",
        "Average":57.38,
        "ARC":55.38,
        "HellaSwag":77.2,
        "MMLU":45.46,
        "TruthfulQA":51.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2204970fc0d96b071e2b1b003fbc5c87cfc46840"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-34b-v1.9",
        "Average":57.38,
        "ARC":54.27,
        "HellaSwag":75.2,
        "MMLU":56.12,
        "TruthfulQA":43.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"68aad9f8452b2abf7d5415d48c09bd55d5b7ca05"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ashercn97\/manatee-7b",
        "Average":57.38,
        "ARC":54.52,
        "HellaSwag":78.95,
        "MMLU":49.26,
        "TruthfulQA":46.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"e66094c43ffe6c5b3f4164cd4ba048d3bc422fd0"
    },
    {
        "T":"?",
        "Model":"chavinlo\/gpt4-x-alpaca",
        "Average":57.37,
        "ARC":52.82,
        "HellaSwag":79.59,
        "MMLU":48.19,
        "TruthfulQA":48.88,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":447,
        "Available on the hub":true,
        "Model Sha":"6a571f458cab9a23d14324ec63e0abd1744c8353"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LTC-AI-Labs\/L2-7b-Beluga-WVG-Test",
        "Average":57.37,
        "ARC":53.75,
        "HellaSwag":78.38,
        "MMLU":51.57,
        "TruthfulQA":45.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b90c207e248c0ad541274c2eb5ef76da1181802f"
    },
    {
        "T":"?",
        "Model":"PocketDoc\/Dans-PersonalityEngine-13b",
        "Average":57.36,
        "ARC":58.45,
        "HellaSwag":82.3,
        "MMLU":47.58,
        "TruthfulQA":41.12,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"3b37c31e04419adcc91eddb57f24fd6f9ac91938"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Technoculture\/Medtulu-2x7b",
        "Average":57.36,
        "ARC":54.61,
        "HellaSwag":75.68,
        "MMLU":49.12,
        "TruthfulQA":50.04,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.07,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"76a032af4d8eec7cd9b621c887cdfaa5d99b4cd9"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/WizardLM-1.0-Uncensored-CodeLlama-34b",
        "Average":57.36,
        "ARC":56.4,
        "HellaSwag":75.45,
        "MMLU":54.51,
        "TruthfulQA":43.06,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"3e8df2cf4a4ee1c0b2d079cb7be70024d425ea8c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LTC-AI-Labs\/L2-7b-Base-test-WVG",
        "Average":57.36,
        "ARC":54.27,
        "HellaSwag":77.81,
        "MMLU":51.07,
        "TruthfulQA":46.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2491546f1219c3e9bb1a8cf37fbecf0b299c2177"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Alpacino13b",
        "Average":57.36,
        "ARC":58.53,
        "HellaSwag":81.31,
        "MMLU":47.92,
        "TruthfulQA":41.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":28,
        "Available on the hub":true,
        "Model Sha":"7092a5c8dec649694dd66ff8cfe5452ce52e6a40"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aboros98\/g8s-preview",
        "Average":57.34,
        "ARC":49.74,
        "HellaSwag":72.27,
        "MMLU":54.85,
        "TruthfulQA":52.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"64f2e6fd94015d09f6a0e9e5b791cac76828aa2a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"edor\/Hermes-Platypus2-mini-7B",
        "Average":57.34,
        "ARC":53.75,
        "HellaSwag":79.24,
        "MMLU":47.08,
        "TruthfulQA":49.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"2797c255626b396cc89c416110a4d785aa5cbe25"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lvkaokao\/llama2-7b-hf-chat-lora-v2",
        "Average":57.31,
        "ARC":55.03,
        "HellaSwag":78.81,
        "MMLU":51.35,
        "TruthfulQA":44.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0b8e61d3325cddbad207cbf885c2b5db6a83a059"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"chargoddard\/llama-2-26b-trenchcoat-stack",
        "Average":57.29,
        "ARC":55.03,
        "HellaSwag":79.9,
        "MMLU":53.73,
        "TruthfulQA":40.48,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":25.7,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"075d67c3223f4b379ab7f997c3787cd0630d80f7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-7B-v2.0",
        "Average":57.23,
        "ARC":55.89,
        "HellaSwag":76.66,
        "MMLU":52.05,
        "TruthfulQA":44.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":26,
        "Available on the hub":false,
        "Model Sha":"d18abf633a1274527b1cb00a5ddfd4dc684ba9bc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/Mistral-7B-random-100000",
        "Average":57.23,
        "ARC":53.75,
        "HellaSwag":78.6,
        "MMLU":53.41,
        "TruthfulQA":43.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2059c4f8e796467b35a2bf28df46cf4b99f1a89f"
    },
    {
        "T":"\u2b55",
        "Model":"Kiddyz\/testlm",
        "Average":57.23,
        "ARC":53.5,
        "HellaSwag":75.8,
        "MMLU":51.21,
        "TruthfulQA":48.41,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e00d8c50a007eb1da3fbfb4d5f5a73c1af3aa104"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kiddyz\/testlm-1",
        "Average":57.23,
        "ARC":53.5,
        "HellaSwag":75.8,
        "MMLU":51.21,
        "TruthfulQA":48.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e00d8c50a007eb1da3fbfb4d5f5a73c1af3aa104"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kiddyz\/testlm-1-1",
        "Average":57.23,
        "ARC":53.5,
        "HellaSwag":75.8,
        "MMLU":51.21,
        "TruthfulQA":48.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e00d8c50a007eb1da3fbfb4d5f5a73c1af3aa104"
    },
    {
        "T":"?",
        "Model":"OpenBuddy\/openbuddy-gemma-7b-v19.1-4k",
        "Average":57.22,
        "ARC":55.29,
        "HellaSwag":71.07,
        "MMLU":53.32,
        "TruthfulQA":49.21,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":8.54,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"546da11ac932df3e3792b7b9f98d546754bbc8d5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Technoculture\/MT7Bi-alpha-dpo-v0.2",
        "Average":57.22,
        "ARC":54.69,
        "HellaSwag":75.89,
        "MMLU":52.82,
        "TruthfulQA":45.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"14f5dfce0fc441490450aa3c3935a495ebebd7d3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AllyArc\/llama_allyarc",
        "Average":57.21,
        "ARC":54.35,
        "HellaSwag":78.24,
        "MMLU":48.28,
        "TruthfulQA":47.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f8aa4cd3c52e97c7b8fdc5107f4622318c523975"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KeyonZeng\/lion-gemma-7b-cn-v2",
        "Average":57.21,
        "ARC":51.79,
        "HellaSwag":73.86,
        "MMLU":55.2,
        "TruthfulQA":47.99,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"64327e196a32cf63d6ccc78a686e923f31a8f283"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kiddyz\/testlm2",
        "Average":57.21,
        "ARC":52.99,
        "HellaSwag":75.64,
        "MMLU":51.53,
        "TruthfulQA":48.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9bffd9acfb12b5da1a1dd09825a633f804126dfa"
    },
    {
        "T":"?",
        "Model":"nnethercott\/orca-open_hermes-llava-v1.5-7b-dpo",
        "Average":57.2,
        "ARC":53.07,
        "HellaSwag":77.11,
        "MMLU":51.03,
        "TruthfulQA":47.6,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3e40fa364e9071af5000bc6fe15ae89b0a7da87c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"radm\/Philosophy-Platypus2-13b",
        "Average":57.19,
        "ARC":58.62,
        "HellaSwag":78.52,
        "MMLU":54.3,
        "TruthfulQA":37.34,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fdf66c76a3c55cbac3ec1e8964d993521343a493"
    },
    {
        "T":"?",
        "Model":"Technoculture\/mtor-2x7b",
        "Average":57.17,
        "ARC":55.2,
        "HellaSwag":73.6,
        "MMLU":51.83,
        "TruthfulQA":48.06,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fbc4cdb251bf62307529a45f5ad267eb259761cc"
    },
    {
        "T":"\u2b55",
        "Model":"starmpcc\/Asclepius-Llama2-13B",
        "Average":57.17,
        "ARC":55.89,
        "HellaSwag":79.66,
        "MMLU":52.38,
        "TruthfulQA":40.76,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"579271bebb894d89369205060d151120a217ce81"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hpcai-tech\/Colossal-LLaMA-2-7b-base",
        "Average":57.15,
        "ARC":53.5,
        "HellaSwag":70.5,
        "MMLU":54.4,
        "TruthfulQA":50.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.76,
        "Hub":75,
        "Available on the hub":true,
        "Model Sha":"1f30e4f2037e1e30122667639b8ef37138e85057"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kodonho\/Solar-M-SakuraSolar-Mixed",
        "Average":57.15,
        "ARC":45.9,
        "HellaSwag":58.56,
        "MMLU":64.51,
        "TruthfulQA":59.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":8.99,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9d67378e58c9b6ec96d1712f5313a49b33028629"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aiplanet\/effi-13b",
        "Average":57.14,
        "ARC":53.41,
        "HellaSwag":79.72,
        "MMLU":52.0,
        "TruthfulQA":43.44,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"59c4629545acbc7f4b6f53167df051b67c665bcc"
    },
    {
        "T":"?",
        "Model":"robinsmits\/Qwen1.5-7B-Dutch-Chat-Sft",
        "Average":57.13,
        "ARC":50.68,
        "HellaSwag":73.49,
        "MMLU":60.47,
        "TruthfulQA":43.89,
        "Type":"Unknown",
        "Precision":"4bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cab7fdeb7f88e65e991b58016837a1da80e3dbf9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TigerResearch\/tigerbot-13b-base",
        "Average":57.13,
        "ARC":53.84,
        "HellaSwag":77.05,
        "MMLU":53.57,
        "TruthfulQA":44.06,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"2df5ed76be7eff0962f2d816a64eca1e78e1cbf3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hiyouga\/Baichuan2-7B-Chat-LLaMAfied",
        "Average":57.11,
        "ARC":52.47,
        "HellaSwag":74.04,
        "MMLU":53.88,
        "TruthfulQA":48.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.99,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"da2cd76e2d61bf0247bd67a4f2835319c54a7d62"
    },
    {
        "T":"\u2b55",
        "Model":"Harshvir\/Llama-2-7B-physics",
        "Average":57.09,
        "ARC":52.9,
        "HellaSwag":77.71,
        "MMLU":48.83,
        "TruthfulQA":48.93,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5e66b59c145586266b2351a63f0cf1b4f62f5454"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-ziya-13b",
        "Average":57.08,
        "ARC":55.38,
        "HellaSwag":78.47,
        "MMLU":45.18,
        "TruthfulQA":49.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.89,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"9a21051ae490d2f8ab8b1181c1b45e0412d71a90"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LinkSoul\/Chinese-Llama-2-7b",
        "Average":57.08,
        "ARC":52.99,
        "HellaSwag":75.64,
        "MMLU":50.74,
        "TruthfulQA":48.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":6.61,
        "Hub":250,
        "Available on the hub":true,
        "Model Sha":"72efd71d7f89d9c46008b7a574faf90300ed9ba8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JCX-kcuf\/Llama-2-7b-hf-gpt-4-80k",
        "Average":57.05,
        "ARC":55.55,
        "HellaSwag":77.27,
        "MMLU":46.75,
        "TruthfulQA":48.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2950495cb5a6eaabf9ae8b31887e47faa80c5d3b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-13b_10e5_r2_a256",
        "Average":57.02,
        "ARC":58.02,
        "HellaSwag":80.99,
        "MMLU":52.71,
        "TruthfulQA":36.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"d897ec2c64828ddf05ca0f51c4839a34060b2cef"
    },
    {
        "T":"?",
        "Model":"robinsmits\/Qwen1.5-7B-Dutch-Chat-Dpo",
        "Average":57.02,
        "ARC":50.77,
        "HellaSwag":74.24,
        "MMLU":60.7,
        "TruthfulQA":42.37,
        "Type":"Unknown",
        "Precision":"4bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"014a7afb6a2043c569bb695c2504b344a26ef8cf"
    },
    {
        "T":"\u2b55",
        "Model":"teknium\/OpenHermes-7B",
        "Average":57.02,
        "ARC":56.14,
        "HellaSwag":78.32,
        "MMLU":48.62,
        "TruthfulQA":45.0,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"74edb1ad58d3d517ef46c4e2a31081084ecbc473"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/vicuna-class-shishya-all-hal-13b-ep3",
        "Average":57.02,
        "ARC":48.63,
        "HellaSwag":80.28,
        "MMLU":56.4,
        "TruthfulQA":42.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d773c696778d4f6fe63282d206ed042003346ed1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fangzhaoz\/pearl7B_tuneonGSM8K",
        "Average":57.01,
        "ARC":55.63,
        "HellaSwag":73.31,
        "MMLU":44.95,
        "TruthfulQA":54.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1b5ac00479a05bb39077a6644e78f1d3a93daf93"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/llama2-13b-FINETUNE3_TEST",
        "Average":57.01,
        "ARC":53.67,
        "HellaSwag":79.66,
        "MMLU":54.48,
        "TruthfulQA":40.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"22cea7bf138eb0d6c962812df2b2235290acbee2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openthaigpt\/openthaigpt-1.0.0-beta-13b-chat-hf",
        "Average":56.99,
        "ARC":53.58,
        "HellaSwag":79.09,
        "MMLU":51.13,
        "TruthfulQA":44.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.9,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"e800c7aec39678c5c0f30b0af16cb43800a0d379"
    },
    {
        "T":"?",
        "Model":"InnerI\/InnerIAI-chat-7b-grok",
        "Average":56.98,
        "ARC":52.13,
        "HellaSwag":75.38,
        "MMLU":53.86,
        "TruthfulQA":46.56,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cda5bfe3c6d6f905461fa4d126ed8357261d5e55"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lazycuber\/L2-7b-Orca-WVG-Test",
        "Average":56.98,
        "ARC":54.86,
        "HellaSwag":78.25,
        "MMLU":51.13,
        "TruthfulQA":43.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6073a87872eb36149404bfb7d60e0108074ee1c3"
    },
    {
        "T":"\u2b55",
        "Model":"hywu\/Camelidae-8x7B",
        "Average":56.94,
        "ARC":55.63,
        "HellaSwag":79.18,
        "MMLU":50.1,
        "TruthfulQA":42.86,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"c12485aa7b31943113d992076cc2d79dce2a73a4"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/LLaMa-2-PeanutButter_v10-7B",
        "Average":56.93,
        "ARC":55.29,
        "HellaSwag":81.69,
        "MMLU":46.97,
        "TruthfulQA":43.78,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f98bb987216448aa3aa89e575a7494fae8b68066"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-llama2-13b-v11-bf16",
        "Average":56.92,
        "ARC":52.99,
        "HellaSwag":75.38,
        "MMLU":51.36,
        "TruthfulQA":47.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.88,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"4d4e72c553e9d60fdc208663b0a1c0364caa2f30"
    },
    {
        "T":"?",
        "Model":"dddsaty\/Open_Ko_SOLAR_DPO_Merge_v0.1",
        "Average":56.92,
        "ARC":55.12,
        "HellaSwag":78.18,
        "MMLU":54.19,
        "TruthfulQA":40.17,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"94acda37b62a5f19af558f921c06a296081b3e30"
    },
    {
        "T":"?",
        "Model":"sail\/Sailor-7B-Chat",
        "Average":56.91,
        "ARC":52.3,
        "HellaSwag":75.01,
        "MMLU":56.24,
        "TruthfulQA":44.09,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.72,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"c7bd0a5e9ec309952f4b8187399314d618da8496"
    },
    {
        "T":"?",
        "Model":"ConvexAI\/Pelican-9b-v0.1",
        "Average":56.91,
        "ARC":47.95,
        "HellaSwag":66.22,
        "MMLU":62.85,
        "TruthfulQA":50.61,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":9.86,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"35f2317f2c1b892daf81011b4b46332f59430f70"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"StudentLLM\/Alpagasus-2-13B-QLoRA-pipeline",
        "Average":56.9,
        "ARC":58.28,
        "HellaSwag":80.98,
        "MMLU":54.14,
        "TruthfulQA":34.21,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"86329885e029c1f4fb6ff6b6f3409007158499e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LLMNewbie\/vic_critT_20pr",
        "Average":56.9,
        "ARC":51.62,
        "HellaSwag":76.39,
        "MMLU":47.66,
        "TruthfulQA":51.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"unknown",
        "#Params (B)":10.48,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8999e6a0bafae2926e2e6bf11cf92d32f276b3cc"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"meta-llama\/Llama-2-13b-hf",
        "Average":56.9,
        "ARC":58.11,
        "HellaSwag":80.97,
        "MMLU":54.34,
        "TruthfulQA":34.17,
        "Type":"pretrained",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":342,
        "Available on the hub":false,
        "Model Sha":"db6b8eb1feabb38985fdf785a89895959e944936"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LTC-AI-Labs\/L2-7b-Hermes-WVG-Test",
        "Average":56.88,
        "ARC":54.95,
        "HellaSwag":78.48,
        "MMLU":48.36,
        "TruthfulQA":45.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"eb5b1d65fdf916ca71f89a46eb91175c1c630a57"
    },
    {
        "T":"\u2b55",
        "Model":"krevas\/LDCC-Instruct-Llama-2-ko-13B",
        "Average":56.88,
        "ARC":56.74,
        "HellaSwag":81.57,
        "MMLU":51.2,
        "TruthfulQA":38.0,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"f7860b6062352942eafbc9f0cedc355a9a0884d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LTC-AI-Labs\/L2-7b-Synthia-WVG-Test",
        "Average":56.86,
        "ARC":55.97,
        "HellaSwag":77.89,
        "MMLU":49.48,
        "TruthfulQA":44.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"23ae02efba01c37abe3cff0fedc7d2d9644fe98e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WizardLM\/WizardCoder-Python-34B-V1.0",
        "Average":56.86,
        "ARC":54.35,
        "HellaSwag":75.37,
        "MMLU":50.75,
        "TruthfulQA":46.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":564,
        "Available on the hub":true,
        "Model Sha":"479dc620564c47366014f311342cef128504025d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AlekseyKorshuk\/vic15-exp-syn-fight-cp3838",
        "Average":56.86,
        "ARC":51.79,
        "HellaSwag":75.79,
        "MMLU":50.23,
        "TruthfulQA":49.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"91ce25dbdb67793ad1fcfdfd59f7603c2be65aea"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airoboros-l2-7b-2.2.1",
        "Average":56.85,
        "ARC":55.03,
        "HellaSwag":80.06,
        "MMLU":47.64,
        "TruthfulQA":44.65,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"eafbba6fec094a17ca7bce6d9605cac97b90a483"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Code-290k-13B",
        "Average":56.81,
        "ARC":56.06,
        "HellaSwag":81.55,
        "MMLU":51.99,
        "TruthfulQA":37.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":12.85,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"e2595df2aedc1decaf73d167ce0114e7a9cb2126"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/llama-13b-pretrained-sft-epoch-1",
        "Average":56.8,
        "ARC":57.25,
        "HellaSwag":79.99,
        "MMLU":45.52,
        "TruthfulQA":44.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1f839c019153789c15bbc45ecbb512d0f5015881"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"speechlessai\/speechless-codellama-34b-v1.0",
        "Average":56.8,
        "ARC":52.47,
        "HellaSwag":74.13,
        "MMLU":53.47,
        "TruthfulQA":47.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1d64d871cd56da3031e19bc267ef8bd0b85b9936"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-dolphin-orca-platypus-34b",
        "Average":56.8,
        "ARC":52.47,
        "HellaSwag":74.13,
        "MMLU":53.47,
        "TruthfulQA":47.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"57e18e617b4fd7ab61bd7da8ee9516513ad76842"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"camel-ai\/CAMEL-13B-Role-Playing-Data",
        "Average":56.79,
        "ARC":54.95,
        "HellaSwag":79.25,
        "MMLU":46.61,
        "TruthfulQA":46.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"762ecb0d85572c8f8bcbca06d27f7f64a4d74615"
    },
    {
        "T":"\u2b55",
        "Model":"chinoll\/Yi-6b-200k-dpo",
        "Average":56.78,
        "ARC":43.09,
        "HellaSwag":74.53,
        "MMLU":64.0,
        "TruthfulQA":45.51,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.06,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"925c5fbaeccb321ba8edbde79c3d994adc460a41"
    },
    {
        "T":"\u2b55",
        "Model":"chinoll\/Yi-7b-dpo",
        "Average":56.78,
        "ARC":43.09,
        "HellaSwag":74.53,
        "MMLU":64.0,
        "TruthfulQA":45.51,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.06,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"925c5fbaeccb321ba8edbde79c3d994adc460a41"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-34B-v1.2",
        "Average":56.77,
        "ARC":54.86,
        "HellaSwag":74.33,
        "MMLU":53.2,
        "TruthfulQA":44.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":34.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"42c2e521c1de5f83f2d3f537ceac71ede63e988d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Technoculture\/MT7Bi-alpha-dpo",
        "Average":56.73,
        "ARC":55.03,
        "HellaSwag":75.45,
        "MMLU":52.63,
        "TruthfulQA":43.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"395626f9971b41065264e9c98c4daa53cdf609cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"maximuslee07\/llama-2-7b-rockwell-final",
        "Average":56.73,
        "ARC":52.73,
        "HellaSwag":79.1,
        "MMLU":47.88,
        "TruthfulQA":47.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"de4cfe99e9e3db62733b40f48b2b11faf9abe4bf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"umd-zhou-lab\/recycled-wizardlm-7b-v2.0",
        "Average":56.72,
        "ARC":54.95,
        "HellaSwag":77.85,
        "MMLU":45.79,
        "TruthfulQA":48.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"4a770caf3509b3fdda5ed54735dc40a8f0442c61"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hamxea\/Llama-2-7b-chat-hf-activity-fine-tuned-v4",
        "Average":56.68,
        "ARC":54.35,
        "HellaSwag":78.12,
        "MMLU":48.42,
        "TruthfulQA":45.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c964836b57483ae83e5b7bc1ece1e121a7727a75"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"allenai\/digital-socrates-7b",
        "Average":56.68,
        "ARC":54.44,
        "HellaSwag":75.99,
        "MMLU":51.41,
        "TruthfulQA":44.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"5d26db18b95778c31dc8425871052f495b267563"
    },
    {
        "T":"?",
        "Model":"Cartinoe5930\/TIES-Merging",
        "Average":56.67,
        "ARC":58.11,
        "HellaSwag":75.74,
        "MMLU":51.57,
        "TruthfulQA":41.25,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"eb4d42de1ed4407e83660f2ab03139c1fd03ac02"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JCX-kcuf\/Llama-2-7b-chat-hf-gpt-4-80k",
        "Average":56.65,
        "ARC":54.78,
        "HellaSwag":74.63,
        "MMLU":48.77,
        "TruthfulQA":48.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8a4c270873b82edf9759cc693028035f36600a22"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Severus27\/BeingWell_llama2_7b",
        "Average":56.65,
        "ARC":54.95,
        "HellaSwag":78.27,
        "MMLU":47.46,
        "TruthfulQA":45.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":6.61,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"d1d27f8f822d083cfb018e9550784a29d97b51a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ziqingyang\/chinese-llama-2-13b",
        "Average":56.65,
        "ARC":55.8,
        "HellaSwag":79.53,
        "MMLU":53.01,
        "TruthfulQA":38.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.97,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"484c8a18b02f95eb2b6f6302105cf9a329e76ec8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hamxea\/Llama-2-7b-chat-hf-activity-fine-tuned-v4",
        "Average":56.65,
        "ARC":54.27,
        "HellaSwag":78.1,
        "MMLU":48.44,
        "TruthfulQA":45.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c964836b57483ae83e5b7bc1ece1e121a7727a75"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KeyonZeng\/lion-gemma-7b-cn",
        "Average":56.63,
        "ARC":50.6,
        "HellaSwag":73.21,
        "MMLU":55.72,
        "TruthfulQA":46.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1ad610e1040eb72e99672ad4a1ba3f33c6aa6b1a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Technoculture\/MedMerge-6-7b-alpha-dpo",
        "Average":56.62,
        "ARC":54.27,
        "HellaSwag":75.6,
        "MMLU":52.65,
        "TruthfulQA":43.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"862c90d07e4614122bce660b8e725d142f6526c4"
    },
    {
        "T":"?",
        "Model":"pansophic\/new_model_test3",
        "Average":56.61,
        "ARC":51.79,
        "HellaSwag":78.61,
        "MMLU":49.14,
        "TruthfulQA":46.89,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5fc0394d59ea72784285eeb2252411b88e9b6d9d"
    },
    {
        "T":"\u2b55",
        "Model":"rombodawg\/LosslessMegaCoder-llama2-7b-mini",
        "Average":56.59,
        "ARC":53.5,
        "HellaSwag":77.38,
        "MMLU":49.72,
        "TruthfulQA":45.77,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"186b105d61054611d0b921a55c220d41c6aefe43"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"nnethercott\/llava-v1.5-7b-hf-vicuna",
        "Average":56.57,
        "ARC":52.65,
        "HellaSwag":76.09,
        "MMLU":51.68,
        "TruthfulQA":45.86,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"62343e8b8383aed371cb04aa01aff8a143b82ff9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"nnethercott\/llava-v1.5-7b_vicuna",
        "Average":56.57,
        "ARC":52.65,
        "HellaSwag":76.09,
        "MMLU":51.68,
        "TruthfulQA":45.86,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d33e2a87f99380302bc77ac270630bd7fd23baaf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Changgil\/K2S3-Llama2-13b-v1.0",
        "Average":56.55,
        "ARC":51.45,
        "HellaSwag":78.57,
        "MMLU":50.99,
        "TruthfulQA":45.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.26,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d946d79639945ec467eae0029696c7af39f15c6e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/koala-13B-HF",
        "Average":56.53,
        "ARC":52.99,
        "HellaSwag":77.59,
        "MMLU":45.32,
        "TruthfulQA":50.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":40,
        "Available on the hub":true,
        "Model Sha":"b20f96a0171ce4c0fa27d6048215ebe710521587"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/Cinder-Phi-2-Test-1",
        "Average":56.49,
        "ARC":57.34,
        "HellaSwag":72.6,
        "MMLU":50.81,
        "TruthfulQA":45.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ed772a328dc0461fc3feb7b92fa1f8dcfedfa1cb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Nekochu\/Luminia-13B-v3",
        "Average":56.47,
        "ARC":52.47,
        "HellaSwag":76.08,
        "MMLU":53.6,
        "TruthfulQA":43.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"602563f3af32b3c6be067ad522e6f3eaff4f8627"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-100step-flan-v2",
        "Average":56.44,
        "ARC":53.24,
        "HellaSwag":78.43,
        "MMLU":48.43,
        "TruthfulQA":45.66,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0f1873b505a5f32ca429c164a229bab663eaf617"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-mmlu-merged",
        "Average":56.43,
        "ARC":51.11,
        "HellaSwag":76.75,
        "MMLU":49.39,
        "TruthfulQA":48.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"189304f388007b3be70818d8f9842b4512faf310"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-7b-delta-v1.1",
        "Average":56.43,
        "ARC":53.67,
        "HellaSwag":77.5,
        "MMLU":45.61,
        "TruthfulQA":48.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":197,
        "Available on the hub":true,
        "Model Sha":"24fb8e1e9cc78e0aa7ef154b026c4a83296e3fc4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eachadea\/vicuna-7b-1.1",
        "Average":56.43,
        "ARC":53.67,
        "HellaSwag":77.46,
        "MMLU":45.63,
        "TruthfulQA":48.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":108,
        "Available on the hub":true,
        "Model Sha":"9d8eea215e00b388a22e8f050768ea8911d41f1d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Ejafa\/vicuna_7B_vanilla_1.1",
        "Average":56.43,
        "ARC":53.67,
        "HellaSwag":77.46,
        "MMLU":45.63,
        "TruthfulQA":48.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"d971d788db19648ad16bf77ec3f1de35ebf9a8e0"
    },
    {
        "T":"?",
        "Model":"Aabbhishekk\/llama2-7b-function-calling-slerp",
        "Average":56.4,
        "ARC":55.46,
        "HellaSwag":79.5,
        "MMLU":50.32,
        "TruthfulQA":40.32,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"41e714527afd7d502e853092286c332f69b37c15"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NurtureAI\/MistralLite-11B",
        "Average":56.39,
        "ARC":57.68,
        "HellaSwag":79.54,
        "MMLU":50.09,
        "TruthfulQA":38.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"1a327551e7b2b4fdfbe27fcdb03d1cf5cbffdab4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lvkaokao\/llama2-7b-hf-chat-lora",
        "Average":56.39,
        "ARC":55.72,
        "HellaSwag":78.75,
        "MMLU":47.99,
        "TruthfulQA":43.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e92a1439ac8d2edb5e311b8a42e13ed7c5e70db5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-atom-13b-v9-bf16",
        "Average":56.38,
        "ARC":51.19,
        "HellaSwag":76.0,
        "MMLU":49.67,
        "TruthfulQA":48.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.94,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"35bb2c73953f6ea40be6f0c8c6b2dfa7ecbaa0df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Norquinal\/llama-2-7b-claude-chat-rp",
        "Average":56.38,
        "ARC":54.95,
        "HellaSwag":80.05,
        "MMLU":47.03,
        "TruthfulQA":43.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4309eedebe8ba5709e0cc7cf186cb783f3bc8060"
    },
    {
        "T":"\u2b55",
        "Model":"abhinand\/tamil-llama-13b-instruct-v0.1",
        "Average":56.37,
        "ARC":54.52,
        "HellaSwag":79.35,
        "MMLU":50.37,
        "TruthfulQA":41.22,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.93,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"7d6d6f23f69d1d8806ac21eec7ef8feba63c0e67"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hamxea\/Llama-2-7b-chat-hf-activity-fine-tuned-v3",
        "Average":56.36,
        "ARC":53.33,
        "HellaSwag":78.1,
        "MMLU":48.31,
        "TruthfulQA":45.7,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"471e796a06138051def6777c3742d9e196b56e08"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Danielbrdz\/Barcenas-7b",
        "Average":56.36,
        "ARC":55.12,
        "HellaSwag":77.4,
        "MMLU":49.27,
        "TruthfulQA":43.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"770fa73981a599e935c21a95b1817a553c726694"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-100step-flan",
        "Average":56.35,
        "ARC":52.9,
        "HellaSwag":78.44,
        "MMLU":48.4,
        "TruthfulQA":45.67,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1d502ae9a15c38118baa5ae55e048a080cb05c89"
    },
    {
        "T":"\u2b55",
        "Model":"heegyu\/WizardVicuna2-13b-hf",
        "Average":56.35,
        "ARC":55.38,
        "HellaSwag":79.14,
        "MMLU":48.46,
        "TruthfulQA":42.43,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6cfd95e2dcdb6996afa9eb5c63273a1a3524c6c6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"moondriller\/llama2-13B-eugeneparkthebest",
        "Average":56.34,
        "ARC":53.41,
        "HellaSwag":79.25,
        "MMLU":48.28,
        "TruthfulQA":44.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.16,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8e0740c8f0a61903c563126bc45d0e9a16547742"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"meta-llama\/Llama-2-7b-chat-hf",
        "Average":56.34,
        "ARC":52.9,
        "HellaSwag":78.55,
        "MMLU":48.32,
        "TruthfulQA":45.57,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":1152,
        "Available on the hub":false,
        "Model Sha":"b7701a9e825e79a5ab18b5801be113c2160cc627"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sethuiyer\/Dr_Samantha-7b",
        "Average":56.33,
        "ARC":53.84,
        "HellaSwag":77.95,
        "MMLU":47.94,
        "TruthfulQA":45.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"b1a643e32e467d8dd722186d6c36d16ea4281003"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"yanolja\/EEVE-Korean-2.8B-v1.0",
        "Average":56.32,
        "ARC":57.25,
        "HellaSwag":72.15,
        "MMLU":51.62,
        "TruthfulQA":44.27,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.82,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"94e55e0dcb1cae43ad0ed29ff889b0b8d906ef8f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Locutusque\/Mistral-7B-SFT",
        "Average":56.31,
        "ARC":46.5,
        "HellaSwag":75.69,
        "MMLU":51.04,
        "TruthfulQA":52.02,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"db1c291a7cbab162ebfb9512f9d27a95b42c6548"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dfurman\/llama-2-7b-instruct-peft",
        "Average":56.31,
        "ARC":51.19,
        "HellaSwag":78.92,
        "MMLU":46.63,
        "TruthfulQA":48.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"unknown",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0fc43413117187e0723cdac133068ab527c80fe2"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/LLaMa-2-PeanutButter_v14-7B",
        "Average":56.3,
        "ARC":54.18,
        "HellaSwag":80.38,
        "MMLU":45.97,
        "TruthfulQA":44.68,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"18b55a04e4537ca77b69311e4144984388ae965c"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/LLaMa-2-PeanutButter_v4-7B",
        "Average":56.3,
        "ARC":54.86,
        "HellaSwag":80.78,
        "MMLU":47.24,
        "TruthfulQA":42.31,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"51aef62f2a8baf37156d13f9ca5a29154d694f57"
    },
    {
        "T":"?",
        "Model":"davzoku\/frankencria-llama2-11b-v1.3-m.1",
        "Average":56.3,
        "ARC":52.82,
        "HellaSwag":77.5,
        "MMLU":48.0,
        "TruthfulQA":46.87,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":9.98,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b528d28472ee973165ca216337e66207a0d7c6e3"
    },
    {
        "T":"\u2b55",
        "Model":"davzoku\/cria-llama2-7b-v1.3",
        "Average":56.3,
        "ARC":52.73,
        "HellaSwag":78.58,
        "MMLU":48.3,
        "TruthfulQA":45.58,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"163a5bec7b6f5aaa4667aa6a95746deff50ceab1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-atom-13b-v9-bf16",
        "Average":56.29,
        "ARC":51.19,
        "HellaSwag":75.99,
        "MMLU":49.33,
        "TruthfulQA":48.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.94,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"35bb2c73953f6ea40be6f0c8c6b2dfa7ecbaa0df"
    },
    {
        "T":"?",
        "Model":"TeeZee\/Buttocks-7B-v1.0",
        "Average":56.29,
        "ARC":54.61,
        "HellaSwag":75.61,
        "MMLU":50.22,
        "TruthfulQA":44.72,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"42c97d578b5daab95a71c7c4b007e882fd91e774"
    },
    {
        "T":"?",
        "Model":"TeeZee\/Buttocks-7B-v1.1",
        "Average":56.29,
        "ARC":54.61,
        "HellaSwag":75.61,
        "MMLU":50.22,
        "TruthfulQA":44.72,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"828ccce85f9dd3744a9438f140ffb8c06187d165"
    },
    {
        "T":"\u2b55",
        "Model":"lvkaokao\/llama2-7b-hf-instruction-lora",
        "Average":56.29,
        "ARC":55.38,
        "HellaSwag":78.57,
        "MMLU":49.39,
        "TruthfulQA":41.83,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f660a40323b29040e78097acca320517ed242512"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"defog\/sqlcoder-34b-alpha",
        "Average":56.29,
        "ARC":54.18,
        "HellaSwag":75.93,
        "MMLU":54.42,
        "TruthfulQA":40.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":33.48,
        "Hub":162,
        "Available on the hub":true,
        "Model Sha":"6712da4d486caec81d6b1b650d0596517052cffe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Tap-M\/Luna-AI-Llama2-Uncensored",
        "Average":56.29,
        "ARC":54.35,
        "HellaSwag":78.6,
        "MMLU":46.7,
        "TruthfulQA":45.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":6.61,
        "Hub":102,
        "Available on the hub":true,
        "Model Sha":"6b5e1067e412cc5750aec7415a065671df3618be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/pygmalion-2-7b",
        "Average":56.28,
        "ARC":54.01,
        "HellaSwag":78.23,
        "MMLU":49.11,
        "TruthfulQA":43.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":51,
        "Available on the hub":true,
        "Model Sha":"983f8ad5c156f4a0e4d2b7b5f1146981ad2e8a8b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"1TuanPham\/T-Llama",
        "Average":56.28,
        "ARC":54.18,
        "HellaSwag":76.48,
        "MMLU":47.98,
        "TruthfulQA":46.47,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.85,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"967eb6cc60c7d648d7630e6501a4ba33767231ba"
    },
    {
        "T":"?",
        "Model":"vilm\/Quyen-v0.1",
        "Average":56.28,
        "ARC":48.21,
        "HellaSwag":72.49,
        "MMLU":52.88,
        "TruthfulQA":51.53,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":3.95,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"e171839fa60645d775b1555c86a1ab52e7de82f2"
    },
    {
        "T":"?",
        "Model":"vicgalleorg\/TruthfulQwen1.5-4B",
        "Average":56.27,
        "ARC":47.1,
        "HellaSwag":71.32,
        "MMLU":56.04,
        "TruthfulQA":50.6,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.95,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"e6dc5fbf051ae3be06259b28cea254e6a76d632e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AdaptLLM\/finance-chat",
        "Average":56.26,
        "ARC":53.75,
        "HellaSwag":76.6,
        "MMLU":50.16,
        "TruthfulQA":44.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":50,
        "Available on the hub":true,
        "Model Sha":"42d449dc4f42960a52130893843136ab3fed1256"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"ehartford\/Samantha-1.11-7b",
        "Average":56.26,
        "ARC":55.03,
        "HellaSwag":79.12,
        "MMLU":40.51,
        "TruthfulQA":50.37,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"730cbd8f3077f3d24001aab714def991f1e4e7e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Severian\/ANIMA-Nectar-v3",
        "Average":56.24,
        "ARC":49.49,
        "HellaSwag":75.99,
        "MMLU":53.34,
        "TruthfulQA":46.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8ff9dd66d8cb8fba5c745e5bdb9928c4fc9889e4"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardCoder-Python-34B-V1.0",
        "Average":56.23,
        "ARC":52.13,
        "HellaSwag":74.78,
        "MMLU":49.15,
        "TruthfulQA":48.85,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":564,
        "Available on the hub":true,
        "Model Sha":"5cdc34e4a81d202f1d4a3b5d60e028aab895dfeb"
    },
    {
        "T":"\u2b55",
        "Model":"wang7776\/Llama-2-7b-chat-hf-10-sparsity",
        "Average":56.22,
        "ARC":53.16,
        "HellaSwag":78.26,
        "MMLU":48.18,
        "TruthfulQA":45.29,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9dda6f163ab399b0ae0fd19d6fe8ec37d9ff97be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-temporal-sharegpt",
        "Average":56.22,
        "ARC":53.5,
        "HellaSwag":75.82,
        "MMLU":50.79,
        "TruthfulQA":44.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"dffa41945e5bdac60bd7541ef775642a02189d15"
    },
    {
        "T":"?",
        "Model":"TheBloke\/Manticore-13B-Chat-Pyg-Guanaco-SuperHOT-8K-GPTQ",
        "Average":56.2,
        "ARC":52.82,
        "HellaSwag":79.63,
        "MMLU":39.83,
        "TruthfulQA":52.55,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.03,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"bd3c66e626c81de4977f197e1534bd3dfa2f569d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"haoranxu\/ALMA-13B-Pretrain",
        "Average":56.2,
        "ARC":56.91,
        "HellaSwag":80.15,
        "MMLU":50.31,
        "TruthfulQA":37.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"69e9e12d8bab66dffdcb15fa534fc3f0dc34acec"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"mosaicml\/mpt-30b",
        "Average":56.2,
        "ARC":55.97,
        "HellaSwag":82.42,
        "MMLU":48.0,
        "TruthfulQA":38.42,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":29.96,
        "Hub":309,
        "Available on the hub":true,
        "Model Sha":"0261af71d7177453889f868d26607dec8d5aaa2e"
    },
    {
        "T":"?",
        "Model":"LTC-AI-Labs\/L2-7b-Hermes-Synthia",
        "Average":56.2,
        "ARC":51.02,
        "HellaSwag":79.12,
        "MMLU":47.88,
        "TruthfulQA":46.77,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"6f9bd33be62c4b5dbbb8d76ad30d61c3ceb01641"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/vicuna-class-shishya-ac-hal-13b-ep3",
        "Average":56.18,
        "ARC":48.46,
        "HellaSwag":80.78,
        "MMLU":56.17,
        "TruthfulQA":39.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5a8ccf2bf67a7ee21c3d3accc8a1c5b318677c25"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mikael110\/llama-2-7b-guanaco-fp16",
        "Average":56.18,
        "ARC":54.86,
        "HellaSwag":79.65,
        "MMLU":46.38,
        "TruthfulQA":43.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"f769fed10874af73ad12115efd044cb4a64506b0"
    },
    {
        "T":"?",
        "Model":"wang7776\/Llama-2-7b-chat-hf-20-attention-sparsity",
        "Average":56.17,
        "ARC":53.41,
        "HellaSwag":77.91,
        "MMLU":47.49,
        "TruthfulQA":45.84,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d095cb8c872249e6b324ec25c7e388aa9203e5b4"
    },
    {
        "T":"\u2b55",
        "Model":"garage-bAInd\/Platypus2-7B",
        "Average":56.16,
        "ARC":55.03,
        "HellaSwag":78.77,
        "MMLU":50.17,
        "TruthfulQA":40.67,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"db8fa2a8e5b44fa578e17417db1860eeaef57570"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/LLaMa-2-PeanutButter_v18_B-7B",
        "Average":56.15,
        "ARC":54.61,
        "HellaSwag":81.0,
        "MMLU":47.07,
        "TruthfulQA":41.93,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bc8c239cacf1e3211f05e27be67a74d84c12aea9"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-200step-flan-v2",
        "Average":56.15,
        "ARC":52.65,
        "HellaSwag":78.04,
        "MMLU":48.51,
        "TruthfulQA":45.42,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"35e4747656b719af659625092174f188584934c1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"haoranxu\/ALMA-13B",
        "Average":56.15,
        "ARC":56.83,
        "HellaSwag":80.29,
        "MMLU":49.92,
        "TruthfulQA":37.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub":26,
        "Available on the hub":true,
        "Model Sha":"6798d9501a71b203be0610e640ec92fc08ea8dc6"
    },
    {
        "T":"?",
        "Model":"wang7776\/Llama-2-7b-chat-hf-10-attention-sparsity",
        "Average":56.15,
        "ARC":52.9,
        "HellaSwag":78.18,
        "MMLU":48.1,
        "TruthfulQA":45.4,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"175ab7c54ff9031936cbcd23edfb82420e438252"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-100step-v2",
        "Average":56.14,
        "ARC":52.65,
        "HellaSwag":78.25,
        "MMLU":48.47,
        "TruthfulQA":45.18,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4ee3182f614473f9ea3b6e429b01872bc90e89f1"
    },
    {
        "T":"\u2b55",
        "Model":"garage-bAInd\/Platypus2-7B",
        "Average":56.13,
        "ARC":55.2,
        "HellaSwag":78.84,
        "MMLU":49.83,
        "TruthfulQA":40.64,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"f784afa7887b0738d92ea470797582756f02e630"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FlagAlpha\/Llama2-Chinese-7b-Chat",
        "Average":56.13,
        "ARC":52.39,
        "HellaSwag":77.52,
        "MMLU":47.72,
        "TruthfulQA":46.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":202,
        "Available on the hub":true,
        "Model Sha":"4c3bc725f71898c6a1acd4ea98a2f8d74d1b1b6b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ncsgobubble\/Llama-7B-rollercoaster_v2",
        "Average":56.11,
        "ARC":52.82,
        "HellaSwag":78.22,
        "MMLU":49.8,
        "TruthfulQA":43.62,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b099c0725af7e984a8dd9d4ba2af2230613aa367"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-general-temporal-merged",
        "Average":56.11,
        "ARC":52.47,
        "HellaSwag":75.83,
        "MMLU":49.09,
        "TruthfulQA":47.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2683a990206099178aab7e09cca5f330151c4e79"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-200step-flan",
        "Average":56.1,
        "ARC":52.47,
        "HellaSwag":78.02,
        "MMLU":48.42,
        "TruthfulQA":45.47,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"03550d05aac147dde6d70b7b63f4a1661ecf5cb3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"umd-zhou-lab\/recycled-alpaca-7b-v2.0",
        "Average":56.09,
        "ARC":54.18,
        "HellaSwag":77.98,
        "MMLU":46.79,
        "TruthfulQA":45.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"12ab9aed495d8129856fdc469ce3ec672c94e6a3"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"huggingface\/llama-13b",
        "Average":56.08,
        "ARC":56.23,
        "HellaSwag":80.93,
        "MMLU":47.67,
        "TruthfulQA":39.48,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4022c52fcc7473ce7364bb5ac166195903ea1efb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/oasst-llama-13b-1000-steps",
        "Average":56.07,
        "ARC":58.11,
        "HellaSwag":81.52,
        "MMLU":48.65,
        "TruthfulQA":35.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d2cd599cc40db3370009f45d6caa7e486cb6d31f"
    },
    {
        "T":"?",
        "Model":"Joseph717171\/Tess-10.7B-v2.0",
        "Average":56.06,
        "ARC":55.12,
        "HellaSwag":74.4,
        "MMLU":50.09,
        "TruthfulQA":44.63,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"39dcb3ee299f888a3c3668e0ee4e6532cf1bf161"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jordiclive\/gpt4all-alpaca-oa-codealpaca-lora-13b",
        "Average":56.06,
        "ARC":56.14,
        "HellaSwag":80.93,
        "MMLU":47.66,
        "TruthfulQA":39.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":13.0,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"13443d633eaa5b7e1a90ac9cdb4a4d51b1c8d0d1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"google\/gemma-7b-it",
        "Average":56.05,
        "ARC":51.45,
        "HellaSwag":71.96,
        "MMLU":53.52,
        "TruthfulQA":47.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dec4b13d574762bd36f0a1b75541439bd852b2e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yam-peleg\/gemma-7b-it-experiment",
        "Average":56.05,
        "ARC":51.45,
        "HellaSwag":71.96,
        "MMLU":53.52,
        "TruthfulQA":47.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2fd51857cf72e1cce2dc8e39c016306162ee94b9"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"ContextualAI\/archangel_sft-kto_llama13b",
        "Average":56.05,
        "ARC":56.14,
        "HellaSwag":80.8,
        "MMLU":47.84,
        "TruthfulQA":39.42,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"d596fb0060168006360610d673c2c35edcbbf110"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"huggyllama\/llama-13b",
        "Average":56.04,
        "ARC":56.14,
        "HellaSwag":80.92,
        "MMLU":47.61,
        "TruthfulQA":39.48,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub":114,
        "Available on the hub":true,
        "Model Sha":"bf57045473f207bb1de1ed035ace226f4d9f9bba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Voicelab\/trurl-2-7b",
        "Average":56.03,
        "ARC":53.41,
        "HellaSwag":75.29,
        "MMLU":50.0,
        "TruthfulQA":45.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"e26ca5f157c60fc527170cc04db7fc0ea04ad26f"
    },
    {
        "T":"?",
        "Model":"wang7776\/vicuna-7b-v1.3-attention-sparsity-10",
        "Average":56.02,
        "ARC":52.22,
        "HellaSwag":77.05,
        "MMLU":47.93,
        "TruthfulQA":46.87,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4f7d536f7c880d75aba888699771281b704485e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vikash06\/doctorLLM10k",
        "Average":56.01,
        "ARC":54.95,
        "HellaSwag":79.94,
        "MMLU":44.4,
        "TruthfulQA":44.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"587b0f8bac27e0f316c41649186d93771d043472"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"codellama\/CodeLlama-34b-hf",
        "Average":56.01,
        "ARC":54.1,
        "HellaSwag":75.82,
        "MMLU":55.02,
        "TruthfulQA":39.11,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub":93,
        "Available on the hub":true,
        "Model Sha":"d3e967887d285343b8e239e26c6778c26931a536"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"codellama\/CodeLlama-34b-hf",
        "Average":56.01,
        "ARC":54.18,
        "HellaSwag":75.82,
        "MMLU":54.92,
        "TruthfulQA":39.11,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub":93,
        "Available on the hub":true,
        "Model Sha":"c778b02fdecd4663d2b0a42bfb340fd29969533b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Delcos\/Mistral-Pygmalion-7b",
        "Average":55.99,
        "ARC":54.44,
        "HellaSwag":78.48,
        "MMLU":49.23,
        "TruthfulQA":41.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.61,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"4e5fa9ae7f572b4841b02c3f96d8a3c7a7e59521"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-300step-flan-v2",
        "Average":55.99,
        "ARC":52.56,
        "HellaSwag":77.76,
        "MMLU":48.51,
        "TruthfulQA":45.14,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a2191bd90b04396016b7420dd14675916056f44a"
    },
    {
        "T":"\u2b55",
        "Model":"stabilityai\/japanese-stablelm-instruct-gamma-7b",
        "Average":55.99,
        "ARC":50.68,
        "HellaSwag":78.68,
        "MMLU":54.82,
        "TruthfulQA":39.77,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":45,
        "Available on the hub":false,
        "Model Sha":"044918151c5b3910d12f2e489fb7c60752048e1e"
    },
    {
        "T":"?",
        "Model":"abideen\/gemma-7b-openhermes",
        "Average":55.99,
        "ARC":51.28,
        "HellaSwag":71.93,
        "MMLU":53.56,
        "TruthfulQA":47.18,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":8.54,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"8798db2d228a8d460966f3078fe65db7616ec1dd"
    },
    {
        "T":"\u2b55",
        "Model":"Unbabel\/TowerInstruct-7B-v0.1",
        "Average":55.98,
        "ARC":55.46,
        "HellaSwag":79.0,
        "MMLU":46.88,
        "TruthfulQA":42.59,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.74,
        "Hub":52,
        "Available on the hub":true,
        "Model Sha":"d97a456da8a218425b5171a906a7d9a0c5cd7b2f"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged",
        "Average":55.98,
        "ARC":53.67,
        "HellaSwag":78.21,
        "MMLU":45.9,
        "TruthfulQA":46.13,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1295069e9fef63aed87d36fe108d6c934cb34ded"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Qwen\/Qwen1.5-4B",
        "Average":55.96,
        "ARC":48.46,
        "HellaSwag":71.58,
        "MMLU":56.52,
        "TruthfulQA":47.27,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":3.95,
        "Hub":20,
        "Available on the hub":false,
        "Model Sha":"294dbdee5dacecc52c9cc6ba2dba4084addc7b2c"
    },
    {
        "T":"\u2b55",
        "Model":"wang7776\/Mistral-7B-Instruct-v0.2-sparsity-20",
        "Average":55.96,
        "ARC":52.65,
        "HellaSwag":76.71,
        "MMLU":47.27,
        "TruthfulQA":47.22,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"afbc5381ebc40d33832702045c8b6cd567f6f1f8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NLUHOPOE\/Mistral-7B-loss-100000",
        "Average":55.95,
        "ARC":51.79,
        "HellaSwag":77.16,
        "MMLU":53.94,
        "TruthfulQA":40.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b29a345c5dd34e4e198fa19814c1538da17512c5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TFLai\/llama-13b-4bit-alpaca",
        "Average":55.95,
        "ARC":55.72,
        "HellaSwag":80.88,
        "MMLU":42.42,
        "TruthfulQA":44.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d717be9d77986fb9100597dc78fbbfbde77bc2b1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"stabilityai\/japanese-stablelm-base-gamma-7b",
        "Average":55.94,
        "ARC":50.34,
        "HellaSwag":77.47,
        "MMLU":54.75,
        "TruthfulQA":41.2,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":18,
        "Available on the hub":false,
        "Model Sha":"e1c3840c716485077b688296fefa8e5641249843"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PotatoOff\/HamSter-0.2",
        "Average":55.94,
        "ARC":50.09,
        "HellaSwag":73.65,
        "MMLU":50.39,
        "TruthfulQA":49.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"85cd65a8a1ac1fc2814a06e11640da72db25935a"
    },
    {
        "T":"\u2b55",
        "Model":"gywy\/llama2-13b-chinese-v2",
        "Average":55.93,
        "ARC":53.92,
        "HellaSwag":74.64,
        "MMLU":49.74,
        "TruthfulQA":45.43,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.94,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"8f6b11ca4344ac230d6b55defa4e04e60a39f9b5"
    },
    {
        "T":"?",
        "Model":"declare-lab\/starling-7B",
        "Average":55.93,
        "ARC":51.02,
        "HellaSwag":76.77,
        "MMLU":47.75,
        "TruthfulQA":48.18,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"837b74bb8904dd025a2b5b2df8916800380d7c12"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Shiki-v2-m7",
        "Average":55.92,
        "ARC":47.35,
        "HellaSwag":51.71,
        "MMLU":62.62,
        "TruthfulQA":61.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4a635851e435669456ef761b134f53a6accee259"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"selfrag\/selfrag_llama2_7b",
        "Average":55.92,
        "ARC":51.45,
        "HellaSwag":78.48,
        "MMLU":52.0,
        "TruthfulQA":41.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub":58,
        "Available on the hub":true,
        "Model Sha":"190261383b0779ff66d2f95a73c7ad267d94b820"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/longchat-13b-16k",
        "Average":55.89,
        "ARC":53.58,
        "HellaSwag":77.67,
        "MMLU":45.24,
        "TruthfulQA":47.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":123,
        "Available on the hub":true,
        "Model Sha":"70e2e38b82f1e25d8b90b50fbfc2361123bef45f"
    },
    {
        "T":"?",
        "Model":"KnutJaegersberg\/Deita-4b",
        "Average":55.89,
        "ARC":46.08,
        "HellaSwag":71.81,
        "MMLU":55.46,
        "TruthfulQA":50.23,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":3.95,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"a22c9858867e4015268c63fcb495ef922f95a097"
    },
    {
        "T":"\u2b55",
        "Model":"krevas\/LDCC-Instruct-Llama-2-ko-13B-v2",
        "Average":55.89,
        "ARC":56.4,
        "HellaSwag":81.82,
        "MMLU":45.57,
        "TruthfulQA":39.78,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"0779b43890c83a02fe7696321c95966717945f58"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"wang7776\/Llama-2-7b-chat-hf-20-sparsity",
        "Average":55.88,
        "ARC":52.47,
        "HellaSwag":77.91,
        "MMLU":47.27,
        "TruthfulQA":45.88,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7b44f4902cde1b21b48c87c0379c7aab819436ef"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pansophic\/new_model_test",
        "Average":55.87,
        "ARC":52.56,
        "HellaSwag":73.65,
        "MMLU":46.02,
        "TruthfulQA":51.25,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":2.51,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b80248dbdf3e3d4ee4a8d498afd8a4d96892ff85"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/trurl-2-7b-pl-instruct_unload",
        "Average":55.85,
        "ARC":53.16,
        "HellaSwag":74.64,
        "MMLU":49.89,
        "TruthfulQA":45.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"768d800e4dbe3fc95334f30ca7cd02113d3e3fd3"
    },
    {
        "T":"?",
        "Model":"wang7776\/vicuna-7b-v1.3-attention-sparsity-20",
        "Average":55.84,
        "ARC":52.3,
        "HellaSwag":77.05,
        "MMLU":47.39,
        "TruthfulQA":46.62,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9ffb1e27f2672d68db2b5b2fb08d38e401e7c18d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AdaptLLM\/law-chat",
        "Average":55.84,
        "ARC":53.41,
        "HellaSwag":76.16,
        "MMLU":50.24,
        "TruthfulQA":43.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"0bf36fdc22bf30632cced8044667d3d46061d619"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-441step-flan-v2",
        "Average":55.83,
        "ARC":52.13,
        "HellaSwag":77.63,
        "MMLU":48.52,
        "TruthfulQA":45.02,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"daede60607179be05b5d6e90b4c6777806b10fb8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AdaptLLM\/medicine-chat",
        "Average":55.83,
        "ARC":53.75,
        "HellaSwag":76.11,
        "MMLU":49.98,
        "TruthfulQA":43.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":27,
        "Available on the hub":true,
        "Model Sha":"32824ba93e88ccfe8464f6d267a5d67024c7722b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WizardLM\/WizardMath-7B-V1.0",
        "Average":55.82,
        "ARC":54.1,
        "HellaSwag":79.55,
        "MMLU":45.97,
        "TruthfulQA":43.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":29,
        "Available on the hub":true,
        "Model Sha":"06dbd3e0da08255c575e585cb82e0554c1d2707a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wang7776\/vicuna-7b-v1.3-sparsity-10",
        "Average":55.81,
        "ARC":51.45,
        "HellaSwag":76.98,
        "MMLU":47.95,
        "TruthfulQA":46.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"134ecba330ed973ff11f87aefaa3b2e19af024b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Norquinal\/llama-2-7b-claude-chat",
        "Average":55.81,
        "ARC":54.44,
        "HellaSwag":80.66,
        "MMLU":46.74,
        "TruthfulQA":41.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e65d34ed31cdcd2637f6284aa0605f30ef5a9381"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/google-gemma-7b-it-dpo-v1",
        "Average":55.8,
        "ARC":51.54,
        "HellaSwag":71.58,
        "MMLU":53.24,
        "TruthfulQA":46.85,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3d6f29fb83a38b1ec0fbfb69af1b345ac657f7cb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-chat-hf-dpo",
        "Average":55.8,
        "ARC":53.67,
        "HellaSwag":78.79,
        "MMLU":46.78,
        "TruthfulQA":43.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ec98429034fc84a4555dd4e3db4d6af534a03832"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NewstaR\/Koss-7B-chat",
        "Average":55.79,
        "ARC":53.67,
        "HellaSwag":78.79,
        "MMLU":46.72,
        "TruthfulQA":43.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b1ab836d9ebf7029fafa07949b51d3838501d537"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/starchat-beta",
        "Average":55.78,
        "ARC":52.47,
        "HellaSwag":80.59,
        "MMLU":42.85,
        "TruthfulQA":47.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":15.52,
        "Hub":209,
        "Available on the hub":true,
        "Model Sha":"b1bcda690655777373f57ea6614eb095ec2c886f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"namirocks\/mistral-tutor-model-7b-ep3",
        "Average":55.76,
        "ARC":49.32,
        "HellaSwag":76.93,
        "MMLU":49.07,
        "TruthfulQA":47.73,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"17fd803a0b6a2d94e0467882728b4df35c294abb"
    },
    {
        "T":"\u2b55",
        "Model":"guardrail\/llama-2-7b-guanaco-instruct-sharded",
        "Average":55.76,
        "ARC":53.75,
        "HellaSwag":78.69,
        "MMLU":46.65,
        "TruthfulQA":43.93,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"fc7a3abbc3b9a9b3e163ef3c4844307ac270fca7"
    },
    {
        "T":"?",
        "Model":"TheSkullery\/Aurora_22e_Test",
        "Average":55.73,
        "ARC":44.8,
        "HellaSwag":64.97,
        "MMLU":62.27,
        "TruthfulQA":50.91,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"fc3d8910521ddb84053f0be970467b30da2cbe9d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-timedial-unit-080091",
        "Average":55.73,
        "ARC":52.82,
        "HellaSwag":76.1,
        "MMLU":50.58,
        "TruthfulQA":43.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ae7e0fb58f4201bb14fd4e641d0d6dcc22674e0e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-timedial-unit-080082",
        "Average":55.72,
        "ARC":52.82,
        "HellaSwag":76.07,
        "MMLU":50.47,
        "TruthfulQA":43.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"372c90543ebb2a317fb9b51ff3890cc270e5ce3a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"joehuangx\/spatial-vicuna-7b-v1.5-LoRA",
        "Average":55.72,
        "ARC":50.77,
        "HellaSwag":74.63,
        "MMLU":48.13,
        "TruthfulQA":49.36,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dc71924cfb214b91461d35178e6ea6fef7946f13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Telugu-LLM-Labs\/Telugu-Llama2-7B-v0-Instruct",
        "Average":55.71,
        "ARC":53.41,
        "HellaSwag":78.35,
        "MMLU":47.79,
        "TruthfulQA":43.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"b8e2895810d82fb82a811f452b2e53fef949718c"
    },
    {
        "T":"?",
        "Model":"Telugu-LLM-Labs\/Telugu-Llama2-7B-v0-Instruct",
        "Average":55.7,
        "ARC":53.58,
        "HellaSwag":78.33,
        "MMLU":47.63,
        "TruthfulQA":43.26,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"b8e2895810d82fb82a811f452b2e53fef949718c"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-200step-merged",
        "Average":55.67,
        "ARC":52.05,
        "HellaSwag":77.38,
        "MMLU":48.65,
        "TruthfulQA":44.6,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"858de1c14854e55d5141b8d1b3954b335044669e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ceadar-ie\/FinanceConnect-13B",
        "Average":55.65,
        "ARC":55.12,
        "HellaSwag":77.73,
        "MMLU":52.08,
        "TruthfulQA":37.68,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"apache-2.0",
        "#Params (B)":13.02,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"9ed6c7154cd14d1a5cdbec603a3ae8c8ce05cb29"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"conceptofmind\/LLongMA-2-13b-16k",
        "Average":55.64,
        "ARC":54.27,
        "HellaSwag":79.63,
        "MMLU":50.97,
        "TruthfulQA":37.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":55,
        "Available on the hub":false,
        "Model Sha":"c2defe28e2f3f10460baf8f778b00986a53aa7a2"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/LLaMa-2-PeanutButter_v37_SFT-R1-DPO-R2-7B",
        "Average":55.63,
        "ARC":54.1,
        "HellaSwag":79.1,
        "MMLU":47.32,
        "TruthfulQA":42.0,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7dbaa1eea3964e3218ed1788fc04a30e058d3daf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jb723\/llama2-ko-7B-model",
        "Average":55.63,
        "ARC":56.31,
        "HellaSwag":79.51,
        "MMLU":45.71,
        "TruthfulQA":40.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.67,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"03d23910fa0f9b0542ce7634cbcd36983321f55a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"tianyil1\/denas-llama2",
        "Average":55.62,
        "ARC":53.92,
        "HellaSwag":77.83,
        "MMLU":45.5,
        "TruthfulQA":45.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b8aebc9157c0e427536aeac9132021fd66615702"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-7b-v1.3",
        "Average":55.62,
        "ARC":50.43,
        "HellaSwag":76.92,
        "MMLU":48.14,
        "TruthfulQA":47.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":110,
        "Available on the hub":true,
        "Model Sha":"ac066c83424c4a7221aa10c0ebe074b24d3bcdb6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"conceptofmind\/LLongMA-2-13b-16k",
        "Average":55.61,
        "ARC":54.27,
        "HellaSwag":79.66,
        "MMLU":50.86,
        "TruthfulQA":37.68,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":55,
        "Available on the hub":false,
        "Model Sha":"c2defe28e2f3f10460baf8f778b00986a53aa7a2"
    },
    {
        "T":"?",
        "Model":"wang7776\/Llama-2-7b-chat-hf-30-attention-sparsity",
        "Average":55.58,
        "ARC":53.41,
        "HellaSwag":76.87,
        "MMLU":47.04,
        "TruthfulQA":45.02,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"06031965747c6a43923b2a84555ceba5f6d2aecc"
    },
    {
        "T":"?",
        "Model":"openchat\/opencoderplus",
        "Average":55.57,
        "ARC":50.6,
        "HellaSwag":78.22,
        "MMLU":42.73,
        "TruthfulQA":50.72,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":15.52,
        "Hub":100,
        "Available on the hub":true,
        "Model Sha":"845e9e4452dd4440760b3d5f680400fc014e91b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"revolutionarybukhari\/Llama-2-7b-chat-finetune-AUTOMATE",
        "Average":55.55,
        "ARC":53.07,
        "HellaSwag":75.59,
        "MMLU":48.8,
        "TruthfulQA":44.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"55862462a23ab43fb73d4c784f1518ab4645764c"
    },
    {
        "T":"?",
        "Model":"Replete-AI\/Phi-5B-Test",
        "Average":55.54,
        "ARC":54.61,
        "HellaSwag":67.6,
        "MMLU":54.31,
        "TruthfulQA":45.66,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":5.3,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"11cfbf1032e08ec542eb6ab577d17ac18039b731"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/scarlett-7b",
        "Average":55.52,
        "ARC":57.17,
        "HellaSwag":80.27,
        "MMLU":36.11,
        "TruthfulQA":48.52,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"0715b738e750830ba7213f26fe32fa1cc1bb15b3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"deepseek-ai\/deepseek-math-7b-instruct",
        "Average":55.51,
        "ARC":53.58,
        "HellaSwag":71.53,
        "MMLU":56.55,
        "TruthfulQA":40.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.49,
        "Hub":58,
        "Available on the hub":true,
        "Model Sha":"0a5828f800a36df0fd7f0ed581b983246c0677ff"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hongzoh\/Yi-6B_Open-Platypus-v2",
        "Average":55.51,
        "ARC":49.91,
        "HellaSwag":72.18,
        "MMLU":57.59,
        "TruthfulQA":42.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.06,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7844a6dbde22616af0f0221d7f26af03ae6e39f1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lorinma\/yi6B_Vicuna",
        "Average":55.5,
        "ARC":46.16,
        "HellaSwag":69.3,
        "MMLU":58.43,
        "TruthfulQA":48.11,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":6.06,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4ba7237cc904a14240f426154dc5233ef47db9e4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dhmeltzer\/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged",
        "Average":55.46,
        "ARC":53.75,
        "HellaSwag":78.76,
        "MMLU":46.02,
        "TruthfulQA":43.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6ba5416f618ed3e11b409326e84c36fa542f0951"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Uncensored-Jordan-7B",
        "Average":55.46,
        "ARC":51.28,
        "HellaSwag":77.37,
        "MMLU":45.69,
        "TruthfulQA":47.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.61,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"96a9fbe5aaef8410a8d0dad25f3cc97b408c4efb"
    },
    {
        "T":"?",
        "Model":"TheSkullery\/Aurora-V2-DLEC",
        "Average":55.46,
        "ARC":47.7,
        "HellaSwag":69.46,
        "MMLU":52.68,
        "TruthfulQA":51.99,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"af67dd0bf327bbf687a024d47095bc9dac6f6b58"
    },
    {
        "T":"\u2b55",
        "Model":"psyche\/kollama2-7b-v2",
        "Average":55.45,
        "ARC":53.33,
        "HellaSwag":78.5,
        "MMLU":43.61,
        "TruthfulQA":46.37,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"d5b6e9d5b882d4f6ba322396e027925ed915f848"
    },
    {
        "T":"\u2b55",
        "Model":"heegyu\/LIMA2-7b-hf",
        "Average":55.45,
        "ARC":53.24,
        "HellaSwag":80.6,
        "MMLU":43.22,
        "TruthfulQA":44.74,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6a1aa59cb7624f059728840ce68b20b1070ebdcb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xriminact\/TarsChattyBasev0.2",
        "Average":55.45,
        "ARC":52.22,
        "HellaSwag":77.78,
        "MMLU":47.99,
        "TruthfulQA":43.79,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"91d90f5feb9c01d8279ed891c72e225356a4ca97"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/vicuna-mmlu-val-mcq-7b-ep2",
        "Average":55.44,
        "ARC":53.33,
        "HellaSwag":77.73,
        "MMLU":46.85,
        "TruthfulQA":43.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a6e6639ddaed9b2a8a549424f8c8a2d2bca241d4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/vicuna-mmlu-val-only-correct-mcq-7b-ep2",
        "Average":55.44,
        "ARC":52.99,
        "HellaSwag":77.67,
        "MMLU":47.92,
        "TruthfulQA":43.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f0606bca9bea0afdd1dd8c26f0664b65f4dc5967"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"dhmeltzer\/llama-7b-SFT-qlora-eli5-wiki_DPO_ds_RM_top_2_1024_r_64_alpha_16",
        "Average":55.42,
        "ARC":54.1,
        "HellaSwag":78.74,
        "MMLU":45.44,
        "TruthfulQA":43.4,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f1f3b9fdb1e2d8d8fa913d57a8fe15d7bdf72c20"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vikash06\/doctorLLM",
        "Average":55.41,
        "ARC":52.9,
        "HellaSwag":79.76,
        "MMLU":46.47,
        "TruthfulQA":42.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.74,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"0c13f4e15ee967c85643bf6c72d673798ea0b7a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LTC-AI-Labs\/Guanaco-Vicuna-7B-L2",
        "Average":55.41,
        "ARC":53.24,
        "HellaSwag":78.89,
        "MMLU":46.77,
        "TruthfulQA":42.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ba8e755feab0bbf90675dcb9f8875a42f92112a5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LTC-AI-Labs\/L2-7b-Base-WVG-Uncensored",
        "Average":55.4,
        "ARC":53.24,
        "HellaSwag":79.13,
        "MMLU":46.65,
        "TruthfulQA":42.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"67ede9be6ceffdf574294351cca937d88d7d448d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishek\/llama2guanacotest",
        "Average":55.39,
        "ARC":51.62,
        "HellaSwag":77.55,
        "MMLU":48.49,
        "TruthfulQA":43.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"679d17809939a0bf9b79bbb027898cbea64045b2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-7b-2.1",
        "Average":55.38,
        "ARC":54.44,
        "HellaSwag":78.68,
        "MMLU":44.45,
        "TruthfulQA":43.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"699491e2e73cc2936205db143f59c1a686b88f14"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-200step-v2",
        "Average":55.36,
        "ARC":51.79,
        "HellaSwag":77.41,
        "MMLU":48.55,
        "TruthfulQA":43.69,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a3575a542e1dc3db4a7794b8f36b104c93b39875"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhinand\/tamil-llama-13b-base-v0.1",
        "Average":55.34,
        "ARC":52.82,
        "HellaSwag":79.95,
        "MMLU":52.05,
        "TruthfulQA":36.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.93,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6cbdb6b6088910459cd104b1752177ab52e7f892"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"llama-anon\/instruct-13b",
        "Average":55.32,
        "ARC":56.14,
        "HellaSwag":80.27,
        "MMLU":47.89,
        "TruthfulQA":36.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":12.85,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"142e198df473fd0cd4370b0d50be5f57e1da399b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"922-CA\/monika-ddlc-7b-v1",
        "Average":55.32,
        "ARC":54.95,
        "HellaSwag":76.78,
        "MMLU":45.61,
        "TruthfulQA":43.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"4157d696bb0015da3ba26a58c1d24925515e4125"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-timedial",
        "Average":55.31,
        "ARC":52.9,
        "HellaSwag":76.29,
        "MMLU":50.47,
        "TruthfulQA":41.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1e1709818cca48af4cd31c07c493f996854aa10f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/vicuna-7B-physics",
        "Average":55.31,
        "ARC":49.49,
        "HellaSwag":75.88,
        "MMLU":46.58,
        "TruthfulQA":49.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2147983e9493347c3424c07403f65e7a81c0b19f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"willnguyen\/lacda-2-7B-chat-v0.1",
        "Average":55.31,
        "ARC":53.07,
        "HellaSwag":77.57,
        "MMLU":46.03,
        "TruthfulQA":44.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"afca346816726b83e331bb4d93246ed5146e1675"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/T1C",
        "Average":55.31,
        "ARC":50.17,
        "HellaSwag":72.21,
        "MMLU":56.34,
        "TruthfulQA":42.52,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1b1185ddc427341df12dd1aa8c68090fde16b5f3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JosephusCheung\/Pwen-VL-Chat-20_30",
        "Average":55.31,
        "ARC":50.17,
        "HellaSwag":72.21,
        "MMLU":56.34,
        "TruthfulQA":42.52,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"gpl-3.0",
        "#Params (B)":7.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"64a9b89fb18140fc1af1f11471dc9fe34ebc7446"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/Llama-2-7b-hf-instruct-pl-lora_unload",
        "Average":55.31,
        "ARC":53.75,
        "HellaSwag":78.34,
        "MMLU":46.8,
        "TruthfulQA":42.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3dfef350be9c8ce92c2d314dbe96a002bd6ca97d"
    },
    {
        "T":"?",
        "Model":"Azure99\/blossom-v5-4b",
        "Average":55.29,
        "ARC":46.76,
        "HellaSwag":71.87,
        "MMLU":55.04,
        "TruthfulQA":47.51,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.95,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"94ac52d3ed32f070362d6005f5e57d8ffc5e81a5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-RetroRodeo-13b",
        "Average":55.28,
        "ARC":53.84,
        "HellaSwag":79.63,
        "MMLU":48.93,
        "TruthfulQA":38.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"102f9fdad903f5eaffe1ed8173ae56081072e429"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewqr2130\/llama_ppo_1e6_new_tokenizerstep_8000",
        "Average":55.28,
        "ARC":54.78,
        "HellaSwag":78.64,
        "MMLU":46.63,
        "TruthfulQA":41.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"04de12f4c9f1fdf9fd4cd0d71dac8bc169813c63"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewqr2130\/llama_ppo_1e6step_4000",
        "Average":55.27,
        "ARC":54.44,
        "HellaSwag":78.66,
        "MMLU":46.74,
        "TruthfulQA":41.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4fa307bac86077a73c3b1a19be4dd12c4d709fae"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewqr2130\/llama_sft_longer",
        "Average":55.26,
        "ARC":54.78,
        "HellaSwag":78.58,
        "MMLU":46.87,
        "TruthfulQA":40.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"cf2d1d1b306395ad3ae92484dc951ade09fb698c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"deepseek-ai\/deepseek-math-7b-instruct",
        "Average":55.26,
        "ARC":53.41,
        "HellaSwag":71.5,
        "MMLU":55.97,
        "TruthfulQA":40.16,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":6.49,
        "Hub":58,
        "Available on the hub":true,
        "Model Sha":"0a5828f800a36df0fd7f0ed581b983246c0677ff"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"sail\/Sailor-7B",
        "Average":55.25,
        "ARC":49.83,
        "HellaSwag":76.21,
        "MMLU":54.84,
        "TruthfulQA":40.12,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.72,
        "Hub":21,
        "Available on the hub":false,
        "Model Sha":"f8a0533c4818d021a7dbf985b9779d0a640bae6b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elliotthwang\/Elliott-Chinese-LLaMa-GPTQ",
        "Average":55.23,
        "ARC":51.02,
        "HellaSwag":75.23,
        "MMLU":49.58,
        "TruthfulQA":45.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"bbbca62bb340b4ae0a19ba93dae38fc9f9787c16"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lazycuber\/L2-7b-Base-Guanaco-Uncensored",
        "Average":55.22,
        "ARC":52.22,
        "HellaSwag":79.08,
        "MMLU":46.63,
        "TruthfulQA":42.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"dd51a3b26ad378e2953c947a1e4c2f8febe0cb52"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shibing624\/chinese-alpaca-plus-13b-hf",
        "Average":55.2,
        "ARC":53.16,
        "HellaSwag":73.51,
        "MMLU":48.81,
        "TruthfulQA":45.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.94,
        "Hub":31,
        "Available on the hub":true,
        "Model Sha":"a118d2c35573b9a70c6f5b56fba4b657f74ce00c"
    },
    {
        "T":"\u2b55",
        "Model":"davzoku\/cria-llama2-7b-v1.3_peft",
        "Average":55.2,
        "ARC":51.45,
        "HellaSwag":77.35,
        "MMLU":46.47,
        "TruthfulQA":45.52,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6864fa8ee43fa4d6b4f3ae055bbf464a5dcca570"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sail\/Sailor-7B",
        "Average":55.19,
        "ARC":49.83,
        "HellaSwag":76.21,
        "MMLU":54.65,
        "TruthfulQA":40.08,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.72,
        "Hub":21,
        "Available on the hub":false,
        "Model Sha":"f8a0533c4818d021a7dbf985b9779d0a640bae6b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elliotthwang\/elliott_Llama-2-7b-hf",
        "Average":55.17,
        "ARC":53.16,
        "HellaSwag":78.33,
        "MMLU":47.09,
        "TruthfulQA":42.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ac5d22e14c2c7a400519da5d12d88e4fe683ccfa"
    },
    {
        "T":"\u2b55",
        "Model":"LeoLM\/leo-hessianai-7b-chat",
        "Average":55.16,
        "ARC":52.56,
        "HellaSwag":77.61,
        "MMLU":45.58,
        "TruthfulQA":44.89,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"7c343a501f5cd3b768d2f78d9941b760fd66815d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"tlphams\/zoyllm-7b-slimorca",
        "Average":55.16,
        "ARC":50.6,
        "HellaSwag":72.12,
        "MMLU":48.78,
        "TruthfulQA":49.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4b49caa2c42b3e8757f986624b047dab485ee26f"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"haoranxu\/ALMA-13B-R",
        "Average":55.15,
        "ARC":55.55,
        "HellaSwag":79.45,
        "MMLU":49.52,
        "TruthfulQA":36.09,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":13.02,
        "Hub":63,
        "Available on the hub":true,
        "Model Sha":"f0a3613c5da62cbe85fb90ea348932ddfc022b22"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"quantumaikr\/llama-2-7b-hf-guanaco-1k",
        "Average":55.15,
        "ARC":51.62,
        "HellaSwag":76.73,
        "MMLU":47.45,
        "TruthfulQA":44.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"bdb57c5c992872ced47f48cb2177a5fa159f926a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/Llama-2-7B-32K-Instruct",
        "Average":55.15,
        "ARC":51.11,
        "HellaSwag":78.51,
        "MMLU":46.11,
        "TruthfulQA":44.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":78,
        "Available on the hub":true,
        "Model Sha":"35696b9a7ab330dcbe240ff76fb44ab1eccf45bf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sia-ai\/llama-2-7b-1-percent-open-orca-1000-steps-v0",
        "Average":55.14,
        "ARC":51.28,
        "HellaSwag":78.75,
        "MMLU":44.68,
        "TruthfulQA":45.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a893ebef4b818de1968dd9e932da2f513d16386a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama2-13b-pretrain",
        "Average":55.13,
        "ARC":53.92,
        "HellaSwag":79.1,
        "MMLU":51.25,
        "TruthfulQA":36.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.97,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f87d66f9c4541c575a6fad3c19a31b11568e0dfb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JCX-kcuf\/Llama-2-7b-chat-hf-gpt-4-80k-base_lora",
        "Average":55.12,
        "ARC":52.56,
        "HellaSwag":71.37,
        "MMLU":48.34,
        "TruthfulQA":48.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d5109138b21f252053aebbd2c469fdcab6116b93"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rufjdk5480\/llama-7b-ludwig-alpaca",
        "Average":55.11,
        "ARC":54.01,
        "HellaSwag":78.73,
        "MMLU":45.8,
        "TruthfulQA":41.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7928584c0329c3ed88915a823033908be90ba657"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"klyang\/MentaLLaMA-chat-7B",
        "Average":55.11,
        "ARC":52.82,
        "HellaSwag":76.1,
        "MMLU":47.51,
        "TruthfulQA":44.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"eb0b119279aada6404042c69763aaadb5be5000d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-hf-guanaco",
        "Average":55.11,
        "ARC":52.47,
        "HellaSwag":78.75,
        "MMLU":45.33,
        "TruthfulQA":43.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"6c1fc95e67b11f1011a3b2fc1aa05c7b83251e40"
    },
    {
        "T":"\u2b55",
        "Model":"togethercomputer\/Llama-2-7B-32K-Instruct",
        "Average":55.09,
        "ARC":51.37,
        "HellaSwag":78.47,
        "MMLU":45.53,
        "TruthfulQA":45.01,
        "Type":"instruction-tuned",
        "Precision":"8bit",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":78,
        "Available on the hub":true,
        "Model Sha":"b050a6f17d46e32c4b90a30492f14746589f74b7"
    },
    {
        "T":"?",
        "Model":"wang7776\/vicuna-7b-v1.3-attention-sparsity-30",
        "Average":55.08,
        "ARC":51.02,
        "HellaSwag":76.41,
        "MMLU":46.83,
        "TruthfulQA":46.06,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"df0a01a17930aed8715cc6ab508dffdbb15ee240"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elliotthwang\/Elliott-Chinese-LLaMa-GPTQ-V2.0",
        "Average":55.06,
        "ARC":50.77,
        "HellaSwag":75.36,
        "MMLU":49.41,
        "TruthfulQA":44.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ebffe57ba6cc70b60ff5295889abc62d91eeb4dd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lajonbot\/Llama-2-7b-chat-hf-instruct-pl-lora_unload",
        "Average":55.05,
        "ARC":52.99,
        "HellaSwag":77.49,
        "MMLU":47.12,
        "TruthfulQA":42.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f838fda8d2b97effae1e8af4dbb6217eab14fb7e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-7b-gpt4-1.4.1",
        "Average":55.04,
        "ARC":55.12,
        "HellaSwag":79.6,
        "MMLU":45.17,
        "TruthfulQA":40.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"77bdd1f049f27876c38b68782fc240518208f391"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wahaha1987\/llama_7b_sharegpt94k_fastchat",
        "Average":55.04,
        "ARC":53.24,
        "HellaSwag":76.94,
        "MMLU":44.64,
        "TruthfulQA":45.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"2d82abff150b7a5ae484f9cd7c64c72fd4eaf7f5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elliotthwang\/Elliott-Chinese-LLaMa-GPTQ-V1.0",
        "Average":55.02,
        "ARC":50.68,
        "HellaSwag":75.36,
        "MMLU":49.33,
        "TruthfulQA":44.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"01305dc473ba231519fe71e7f4b2d1e3f6aa9bc8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/vicuna-7B-chemical",
        "Average":55.01,
        "ARC":49.83,
        "HellaSwag":74.42,
        "MMLU":44.1,
        "TruthfulQA":51.7,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"fbf6476ebfa856ffe743e41f8d4413c15b2127c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vikash06\/doctorLLM5k",
        "Average":54.99,
        "ARC":52.47,
        "HellaSwag":79.66,
        "MMLU":44.68,
        "TruthfulQA":43.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e7b7ac695d46be0832e404a932f6e9a60159333a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lazycuber\/L2-7b-Guanaco-Uncensored",
        "Average":54.98,
        "ARC":50.6,
        "HellaSwag":76.99,
        "MMLU":48.93,
        "TruthfulQA":43.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9d49378c69c00113cf7f6e66d1ddb9d9b003dddc"
    },
    {
        "T":"\u2b55",
        "Model":"martyn\/mistral-megamerge-dare-7b",
        "Average":54.98,
        "ARC":55.29,
        "HellaSwag":70.48,
        "MMLU":43.05,
        "TruthfulQA":51.08,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f116230ee12e55d1716b89e1b114dd2ee3d397bd"
    },
    {
        "T":"\u2b55",
        "Model":"TinyPixel\/llama2-7b-oa",
        "Average":54.97,
        "ARC":53.41,
        "HellaSwag":78.72,
        "MMLU":46.68,
        "TruthfulQA":41.06,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f346cbe795a2dadb6da0b40d70afd4976bcae90e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BelalTab\/finetuned-llama2-2048-v3.0",
        "Average":54.95,
        "ARC":49.83,
        "HellaSwag":77.09,
        "MMLU":46.69,
        "TruthfulQA":46.21,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"474902c7d83bd67edbb732502e0fe4cf93ed1fc8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chaoyi-wu\/MedLLaMA_13B",
        "Average":54.93,
        "ARC":54.27,
        "HellaSwag":78.53,
        "MMLU":46.4,
        "TruthfulQA":40.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":27,
        "Available on the hub":true,
        "Model Sha":"893557ef32f98cd01deb1c5d063be6d640ffa657"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"edor\/Platypus2-mini-7B",
        "Average":54.93,
        "ARC":53.33,
        "HellaSwag":78.81,
        "MMLU":45.58,
        "TruthfulQA":42.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4ede4a6f8a8d6cc3bfff8b98837116c74c280f63"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abdulrahman-nuzha\/finetuned-llama2-chat-5000-v2.0",
        "Average":54.92,
        "ARC":52.05,
        "HellaSwag":76.13,
        "MMLU":46.33,
        "TruthfulQA":45.18,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3e740254650b5f41e77d04c66806e6a0d3145195"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"deepseek-ai\/deepseek-math-7b-base",
        "Average":54.92,
        "ARC":52.22,
        "HellaSwag":69.49,
        "MMLU":57.25,
        "TruthfulQA":40.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.49,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"036a8c6189aac6e2fc4e07b46e1e57c6b647bca5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RoversX\/llama-2-7b-hf-small-shards-Samantha-V1-SFT",
        "Average":54.91,
        "ARC":53.16,
        "HellaSwag":77.71,
        "MMLU":43.47,
        "TruthfulQA":45.28,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c39cee3821269e7fdffa690c2d0836c74dfebd25"
    },
    {
        "T":"\u2b55",
        "Model":"budecosystem\/code-millenials-34b",
        "Average":54.89,
        "ARC":49.83,
        "HellaSwag":75.09,
        "MMLU":49.28,
        "TruthfulQA":45.37,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"fdb4dc33b18c884e51f9d8258f192b4ed0f93dc3"
    },
    {
        "T":"\u2b55",
        "Model":"venkycs\/llama-v2-7b-32kC-Security",
        "Average":54.88,
        "ARC":49.83,
        "HellaSwag":77.33,
        "MMLU":44.41,
        "TruthfulQA":47.96,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"0ae2abdc539a79ad84b141f894d614adf3754882"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"wang7776\/Llama-2-7b-chat-hf-30-sparsity",
        "Average":54.86,
        "ARC":52.47,
        "HellaSwag":76.58,
        "MMLU":45.57,
        "TruthfulQA":44.82,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c3d07c4f8b6a509334d0f63e5057e9447f01b318"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airocoder-34b-2.1",
        "Average":54.85,
        "ARC":54.18,
        "HellaSwag":73.84,
        "MMLU":50.67,
        "TruthfulQA":40.7,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"f66e783ac783837b3f59f274ecf55f18a9221cd0"
    },
    {
        "T":"?",
        "Model":"Azure99\/blossom-v4-qwen1_5-4b",
        "Average":54.82,
        "ARC":46.08,
        "HellaSwag":70.8,
        "MMLU":55.11,
        "TruthfulQA":47.29,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.95,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"574e846dbb8842b1b578b7e44eec318588579cc6"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged",
        "Average":54.78,
        "ARC":53.67,
        "HellaSwag":78.09,
        "MMLU":45.63,
        "TruthfulQA":41.72,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2af3d3acb0466fef466512bc17b9bf57024629e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/vicuna-class-shishya-all-hal-7b-ep3",
        "Average":54.77,
        "ARC":45.48,
        "HellaSwag":77.21,
        "MMLU":51.54,
        "TruthfulQA":44.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5a1424eb777c8a3ce94ab31486510da8f617d17e"
    },
    {
        "T":"?",
        "Model":"kalisai\/Nusantara-7b-Indo-Chat",
        "Average":54.76,
        "ARC":48.55,
        "HellaSwag":72.84,
        "MMLU":52.03,
        "TruthfulQA":45.63,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.72,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e6e0dfe507ba70b5f33c1631cd67d35c6484a1cb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"namirocks\/student-model-13b-ep3",
        "Average":54.73,
        "ARC":46.5,
        "HellaSwag":80.36,
        "MMLU":57.06,
        "TruthfulQA":35.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1f21e9d0506e908a10d5e611d5f1c022fdee6585"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/vicuna-class-shishya-13b-ep3",
        "Average":54.73,
        "ARC":46.5,
        "HellaSwag":80.36,
        "MMLU":57.06,
        "TruthfulQA":35.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"870fe04090a6a6cfe27d0bf4b06cc9f18dd4c67d"
    },
    {
        "T":"\u2b55",
        "Model":"wang7776\/Mistral-7B-Instruct-v0.2-sparsity-30",
        "Average":54.73,
        "ARC":51.11,
        "HellaSwag":75.72,
        "MMLU":46.54,
        "TruthfulQA":45.53,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8454c7220f153f57b84d789225a141e3cdc3ba00"
    },
    {
        "T":"\u2b55",
        "Model":"LeoLM\/leo-hessianai-7b-chat-bilingual",
        "Average":54.73,
        "ARC":51.02,
        "HellaSwag":76.03,
        "MMLU":44.68,
        "TruthfulQA":47.16,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"5ee98fd03b310e3081f0c9986c5153b27ec5dce6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psyche\/kollama2-7b",
        "Average":54.72,
        "ARC":53.24,
        "HellaSwag":78.78,
        "MMLU":42.31,
        "TruthfulQA":44.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"48fca4ba1e2d31ff4fbe6856b9b93ad2d97da8b7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"tyson0420\/stack_llama_full",
        "Average":54.71,
        "ARC":54.27,
        "HellaSwag":78.76,
        "MMLU":45.55,
        "TruthfulQA":40.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9a9a38b4d1c9b4d3f30f6407558470e74b1e56fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/Guanaco-7B-Uncensored",
        "Average":54.69,
        "ARC":52.13,
        "HellaSwag":78.77,
        "MMLU":43.42,
        "TruthfulQA":44.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"db068e363e66e5d4b131e1d7a42a3a849e406a9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TinyPixel\/elm-test",
        "Average":54.67,
        "ARC":53.16,
        "HellaSwag":78.98,
        "MMLU":47.04,
        "TruthfulQA":39.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"aa8f81624d897aa493474bcd96dc3feae9f7a535"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-chat-hf-guanaco",
        "Average":54.66,
        "ARC":50.51,
        "HellaSwag":76.72,
        "MMLU":48.03,
        "TruthfulQA":43.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5d33696ee324899d52fc43794b46009fea08a9af"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"XuanXuanXuanXuan\/Llama-2-7b-chat-hf-gpt-3.5-80k",
        "Average":54.62,
        "ARC":52.05,
        "HellaSwag":73.89,
        "MMLU":48.19,
        "TruthfulQA":44.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3dac39a014a4535f0e5075f0cac0b0e0b4740b01"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abdulrahman-nuzha\/finetuned-llama2-chat-5000-v1.0-squad",
        "Average":54.61,
        "ARC":50.94,
        "HellaSwag":76.61,
        "MMLU":46.43,
        "TruthfulQA":44.45,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"67c1301cb8a9ea7eb6e2b2c1829719ef746465d3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xwin-LM\/XwinCoder-34B",
        "Average":54.6,
        "ARC":51.02,
        "HellaSwag":74.02,
        "MMLU":49.53,
        "TruthfulQA":43.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":24,
        "Available on the hub":true,
        "Model Sha":"c331e0f9203efcdfcc8916a24fcec52cd8897dcc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/YaYi-30b-EverythingLM",
        "Average":54.6,
        "ARC":37.97,
        "HellaSwag":61.05,
        "MMLU":69.63,
        "TruthfulQA":49.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":30.0,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"5e1c06e0d4c54df59355b6d2fb4e868bf8922346"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"leonarad\/hope_for",
        "Average":54.58,
        "ARC":51.28,
        "HellaSwag":74.74,
        "MMLU":51.56,
        "TruthfulQA":40.73,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8a5c594dbd9b000a00ed26c96e5cda964afe5935"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/WizardLM-13B-Uncensored",
        "Average":54.57,
        "ARC":50.94,
        "HellaSwag":76.64,
        "MMLU":43.96,
        "TruthfulQA":46.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":423,
        "Available on the hub":true,
        "Model Sha":"9025c5f96fef9525da9238369ad082961b0e9494"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-CreepingSenseOfDoom",
        "Average":54.54,
        "ARC":53.33,
        "HellaSwag":78.9,
        "MMLU":48.09,
        "TruthfulQA":37.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"efc7cbc5d0461c137e8ea0c83e54bc5357188783"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/LLaMa-2-PeanutButter_v19_R8-7B",
        "Average":54.54,
        "ARC":53.33,
        "HellaSwag":78.72,
        "MMLU":46.48,
        "TruthfulQA":39.61,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"b290ba1cfd60c5bb7b8bf4e9c08da1d3adb2d7b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-mixed-datasets-time-unit",
        "Average":54.53,
        "ARC":51.79,
        "HellaSwag":76.41,
        "MMLU":49.58,
        "TruthfulQA":40.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"26626ea669172be6bc8e6b2b0bc5f14aef8061aa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wenge-research\/yayi-7b-llama2",
        "Average":54.52,
        "ARC":54.78,
        "HellaSwag":77.94,
        "MMLU":41.35,
        "TruthfulQA":44.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"18a4ed38285c732efc583a4bd883b3a681f8d005"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"deepseek-ai\/deepseek-coder-7b-instruct-v1.5",
        "Average":54.52,
        "ARC":48.55,
        "HellaSwag":72.35,
        "MMLU":50.45,
        "TruthfulQA":46.73,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":6.91,
        "Hub":71,
        "Available on the hub":true,
        "Model Sha":"2a050a4c59d687a85324d32e147517992117ed30"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"georgesung\/llama2_7b_chat_uncensored",
        "Average":54.52,
        "ARC":53.58,
        "HellaSwag":78.66,
        "MMLU":44.49,
        "TruthfulQA":41.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":137,
        "Available on the hub":true,
        "Model Sha":"e9a972b12c6b59bfbcf30fe3779c2c933ce755bd"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Wanfq\/FuseLLM-7B",
        "Average":54.52,
        "ARC":53.24,
        "HellaSwag":78.72,
        "MMLU":47.93,
        "TruthfulQA":38.17,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":18,
        "Available on the hub":true,
        "Model Sha":"503725445dc3bd0377948b3042bda80fc957f85e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TencentARC\/LLaMA-Pro-8B",
        "Average":54.5,
        "ARC":53.75,
        "HellaSwag":77.91,
        "MMLU":47.49,
        "TruthfulQA":38.86,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":8.36,
        "Hub":165,
        "Available on the hub":true,
        "Model Sha":"7a2b46875f68ca276562a44ea99b713d86ddb9f2"
    },
    {
        "T":"\u2b55",
        "Model":"liuda1\/Mistral-7B-golden",
        "Average":54.49,
        "ARC":60.75,
        "HellaSwag":44.42,
        "MMLU":59.29,
        "TruthfulQA":53.51,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"unknown",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bc4624485fef5a2e3fcde465eaf2191cb1df1877"
    },
    {
        "T":"\u2b55",
        "Model":"TinyPixel\/llama2-7b-instruct",
        "Average":54.49,
        "ARC":53.58,
        "HellaSwag":78.78,
        "MMLU":46.11,
        "TruthfulQA":39.48,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4c0aa1032cbebeef1aad2becb5dcb613b8a1cc97"
    },
    {
        "T":"?",
        "Model":"quantumaikr\/QuantumLM-7B",
        "Average":54.47,
        "ARC":50.26,
        "HellaSwag":76.1,
        "MMLU":45.27,
        "TruthfulQA":46.25,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"f44998432fb90d88094ddf42e57ec458877a197f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"decruz07\/llama-2-7b-miniguanaco",
        "Average":54.46,
        "ARC":50.0,
        "HellaSwag":76.96,
        "MMLU":48.05,
        "TruthfulQA":42.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ef3fa61b50387f5a982aa2578933dfc20afb7237"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-mixed-datasets",
        "Average":54.46,
        "ARC":51.71,
        "HellaSwag":76.44,
        "MMLU":50.13,
        "TruthfulQA":39.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9c74b9396ff6b33e7a7622e59aa1f46103d993fe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xzuyn\/MedicWizard-7B",
        "Average":54.45,
        "ARC":53.5,
        "HellaSwag":78.39,
        "MMLU":44.61,
        "TruthfulQA":41.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"0b3ef975fb5e8ac1eae775160ab54c98221889df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wenge-research\/yayi-7b-llama2",
        "Average":54.45,
        "ARC":55.03,
        "HellaSwag":77.84,
        "MMLU":40.92,
        "TruthfulQA":44.02,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"f1a9e8d91e5b636cde3ea7fcf752a9f0234bd92a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TinyPixel\/testmodel2",
        "Average":54.45,
        "ARC":53.24,
        "HellaSwag":78.78,
        "MMLU":46.61,
        "TruthfulQA":39.17,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"cb1111653997cee2818ffcf13a1c37237ea2934d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lazycuber\/L2-7b-Guanaco-Random-Test",
        "Average":54.45,
        "ARC":50.6,
        "HellaSwag":77.21,
        "MMLU":47.66,
        "TruthfulQA":42.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9ffff7d0f58ba1de5e5fc59a61b7dc6ca571c9bf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TinyPixel\/lima-test",
        "Average":54.44,
        "ARC":53.07,
        "HellaSwag":78.88,
        "MMLU":46.42,
        "TruthfulQA":39.4,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4d6a006c6341f29b11c02f19bf9535f51b4da1b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"tyson0420\/stack_llama-clang",
        "Average":54.41,
        "ARC":54.1,
        "HellaSwag":78.93,
        "MMLU":45.97,
        "TruthfulQA":38.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":6.74,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"94d0bb2e81c094c2b0fc48d6c897827fd5650002"
    },
    {
        "T":"\u2b55",
        "Model":"elyza\/ELYZA-japanese-Llama-2-7b-instruct",
        "Average":54.39,
        "ARC":53.16,
        "HellaSwag":78.25,
        "MMLU":47.07,
        "TruthfulQA":39.08,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"48fa08b3098a23d3671e09565499a4cfbaff1923"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"tyson0420\/stack_llama_fil_ai",
        "Average":54.39,
        "ARC":53.75,
        "HellaSwag":78.59,
        "MMLU":46.5,
        "TruthfulQA":38.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"06deb8bab434bb45083a39aed732165bdec0fb0b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hongzoh\/Yi-6B_Open-Orca",
        "Average":54.37,
        "ARC":51.19,
        "HellaSwag":69.6,
        "MMLU":58.06,
        "TruthfulQA":38.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.8,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1b4918ab9c4fe63dfc38871ecaf59bea7c38a2d9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/llama2-MultiLoRA-sharegpt-mmlu-drop-ffn-1.0general",
        "Average":54.35,
        "ARC":53.16,
        "HellaSwag":78.59,
        "MMLU":46.89,
        "TruthfulQA":38.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"33c7717bb3fdfcb3560b0c7c793e5efa6050a392"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/llama-2-7b-hf_open-platypus",
        "Average":54.35,
        "ARC":51.45,
        "HellaSwag":78.63,
        "MMLU":43.6,
        "TruthfulQA":43.71,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c7e776f3f3afc0fa22cb7aff0d00522e571e9b29"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/GML-Mistral-merged-v1",
        "Average":54.34,
        "ARC":43.77,
        "HellaSwag":57.89,
        "MMLU":64.13,
        "TruthfulQA":51.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"3ec981e2e8c018f9e34a7553df2a2ed0d032dd37"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-7b_10e4",
        "Average":54.34,
        "ARC":53.84,
        "HellaSwag":78.46,
        "MMLU":46.76,
        "TruthfulQA":38.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"770088da097d9fe992d0847071d92e3af1923d90"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ziqingyang\/chinese-alpaca-2-7b",
        "Average":54.33,
        "ARC":49.57,
        "HellaSwag":72.62,
        "MMLU":46.5,
        "TruthfulQA":48.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.7,
        "Hub":86,
        "Available on the hub":true,
        "Model Sha":"ab2476bffedeed752daedd77e71900578e136e7c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-7b_10e5",
        "Average":54.32,
        "ARC":53.84,
        "HellaSwag":78.32,
        "MMLU":46.16,
        "TruthfulQA":38.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"43d9d3339870dfcce7a8775a2b4284c25dbeba66"
    },
    {
        "T":"?",
        "Model":"TehVenom\/Pygmalion-Vicuna-1.1-7b",
        "Average":54.32,
        "ARC":52.82,
        "HellaSwag":78.66,
        "MMLU":43.61,
        "TruthfulQA":42.21,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":25,
        "Available on the hub":true,
        "Model Sha":"bdac596568769d1ba4af8df9a611eee9723adf29"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"meta-llama\/Llama-2-7b-hf",
        "Average":54.32,
        "ARC":53.07,
        "HellaSwag":78.59,
        "MMLU":46.87,
        "TruthfulQA":38.76,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":554,
        "Available on the hub":false,
        "Model Sha":"e8f058fa738b6b308540024e9aa12e274e291f75"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TinyPixel\/testmodel-3",
        "Average":54.32,
        "ARC":53.24,
        "HellaSwag":78.72,
        "MMLU":46.57,
        "TruthfulQA":38.75,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a1fbc4d8a2c1a3d211325bdff9e7f0539fa7a2b1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/base_7b",
        "Average":54.32,
        "ARC":53.16,
        "HellaSwag":78.59,
        "MMLU":46.78,
        "TruthfulQA":38.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"7971b900b2e0b2b3657bc70f7cc2e1251b7ac155"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"codellama\/CodeLlama-34b-Python-hf",
        "Average":54.32,
        "ARC":50.43,
        "HellaSwag":76.36,
        "MMLU":49.11,
        "TruthfulQA":41.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub":51,
        "Available on the hub":true,
        "Model Sha":"6a39a8f3839cfc8c6a966f6b4e70472ac6fb719b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeen214\/test_llama2_7b",
        "Average":54.31,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.86,
        "TruthfulQA":38.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"69a4886f51ed752216cdd7f41a584d14240126f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bongchoi\/test-llama-2-7b",
        "Average":54.31,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.86,
        "TruthfulQA":38.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ebe2e68699cb7ab6bb22688f265c89be2ac0fa6d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bongchoi\/test-llama2-7b",
        "Average":54.31,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.86,
        "TruthfulQA":38.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ebe2e68699cb7ab6bb22688f265c89be2ac0fa6d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aboros98\/lilo2",
        "Average":54.31,
        "ARC":51.88,
        "HellaSwag":72.2,
        "MMLU":46.15,
        "TruthfulQA":47.02,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"1e5b27a424bc9372174b76b30a532ee73f392fdb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibranze\/araproje-llama2-7b-hf",
        "Average":54.3,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7fe54f507e762b0f62265813aef908765b1298c0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TaylorAI\/Flash-Llama-7B",
        "Average":54.3,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"27c84ef23d850582453e1cc2dcea13de48da090f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-chat-hf-v4",
        "Average":54.3,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"405c54ec7aea0735996ef5ff6ede6c35ab930381"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NewstaR\/Starlight-7B",
        "Average":54.3,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1f7436c458ebc3d8d31b91091c1a7a48e942cd3b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kashif\/stack-llama-2",
        "Average":54.3,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":6.61,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"28a206689c0097738177840a40e455a308db2d7d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-chat-hf-v2",
        "Average":54.3,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1c97650d4b919e2c6a2829778caa3a109935a58c"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/LLaMa-2-PeanutButter_v18_A-7B",
        "Average":54.3,
        "ARC":53.16,
        "HellaSwag":78.11,
        "MMLU":45.54,
        "TruthfulQA":40.37,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"15b2fa81418792841014f589e61d1d9e30457040"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ToolBench\/ToolLLaMA-7b-LoRA",
        "Average":54.28,
        "ARC":52.99,
        "HellaSwag":78.62,
        "MMLU":46.87,
        "TruthfulQA":38.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"67f2e8af850049a86fb9ee8ef581deb0f51e58e6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Monero\/WizardLM-13b-OpenAssistant-Uncensored",
        "Average":54.28,
        "ARC":48.55,
        "HellaSwag":76.03,
        "MMLU":43.15,
        "TruthfulQA":49.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"ff8e15fd68119d36ae1f0cebaa87f16e2ad3c732"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"undi95\/llama2-to-mistral-diff",
        "Average":54.28,
        "ARC":53.41,
        "HellaSwag":78.56,
        "MMLU":46.43,
        "TruthfulQA":38.71,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"16c279c5e7d12b8a6ff7771881808ef253a406b9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-chat-hf-v4",
        "Average":54.28,
        "ARC":53.41,
        "HellaSwag":78.56,
        "MMLU":46.43,
        "TruthfulQA":38.71,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"405c54ec7aea0735996ef5ff6ede6c35ab930381"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-chat-hf-v2",
        "Average":54.28,
        "ARC":53.41,
        "HellaSwag":78.56,
        "MMLU":46.43,
        "TruthfulQA":38.71,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1c97650d4b919e2c6a2829778caa3a109935a58c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"XuanXuanXuanXuan\/Llama-2-7b-hf-llama2-raw-80k",
        "Average":54.28,
        "ARC":53.41,
        "HellaSwag":78.62,
        "MMLU":46.26,
        "TruthfulQA":38.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f95d50673d3ed6bc65fda776997dbe9069bad1ed"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mrm8488\/llama-2-coder-7b",
        "Average":54.27,
        "ARC":54.01,
        "HellaSwag":78.35,
        "MMLU":46.25,
        "TruthfulQA":38.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":32,
        "Available on the hub":true,
        "Model Sha":"f21c0d5e3f9f8c5addf093358e6885afa9602296"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"tyson0420\/stack_llama_fil_ai",
        "Average":54.27,
        "ARC":53.5,
        "HellaSwag":78.63,
        "MMLU":46.23,
        "TruthfulQA":38.72,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"06deb8bab434bb45083a39aed732165bdec0fb0b"
    },
    {
        "T":"\u2b55",
        "Model":"Rijgersberg\/GEITje-7B-chat-v2",
        "Average":54.26,
        "ARC":50.34,
        "HellaSwag":74.13,
        "MMLU":49.0,
        "TruthfulQA":43.55,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"7e45fc97dcf957b845a98605a08fa13506023d15"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"XuanXuanXuanXuan\/Llama-2-7b-hf-gpt-3.5-80k",
        "Average":54.25,
        "ARC":53.84,
        "HellaSwag":75.75,
        "MMLU":46.0,
        "TruthfulQA":41.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4a1647074aeb3894032fd3dfc5a9f4b214c415bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JCX-kcuf\/Llama-2-7b-hf-gpt-3.5-80k",
        "Average":54.25,
        "ARC":53.84,
        "HellaSwag":75.77,
        "MMLU":45.98,
        "TruthfulQA":41.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"470a75ef677ce4e05306678cdac2174dbbf234e6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elyza\/ELYZA-japanese-Llama-2-7b-fast-instruct",
        "Average":54.25,
        "ARC":53.75,
        "HellaSwag":77.55,
        "MMLU":46.85,
        "TruthfulQA":38.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.66,
        "Hub":28,
        "Available on the hub":true,
        "Model Sha":"89de33d1ad568855853196802aeaecd799c6586f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yash21\/Mistral-Quantum-dpo",
        "Average":54.24,
        "ARC":43.43,
        "HellaSwag":57.76,
        "MMLU":64.29,
        "TruthfulQA":51.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8aba300293cd8abbf71517e7d7c80fe26bf07baa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"clibrain\/Llama-2-7b-ft-instruct-es",
        "Average":54.23,
        "ARC":53.67,
        "HellaSwag":77.83,
        "MMLU":46.58,
        "TruthfulQA":38.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"b62f431c88b232204ea7046f9d906ae1daa68437"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama2-7b-chat-temp",
        "Average":54.19,
        "ARC":51.19,
        "HellaSwag":73.32,
        "MMLU":45.47,
        "TruthfulQA":46.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":20,
        "Available on the hub":false,
        "Model Sha":"53d99a756c790f231e20c5aec2f10b2546ce0d38"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"medalpaca\/medalpaca-7b",
        "Average":54.11,
        "ARC":54.1,
        "HellaSwag":80.42,
        "MMLU":41.47,
        "TruthfulQA":40.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc",
        "#Params (B)":6.61,
        "Hub":30,
        "Available on the hub":true,
        "Model Sha":"b57b9f5ff34059e485b769973d023021fc66a8f7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/tulu-7B-fp16",
        "Average":54.11,
        "ARC":50.17,
        "HellaSwag":77.04,
        "MMLU":47.63,
        "TruthfulQA":41.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"8a026683f79119643f4007da4e9155c7849792cc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-openllama-13b-v7-fp16",
        "Average":54.08,
        "ARC":47.61,
        "HellaSwag":72.24,
        "MMLU":47.74,
        "TruthfulQA":48.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.89,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"8690c065bccd3e897ccbf3d8aa24b0216a6f5dba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_llama2-7b_10e6",
        "Average":54.07,
        "ARC":53.41,
        "HellaSwag":78.03,
        "MMLU":46.07,
        "TruthfulQA":38.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"d355af2ca568135fe1a7fc403ac334345949f0d3"
    },
    {
        "T":"\u2b55",
        "Model":"TaylorAI\/FLAN-Llama-7B-2_Llama2-7B-Flash_868_full_model",
        "Average":54.07,
        "ARC":52.47,
        "HellaSwag":79.08,
        "MMLU":47.58,
        "TruthfulQA":37.14,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"819f3f384e37f8906a62a8048556c9e58e495c02"
    },
    {
        "T":"?",
        "Model":"AtAndDev\/Ogno-Monarch-Neurotic-9B-Passthrough",
        "Average":54.06,
        "ARC":46.25,
        "HellaSwag":56.06,
        "MMLU":62.92,
        "TruthfulQA":51.03,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.99,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"96b43edd20bf553075e991cda3a8464fd65514f5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beomi\/Yi-Ko-6B",
        "Average":54.05,
        "ARC":48.89,
        "HellaSwag":74.48,
        "MMLU":55.72,
        "TruthfulQA":37.09,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":6.18,
        "Hub":32,
        "Available on the hub":true,
        "Model Sha":"8f2f500574cd3c2972f05b7ae6e2807819cce051"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mayacinka\/frankencup-dpo",
        "Average":54.03,
        "ARC":42.66,
        "HellaSwag":60.55,
        "MMLU":62.21,
        "TruthfulQA":50.72,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9248c7340053361655743f40acd4b9c1b5d0815d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/vicuna-7b-v1.3-instruct-pl-lora_unload",
        "Average":54.03,
        "ARC":48.04,
        "HellaSwag":76.28,
        "MMLU":47.42,
        "TruthfulQA":44.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e4b19d9d6168b32402da4ab2b5ec7ff27cf40d9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CobraMamba\/mamba-gpt-7b",
        "Average":54.03,
        "ARC":51.19,
        "HellaSwag":75.4,
        "MMLU":47.47,
        "TruthfulQA":42.06,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"cb0b04b1bff7921614efbd87d5b87bac04c58d13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-7b-gpt4-2.0",
        "Average":53.99,
        "ARC":52.9,
        "HellaSwag":78.53,
        "MMLU":45.09,
        "TruthfulQA":39.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"8432fe95c426ca7709cf2d31a64eee612c4dea42"
    },
    {
        "T":"?",
        "Model":"sambanovasystems\/SambaLingo-Thai-Chat",
        "Average":53.99,
        "ARC":52.73,
        "HellaSwag":78.42,
        "MMLU":43.95,
        "TruthfulQA":40.84,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.95,
        "Hub":33,
        "Available on the hub":true,
        "Model Sha":"fbe817bea4967720268af0e5793000b109147bde"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JCX-kcuf\/Llama-2-7b-chat-hf-gpt-3.5-80k-base_lora",
        "Average":53.95,
        "ARC":51.45,
        "HellaSwag":69.38,
        "MMLU":48.37,
        "TruthfulQA":46.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c975887e34bc74105f8efbded1102c82065de35b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"allbyai\/ToRoLaMa-7b-v1.0",
        "Average":53.94,
        "ARC":51.71,
        "HellaSwag":73.82,
        "MMLU":45.34,
        "TruthfulQA":44.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.67,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"9dd9ebe69ae8b391722c4edbfe70bd6c59b3b14d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dhmeltzer\/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged",
        "Average":53.92,
        "ARC":53.41,
        "HellaSwag":77.9,
        "MMLU":43.56,
        "TruthfulQA":40.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6ca41503b383c654aee8d5496e70fbdfaa33db10"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Menouar\/phi-2-basic-maths",
        "Average":53.91,
        "ARC":55.8,
        "HellaSwag":71.15,
        "MMLU":47.27,
        "TruthfulQA":41.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"c8e856be5f951cc93588c7af07d6c6a9da058490"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/vicuna-class-shishya-ac-hal-7b-ep3",
        "Average":53.9,
        "ARC":44.62,
        "HellaSwag":76.98,
        "MMLU":50.96,
        "TruthfulQA":43.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a76df6b71b959745a5f1804791071332ee6522ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishek\/autotrain-8kfjk-b3gva",
        "Average":53.9,
        "ARC":50.17,
        "HellaSwag":70.84,
        "MMLU":51.15,
        "TruthfulQA":43.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c576dc972d25f57084ed8c80afdae74742eacbda"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"adamo1139\/LWM-7B-1M-1000000ctx-AEZAKMI-3_1-1702",
        "Average":53.89,
        "ARC":51.19,
        "HellaSwag":77.08,
        "MMLU":43.12,
        "TruthfulQA":44.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"fdb6787960ddbe5c867ad1b6e61793c9a76da740"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GOAT-AI\/GOAT-7B-Community",
        "Average":53.87,
        "ARC":48.81,
        "HellaSwag":74.63,
        "MMLU":49.58,
        "TruthfulQA":42.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":33,
        "Available on the hub":true,
        "Model Sha":"a7073a0f5142ce04aaa1603b0812b358f62a8de8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AtomEchoAI\/AtomGPT_56k",
        "Average":53.87,
        "ARC":53.16,
        "HellaSwag":76.73,
        "MMLU":45.31,
        "TruthfulQA":40.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"f69ecfd630ec89afffa4ca7bd8a5eda0daf57643"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hiyouga\/Baichuan2-7B-Base-LLaMAfied",
        "Average":53.85,
        "ARC":49.57,
        "HellaSwag":73.45,
        "MMLU":54.86,
        "TruthfulQA":37.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.99,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"dc5bda435771212fc73a8c6556fbdf4fcd87f96d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"deepseek-ai\/deepseek-moe-16b-base",
        "Average":53.85,
        "ARC":53.24,
        "HellaSwag":79.77,
        "MMLU":46.31,
        "TruthfulQA":36.08,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":16.38,
        "Hub":68,
        "Available on the hub":false,
        "Model Sha":"521d2bc4fb69a3f3ae565310fcc3b65f97af2580"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vitruv\/vitruv_1",
        "Average":53.85,
        "ARC":49.91,
        "HellaSwag":76.05,
        "MMLU":48.21,
        "TruthfulQA":41.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1bf67cd0e4e1fd3bb753b51e693a7e11a3c240ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishek\/autotrain-8kfjk-b3gva",
        "Average":53.85,
        "ARC":50.0,
        "HellaSwag":70.79,
        "MMLU":51.09,
        "TruthfulQA":43.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c576dc972d25f57084ed8c80afdae74742eacbda"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"julianweng\/Llama-2-7b-chat-orcah",
        "Average":53.84,
        "ARC":45.39,
        "HellaSwag":76.62,
        "MMLU":48.11,
        "TruthfulQA":45.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"9ef8658d7d9816b62b61dcb34833a1ece3a3f967"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/Pygmalion-13b-Merged",
        "Average":53.83,
        "ARC":56.48,
        "HellaSwag":80.02,
        "MMLU":42.93,
        "TruthfulQA":35.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":26,
        "Available on the hub":true,
        "Model Sha":"f96308083033c84db47b6c093da3817c085c87c7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"meta-math\/MetaMath-13B-V1.0",
        "Average":53.82,
        "ARC":49.49,
        "HellaSwag":76.48,
        "MMLU":47.74,
        "TruthfulQA":41.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"0b448f6f64808f8bca94dc871e96a3eae7e95621"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"namirocks\/vicuna-tutor-shishya-model-7b-ep3",
        "Average":53.81,
        "ARC":43.86,
        "HellaSwag":76.63,
        "MMLU":51.24,
        "TruthfulQA":43.53,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e1c3514e5cec2e7c871adc85e27a19d596f2c70e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-7b-gpt4-1.1",
        "Average":53.81,
        "ARC":54.61,
        "HellaSwag":80.15,
        "MMLU":39.25,
        "TruthfulQA":41.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"5a45a16bac51ed9529a6dc2eab7355cc61eefb5b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"leonarad\/hope_for_7b_1.0v",
        "Average":53.8,
        "ARC":50.43,
        "HellaSwag":76.44,
        "MMLU":49.68,
        "TruthfulQA":38.66,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c5382de58d8be5d79689b9a1f0dcdea72e8b45ef"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"malhajar\/meditron-7b-chat",
        "Average":53.8,
        "ARC":50.77,
        "HellaSwag":75.37,
        "MMLU":40.49,
        "TruthfulQA":48.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"b2e32b581d1484c831654fb2c03d2d29e7f520d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ConvexAI\/Pelican-9b-v0.1",
        "Average":53.79,
        "ARC":43.34,
        "HellaSwag":57.86,
        "MMLU":63.31,
        "TruthfulQA":50.63,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":9.86,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"9be2074aae989c40e9c77b6ade2ab3a3d2c37677"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abdulrahman-nuzha\/belal-finetuned-llama2-1024-v2.2",
        "Average":53.78,
        "ARC":52.65,
        "HellaSwag":77.81,
        "MMLU":44.65,
        "TruthfulQA":40.02,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"9981b2c54d1cbf0d7d11b0855e27dada1e99434c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yys\/gemma-7B-it-firefly",
        "Average":53.78,
        "ARC":48.29,
        "HellaSwag":71.59,
        "MMLU":52.99,
        "TruthfulQA":42.25,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a45132c4f73c4b6cbddf24b6df519381f3a1b66d"
    },
    {
        "T":"\u2b55",
        "Model":"llm-agents\/tora-7b-v1.0",
        "Average":53.74,
        "ARC":52.47,
        "HellaSwag":78.68,
        "MMLU":45.9,
        "TruthfulQA":37.9,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"717edbee98945192b1a396fc9c337c5b32d6c79c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/LongQLoRA-Llama2-7b-8k",
        "Average":53.73,
        "ARC":52.47,
        "HellaSwag":78.11,
        "MMLU":45.37,
        "TruthfulQA":38.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d29069d302700fcbd9322c4b4189a0eac4bccaa7"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged",
        "Average":53.72,
        "ARC":54.35,
        "HellaSwag":78.06,
        "MMLU":45.35,
        "TruthfulQA":37.11,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"684c4f4612fadae47c2c7db9fe9e9be4aaafc7e2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-7b-chat",
        "Average":53.71,
        "ARC":52.47,
        "HellaSwag":78.35,
        "MMLU":39.51,
        "TruthfulQA":44.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"9af636df9c8693ea857b62442bd1c6c73d657dc6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aloobun\/Reyna-CoT-4B-v0.1",
        "Average":53.7,
        "ARC":44.71,
        "HellaSwag":71.12,
        "MMLU":55.9,
        "TruthfulQA":43.09,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":3.95,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"39785f4cca88485960fb2eefe9845fa17a614d6d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"indischepartij\/MiniCPM-3B-OpenHermes-2.5-v2",
        "Average":53.69,
        "ARC":47.44,
        "HellaSwag":72.0,
        "MMLU":53.06,
        "TruthfulQA":42.28,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.01,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"e192b35bd2b59f3fe7f6987b4459eaa59b69fc9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"itsliupeng\/llama2_7b_code",
        "Average":53.66,
        "ARC":52.13,
        "HellaSwag":75.71,
        "MMLU":48.05,
        "TruthfulQA":38.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"0e6d1edd87c8753b55d280179c8fb0e65ebf5fa2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"qualis2006\/llama-2-7b-int4-python-code-18k",
        "Average":53.66,
        "ARC":52.13,
        "HellaSwag":78.55,
        "MMLU":46.25,
        "TruthfulQA":37.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"aed968a4b3f3b716064eb8b50c5ae24b38007627"
    },
    {
        "T":"?",
        "Model":"damerajee\/Gaja-v1.00",
        "Average":53.65,
        "ARC":52.82,
        "HellaSwag":76.31,
        "MMLU":40.83,
        "TruthfulQA":44.64,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.87,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c5583d6a15a238e6d28c889ab00bf659afd47ef3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"decruz07\/llama-2-7b-miniguanaco",
        "Average":53.63,
        "ARC":49.06,
        "HellaSwag":75.59,
        "MMLU":46.14,
        "TruthfulQA":43.73,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ef3fa61b50387f5a982aa2578933dfc20afb7237"
    },
    {
        "T":"\u2b55",
        "Model":"rinna\/youri-7b-chat",
        "Average":53.63,
        "ARC":51.19,
        "HellaSwag":76.09,
        "MMLU":46.06,
        "TruthfulQA":41.17,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"96d1690c4a1fa192ab26c4be8f9c79e1faed8346"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dotvignesh\/perry-7b",
        "Average":53.62,
        "ARC":51.79,
        "HellaSwag":76.43,
        "MMLU":46.18,
        "TruthfulQA":40.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f35ae37b436637cd3e14d086324ccdaccfd69045"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Locutusque\/Rhino-Mistral-7B",
        "Average":53.6,
        "ARC":48.12,
        "HellaSwag":71.42,
        "MMLU":48.95,
        "TruthfulQA":45.9,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"9d23ebfc46951058a44d99c3ee45abf0c55d08ef"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gmonsoon\/MiniCPM-2B-Base-v3",
        "Average":53.6,
        "ARC":47.01,
        "HellaSwag":73.12,
        "MMLU":52.42,
        "TruthfulQA":41.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"114ef55021443ac68a51fc131156a5796e72541d"
    },
    {
        "T":"\u2b55",
        "Model":"starmpcc\/Asclepius-Llama2-7B",
        "Average":53.58,
        "ARC":50.85,
        "HellaSwag":76.53,
        "MMLU":43.61,
        "TruthfulQA":43.31,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"2f15bd8250d7825307e59cc2c785074ebbec3395"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/longchat-7b-v1.5-32k",
        "Average":53.56,
        "ARC":51.71,
        "HellaSwag":74.97,
        "MMLU":43.16,
        "TruthfulQA":44.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":37,
        "Available on the hub":true,
        "Model Sha":"16deb633ef4d6a18d5750239edc5a85ffeaf3918"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"juhwanlee\/gemma-7B-alpaca-case-2-2",
        "Average":53.55,
        "ARC":51.45,
        "HellaSwag":65.86,
        "MMLU":51.77,
        "TruthfulQA":45.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"03e519df53f8bf918460b63c593d951d09403907"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-7b-gpt4-m2.0",
        "Average":53.52,
        "ARC":50.51,
        "HellaSwag":76.87,
        "MMLU":45.35,
        "TruthfulQA":41.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"67729407add902e3d4d36bb105d7c011fb368ea5"
    },
    {
        "T":"\u2b55",
        "Model":"synapsoft\/Llama-2-7b-chat-hf-flan2022-1.2M",
        "Average":53.5,
        "ARC":49.57,
        "HellaSwag":76.25,
        "MMLU":45.99,
        "TruthfulQA":42.17,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"825506858e4603745a479215b8dea1524bfab6a0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-7b-gpt4-1.4",
        "Average":53.48,
        "ARC":53.92,
        "HellaSwag":80.33,
        "MMLU":38.61,
        "TruthfulQA":41.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"cae1ab8991f66bbe66ae95ed23a87846e7343047"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JCX-kcuf\/Llama-2-7b-hf-llama2-chat-80k",
        "Average":53.48,
        "ARC":53.84,
        "HellaSwag":74.65,
        "MMLU":46.36,
        "TruthfulQA":39.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"945429e199de13ccd374ba7e1d351c9a2d5fe561"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-mctaco",
        "Average":53.42,
        "ARC":45.65,
        "HellaSwag":75.65,
        "MMLU":49.27,
        "TruthfulQA":43.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"883b0fa4158de8207d0a94f4b8cb188e6250aa9d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"webbigdata\/ALMA-7B-Ja-V2",
        "Average":53.42,
        "ARC":52.39,
        "HellaSwag":77.92,
        "MMLU":44.72,
        "TruthfulQA":38.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"c2497586b28f419ad12c734600d08b2a5784ddc1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abdulrahman-nuzha\/finetuned-llama-v2.0",
        "Average":53.42,
        "ARC":53.16,
        "HellaSwag":77.75,
        "MMLU":43.69,
        "TruthfulQA":39.08,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9ffa847a1a0b229ea9c218e865bcf20f78556a8e"
    },
    {
        "T":"\u2b55",
        "Model":"yihan6324\/instructmining-platypus-15k",
        "Average":53.4,
        "ARC":54.35,
        "HellaSwag":80.01,
        "MMLU":37.44,
        "TruthfulQA":41.8,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":0.0,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"68342e1edd3c922943c1ea3bb34efff2248c149c"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"meta-llama\/Llama-2-7b-hf",
        "Average":53.4,
        "ARC":53.07,
        "HellaSwag":77.74,
        "MMLU":43.8,
        "TruthfulQA":38.98,
        "Type":"pretrained",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":554,
        "Available on the hub":false,
        "Model Sha":"6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/MultiLoRA-llama2-mmlu",
        "Average":53.34,
        "ARC":52.22,
        "HellaSwag":77.59,
        "MMLU":42.61,
        "TruthfulQA":40.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5cd015af30f8676b65320c8e597f12389aa4c576"
    },
    {
        "T":"?",
        "Model":"Qwen\/Qwen1.5-4B-Chat",
        "Average":53.33,
        "ARC":43.26,
        "HellaSwag":69.73,
        "MMLU":55.55,
        "TruthfulQA":44.79,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":3.95,
        "Hub":25,
        "Available on the hub":false,
        "Model Sha":"15bf46b13b1e6b6dd18ff7fa3242af406cc7e791"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"leonarad\/hope_for_7b_1.1v",
        "Average":53.33,
        "ARC":49.49,
        "HellaSwag":75.08,
        "MMLU":48.49,
        "TruthfulQA":40.26,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"30361b72ced8acd7cf96ddd136f70407d66ab92e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-chat-hf-v3",
        "Average":53.32,
        "ARC":52.22,
        "HellaSwag":76.78,
        "MMLU":45.89,
        "TruthfulQA":38.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a5269bc93a7f98e192e34553cec1302877ca4327"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Cluj-Napoca-0.3",
        "Average":53.31,
        "ARC":49.23,
        "HellaSwag":70.2,
        "MMLU":46.67,
        "TruthfulQA":47.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":25.46,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"eaa766e5186938eb27048347e0fdf52fd459a185"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abdulrahman-nuzha\/belal-finetuned-llama2-v1.0",
        "Average":53.29,
        "ARC":52.82,
        "HellaSwag":77.75,
        "MMLU":43.51,
        "TruthfulQA":39.09,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9280900648e021d222ebba3689e663d31227d9a1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jan-hq\/komodo-7b-chat",
        "Average":53.29,
        "ARC":51.45,
        "HellaSwag":77.05,
        "MMLU":44.63,
        "TruthfulQA":40.05,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.76,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"a255f2cf5038966136382b2d61c6258ae5b3b20a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"quantumaikr\/KoreanLM-hf",
        "Average":53.29,
        "ARC":51.45,
        "HellaSwag":76.77,
        "MMLU":40.61,
        "TruthfulQA":44.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"a7261e7ae6ee76c78e1ba1ac8c59bcc3e0868bf9"
    },
    {
        "T":"\u2b55",
        "Model":"openthaigpt\/openthaigpt-1.0.0-alpha-7b-chat-ckpt-hf",
        "Average":53.25,
        "ARC":50.85,
        "HellaSwag":74.89,
        "MMLU":40.02,
        "TruthfulQA":47.23,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"cdffb3488c5cb1a9aa5039a6b3bc72af24827db0"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TheBloke\/Llama-2-7B-GPTQ",
        "Average":53.24,
        "ARC":52.05,
        "HellaSwag":77.59,
        "MMLU":43.99,
        "TruthfulQA":39.32,
        "Type":"pretrained",
        "Precision":"None",
        "Hub License":"llama2",
        "#Params (B)":1.13,
        "Hub":47,
        "Available on the hub":true,
        "Model Sha":"ecd7ab9f6adc36ecbe0d751eeea0d90ae1863c3b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TheBloke\/Llama-2-7B-GPTQ",
        "Average":53.24,
        "ARC":52.05,
        "HellaSwag":77.59,
        "MMLU":43.99,
        "TruthfulQA":39.32,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":1.13,
        "Hub":47,
        "Available on the hub":true,
        "Model Sha":"ecd7ab9f6adc36ecbe0d751eeea0d90ae1863c3b"
    },
    {
        "T":"?",
        "Model":"stabilityai\/stablelm-zephyr-3b",
        "Average":53.22,
        "ARC":46.08,
        "HellaSwag":74.16,
        "MMLU":46.17,
        "TruthfulQA":46.49,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":2.8,
        "Hub":226,
        "Available on the hub":false,
        "Model Sha":"8b471c751c0e78cb46cf9f47738dd0eb45392071"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FreedomIntelligence\/AceGPT-7B",
        "Average":53.22,
        "ARC":53.58,
        "HellaSwag":77.54,
        "MMLU":43.0,
        "TruthfulQA":38.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"c907e27abebb1237ff62450b0e84cab7ad4dfb5f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LLMs\/AlpacaGPT4-7B-elina",
        "Average":53.21,
        "ARC":55.03,
        "HellaSwag":78.79,
        "MMLU":37.5,
        "TruthfulQA":41.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"bbece5e3f8ee9be09c8defc536a95c6ef780c681"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-temporal-without-mctaco-1",
        "Average":53.21,
        "ARC":49.49,
        "HellaSwag":75.93,
        "MMLU":47.47,
        "TruthfulQA":39.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f023eda0d38152e826136ecb988e8d2bdcc6f46c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Wizard-Vicuna-7B-Uncensored-HF",
        "Average":53.21,
        "ARC":53.41,
        "HellaSwag":78.85,
        "MMLU":37.09,
        "TruthfulQA":43.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":18,
        "Available on the hub":true,
        "Model Sha":"b802f1b4401d0b2242137160c20cc11b9ffd3a4c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/Wizard-Vicuna-7B-Uncensored",
        "Average":53.21,
        "ARC":53.41,
        "HellaSwag":78.85,
        "MMLU":37.09,
        "TruthfulQA":43.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":62,
        "Available on the hub":true,
        "Model Sha":"1097285acd9c48a1d09bc0a9844d365384732111"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/vicuna-shishya-7b-ep3-v1",
        "Average":53.16,
        "ARC":45.9,
        "HellaSwag":76.36,
        "MMLU":50.04,
        "TruthfulQA":40.32,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"082cf758aa3f6d8f956056003b5b3b6cde447d88"
    },
    {
        "T":"\u2b55",
        "Model":"llm-agents\/tora-code-34b-v1.0",
        "Average":53.1,
        "ARC":50.43,
        "HellaSwag":75.54,
        "MMLU":46.78,
        "TruthfulQA":39.66,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"cbb33eea774cc03d4363c424d81e8c9d58332274"
    },
    {
        "T":"?",
        "Model":"Replete-AI\/Phi-Stoma",
        "Average":53.08,
        "ARC":48.46,
        "HellaSwag":60.29,
        "MMLU":51.53,
        "TruthfulQA":52.05,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.82,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"60db09130992566859447366590a4c06256a737f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-chat-hf-v3",
        "Average":53.08,
        "ARC":51.96,
        "HellaSwag":76.7,
        "MMLU":45.36,
        "TruthfulQA":38.31,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a5269bc93a7f98e192e34553cec1302877ca4327"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"golaxy\/gowizardlm",
        "Average":53.06,
        "ARC":49.74,
        "HellaSwag":71.9,
        "MMLU":42.96,
        "TruthfulQA":47.66,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.76,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"385f2d164e7fe780e053276d95d36240f2368c21"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rameshm\/llama-2-13b-mathgpt-v4",
        "Average":53.06,
        "ARC":50.94,
        "HellaSwag":75.56,
        "MMLU":43.78,
        "TruthfulQA":41.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c5072a762070c6b3756385c63805348c155004b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/dolphin-llama2-7b",
        "Average":53.05,
        "ARC":46.59,
        "HellaSwag":67.52,
        "MMLU":48.37,
        "TruthfulQA":49.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":60,
        "Available on the hub":true,
        "Model Sha":"85aa4f67191fd016ab7ea8c389fddb5d9e5a9a52"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-7b-dolphin_10w-test",
        "Average":53.05,
        "ARC":51.71,
        "HellaSwag":74.5,
        "MMLU":43.9,
        "TruthfulQA":42.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"180647eeb8098b510002b0474723f801584046b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GeneZC\/MiniChat-1.5-3B",
        "Average":53.04,
        "ARC":46.5,
        "HellaSwag":68.28,
        "MMLU":46.67,
        "TruthfulQA":50.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.87,
        "Hub":32,
        "Available on the hub":true,
        "Model Sha":"886af9601d57d8675c09bab02144b68366cd4437"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Uncensored-Frank-7B",
        "Average":53.04,
        "ARC":54.27,
        "HellaSwag":76.52,
        "MMLU":37.5,
        "TruthfulQA":43.86,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"65bbcb80158a6d2e133bba99a90142caf4e2e242"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"llm-agents\/tora-code-34b-v1.0",
        "Average":53.0,
        "ARC":50.26,
        "HellaSwag":75.48,
        "MMLU":46.65,
        "TruthfulQA":39.62,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"cbb33eea774cc03d4363c424d81e8c9d58332274"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cognitivecomputations\/dolphin-2.2-yi-34b-200k",
        "Average":52.98,
        "ARC":42.24,
        "HellaSwag":68.22,
        "MMLU":55.51,
        "TruthfulQA":45.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":33.93,
        "Hub":34,
        "Available on the hub":true,
        "Model Sha":"33950ffa68b9f8cd5dc2f046c6c9a2d0f0bf7eff"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mncai\/chatdoctor",
        "Average":52.95,
        "ARC":53.75,
        "HellaSwag":78.54,
        "MMLU":35.95,
        "TruthfulQA":43.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"8fdcfdda6877d7f21173dfac48b2c14499ba8264"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/dolphin-2.2-yi-34b-200k",
        "Average":52.93,
        "ARC":42.15,
        "HellaSwag":68.18,
        "MMLU":55.47,
        "TruthfulQA":45.93,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":33.93,
        "Hub":34,
        "Available on the hub":true,
        "Model Sha":"c4e02a3a5c7a9d51f8b0cad85952dfdfb34c9413"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"stanford-oval\/Llama-2-7b-WikiChat-fused",
        "Average":52.93,
        "ARC":50.68,
        "HellaSwag":75.0,
        "MMLU":39.69,
        "TruthfulQA":46.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"47cc2d3e1719da0f0300d07111ea6a9b6e3aa2d0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gmonsoon\/MaxiCPM-3x3B-Test",
        "Average":52.92,
        "ARC":45.99,
        "HellaSwag":71.74,
        "MMLU":52.88,
        "TruthfulQA":41.06,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.19,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d353013fe9622a31794ea0837ba21e34a39ea465"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Juniplayground\/Mist_LLaMA-2-7B-1024_V3",
        "Average":52.91,
        "ARC":51.37,
        "HellaSwag":77.74,
        "MMLU":41.34,
        "TruthfulQA":41.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"05ec8f4a568777e1e543acdf8a587e080fb18fba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rishiraj\/smol-3b",
        "Average":52.91,
        "ARC":46.33,
        "HellaSwag":68.23,
        "MMLU":46.33,
        "TruthfulQA":50.73,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.02,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"21c18e02cbd8becf5cb48eaff485379b6d62a2cd"
    },
    {
        "T":"\u2b55",
        "Model":"psyche\/kollama2-7b-v3",
        "Average":52.88,
        "ARC":49.74,
        "HellaSwag":78.45,
        "MMLU":40.41,
        "TruthfulQA":42.92,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"b211c57902035342b6a92d61cd5f3afd306ddc9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cognitivecomputations\/dolphin-2.2-yi-34b-200k",
        "Average":52.87,
        "ARC":42.06,
        "HellaSwag":68.13,
        "MMLU":55.35,
        "TruthfulQA":45.93,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":33.93,
        "Hub":34,
        "Available on the hub":true,
        "Model Sha":"33950ffa68b9f8cd5dc2f046c6c9a2d0f0bf7eff"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-7b-gpt4",
        "Average":52.85,
        "ARC":53.07,
        "HellaSwag":78.69,
        "MMLU":38.9,
        "TruthfulQA":40.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"d9bcb0ad365bfacdf95128bc1272b4106aff7be9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/zephyr-7b-beta-lora-mmlu-merged",
        "Average":52.84,
        "ARC":52.82,
        "HellaSwag":76.12,
        "MMLU":37.82,
        "TruthfulQA":44.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9e73428dfba192ac2db8d9ec550254858af03ba6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-7b",
        "Average":52.84,
        "ARC":53.07,
        "HellaSwag":77.65,
        "MMLU":37.23,
        "TruthfulQA":43.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"7ea67f85ff3a7a8ec77f1819dec3e56779b764b1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/airoboros-7b-gpt4-fp16",
        "Average":52.84,
        "ARC":53.07,
        "HellaSwag":78.67,
        "MMLU":38.88,
        "TruthfulQA":40.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"14aa50fba9f6418c0d5e2d24087eb802931040ef"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mosaicml\/mpt-7b-8k-chat",
        "Average":52.81,
        "ARC":48.04,
        "HellaSwag":77.62,
        "MMLU":41.88,
        "TruthfulQA":43.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.65,
        "Hub":24,
        "Available on the hub":true,
        "Model Sha":"ef97b878a279cd1765fbed7b8321fb3cff1aa5b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elyza\/ELYZA-japanese-Llama-2-7b",
        "Average":52.79,
        "ARC":52.22,
        "HellaSwag":76.42,
        "MMLU":44.6,
        "TruthfulQA":37.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":26,
        "Available on the hub":true,
        "Model Sha":"976887c5891284db204320860bb84b71d598063e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gmonsoon\/MiniCPM-2B-Base-v2",
        "Average":52.78,
        "ARC":45.99,
        "HellaSwag":72.22,
        "MMLU":52.63,
        "TruthfulQA":40.27,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c9865131d0c5a530d04b9aa3f2d678ee6fbc8cb9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-7b-instruct",
        "Average":52.74,
        "ARC":51.96,
        "HellaSwag":78.11,
        "MMLU":38.43,
        "TruthfulQA":42.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":6.61,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"c6e2f515a0b289478118b5b75ff74107002ad962"
    },
    {
        "T":"?",
        "Model":"Inv\/MoECPM-Untrained-4x2b",
        "Average":52.74,
        "ARC":46.76,
        "HellaSwag":72.58,
        "MMLU":53.21,
        "TruthfulQA":38.41,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.79,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c7d5d78d4d938d772fe22c7302a8312a40a20645"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jxhong\/CAlign-alpaca-7b",
        "Average":52.74,
        "ARC":50.94,
        "HellaSwag":74.55,
        "MMLU":38.56,
        "TruthfulQA":46.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f5cc642a10160a014e2afeefcd57d4781994c51e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"golaxy\/goims",
        "Average":52.7,
        "ARC":49.49,
        "HellaSwag":72.67,
        "MMLU":43.85,
        "TruthfulQA":44.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.76,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9ef1045ca31f670d9cbf820af904b33a097cd787"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-7b-gpt4-1.2",
        "Average":52.68,
        "ARC":52.13,
        "HellaSwag":78.14,
        "MMLU":38.64,
        "TruthfulQA":41.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub":28,
        "Available on the hub":true,
        "Model Sha":"431fda60009d9b37a73211123ffb9c797764e182"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Rijgersberg\/GEITje-7B",
        "Average":52.67,
        "ARC":44.8,
        "HellaSwag":75.31,
        "MMLU":50.1,
        "TruthfulQA":40.45,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"80064d11c557eb1e0ddb52ed060d0e152c764c7c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DevaMalla\/llama_7b_qlora_pds-eval",
        "Average":52.66,
        "ARC":53.92,
        "HellaSwag":78.13,
        "MMLU":32.98,
        "TruthfulQA":45.6,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d20419e1d9e9a6a59ced3edf5169e8e7b3e8394c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gmonsoon\/MiniCPM-2B-Base",
        "Average":52.65,
        "ARC":46.08,
        "HellaSwag":70.52,
        "MMLU":52.61,
        "TruthfulQA":41.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"cecd6b3d629a72aec5a21858ca043fa1e737522d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"azale-ai\/DukunLM-7B-V1.0-Uncensored",
        "Average":52.63,
        "ARC":51.11,
        "HellaSwag":75.62,
        "MMLU":39.82,
        "TruthfulQA":43.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"11a78fd948f70407f78b74fe599af572dbda15fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vibhorag101\/llama-2-7b-chat-hf-phr_mental_health-2048",
        "Average":52.61,
        "ARC":52.39,
        "HellaSwag":75.39,
        "MMLU":39.77,
        "TruthfulQA":42.89,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"81d424a431ab7fa4ff725925b6d0e4269d4563e4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dominguesm\/canarim-7b",
        "Average":52.61,
        "ARC":51.96,
        "HellaSwag":77.52,
        "MMLU":40.92,
        "TruthfulQA":40.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"18d34bd9ad2d9674675b2e0d88dee9324b52f2b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"augmxnt\/shisa-7b-v1",
        "Average":52.6,
        "ARC":56.14,
        "HellaSwag":78.63,
        "MMLU":23.12,
        "TruthfulQA":52.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.96,
        "Hub":26,
        "Available on the hub":false,
        "Model Sha":"131c2f3bf4955d1e2b6762380132bdd8688c0646"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Tensoic\/Kan-Llama-SFT-v0.5",
        "Average":52.58,
        "ARC":47.44,
        "HellaSwag":72.71,
        "MMLU":42.71,
        "TruthfulQA":47.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"6307576da2b462571e804356a4b96803f56197d7"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"mosaicml\/mpt-7b-8k-chat",
        "Average":52.58,
        "ARC":47.7,
        "HellaSwag":77.48,
        "MMLU":41.47,
        "TruthfulQA":43.65,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.65,
        "Hub":24,
        "Available on the hub":true,
        "Model Sha":"ef97b878a279cd1765fbed7b8321fb3cff1aa5b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jphme\/orca_mini_v2_ger_7b",
        "Average":52.54,
        "ARC":49.83,
        "HellaSwag":75.5,
        "MMLU":39.1,
        "TruthfulQA":45.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.61,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"175965f50907c6a8cd40f1a4b10d28342969c066"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/orca_mini_v2_7b",
        "Average":52.54,
        "ARC":50.77,
        "HellaSwag":76.02,
        "MMLU":39.5,
        "TruthfulQA":43.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.61,
        "Hub":34,
        "Available on the hub":true,
        "Model Sha":"165850882991d7fa4eabab577a03ed84e0713bfa"
    },
    {
        "T":"\u2b55",
        "Model":"OdiaGenAI\/odia_llama2_7B_base",
        "Average":52.52,
        "ARC":50.77,
        "HellaSwag":75.94,
        "MMLU":46.1,
        "TruthfulQA":37.27,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"d3782ee2a527fd90dcd86359ab19417936089538"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"notstoic\/PygmalionCoT-7b",
        "Average":52.46,
        "ARC":51.45,
        "HellaSwag":76.92,
        "MMLU":33.35,
        "TruthfulQA":48.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":15,
        "Available on the hub":false,
        "Model Sha":"c03ac527360663d17bb142405251028eec843ed9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GeneZC\/MiniChat-2-3B",
        "Average":52.45,
        "ARC":44.88,
        "HellaSwag":67.69,
        "MMLU":47.59,
        "TruthfulQA":49.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.02,
        "Hub":22,
        "Available on the hub":true,
        "Model Sha":"f9c59fdc14c42d1a84539e4195335da0a10af955"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"damerajee\/Gaja-v2.00",
        "Average":52.44,
        "ARC":51.79,
        "HellaSwag":75.79,
        "MMLU":40.69,
        "TruthfulQA":41.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.87,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"a32aaae72fae89719bb3f0f00350d9fa8a3b37cd"
    },
    {
        "T":"?",
        "Model":"damerajee\/Gaja-v2.00-dpo",
        "Average":52.42,
        "ARC":51.71,
        "HellaSwag":75.87,
        "MMLU":40.79,
        "TruthfulQA":41.29,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.87,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"00fc1c19e31ae84eda0979468ed4a8ae31d5516d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Unbabel\/TowerBase-7B-v0.1",
        "Average":52.37,
        "ARC":51.02,
        "HellaSwag":77.68,
        "MMLU":43.48,
        "TruthfulQA":37.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.74,
        "Hub":44,
        "Available on the hub":true,
        "Model Sha":"227253877d67620f45c7b45ff22ead1dc6e03e4f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NurtureAI\/Orca-2-13B-16k",
        "Average":52.37,
        "ARC":53.67,
        "HellaSwag":69.48,
        "MMLU":41.02,
        "TruthfulQA":45.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"0daee08a5e065d02726e9ae0f05cdfd78992cfba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"golaxy\/gogpt2-13b-chat",
        "Average":52.35,
        "ARC":48.38,
        "HellaSwag":71.78,
        "MMLU":44.5,
        "TruthfulQA":44.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"6750491b8c720f2cc6f7ec53bbd61fb6efca6c04"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"golaxy\/gogpt2-13b",
        "Average":52.35,
        "ARC":48.38,
        "HellaSwag":71.78,
        "MMLU":44.5,
        "TruthfulQA":44.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.04,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"16d4c4214fa8d5a962b9064a8b958076b7c79a17"
    },
    {
        "T":"?",
        "Model":"liminerity\/dhbacmes-3b-slerp",
        "Average":52.34,
        "ARC":45.22,
        "HellaSwag":70.77,
        "MMLU":52.94,
        "TruthfulQA":40.41,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.01,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9e9ceb32aaf18db6e0a122f69356d06c0e36c5f8"
    },
    {
        "T":"?",
        "Model":"togethercomputer\/LLaMA-2-7B-32K",
        "Average":52.33,
        "ARC":48.04,
        "HellaSwag":77.49,
        "MMLU":45.35,
        "TruthfulQA":38.46,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":436,
        "Available on the hub":true,
        "Model Sha":"aef6d8946ae1015bdb65c478a2dd73b58daaef47"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"h2m\/mhm-7b-v1.3-DPO-1",
        "Average":52.33,
        "ARC":49.57,
        "HellaSwag":68.1,
        "MMLU":45.76,
        "TruthfulQA":45.88,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6ebd98fba486278e82be038bdc4b410c6bbd9c2d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"liminerity\/mm4-3b",
        "Average":52.33,
        "ARC":44.8,
        "HellaSwag":70.41,
        "MMLU":50.9,
        "TruthfulQA":43.2,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.72,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"0c43811e69b29c71d87b51b9ae94812616111293"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"damerajee\/Gaja-vv1",
        "Average":52.32,
        "ARC":51.54,
        "HellaSwag":75.49,
        "MMLU":39.94,
        "TruthfulQA":42.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.87,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2fda9f5ba3f3d89420ccf442446cf6d442d6cefd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PotatoOff\/HamSter-0.1",
        "Average":52.32,
        "ARC":46.93,
        "HellaSwag":68.08,
        "MMLU":43.03,
        "TruthfulQA":51.24,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"14b81a0c6870d400cd6216682f182d4615203c2a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/wizardLM-7B-HF",
        "Average":52.32,
        "ARC":50.34,
        "HellaSwag":75.27,
        "MMLU":38.07,
        "TruthfulQA":45.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":89,
        "Available on the hub":true,
        "Model Sha":"a8e22531a48cece989e670f539eb18ebd2dbd0cf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/una-llama-7b",
        "Average":52.28,
        "ARC":53.67,
        "HellaSwag":80.07,
        "MMLU":37.37,
        "TruthfulQA":38.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6321d1b950c6a3997a424b20273d66cb2b9395a5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ValiantLabs\/Fireplace-13b",
        "Average":52.28,
        "ARC":47.7,
        "HellaSwag":69.61,
        "MMLU":43.56,
        "TruthfulQA":48.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.02,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"1c37006534c4352f19c0b7ee857ed00601644068"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"keyfan\/vicuna-chinese-replication-v1.1",
        "Average":52.25,
        "ARC":42.83,
        "HellaSwag":71.47,
        "MMLU":47.47,
        "TruthfulQA":47.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.94,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"259ab0967975012a546f2362d6cd03ab10768157"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"golaxy\/gogpt2-7b",
        "Average":52.25,
        "ARC":46.76,
        "HellaSwag":71.53,
        "MMLU":42.85,
        "TruthfulQA":47.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.76,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"ee60ed402dedf24b6154aef05df54512e02fc9e2"
    },
    {
        "T":"?",
        "Model":"Replete-AI\/Phi-Delthanar",
        "Average":52.24,
        "ARC":46.67,
        "HellaSwag":60.19,
        "MMLU":51.16,
        "TruthfulQA":50.92,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.82,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1440f68ce368b8672e43121147592b4fdbbb64d8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aiplanet\/effi-7b",
        "Average":52.2,
        "ARC":55.12,
        "HellaSwag":78.07,
        "MMLU":35.91,
        "TruthfulQA":39.71,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"d58c62ee27cae60392bd0bd53e1fd05ea82e273b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"arver\/llama7b-qlora",
        "Average":52.2,
        "ARC":55.12,
        "HellaSwag":78.07,
        "MMLU":35.91,
        "TruthfulQA":39.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"5feaf84c888913e1dcd5e52e190660335e4d50f5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DevaMalla\/llama_7b_qlora_cds",
        "Average":52.19,
        "ARC":52.47,
        "HellaSwag":77.76,
        "MMLU":32.38,
        "TruthfulQA":46.14,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b6b5c65c5c1cce34d24c8f790bb0cc011e0f0808"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"u-chom\/preded-title-amazongoogle-abtbuy",
        "Average":52.18,
        "ARC":50.94,
        "HellaSwag":78.14,
        "MMLU":37.99,
        "TruthfulQA":41.65,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ab36321d76775d6e276d157e27de23854d21be3a"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cognitivecomputations\/yayi2-30b-llama",
        "Average":52.18,
        "ARC":35.67,
        "HellaSwag":53.37,
        "MMLU":70.6,
        "TruthfulQA":49.08,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":30.4,
        "Hub":22,
        "Available on the hub":true,
        "Model Sha":"01b331f04153b84a4ac049e71fd122d891424756"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LeoLM\/leo-hessianai-7b",
        "Average":52.15,
        "ARC":51.96,
        "HellaSwag":75.84,
        "MMLU":42.85,
        "TruthfulQA":37.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":40,
        "Available on the hub":true,
        "Model Sha":"88c5ac07006ea8f1b5d10aa4f03f0d624dd27e56"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"titan087\/OpenLlama13B-Guanaco",
        "Average":52.15,
        "ARC":51.19,
        "HellaSwag":75.24,
        "MMLU":43.76,
        "TruthfulQA":38.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"42ed3023ae1afe861f533570be881a03b10fc860"
    },
    {
        "T":"?",
        "Model":"chavinlo\/alpaca-native",
        "Average":52.14,
        "ARC":52.3,
        "HellaSwag":77.09,
        "MMLU":41.6,
        "TruthfulQA":37.58,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":247,
        "Available on the hub":true,
        "Model Sha":"cc7773cac2478231807c56ef2f02292d98f85cf5"
    },
    {
        "T":"\u2b55",
        "Model":"project-baize\/baize-healthcare-lora-7B",
        "Average":52.12,
        "ARC":54.1,
        "HellaSwag":77.32,
        "MMLU":37.09,
        "TruthfulQA":39.96,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"e3eb8bb0d8840431afe24760d964f8ba57edd83e"
    },
    {
        "T":"?",
        "Model":"AlpinDale\/pygmalion-instruct",
        "Average":52.07,
        "ARC":52.56,
        "HellaSwag":77.65,
        "MMLU":35.94,
        "TruthfulQA":42.13,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.74,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"1665b271316dfee05b2a8daf8b9d6c22ed0aef60"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"openlm-research\/open_llama_13b",
        "Average":52.06,
        "ARC":51.19,
        "HellaSwag":75.23,
        "MMLU":43.75,
        "TruthfulQA":38.08,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":428,
        "Available on the hub":true,
        "Model Sha":"b6d7fde8392250730d24cc2fcfa3b7e5f9a03ce8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-7b-gpt4-1.3",
        "Average":52.04,
        "ARC":52.47,
        "HellaSwag":77.98,
        "MMLU":41.97,
        "TruthfulQA":35.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7b5f77827636bbf3174c48ca16e774c89d71d7bd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elyza\/ELYZA-japanese-Llama-2-7b-fast",
        "Average":52.03,
        "ARC":51.88,
        "HellaSwag":75.46,
        "MMLU":44.34,
        "TruthfulQA":36.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.66,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"e326078aa122fb1c4973997952d7b8630071776a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chavinlo\/alpaca-native",
        "Average":52.02,
        "ARC":52.05,
        "HellaSwag":77.0,
        "MMLU":41.45,
        "TruthfulQA":37.6,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":247,
        "Available on the hub":true,
        "Model Sha":"cc7773cac2478231807c56ef2f02292d98f85cf5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"indischepartij\/MiniCPM-3B-Bacchus",
        "Average":51.99,
        "ARC":43.52,
        "HellaSwag":70.45,
        "MMLU":50.49,
        "TruthfulQA":43.52,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.01,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a950abcd65c24f7b3de09298c38ca8890e8fa269"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/multimaster-7b",
        "Average":51.99,
        "ARC":41.04,
        "HellaSwag":75.0,
        "MMLU":46.93,
        "TruthfulQA":44.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ae4dbb285559be9ae6f1eb4bd75db30d08dde5c6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-gpt-3.5-turbo-100k-7b",
        "Average":51.98,
        "ARC":53.07,
        "HellaSwag":76.16,
        "MMLU":33.63,
        "TruthfulQA":45.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"53887996c0f17f7711d182537505a895fb404542"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-orca-13b",
        "Average":51.97,
        "ARC":46.33,
        "HellaSwag":67.71,
        "MMLU":47.19,
        "TruthfulQA":46.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a82467de3cb9438aa8f9e0ea8ea692f16a5724b2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bigcode\/starcoderplus",
        "Average":51.9,
        "ARC":48.72,
        "HellaSwag":77.3,
        "MMLU":43.72,
        "TruthfulQA":37.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":15.52,
        "Hub":177,
        "Available on the hub":false,
        "Model Sha":"95be82087c33f14ee9941c812a154a9dd66efe72"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/guanaco-7B-HF",
        "Average":51.89,
        "ARC":52.99,
        "HellaSwag":80.05,
        "MMLU":35.32,
        "TruthfulQA":39.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"293c24105fa15afa127a2ec3905fdc2a0a3a6dac"
    },
    {
        "T":"?",
        "Model":"KnutJaegersberg\/Deita-2b",
        "Average":51.88,
        "ARC":44.71,
        "HellaSwag":70.39,
        "MMLU":52.79,
        "TruthfulQA":39.61,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":3.01,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"11267fbddbe8652cff32c1f6c7e3e8ca2f48f28c"
    },
    {
        "T":"\u2b55",
        "Model":"0x7194633\/fialka-7B-v3",
        "Average":51.86,
        "ARC":48.55,
        "HellaSwag":71.05,
        "MMLU":43.06,
        "TruthfulQA":44.79,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"d0dae57538d9379526726e66d5156ec0042528be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Neko-Institute-of-Science\/metharme-7b",
        "Average":51.84,
        "ARC":53.67,
        "HellaSwag":78.62,
        "MMLU":35.91,
        "TruthfulQA":39.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"62ca156891feead8db117be8f5f35687b6274e6e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"universitytehran\/PersianMind-v1.0",
        "Average":51.82,
        "ARC":47.18,
        "HellaSwag":71.39,
        "MMLU":47.34,
        "TruthfulQA":41.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.82,
        "Hub":18,
        "Available on the hub":true,
        "Model Sha":"e8acab7aa7d8a5d242cb5fb071a5497c6d1d7377"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"s3nh\/nsfw-noromaid-mistral-instruct",
        "Average":51.79,
        "ARC":51.79,
        "HellaSwag":75.39,
        "MMLU":46.47,
        "TruthfulQA":33.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a1f9d4f788c52967433396cbbb46e8bec4e0d891"
    },
    {
        "T":"?",
        "Model":"TigerResearch\/tigerbot-7b-base",
        "Average":51.79,
        "ARC":47.7,
        "HellaSwag":72.08,
        "MMLU":45.11,
        "TruthfulQA":42.27,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"300831494aa1eb16e59799310a09531f60dcc904"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aqweteddy\/llama_chat-tv_en_luban-tv_stable_platypus2",
        "Average":51.76,
        "ARC":44.54,
        "HellaSwag":61.02,
        "MMLU":49.6,
        "TruthfulQA":51.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"8c7858640053c11058906b0e3c73f3d3d1bf08ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"golaxy\/gogpt-7b",
        "Average":51.66,
        "ARC":48.81,
        "HellaSwag":73.79,
        "MMLU":43.03,
        "TruthfulQA":41.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.76,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"7eb70c0e330b7d3ff490047ddbb153bb96294882"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"indischepartij\/MiniCPM-3B-Hercules-v2.0",
        "Average":51.64,
        "ARC":43.26,
        "HellaSwag":71.11,
        "MMLU":51.82,
        "TruthfulQA":40.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.01,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"7564c055f9aaca4094e955b62f68975ec305d675"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"itsliupeng\/openllama-7b-icl",
        "Average":51.6,
        "ARC":47.95,
        "HellaSwag":77.04,
        "MMLU":44.37,
        "TruthfulQA":37.06,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"d6317fed3b190cc4d4c27b9f27ccf7c77f0b2e3b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azazelle\/Sina-Odin-7b-Merge",
        "Average":51.6,
        "ARC":52.82,
        "HellaSwag":68.86,
        "MMLU":45.54,
        "TruthfulQA":39.2,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c60ddc48eabbd4e7629afd26eb5a79efb4278084"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lodrick-the-lafted\/Winged-Lagomorph-2x13B",
        "Average":51.59,
        "ARC":47.95,
        "HellaSwag":69.39,
        "MMLU":44.5,
        "TruthfulQA":44.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":21.51,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"f3959f69559f531fb9202798baf641b4af90c1bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vikash06\/llama-2-7b-small-model-new",
        "Average":51.57,
        "ARC":45.22,
        "HellaSwag":72.35,
        "MMLU":46.23,
        "TruthfulQA":42.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.74,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"eefcb721d116ff2e486c4b70cf506e6c0d00fb0f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/LLaMA-2-7B-32K",
        "Average":51.56,
        "ARC":47.53,
        "HellaSwag":76.14,
        "MMLU":43.33,
        "TruthfulQA":39.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":436,
        "Available on the hub":true,
        "Model Sha":"aef6d8946ae1015bdb65c478a2dd73b58daaef47"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BarraHome\/LLaMarada-7B-v0.1-16bit",
        "Average":51.54,
        "ARC":53.33,
        "HellaSwag":76.02,
        "MMLU":39.68,
        "TruthfulQA":37.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3ad8fb00e2646d606fe53989de8d7449b6c542b3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Mixtral-8x7B-MoE-RP-Story",
        "Average":51.53,
        "ARC":51.54,
        "HellaSwag":70.0,
        "MMLU":43.04,
        "TruthfulQA":41.53,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":46.7,
        "Hub":34,
        "Available on the hub":false,
        "Model Sha":"ce4a4e4ffec063a3e338b6ebc328365270b6c5f0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yhyhy3\/open_llama_7b_v2_med_instruct",
        "Average":51.52,
        "ARC":46.5,
        "HellaSwag":76.91,
        "MMLU":42.32,
        "TruthfulQA":40.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"cabb47abd422a2d67161e2d038265ee23be45fb8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/carl-7b",
        "Average":51.51,
        "ARC":53.5,
        "HellaSwag":78.29,
        "MMLU":33.96,
        "TruthfulQA":40.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"de4c7af9598bebc47dd43253c972be719f3195d6"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"stabilityai\/stablelm-base-alpha-7b-v2",
        "Average":51.5,
        "ARC":47.35,
        "HellaSwag":77.08,
        "MMLU":45.1,
        "TruthfulQA":36.46,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":6.89,
        "Hub":25,
        "Available on the hub":false,
        "Model Sha":"eb3b56fee1ad4b1efe6625bbbc7a277df8ab5b96"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"conceptofmind\/LLongMA-2-7b-16k",
        "Average":51.49,
        "ARC":52.22,
        "HellaSwag":76.21,
        "MMLU":38.46,
        "TruthfulQA":39.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":40,
        "Available on the hub":false,
        "Model Sha":"5ffe363bb3e9ca7a24816981f399f67163f3c116"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Cluj-Napoca-0.2",
        "Average":51.48,
        "ARC":48.89,
        "HellaSwag":68.72,
        "MMLU":43.52,
        "TruthfulQA":44.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":25.46,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"424f45c510410d6890a928d83061cea53dd078e8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"nxn1231\/yi6",
        "Average":51.47,
        "ARC":47.78,
        "HellaSwag":68.25,
        "MMLU":54.05,
        "TruthfulQA":35.8,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":6.06,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"cfd5055e80eef946245f0ff4a49d46f9857ba482"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-mctaco-modified2",
        "Average":51.45,
        "ARC":42.92,
        "HellaSwag":73.97,
        "MMLU":48.49,
        "TruthfulQA":40.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8e1930bbbbdeb4f6f4639e837f09d9878bbf7831"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/Pygmalion_AlpacaLora-7b",
        "Average":51.38,
        "ARC":53.24,
        "HellaSwag":76.92,
        "MMLU":35.92,
        "TruthfulQA":39.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"1f61442e1238062095b31b4909c5e9ab26105794"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-7b-gpt4-1.4.1-qlora",
        "Average":51.37,
        "ARC":52.73,
        "HellaSwag":77.89,
        "MMLU":38.77,
        "TruthfulQA":36.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"91ffa900ed637cf5fd904d96e6985b6f7857ad64"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"feidfoe\/Metamath-reproduce-7b",
        "Average":51.34,
        "ARC":47.18,
        "HellaSwag":73.65,
        "MMLU":42.94,
        "TruthfulQA":41.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9955b88b535863a36ee9d9a255260bbc2cdab47b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wannaphong\/openthaigpt-0.1.0-beta-full-model_for_open_llm_leaderboard",
        "Average":51.3,
        "ARC":51.28,
        "HellaSwag":77.46,
        "MMLU":33.18,
        "TruthfulQA":43.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c1068f859a225e50d9d9ec74c572bfaf38573051"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/CodeLlama13B-Finetune-v1",
        "Average":51.3,
        "ARC":45.82,
        "HellaSwag":69.36,
        "MMLU":45.05,
        "TruthfulQA":44.97,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"40ff78ce37efcaf83718534c494829a573b9d719"
    },
    {
        "T":"?",
        "Model":"Abhaykoul\/MediKAI",
        "Average":51.28,
        "ARC":46.5,
        "HellaSwag":60.56,
        "MMLU":49.3,
        "TruthfulQA":48.77,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":14.07,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"ba58c6cfe070d77c943251f7e1366cac05a85565"
    },
    {
        "T":"?",
        "Model":"ausboss\/llama7b-wizardlm-unfiltered",
        "Average":51.26,
        "ARC":52.99,
        "HellaSwag":77.89,
        "MMLU":36.41,
        "TruthfulQA":37.75,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"2123beec77083c414b2ae51dd25b7a870b0b936c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"project-baize\/baize-v2-7b",
        "Average":51.26,
        "ARC":48.98,
        "HellaSwag":75.06,
        "MMLU":39.6,
        "TruthfulQA":41.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub":22,
        "Available on the hub":true,
        "Model Sha":"e4731c2c2671e2d0b47b5eba08c753ca21671fab"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"namirocks\/mistral-class-tutor-7b-ep3",
        "Average":51.25,
        "ARC":47.95,
        "HellaSwag":77.8,
        "MMLU":34.57,
        "TruthfulQA":44.69,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e571e0278702171cc460f8fe35b053278b0a9d7b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/vicuna-class-shishya-7b-ep3",
        "Average":51.24,
        "ARC":40.61,
        "HellaSwag":76.72,
        "MMLU":50.77,
        "TruthfulQA":36.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c2bd682b9f3babbb3bc84f84856fabe69a3c21d0"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"stabilityai\/stablelm-3b-4e1t",
        "Average":51.24,
        "ARC":46.59,
        "HellaSwag":75.94,
        "MMLU":45.23,
        "TruthfulQA":37.2,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":2.8,
        "Hub":305,
        "Available on the hub":false,
        "Model Sha":"a4750ace0db6f08d7bbba0aa52a585f231ea3cde"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HanningZhang\/Robin-v2",
        "Average":51.22,
        "ARC":48.81,
        "HellaSwag":74.48,
        "MMLU":39.27,
        "TruthfulQA":42.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LMFlow\/Robin-v2",
        "Average":51.22,
        "ARC":48.81,
        "HellaSwag":74.48,
        "MMLU":39.27,
        "TruthfulQA":42.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":""
    },
    {
        "T":"?",
        "Model":"LMFlow\/Robin-7b-v2",
        "Average":51.22,
        "ARC":48.81,
        "HellaSwag":74.48,
        "MMLU":39.27,
        "TruthfulQA":42.33,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"ec74e3955d91ae04e48250a658b37093e839e65c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"whiterabbitneo\/WhiteRabbitNeo-13B",
        "Average":51.22,
        "ARC":48.55,
        "HellaSwag":68.7,
        "MMLU":43.04,
        "TruthfulQA":44.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":341,
        "Available on the hub":true,
        "Model Sha":"594b9222df90074334697d0ed36ffeb3b478e9ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WhiteRabbitNeo\/WhiteRabbitNeo-13B-v1",
        "Average":51.22,
        "ARC":48.55,
        "HellaSwag":68.7,
        "MMLU":43.04,
        "TruthfulQA":44.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":341,
        "Available on the hub":true,
        "Model Sha":"594b9222df90074334697d0ed36ffeb3b478e9ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OptimalScale\/robin-7b-v2-delta",
        "Average":51.2,
        "ARC":49.15,
        "HellaSwag":74.43,
        "MMLU":38.96,
        "TruthfulQA":42.27,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"85eef39d89c100d860e53ff915ad3ab9668e1d1e"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"h2m\/mhm-7b-v1.3",
        "Average":51.2,
        "ARC":47.53,
        "HellaSwag":65.31,
        "MMLU":45.74,
        "TruthfulQA":46.22,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"0e8363818fdbdc8bacb1467e019f49fa8a9f4329"
    },
    {
        "T":"\u2b55",
        "Model":"OpenAssistant\/codellama-13b-oasst-sft-v10",
        "Average":51.16,
        "ARC":47.35,
        "HellaSwag":68.44,
        "MMLU":45.62,
        "TruthfulQA":43.23,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":34,
        "Available on the hub":true,
        "Model Sha":"97e936a2ec60bb31787a2428e1de49e32c213965"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-platypus-13b",
        "Average":51.14,
        "ARC":46.16,
        "HellaSwag":68.88,
        "MMLU":44.55,
        "TruthfulQA":44.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7a771bd8899b9ef4ba9680e96f84dc85810a67d6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wenge-research\/yayi-13b-llama2",
        "Average":51.11,
        "ARC":48.46,
        "HellaSwag":74.79,
        "MMLU":38.84,
        "TruthfulQA":42.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"d2effd5b6ba2e38964c205bf9946b1c19cf4a4be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JosephusCheung\/Qwen-VL-LLaMAfied-7B-Chat",
        "Average":51.08,
        "ARC":47.35,
        "HellaSwag":69.97,
        "MMLU":44.12,
        "TruthfulQA":42.87,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"gpl-3.0",
        "#Params (B)":7.1,
        "Hub":32,
        "Available on the hub":true,
        "Model Sha":"ccbd599ac46bcfbf7020be393afeecef404bce2b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jerryjalapeno\/nart-100k-7b",
        "Average":51.07,
        "ARC":54.1,
        "HellaSwag":78.47,
        "MMLU":34.98,
        "TruthfulQA":36.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.61,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"50e61b8e6cc17cb3fbcb490fe3dc7e2c8b248378"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wenge-research\/yayi-13b-llama2",
        "Average":51.06,
        "ARC":48.55,
        "HellaSwag":74.82,
        "MMLU":38.68,
        "TruthfulQA":42.19,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"9fc1bc4409b9e71f54213245a91c2742fbf7b3d0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"speechlessai\/speechless-codellama-dolphin-orca-platypus-13b",
        "Average":51.02,
        "ARC":45.82,
        "HellaSwag":67.71,
        "MMLU":45.88,
        "TruthfulQA":44.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"25e1c346c2a01588a728307d5c35fbeecd58b51b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhinand\/gemma-2b-it-tamil-v0.1-alpha",
        "Average":51.02,
        "ARC":50.09,
        "HellaSwag":71.41,
        "MMLU":39.94,
        "TruthfulQA":42.63,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":2.51,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"c1279a2cb3396028129ea74a935c638cb7e3dc95"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"mosaicml\/mpt-7b-8k",
        "Average":51.0,
        "ARC":47.35,
        "HellaSwag":77.4,
        "MMLU":42.58,
        "TruthfulQA":36.65,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":26,
        "Available on the hub":true,
        "Model Sha":"c94f57239fed80eac0dc62507aee049681c799a1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fireballoon\/baichuan-vicuna-chinese-7b",
        "Average":50.99,
        "ARC":43.52,
        "HellaSwag":71.12,
        "MMLU":46.87,
        "TruthfulQA":42.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":61,
        "Available on the hub":true,
        "Model Sha":"6cdb9e75cd473e31e87067c2a0b646083247d9ab"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-dolphin-orca-platypus-13b",
        "Average":50.93,
        "ARC":44.8,
        "HellaSwag":68.6,
        "MMLU":44.03,
        "TruthfulQA":46.28,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0c41023f8f665946a2c46c3823afee431408bcbd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama2-7b-pretrain",
        "Average":50.89,
        "ARC":48.63,
        "HellaSwag":74.83,
        "MMLU":41.04,
        "TruthfulQA":39.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.7,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"444c85ef809f8793d84b0813ab78bec50700cfcf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/TheTop-5x7B-Instruct-P-v0.1",
        "Average":50.88,
        "ARC":38.57,
        "HellaSwag":51.54,
        "MMLU":63.36,
        "TruthfulQA":50.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":14.22,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"22669ba7d1924596b6cd224b5909b6ba6c646475"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fierysurf\/Kan-LLaMA-7B-SFT-v0.1-sharded",
        "Average":50.81,
        "ARC":45.9,
        "HellaSwag":71.43,
        "MMLU":40.86,
        "TruthfulQA":45.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.88,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a04fd8b0958c11d7316965207d67b707cf4702f5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DevaMalla\/llama_7b_qlora",
        "Average":50.77,
        "ARC":55.12,
        "HellaSwag":78.26,
        "MMLU":35.71,
        "TruthfulQA":33.98,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7f94b0be78193abc54722cf723541c3800426f7b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fierysurf\/Ambari-7B-Instruct-v0.1-sharded",
        "Average":50.75,
        "ARC":50.0,
        "HellaSwag":74.59,
        "MMLU":38.03,
        "TruthfulQA":40.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.88,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d5f311d103dab0eeac1d5208130645c5a3dbfcd5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jlevin\/guanaco-unchained-llama-2-7b",
        "Average":50.69,
        "ARC":47.35,
        "HellaSwag":72.16,
        "MMLU":41.76,
        "TruthfulQA":41.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"43f3de8bcef63eec03a1b00079c08b5932c1a429"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sarvamai\/OpenHathi-7B-Hi-v0.1-Base",
        "Average":50.67,
        "ARC":49.49,
        "HellaSwag":74.34,
        "MMLU":41.38,
        "TruthfulQA":37.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.87,
        "Hub":80,
        "Available on the hub":true,
        "Model Sha":"2cbb156ab4426113115bc3387b06d1940015119a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-mctaco-modified4",
        "Average":50.66,
        "ARC":40.7,
        "HellaSwag":73.08,
        "MMLU":47.26,
        "TruthfulQA":41.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"715b03c8573df06f3825d1c08b307e2a83fa8bf9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DevaMalla\/llama_7b_lora",
        "Average":50.58,
        "ARC":54.86,
        "HellaSwag":79.1,
        "MMLU":33.63,
        "TruthfulQA":34.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7f4cbd810b4bef0d75c1fd3f551146b4ea97d9fd"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"NYTK\/PULI-LlumiX-32K",
        "Average":50.55,
        "ARC":48.63,
        "HellaSwag":75.0,
        "MMLU":41.65,
        "TruthfulQA":36.93,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"80271481150d842bd15bbb830fa5197296e32c72"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rinna\/youri-7b",
        "Average":50.55,
        "ARC":49.06,
        "HellaSwag":74.89,
        "MMLU":42.22,
        "TruthfulQA":36.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"2be40b8a7b669c4520bc04ce954bdbd7d4b0da7e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"namirocks\/mistral-shishya-model-7b-ep3",
        "Average":50.54,
        "ARC":44.71,
        "HellaSwag":76.81,
        "MMLU":46.77,
        "TruthfulQA":33.87,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ff7ee1544ff0d270c81146e9c9b681202bcf59be"
    },
    {
        "T":"?",
        "Model":"csitfun\/llama-7b-logicot",
        "Average":50.53,
        "ARC":47.01,
        "HellaSwag":72.56,
        "MMLU":38.93,
        "TruthfulQA":43.63,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"8e9c93c09e6a6c7d504c88d6ca598144829bced8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"itsliupeng\/openllama-7b-base",
        "Average":50.51,
        "ARC":46.16,
        "HellaSwag":76.4,
        "MMLU":42.82,
        "TruthfulQA":36.65,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"24d98f339fabfa479e3c85404f5e4dda9e43dcd1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Nexusflow\/NexusRaven-V2-13B",
        "Average":50.49,
        "ARC":45.14,
        "HellaSwag":67.4,
        "MMLU":44.88,
        "TruthfulQA":44.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":395,
        "Available on the hub":true,
        "Model Sha":"3bec1dcc7cb6f1895a923e66d87438e903bebb57"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fierysurf\/Ambari-7B-base-v0.1-sharded",
        "Average":50.47,
        "ARC":47.95,
        "HellaSwag":74.62,
        "MMLU":40.39,
        "TruthfulQA":38.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.88,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a8305467fb07f667c4aa1ba61a78ab3b3c0c23e1"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"freecs\/ThetaWave-14B-v0.1",
        "Average":50.45,
        "ARC":42.83,
        "HellaSwag":47.09,
        "MMLU":61.45,
        "TruthfulQA":50.41,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":14.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9e9745166b6f4e125511739d06900e72e5859617"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/llama-shishya-7b-ep3-v1",
        "Average":50.42,
        "ARC":48.04,
        "HellaSwag":76.63,
        "MMLU":46.12,
        "TruthfulQA":30.9,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8dc109f45ef36cc7bbd0f5d83fb65ac8e768d1bd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-mctaco-modified1",
        "Average":50.39,
        "ARC":40.87,
        "HellaSwag":73.4,
        "MMLU":47.42,
        "TruthfulQA":39.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a7749ff092ef03900de34b69d41c767a6a48ea9e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DevaMalla\/llama7b_alpaca_1gpu_bf16",
        "Average":50.37,
        "ARC":52.73,
        "HellaSwag":78.78,
        "MMLU":36.26,
        "TruthfulQA":33.71,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"305683c1b95f6888b8668dbc6b56d9efa5d07fef"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"microsoft\/phi-1_5",
        "Average":50.37,
        "ARC":52.9,
        "HellaSwag":63.79,
        "MMLU":43.89,
        "TruthfulQA":40.89,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":456,
        "Available on the hub":false,
        "Model Sha":"ea95720a352172db6fcbcd89032bfb1cb8481797"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openthaigpt\/openthaigpt-1.0.0-beta-7b-chat-ckpt-hf",
        "Average":50.34,
        "ARC":44.97,
        "HellaSwag":70.19,
        "MMLU":36.22,
        "TruthfulQA":49.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.71,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"dfc8a1e7ac47765466764dc48c285c5bd23de1fd"
    },
    {
        "T":"?",
        "Model":"shuvom\/yuj-v1",
        "Average":50.3,
        "ARC":45.65,
        "HellaSwag":70.1,
        "MMLU":43.78,
        "TruthfulQA":41.69,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.87,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"acf11b386f17e81d357b93bc6c89efd743b5ddfc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"teilomillet\/MiniMerlin-3B",
        "Average":50.3,
        "ARC":44.37,
        "HellaSwag":66.56,
        "MMLU":43.21,
        "TruthfulQA":47.07,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7fefc3d23e77c699aadba55c40d9e364eb73baf0"
    },
    {
        "T":"?",
        "Model":"vicgalle\/OpenHermes-Gemma-2B",
        "Average":50.23,
        "ARC":49.32,
        "HellaSwag":72.26,
        "MMLU":37.67,
        "TruthfulQA":41.69,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.51,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"b21e7a0b55ceb868e48181071a18fe8d4179fa2a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vikash06\/mistral_v1",
        "Average":50.2,
        "ARC":47.01,
        "HellaSwag":67.58,
        "MMLU":48.68,
        "TruthfulQA":37.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"9b7bd68c8105ff8ab2b6a5d6c9ad32f82c3190a1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WeOpenML\/PandaLM-Alpaca-7B-v1",
        "Average":50.19,
        "ARC":50.85,
        "HellaSwag":77.36,
        "MMLU":35.91,
        "TruthfulQA":36.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7fe5cb1a7009fdade8dfcfec335527997a730fcf"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigcode\/starcoder2-15b",
        "Average":50.17,
        "ARC":47.35,
        "HellaSwag":64.09,
        "MMLU":51.35,
        "TruthfulQA":37.87,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":15.96,
        "Hub":472,
        "Available on the hub":false,
        "Model Sha":"995200dd02e1e5080004d1967664933b28d5e577"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhinand\/tamil-llama-7b-instruct-v0.1",
        "Average":50.17,
        "ARC":48.04,
        "HellaSwag":70.97,
        "MMLU":39.95,
        "TruthfulQA":41.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.67,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"36f04b36c781ff994af41060df09491bde54105d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ai4bharat\/Airavata",
        "Average":50.07,
        "ARC":46.5,
        "HellaSwag":69.26,
        "MMLU":43.9,
        "TruthfulQA":40.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.87,
        "Hub":24,
        "Available on the hub":true,
        "Model Sha":"3fd8340a3683c8e7695c89a463428fcc0b2a875a"
    },
    {
        "T":"\u2b55",
        "Model":"Writer\/palmyra-med-20b",
        "Average":50.06,
        "ARC":46.93,
        "HellaSwag":73.51,
        "MMLU":44.34,
        "TruthfulQA":35.47,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.26,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"407810f75698c95000dc0ae1a9a0457be625e972"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"illuin\/test-custom-llama",
        "Average":50.06,
        "ARC":52.3,
        "HellaSwag":77.49,
        "MMLU":36.61,
        "TruthfulQA":33.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d985610bef080473e40f01c53266083c5f0c3169"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Writer\/palmyra-med-20b",
        "Average":50.05,
        "ARC":46.76,
        "HellaSwag":73.54,
        "MMLU":44.36,
        "TruthfulQA":35.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.26,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"407810f75698c95000dc0ae1a9a0457be625e972"
    },
    {
        "T":"?",
        "Model":"stabilityai\/stablelm-2-zephyr-1_6b",
        "Average":50.03,
        "ARC":43.69,
        "HellaSwag":69.3,
        "MMLU":42.03,
        "TruthfulQA":45.11,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.64,
        "Hub":150,
        "Available on the hub":false,
        "Model Sha":"c89d7d19e9781974793a7e9b0fe55bcabcf8abc5"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/WizardCoder-Python-13B-LoRa",
        "Average":50.03,
        "ARC":47.78,
        "HellaSwag":69.6,
        "MMLU":38.76,
        "TruthfulQA":43.97,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"32ffc44ffdf1adfe2d8ef219327fbd534f3d5955"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ashercn97\/giraffe-7b",
        "Average":50.02,
        "ARC":47.18,
        "HellaSwag":75.53,
        "MMLU":38.89,
        "TruthfulQA":38.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9af88449bed5be4709befcfbbba123ee75805479"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mosaicml\/mpt-7b-chat",
        "Average":49.95,
        "ARC":46.5,
        "HellaSwag":75.51,
        "MMLU":37.62,
        "TruthfulQA":40.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.65,
        "Hub":486,
        "Available on the hub":true,
        "Model Sha":"64e5c9c9fb53a8e89690c2dee75a5add37f7113e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishek\/autotrain-llama-alpaca-peft-52508123785",
        "Average":49.9,
        "ARC":52.22,
        "HellaSwag":76.92,
        "MMLU":37.6,
        "TruthfulQA":32.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"castorini\/rank_vicuna_7b_v1_fp16",
        "Average":49.89,
        "ARC":44.62,
        "HellaSwag":65.67,
        "MMLU":44.14,
        "TruthfulQA":45.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"0f3556bb0227cb59bcc652584d879f3bc40102e6"
    },
    {
        "T":"?",
        "Model":"voidful\/phi-1_5_chat",
        "Average":49.89,
        "ARC":51.02,
        "HellaSwag":63.39,
        "MMLU":39.35,
        "TruthfulQA":45.79,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":1.42,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3d1c4bc5437b9aacfa1afff4bcacdfe57e92ea43"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"haoranxu\/ALMA-7B",
        "Average":49.88,
        "ARC":50.34,
        "HellaSwag":75.5,
        "MMLU":38.04,
        "TruthfulQA":35.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"b570315dd26452a07cf15cf6feecce839e1327a6"
    },
    {
        "T":"?",
        "Model":"sail\/Sailor-4B-Chat",
        "Average":49.86,
        "ARC":45.05,
        "HellaSwag":68.36,
        "MMLU":43.96,
        "TruthfulQA":42.09,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.95,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"462e04484d1b1dd9c4dffe4f3d2d313e01a7abda"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/MultiLora-drop-sharegpt",
        "Average":49.85,
        "ARC":47.61,
        "HellaSwag":65.97,
        "MMLU":40.99,
        "TruthfulQA":44.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"00dea3ae0d995e8afc17f13e1757d954d68c089e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Neko-Institute-of-Science\/pygmalion-7b",
        "Average":49.85,
        "ARC":51.37,
        "HellaSwag":77.81,
        "MMLU":35.68,
        "TruthfulQA":34.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":37,
        "Available on the hub":true,
        "Model Sha":"6473f9996d758fde48a181f37cc5de575aff1606"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TFLai\/llama-7b-4bit-alpaca",
        "Average":49.81,
        "ARC":52.65,
        "HellaSwag":77.78,
        "MMLU":34.57,
        "TruthfulQA":34.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"74fddbcad2dfc24d476efda6bf97b08194625e91"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"adept\/persimmon-8b-chat",
        "Average":49.79,
        "ARC":44.97,
        "HellaSwag":73.3,
        "MMLU":44.98,
        "TruthfulQA":35.93,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.0,
        "Hub":41,
        "Available on the hub":false,
        "Model Sha":"7f1c23bce0eb2a41a5c7417f10ef15405819286e"
    },
    {
        "T":"\u2b55",
        "Model":"uukuguy\/speechless-codellama-platypus-13b",
        "Average":49.78,
        "ARC":45.31,
        "HellaSwag":68.63,
        "MMLU":42.82,
        "TruthfulQA":42.38,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"81cb1bca46ce646b8339501537837e02116de1b8"
    },
    {
        "T":"?",
        "Model":"luqmanxyz\/FrankenVillain-7B-v1",
        "Average":49.76,
        "ARC":42.75,
        "HellaSwag":51.52,
        "MMLU":48.6,
        "TruthfulQA":56.19,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"62078c66843dc86acb23ed546f6facb0199a489b"
    },
    {
        "T":"\u2b55",
        "Model":"rufjdk5480\/mixtral-ko-qna-merged",
        "Average":49.76,
        "ARC":39.51,
        "HellaSwag":39.06,
        "MMLU":71.86,
        "TruthfulQA":48.61,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":46.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"810c90db1842f6c5f314f23b7549d58316e0db95"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rufjdk5480\/gov-qna-ko-merged",
        "Average":49.76,
        "ARC":39.51,
        "HellaSwag":39.06,
        "MMLU":71.86,
        "TruthfulQA":48.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":46.7,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"810c90db1842f6c5f314f23b7549d58316e0db95"
    },
    {
        "T":"\u2b55",
        "Model":"uukuguy\/speechless-codellama-orca-13b",
        "Average":49.74,
        "ARC":44.37,
        "HellaSwag":65.2,
        "MMLU":43.46,
        "TruthfulQA":45.94,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6fdfeabe817235df3d560a6e6465c3722bc3a4ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheDrummer\/Moistral-11B-v2",
        "Average":49.74,
        "ARC":45.14,
        "HellaSwag":71.9,
        "MMLU":39.01,
        "TruthfulQA":42.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":10.6,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"d2e397dae95fca518e5ef43a1c3e3c7231ffdcf7"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"huggingface\/llama-7b",
        "Average":49.72,
        "ARC":51.02,
        "HellaSwag":77.82,
        "MMLU":35.71,
        "TruthfulQA":34.33,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f356572651e58fb337d610470d4b36976e7fb802"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Planner-7B-fp16",
        "Average":49.72,
        "ARC":51.02,
        "HellaSwag":77.82,
        "MMLU":35.71,
        "TruthfulQA":34.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"afb4604a06c8541960fb51240259777764c4ce7e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"VAGOsolutions\/SauerkrautLM-Gemma-2b",
        "Average":49.7,
        "ARC":48.72,
        "HellaSwag":71.41,
        "MMLU":42.9,
        "TruthfulQA":35.77,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":2.51,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"29075d62fc6ffe23c3c517aa9afe5c9fc1621b81"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AA051610\/VA",
        "Average":49.7,
        "ARC":41.38,
        "HellaSwag":62.52,
        "MMLU":49.96,
        "TruthfulQA":44.93,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"3c60daea2db0840475b3f67149122d9e033eab5b"
    },
    {
        "T":"?",
        "Model":"huggyllama\/llama-7b",
        "Average":49.69,
        "ARC":50.94,
        "HellaSwag":77.81,
        "MMLU":35.69,
        "TruthfulQA":34.33,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":253,
        "Available on the hub":true,
        "Model Sha":"8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shibing624\/chinese-llama-plus-13b-hf",
        "Average":49.69,
        "ARC":46.25,
        "HellaSwag":71.88,
        "MMLU":40.74,
        "TruthfulQA":39.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.94,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"f17a52b8067d551a814069d2c710e1f5c487a3ce"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"DevaMalla\/llama-base-7b",
        "Average":49.69,
        "ARC":50.94,
        "HellaSwag":77.8,
        "MMLU":35.67,
        "TruthfulQA":34.34,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e01d89d8e444f7d751ea58feaf22ff8c9af69d2a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AGI-inc\/lora_moe_7b",
        "Average":49.69,
        "ARC":50.94,
        "HellaSwag":77.8,
        "MMLU":35.67,
        "TruthfulQA":34.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3a528bdd73a12adc73f841a6d46bd363fe690023"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AGI-inc\/lora_moe_7b_baseline",
        "Average":49.69,
        "ARC":50.94,
        "HellaSwag":77.8,
        "MMLU":35.67,
        "TruthfulQA":34.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ad8065c8357945e6c07569033f5eba82c67c72ed"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/GPT-JT-6B-v0",
        "Average":49.57,
        "ARC":42.06,
        "HellaSwag":67.96,
        "MMLU":49.34,
        "TruthfulQA":38.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"41bd1937dbc51f9e589d310bddab5b4c1409e783"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wandb\/gemma-2b-zephyr-sft",
        "Average":49.48,
        "ARC":49.74,
        "HellaSwag":72.38,
        "MMLU":41.37,
        "TruthfulQA":34.42,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":2.51,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"31ba7c6d5ce2db815e874220a107cfa1e36c1e97"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/WizardLM-7B-Uncensored",
        "Average":49.46,
        "ARC":47.87,
        "HellaSwag":73.08,
        "MMLU":35.42,
        "TruthfulQA":41.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":325,
        "Available on the hub":true,
        "Model Sha":"14c23f9fa775ab5ce49010418f00df06d92b0b13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shibing624\/chinese-alpaca-plus-7b-hf",
        "Average":49.46,
        "ARC":49.23,
        "HellaSwag":70.48,
        "MMLU":38.39,
        "TruthfulQA":39.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.68,
        "Hub":44,
        "Available on the hub":true,
        "Model Sha":"0deb5a13732f1e3e3240ea83f403c57283fe2dc8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beomi\/llama-2-ko-7b",
        "Average":49.45,
        "ARC":48.46,
        "HellaSwag":75.28,
        "MMLU":39.56,
        "TruthfulQA":34.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.86,
        "Hub":74,
        "Available on the hub":true,
        "Model Sha":"d5c58cc2cae21b4fb96aaad2658acc898ab22d99"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/Gemmalpaca-2B",
        "Average":49.4,
        "ARC":48.72,
        "HellaSwag":71.36,
        "MMLU":36.3,
        "TruthfulQA":41.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.51,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"33fa56fd6dde243144c8d6ed2e91830f43b69c15"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Menouar\/gemma-2b-chat",
        "Average":49.4,
        "ARC":48.72,
        "HellaSwag":70.27,
        "MMLU":39.81,
        "TruthfulQA":38.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.51,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"d6e530eee6c7c99a43a5794055854cef6b1e5876"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mosaicml\/mpt-7b-8k-instruct",
        "Average":49.39,
        "ARC":45.9,
        "HellaSwag":74.47,
        "MMLU":41.97,
        "TruthfulQA":35.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":6.65,
        "Hub":22,
        "Available on the hub":true,
        "Model Sha":"736f68aceeb61298a5de3cf5ae81d0bc2697edf4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wandb\/gemma-2b-zephyr-dpo",
        "Average":49.37,
        "ARC":49.66,
        "HellaSwag":72.23,
        "MMLU":41.13,
        "TruthfulQA":34.47,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":2.51,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"7e2818ec0fd8079c97adebe2bff990313fc0b92e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigscience\/bloom",
        "Average":49.36,
        "ARC":50.43,
        "HellaSwag":76.41,
        "MMLU":30.85,
        "TruthfulQA":39.76,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":176.25,
        "Hub":4506,
        "Available on the hub":true,
        "Model Sha":"053d9cd9fbe814e091294f67fcfedb3397b954bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewqr2130\/llama2-7b-raw-sft",
        "Average":49.33,
        "ARC":47.44,
        "HellaSwag":75.25,
        "MMLU":33.86,
        "TruthfulQA":40.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"cd167d27b6c116b23863da859a07d08c6359c207"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewqr2130\/mistral-7b-raw-sft",
        "Average":49.33,
        "ARC":47.44,
        "HellaSwag":75.25,
        "MMLU":33.86,
        "TruthfulQA":40.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e1b241a26e35b87137fba8a54e352f1e4c98eebf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/llama-shishya-7b-ep3-v2",
        "Average":49.31,
        "ARC":47.35,
        "HellaSwag":75.88,
        "MMLU":43.84,
        "TruthfulQA":30.16,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"679c6cb9e869df686b1ae415ed440e6cfc05f80b"
    },
    {
        "T":"\u2b55",
        "Model":"mosaicml\/mpt-7b-8k-instruct",
        "Average":49.26,
        "ARC":45.48,
        "HellaSwag":74.41,
        "MMLU":42.11,
        "TruthfulQA":35.06,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":6.65,
        "Hub":22,
        "Available on the hub":true,
        "Model Sha":"736f68aceeb61298a5de3cf5ae81d0bc2697edf4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LLM360\/AmberChat",
        "Average":49.22,
        "ARC":42.92,
        "HellaSwag":74.01,
        "MMLU":38.75,
        "TruthfulQA":41.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":19,
        "Available on the hub":true,
        "Model Sha":"4c6dc7ae57586801a8d8efe8fcabf98cfe166427"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Menouar\/gemma-2b-chat-ultra",
        "Average":49.18,
        "ARC":48.29,
        "HellaSwag":70.18,
        "MMLU":39.19,
        "TruthfulQA":39.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.51,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"56cbd4ec1d6dffe7651e9d5a4e34f584b8067627"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"speechlessai\/speechless-codellama-airoboros-orca-platypus-13b",
        "Average":49.15,
        "ARC":44.88,
        "HellaSwag":67.7,
        "MMLU":43.16,
        "TruthfulQA":40.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f01d3ab70cc23e31dcf5d6418406b08dc2003153"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NurtureAI\/Orca-2-7B-16k",
        "Average":49.14,
        "ARC":50.6,
        "HellaSwag":63.89,
        "MMLU":36.68,
        "TruthfulQA":45.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"ab373033e98dcdbcc3aadb51374ae392656c6603"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/3BigReasonCinder",
        "Average":49.11,
        "ARC":41.72,
        "HellaSwag":65.16,
        "MMLU":44.79,
        "TruthfulQA":44.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":3.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ec1da6e96831dcebcc044280fb2ac5cd7e3d49ee"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhinand\/tamil-llama-7b-base-v0.1",
        "Average":49.1,
        "ARC":46.67,
        "HellaSwag":72.85,
        "MMLU":40.95,
        "TruthfulQA":35.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.67,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"e40f072bf68a157a18247eb08bf5b18ab8138986"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Linly-AI\/Chinese-LLaMA-2-7B-hf",
        "Average":49.06,
        "ARC":48.04,
        "HellaSwag":73.25,
        "MMLU":35.04,
        "TruthfulQA":39.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.64,
        "Hub":30,
        "Available on the hub":true,
        "Model Sha":"a2d55220b3d0693825fe69e1174653dc6cc4a920"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"senseable\/33x-coder",
        "Average":49.04,
        "ARC":45.9,
        "HellaSwag":62.64,
        "MMLU":42.02,
        "TruthfulQA":45.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":33.34,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"352e5249cd84f34ea9265b4218ddfdd1e9b73cc6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Enno-Ai\/ennodata-7b",
        "Average":49.03,
        "ARC":51.02,
        "HellaSwag":77.62,
        "MMLU":33.95,
        "TruthfulQA":33.53,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7872a492ebbb3c6a899f9acbd34dfd5f7e674fdd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GeneZC\/MiniChat-3B",
        "Average":49.01,
        "ARC":44.03,
        "HellaSwag":67.19,
        "MMLU":39.17,
        "TruthfulQA":45.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.02,
        "Hub":32,
        "Available on the hub":true,
        "Model Sha":"123d23bd291bb2d5fdb3b91dc1570d0b11654a78"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Telugu-LLM-Labs\/Indic-gemma-2b-finetuned-sft-Navarasa-2.0",
        "Average":49.0,
        "ARC":44.71,
        "HellaSwag":68.4,
        "MMLU":38.21,
        "TruthfulQA":44.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.51,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"90c88dbcd23acbd412378cdd5157a62c6895ff5f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pmking27\/PrathameshLLM-7B",
        "Average":49.0,
        "ARC":44.71,
        "HellaSwag":68.4,
        "MMLU":38.21,
        "TruthfulQA":44.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.51,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"f93a2a648522f274e7178512c4561ab1769dd5e0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/falcon-7B-case-8",
        "Average":48.98,
        "ARC":49.49,
        "HellaSwag":78.55,
        "MMLU":30.3,
        "TruthfulQA":37.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"36a32f6892c9e0b537b8560dd548b29fd5ccb86a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zorobin\/mistral-class-shishya-all-hal-7b-ep3",
        "Average":48.97,
        "ARC":46.59,
        "HellaSwag":78.87,
        "MMLU":34.45,
        "TruthfulQA":35.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8f15bc3f0d0235fdb67a8dfb6be36a1ac9c1b8b8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"EleutherAI\/llemma_7b",
        "Average":48.97,
        "ARC":46.16,
        "HellaSwag":62.98,
        "MMLU":47.87,
        "TruthfulQA":38.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":75,
        "Available on the hub":true,
        "Model Sha":"acc26c54609e9f18bf31fc5d58b5b533239e0430"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zorobin\/mistral-class-shishya-7b-ep3",
        "Average":48.95,
        "ARC":46.59,
        "HellaSwag":76.62,
        "MMLU":39.07,
        "TruthfulQA":33.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e85b73ce67deaa5b40633c5ce2545b23fa3ff3a0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-openllama-7b-v12-bf16",
        "Average":48.95,
        "ARC":42.06,
        "HellaSwag":62.01,
        "MMLU":46.53,
        "TruthfulQA":45.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.63,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"bb94ff691996484b1a9d899a6c0956ef6750d86a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mosaicml\/mpt-7b-instruct",
        "Average":48.92,
        "ARC":50.34,
        "HellaSwag":77.91,
        "MMLU":32.35,
        "TruthfulQA":35.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":6.65,
        "Hub":437,
        "Available on the hub":true,
        "Model Sha":"925e0d80e50e77aaddaf9c3ced41ca4ea23a1025"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"glenn2\/gemma-7b-lora-distilabel-intel-orca-dpo-pairs",
        "Average":48.89,
        "ARC":49.15,
        "HellaSwag":71.78,
        "MMLU":41.52,
        "TruthfulQA":33.1,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a2b71eb940722a0a5597d62044ad2e5d37352a94"
    },
    {
        "T":"?",
        "Model":"facebook\/opt-iml-max-30b",
        "Average":48.87,
        "ARC":43.86,
        "HellaSwag":72.39,
        "MMLU":41.09,
        "TruthfulQA":38.16,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":29.98,
        "Hub":34,
        "Available on the hub":true,
        "Model Sha":"291753b04817a31a742631053ee361874d6db8a4"
    },
    {
        "T":"?",
        "Model":"Fizzarolli\/sappha-2b-v3",
        "Average":48.87,
        "ARC":46.16,
        "HellaSwag":70.73,
        "MMLU":38.63,
        "TruthfulQA":39.94,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.51,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"57115346c5f21152c58caf36c5359c8283fe258a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"meta-math\/MetaMath-Llemma-7B",
        "Average":48.87,
        "ARC":46.5,
        "HellaSwag":61.69,
        "MMLU":47.66,
        "TruthfulQA":39.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"e31ec61dccd8fa24f44f0592a518491ef76a2235"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"augmxnt\/shisa-base-7b-v1",
        "Average":48.86,
        "ARC":52.3,
        "HellaSwag":77.63,
        "MMLU":23.12,
        "TruthfulQA":42.4,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.96,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"5aa465caca707816a4bb36b4980aef5d102d76fb"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"adept\/persimmon-8b-base",
        "Average":48.84,
        "ARC":42.75,
        "HellaSwag":71.14,
        "MMLU":43.63,
        "TruthfulQA":37.85,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.0,
        "Hub":27,
        "Available on the hub":false,
        "Model Sha":"94dc4e0bb7eeb26ec521eb3f78c36c91f6fe866b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RESMPDEV\/Gemma-Wukong-2b",
        "Average":48.76,
        "ARC":45.9,
        "HellaSwag":66.83,
        "MMLU":38.01,
        "TruthfulQA":44.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":2.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"532616da44668d93da159c4f823ac94772cc2a7c"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"google\/gemma-2b",
        "Average":48.75,
        "ARC":48.38,
        "HellaSwag":71.77,
        "MMLU":41.77,
        "TruthfulQA":33.08,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9d067f00def958594aaa16b39a65b07d69ca655b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/MultiLora-sharegpt",
        "Average":48.75,
        "ARC":45.65,
        "HellaSwag":65.54,
        "MMLU":37.95,
        "TruthfulQA":45.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9375b805eaaf89eff195d7a2b74a3590a1c525f6"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"google\/gemma-2b",
        "Average":48.73,
        "ARC":48.46,
        "HellaSwag":71.65,
        "MMLU":41.68,
        "TruthfulQA":33.13,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b903623f4be99493dba7e415a6f6c7c609ecf674"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JiheonJeong\/v1",
        "Average":48.73,
        "ARC":48.29,
        "HellaSwag":71.74,
        "MMLU":41.78,
        "TruthfulQA":33.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"37a45681356a4e8ba769e34ae06e620a7990e7d6"
    },
    {
        "T":"?",
        "Model":"vicgalle\/TruthfulQwen1.5-1.8B",
        "Average":48.7,
        "ARC":38.99,
        "HellaSwag":60.43,
        "MMLU":44.54,
        "TruthfulQA":50.86,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.84,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"22a0a36aa698afbd83c29bc08d1e91cbb97d4b62"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WeOpenML\/Alpaca-7B-v1",
        "Average":48.7,
        "ARC":49.06,
        "HellaSwag":75.71,
        "MMLU":33.76,
        "TruthfulQA":36.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"be5cb84a84a859dd6e5e3efc4648d6d5d1a5d188"
    },
    {
        "T":"?",
        "Model":"TheBloke\/Project-Baize-v2-7B-GPTQ",
        "Average":48.7,
        "ARC":45.99,
        "HellaSwag":73.44,
        "MMLU":35.46,
        "TruthfulQA":39.92,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.13,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"5dc039834e1ea42ac334458b2e3090fe3705cc59"
    },
    {
        "T":"\u2b55",
        "Model":"Writer\/palmyra-20b-chat",
        "Average":48.67,
        "ARC":43.52,
        "HellaSwag":72.83,
        "MMLU":35.18,
        "TruthfulQA":43.17,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":20.26,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"3b7442b7e2240846bc9cfac545bd8861c1660aa2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RESMPDEV\/Gemma-Wukong-2b",
        "Average":48.66,
        "ARC":45.31,
        "HellaSwag":66.94,
        "MMLU":38.1,
        "TruthfulQA":44.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"532616da44668d93da159c4f823ac94772cc2a7c"
    },
    {
        "T":"?",
        "Model":"JiheonJeong\/v1",
        "Average":48.65,
        "ARC":48.12,
        "HellaSwag":71.6,
        "MMLU":41.83,
        "TruthfulQA":33.04,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"37a45681356a4e8ba769e34ae06e620a7990e7d6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/falcon-7B-case-3",
        "Average":48.62,
        "ARC":47.78,
        "HellaSwag":78.3,
        "MMLU":31.96,
        "TruthfulQA":36.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"86c14bd09f6ebb9d3ebf59bb08b773c2b15630d4"
    },
    {
        "T":"\u2b55",
        "Model":"Heng666\/EastAsia-4x7B-Moe-experiment",
        "Average":48.62,
        "ARC":39.51,
        "HellaSwag":48.92,
        "MMLU":56.2,
        "TruthfulQA":49.83,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":18.52,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"44d2f9bfc6538102d101054d2366cb389fb713d9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vihangd\/dopeyshearedplats-2.7b-v1",
        "Average":48.6,
        "ARC":46.08,
        "HellaSwag":75.17,
        "MMLU":29.01,
        "TruthfulQA":44.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":2.62,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"c125218041c01662dc4c59b3f344aaa4e53dfd18"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-13b-Instruct-hf",
        "Average":48.56,
        "ARC":44.54,
        "HellaSwag":64.93,
        "MMLU":38.89,
        "TruthfulQA":45.88,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":53,
        "Available on the hub":true,
        "Model Sha":"b9f91b7351ecd589118d883afa23d5c93a38c612"
    },
    {
        "T":"\u2b55",
        "Model":"TheBloke\/CodeLlama-13B-Instruct-fp16",
        "Average":48.55,
        "ARC":44.62,
        "HellaSwag":64.94,
        "MMLU":38.77,
        "TruthfulQA":45.88,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":26,
        "Available on the hub":true,
        "Model Sha":"521c208c7251ccd3e44ccd9500b6bed419bca565"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/falcon-7B-case-c",
        "Average":48.55,
        "ARC":48.55,
        "HellaSwag":78.67,
        "MMLU":28.72,
        "TruthfulQA":38.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4e64962942d53640cc86ec50e3c75b86f1e65d1c"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/galactica-30b",
        "Average":48.53,
        "ARC":47.35,
        "HellaSwag":61.21,
        "MMLU":47.56,
        "TruthfulQA":38.01,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":29.97,
        "Hub":32,
        "Available on the hub":true,
        "Model Sha":"80bd55898b06c7c363c467dec877b8b32702a2c4"
    },
    {
        "T":"?",
        "Model":"FreedomIntelligence\/phoenix-inst-chat-7b",
        "Average":48.52,
        "ARC":44.71,
        "HellaSwag":63.23,
        "MMLU":39.06,
        "TruthfulQA":47.08,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.07,
        "Hub":44,
        "Available on the hub":true,
        "Model Sha":"5ed4d9570e0f76e1becb05bf467a7b4ff7b66055"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"glenn2\/gemma-2b-lora16b2",
        "Average":48.51,
        "ARC":47.53,
        "HellaSwag":71.97,
        "MMLU":38.12,
        "TruthfulQA":36.45,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":2.51,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ef7b5be9df2cabbef92b3021ed16cc869e91c054"
    },
    {
        "T":"?",
        "Model":"rombodawg\/Everyone-Coder-33b-Base",
        "Average":48.5,
        "ARC":45.99,
        "HellaSwag":61.71,
        "MMLU":44.05,
        "TruthfulQA":42.26,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":33.34,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"5f595eabc02a322d232e9d06e424e72606e9b0f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GeneZC\/MiniMA-2-3B",
        "Average":48.42,
        "ARC":44.71,
        "HellaSwag":69.33,
        "MMLU":41.22,
        "TruthfulQA":38.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.87,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"03c9985b5427e143a4e8b513393d65b9bb24a2d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"StarFox7\/gemma-2b-dpo-v1",
        "Average":48.4,
        "ARC":51.88,
        "HellaSwag":70.87,
        "MMLU":37.7,
        "TruthfulQA":33.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":2.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1caf66dac8b06385eb8d65f3ae5697fe85d0cf6d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-deepseekcoder-33b-v16.1-32k",
        "Average":48.39,
        "ARC":45.05,
        "HellaSwag":60.79,
        "MMLU":43.24,
        "TruthfulQA":44.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":33.4,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"afab8e521c80d127a2795539a48de4d93bd02e88"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"glenn2\/gemma-2b-lora3",
        "Average":48.39,
        "ARC":47.27,
        "HellaSwag":71.83,
        "MMLU":38.04,
        "TruthfulQA":36.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.51,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6bb2412c1392c2caa989b15d61a14ea7210132f3"
    },
    {
        "T":"?",
        "Model":"aevalone\/Test-7B-pthrough",
        "Average":48.36,
        "ARC":44.37,
        "HellaSwag":51.19,
        "MMLU":49.31,
        "TruthfulQA":48.57,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"67127c0796b2c49f86f68ebb10e6a5707e0d59cf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/falcon-7B-case-4",
        "Average":48.33,
        "ARC":47.61,
        "HellaSwag":78.69,
        "MMLU":29.21,
        "TruthfulQA":37.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1e628df64fbce4c4e5e913ddaf4b8c861ffe1fea"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/falcon-7B-case-1",
        "Average":48.33,
        "ARC":47.61,
        "HellaSwag":78.69,
        "MMLU":29.21,
        "TruthfulQA":37.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0eea42282e92b4f2e90d2d37f660ac9b192aa171"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/falcon-7B-case-2",
        "Average":48.28,
        "ARC":47.18,
        "HellaSwag":78.47,
        "MMLU":28.83,
        "TruthfulQA":38.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c58c75c6454865f7f446cd2a4b8dd98b21f607b0"
    },
    {
        "T":"?",
        "Model":"cyberagent\/calm2-7b-chat-dpo-experimental",
        "Average":48.24,
        "ARC":41.04,
        "HellaSwag":68.99,
        "MMLU":39.82,
        "TruthfulQA":43.13,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.01,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"d55a77e9843b4c3848f4e82a4bc303d5a9ec47ff"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"openlm-research\/open_llama_7b_v2",
        "Average":48.18,
        "ARC":43.69,
        "HellaSwag":72.2,
        "MMLU":41.29,
        "TruthfulQA":35.54,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":80,
        "Available on the hub":true,
        "Model Sha":"e5961def23172a2384543940e773ab676033c963"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GeorgiaTechResearchInstitute\/galpaca-30b",
        "Average":48.18,
        "ARC":49.57,
        "HellaSwag":58.2,
        "MMLU":43.78,
        "TruthfulQA":41.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":29.97,
        "Hub":55,
        "Available on the hub":true,
        "Model Sha":"a1f0c4bedd65b485a0d4d3a3bd60d7a4599f1eaf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hyunseoki\/ko-ref-llama2-13b",
        "Average":48.15,
        "ARC":48.38,
        "HellaSwag":73.56,
        "MMLU":34.83,
        "TruthfulQA":35.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"c5d09631c88ab5012b48187ecd90ae773cd4bbd9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"THUDM\/chatglm2-6b",
        "Average":48.15,
        "ARC":38.82,
        "HellaSwag":59.02,
        "MMLU":46.66,
        "TruthfulQA":48.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.0,
        "Hub":1692,
        "Available on the hub":false,
        "Model Sha":"162b620e3078b03eefff94eb5f762d4093425fb5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/llama-class-shishya-7b-ep3",
        "Average":48.13,
        "ARC":40.78,
        "HellaSwag":77.04,
        "MMLU":46.74,
        "TruthfulQA":27.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"92802ec9c58b1ed64d758c0f0c8420f4000636ff"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/falcon-7B-case-0",
        "Average":48.12,
        "ARC":49.15,
        "HellaSwag":78.25,
        "MMLU":28.89,
        "TruthfulQA":36.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c27fc771cead6c5556084ea1603a93b5ee29122e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/falcon-7B-case-5",
        "Average":48.11,
        "ARC":48.38,
        "HellaSwag":78.51,
        "MMLU":29.52,
        "TruthfulQA":36.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7dc60a92b8836324e45efe6e6a769bdf5b964539"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"csujeong\/Falcon-7B-Fintued-Finance-Stock-E",
        "Average":48.1,
        "ARC":50.09,
        "HellaSwag":78.26,
        "MMLU":27.36,
        "TruthfulQA":36.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9f0bd2f7301a8ca44954c2b93359e564b9b61678"
    },
    {
        "T":"?",
        "Model":"kalisai\/Nusantara-4b-Indo-Chat",
        "Average":48.08,
        "ARC":45.39,
        "HellaSwag":70.16,
        "MMLU":38.39,
        "TruthfulQA":38.38,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.95,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"9cbf28c0f0ae444f15abe0eaa7955186865ba49b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/GPT-JT-6B-v1",
        "Average":48.07,
        "ARC":40.87,
        "HellaSwag":67.15,
        "MMLU":47.19,
        "TruthfulQA":37.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub":300,
        "Available on the hub":true,
        "Model Sha":"f34aa35f906895602c1f86f5685e598afdea8051"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/BigTranslate-13B-GPTQ",
        "Average":48.05,
        "ARC":45.31,
        "HellaSwag":75.1,
        "MMLU":31.18,
        "TruthfulQA":40.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.25,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"f2968552d2f522023f3289747234aea5508980e2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/koala-7B-HF",
        "Average":48.04,
        "ARC":47.1,
        "HellaSwag":73.58,
        "MMLU":25.53,
        "TruthfulQA":45.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":18,
        "Available on the hub":true,
        "Model Sha":"d102fe3b68f1a5a50d547e4fd1c8b33b783c993b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lvxy1117\/amber_fine_tune_001",
        "Average":47.98,
        "ARC":44.8,
        "HellaSwag":73.78,
        "MMLU":30.41,
        "TruthfulQA":42.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"389916bd805c635b3c118b896ed1a8f2333a3e4d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"abhinand\/gemma-2b-tamil",
        "Average":47.97,
        "ARC":47.44,
        "HellaSwag":71.3,
        "MMLU":38.21,
        "TruthfulQA":34.93,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":2.51,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4e0a9e1f7290f59fe0bef30f38cea5bee7a158db"
    },
    {
        "T":"\u2b55",
        "Model":"TheBloke\/Poro-34B-GPTQ",
        "Average":47.9,
        "ARC":47.01,
        "HellaSwag":73.75,
        "MMLU":32.47,
        "TruthfulQA":38.37,
        "Type":"instruction-tuned",
        "Precision":"None",
        "Hub License":"apache-2.0",
        "#Params (B)":6.01,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"f6e034384e36b411d6b831157fb6063060ec1169"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Galpaca-30b-MiniOrca",
        "Average":47.88,
        "ARC":48.89,
        "HellaSwag":57.8,
        "MMLU":43.72,
        "TruthfulQA":41.1,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":29.97,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"681d92f8f71ca3e8425da19afee89ed84baedf1d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gmonsoon\/Qwenchana-4B-restart-OH",
        "Average":47.83,
        "ARC":45.31,
        "HellaSwag":70.42,
        "MMLU":37.93,
        "TruthfulQA":37.68,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":4.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"335319112c7100f8a8b7d54986859157e477b129"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fierysurf\/Kan-LLaMA-7B-base",
        "Average":47.83,
        "ARC":43.94,
        "HellaSwag":70.75,
        "MMLU":37.06,
        "TruthfulQA":39.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.88,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"66ae057862e1201128113b4c8f3875c1a3fd8ef2"
    },
    {
        "T":"\u2b55",
        "Model":"nathan0\/mpt_delta_tuned_model_v2",
        "Average":47.82,
        "ARC":50.68,
        "HellaSwag":76.41,
        "MMLU":28.73,
        "TruthfulQA":35.47,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6adb4cb4ba37f4ce9e9c3051d343addf1098182c"
    },
    {
        "T":"\u2b55",
        "Model":"nathan0\/mpt_delta_tuned_model_v3",
        "Average":47.82,
        "ARC":50.68,
        "HellaSwag":76.41,
        "MMLU":28.73,
        "TruthfulQA":35.47,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6adb4cb4ba37f4ce9e9c3051d343addf1098182c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MAISAAI\/gemma-2b-coder",
        "Average":47.74,
        "ARC":48.98,
        "HellaSwag":71.43,
        "MMLU":37.02,
        "TruthfulQA":33.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":2.51,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"e5e4e5bfb8eb4cc11a82cff08db51a213fa66e42"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"qblocks\/falcon_7b_norobots",
        "Average":47.72,
        "ARC":48.12,
        "HellaSwag":77.9,
        "MMLU":28.11,
        "TruthfulQA":36.76,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"bbe8e4a0c19ec5a94f6eff680b5a55bd08e11e31"
    },
    {
        "T":"\u2b55",
        "Model":"souvik0306\/falcon_7b_3epoch_norobots",
        "Average":47.71,
        "ARC":47.61,
        "HellaSwag":77.24,
        "MMLU":29.73,
        "TruthfulQA":36.27,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"55b11c279d1a5b83f59cec0381fb41c31fd02d8d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aqweteddy\/Tulpar-tv_marcoroni-7b",
        "Average":47.71,
        "ARC":41.64,
        "HellaSwag":67.11,
        "MMLU":32.72,
        "TruthfulQA":49.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"89f76fc1520fdf54dab892c63196e3a871b7d1ad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lvxy1117\/amber_fine_tune_sgall",
        "Average":47.7,
        "ARC":44.28,
        "HellaSwag":74.77,
        "MMLU":31.29,
        "TruthfulQA":40.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"242e01d032be75c8e3282917a8f39b721296f645"
    },
    {
        "T":"\u2b55",
        "Model":"GeorgiaTechResearchInstitute\/galpaca-30b",
        "Average":47.7,
        "ARC":49.32,
        "HellaSwag":58.31,
        "MMLU":42.1,
        "TruthfulQA":41.09,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":29.97,
        "Hub":55,
        "Available on the hub":true,
        "Model Sha":"a1f0c4bedd65b485a0d4d3a3bd60d7a4599f1eaf"
    },
    {
        "T":"?",
        "Model":"JosephusCheung\/Guanaco",
        "Average":47.7,
        "ARC":50.17,
        "HellaSwag":72.69,
        "MMLU":30.3,
        "TruthfulQA":37.64,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":6.61,
        "Hub":210,
        "Available on the hub":true,
        "Model Sha":"bed6f3bd18f07a4a379525645cbd86d622b12836"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Menouar\/saqr-7b-beta",
        "Average":47.64,
        "ARC":47.78,
        "HellaSwag":77.61,
        "MMLU":25.8,
        "TruthfulQA":39.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"9f1f11790bb0ac4ae6ab8081bf798f5b7cd2331d"
    },
    {
        "T":"\u2b55",
        "Model":"qblocks\/falcon_7b_norobots",
        "Average":47.64,
        "ARC":47.87,
        "HellaSwag":77.92,
        "MMLU":27.94,
        "TruthfulQA":36.81,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"bbe8e4a0c19ec5a94f6eff680b5a55bd08e11e31"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jisukim8873\/falcon-7B-case-6",
        "Average":47.6,
        "ARC":46.5,
        "HellaSwag":78.49,
        "MMLU":28.97,
        "TruthfulQA":36.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"af9e538c8c2758bfe09a538f2093a6f4196a2b76"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shareAI\/CodeLLaMA-chat-13b-Chinese",
        "Average":47.6,
        "ARC":43.26,
        "HellaSwag":63.87,
        "MMLU":34.29,
        "TruthfulQA":48.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":12.85,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"675b3e35a9601683c2cb4ec7f1b11d2869842f36"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Menouar\/saqr-7b-merged",
        "Average":47.59,
        "ARC":47.7,
        "HellaSwag":77.51,
        "MMLU":25.78,
        "TruthfulQA":39.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"017423b094eea1bf4b2e8df0939627c7d68c7db6"
    },
    {
        "T":"?",
        "Model":"azarafrooz\/gemma-2b-it-sp-test-openherms-step500",
        "Average":47.57,
        "ARC":44.03,
        "HellaSwag":62.82,
        "MMLU":37.67,
        "TruthfulQA":45.77,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":2.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"e545006c78cef3250fb092aa0ffb9a06c6d7487e"
    },
    {
        "T":"?",
        "Model":"azarafrooz\/gemma-2b-it-sp-test",
        "Average":47.57,
        "ARC":44.03,
        "HellaSwag":62.82,
        "MMLU":37.67,
        "TruthfulQA":45.77,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":2.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"6cfd509a3b78e16429de3ce822f8ebc086e31a27"
    },
    {
        "T":"?",
        "Model":"azarafrooz\/gemma-2b-it-sp-test1",
        "Average":47.57,
        "ARC":44.03,
        "HellaSwag":62.82,
        "MMLU":37.67,
        "TruthfulQA":45.77,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":2.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"788d71c479bb22a4030e48ae4eb1378bc1631f08"
    },
    {
        "T":"?",
        "Model":"TigerResearch\/tigerbot-7b-sft",
        "Average":47.57,
        "ARC":41.64,
        "HellaSwag":60.56,
        "MMLU":29.89,
        "TruthfulQA":58.18,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.07,
        "Hub":13,
        "Available on the hub":false,
        "Model Sha":"98b847905d63f74624e834db1ff95ee2814cbbd3"
    },
    {
        "T":"?",
        "Model":"vilm\/Quyen-Mini-v0.1",
        "Average":47.57,
        "ARC":39.33,
        "HellaSwag":60.57,
        "MMLU":43.93,
        "TruthfulQA":46.44,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"e6178976a00495b6e0b9cec54ee6ac342bbd4d71"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lvxy1117\/amber_fine_tune_sg_part1",
        "Average":47.55,
        "ARC":44.88,
        "HellaSwag":75.1,
        "MMLU":29.36,
        "TruthfulQA":40.85,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b9ed86588ba7f315e10072c4976b6a71cbf0f747"
    },
    {
        "T":"\u2b55",
        "Model":"Writer\/InstructPalmyra-20b",
        "Average":47.55,
        "ARC":47.1,
        "HellaSwag":73.0,
        "MMLU":28.26,
        "TruthfulQA":41.81,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.26,
        "Hub":34,
        "Available on the hub":true,
        "Model Sha":"c78df447c70d4677b128b1df864b9fff8338d900"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Syed-Hasan-8503\/openhermes-gemma-2b-it",
        "Average":47.53,
        "ARC":43.94,
        "HellaSwag":62.74,
        "MMLU":37.62,
        "TruthfulQA":45.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":2.51,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"a104bc34d3a72e1ed7f3d469591b15dc03dd9725"
    },
    {
        "T":"?",
        "Model":"abideen\/gemma-2b-openhermes",
        "Average":47.53,
        "ARC":43.94,
        "HellaSwag":62.74,
        "MMLU":37.62,
        "TruthfulQA":45.83,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":2.51,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"1a8acd4de3c052bd07b6acc89c416d75033e710b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"namirocks\/mistral-shishya-all-hal-7b-ep3-v2",
        "Average":47.53,
        "ARC":45.9,
        "HellaSwag":74.29,
        "MMLU":30.21,
        "TruthfulQA":39.71,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"1f0c52aff9af9a5b49ed2dc255670946f98c04cb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"google\/gemma-2b-it",
        "Average":47.53,
        "ARC":43.94,
        "HellaSwag":62.7,
        "MMLU":37.65,
        "TruthfulQA":45.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9642e777f24fde593d204a9b2471dce33334e64a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lyogavin\/Anima-7B-100K",
        "Average":47.53,
        "ARC":46.59,
        "HellaSwag":72.28,
        "MMLU":33.4,
        "TruthfulQA":37.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":31,
        "Available on the hub":true,
        "Model Sha":"e303cf09e553c38ca5e0c0816d83631801ca5776"
    },
    {
        "T":"\u2b55",
        "Model":"OpenAssistant\/codellama-13b-oasst-sft-v10",
        "Average":47.52,
        "ARC":43.94,
        "HellaSwag":62.82,
        "MMLU":37.21,
        "TruthfulQA":46.1,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":34,
        "Available on the hub":true,
        "Model Sha":"612dab2a8b2d77edb4fd36cfc28b3ffbbb20ffc1"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"sail\/Sailor-4B",
        "Average":47.5,
        "ARC":44.45,
        "HellaSwag":69.53,
        "MMLU":38.99,
        "TruthfulQA":37.02,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.95,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"bc4d4e338bf7e64e52dd05c69bc7e893a21d9dad"
    },
    {
        "T":"\u2b55",
        "Model":"conceptofmind\/Hermes-LLongMA-2-7b-8k",
        "Average":47.49,
        "ARC":49.74,
        "HellaSwag":72.88,
        "MMLU":28.51,
        "TruthfulQA":38.84,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"c8755804d0e7d97ec059b1fd867ae3dba742c275"
    },
    {
        "T":"?",
        "Model":"bigscience\/bloomz-7b1-mt",
        "Average":47.44,
        "ARC":43.86,
        "HellaSwag":62.91,
        "MMLU":37.35,
        "TruthfulQA":45.65,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":7.07,
        "Hub":116,
        "Available on the hub":true,
        "Model Sha":"76875e6ea8df98157fb032c48ad6e354fd6a077b"
    },
    {
        "T":"\u2b55",
        "Model":"cyberagent\/calm2-7b-chat",
        "Average":47.44,
        "ARC":40.27,
        "HellaSwag":68.12,
        "MMLU":39.39,
        "TruthfulQA":41.96,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.01,
        "Hub":67,
        "Available on the hub":true,
        "Model Sha":"f666a1e43500643cb3ff8c988a6ea5b56afe934a"
    },
    {
        "T":"\u2b55",
        "Model":"nathan0\/mpt_delta_tuned_model_v3",
        "Average":47.44,
        "ARC":50.6,
        "HellaSwag":76.4,
        "MMLU":27.29,
        "TruthfulQA":35.46,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6adb4cb4ba37f4ce9e9c3051d343addf1098182c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-coding-7b-16k-tora",
        "Average":47.43,
        "ARC":41.21,
        "HellaSwag":64.45,
        "MMLU":39.14,
        "TruthfulQA":44.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"d56b5c4f649d8e722efb927d16d7589967a67fbe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JosephusCheung\/LL7M",
        "Average":47.4,
        "ARC":44.97,
        "HellaSwag":68.81,
        "MMLU":34.44,
        "TruthfulQA":41.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.64,
        "Hub":35,
        "Available on the hub":true,
        "Model Sha":"9b31bbf38a43d41eaf166fb3573f706b23cb1c13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cmarkea\/bloomz-7b1-mt-sft-chat",
        "Average":47.4,
        "ARC":44.03,
        "HellaSwag":62.6,
        "MMLU":38.64,
        "TruthfulQA":44.34,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":7.07,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"8c2dc302780fe320ee3428f3db2ee7ff3684dcef"
    },
    {
        "T":"?",
        "Model":"VMware\/open-llama-7b-open-instruct",
        "Average":47.4,
        "ARC":49.74,
        "HellaSwag":73.67,
        "MMLU":31.52,
        "TruthfulQA":34.65,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":6.61,
        "Hub":25,
        "Available on the hub":true,
        "Model Sha":"fdf9f034163cce67e04d55172155f0e07b1b19a0"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"baichuan-inc\/Baichuan-7B",
        "Average":47.38,
        "ARC":40.7,
        "HellaSwag":69.02,
        "MMLU":43.59,
        "TruthfulQA":36.23,
        "Type":"pretrained",
        "Precision":"torch.float32",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":769,
        "Available on the hub":false,
        "Model Sha":"8baef65be8363f3b5670adfe9a0b9c0311962d90"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"stabilityai\/stablelm-2-1_6b",
        "Average":47.38,
        "ARC":43.34,
        "HellaSwag":70.45,
        "MMLU":38.95,
        "TruthfulQA":36.78,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":1.64,
        "Hub":149,
        "Available on the hub":false,
        "Model Sha":"810b45c00ea0af42ded794f9e613f6fc52330921"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"anas-awadalla\/mpt-7b",
        "Average":47.38,
        "ARC":47.7,
        "HellaSwag":77.57,
        "MMLU":30.8,
        "TruthfulQA":33.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.65,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"b772e556c8e8a17d087db6935e7cd019e5eefb0f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"mosaicml\/mpt-7b",
        "Average":47.38,
        "ARC":47.7,
        "HellaSwag":77.57,
        "MMLU":30.8,
        "TruthfulQA":33.44,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":1068,
        "Available on the hub":true,
        "Model Sha":"72e5f594ce36f9cabfa2a9fd8f58b491eb467ee7"
    },
    {
        "T":"\u2b55",
        "Model":"u-chom\/ex-llm-e1",
        "Average":47.37,
        "ARC":39.93,
        "HellaSwag":68.11,
        "MMLU":39.44,
        "TruthfulQA":42.01,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5838bea0ad7153520a0a105fb81c5b895820f710"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"speechlessai\/speechless-coding-7b-16k-tora",
        "Average":47.35,
        "ARC":41.13,
        "HellaSwag":64.48,
        "MMLU":38.86,
        "TruthfulQA":44.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"37281f20d54d895f8e3bc660e68564244c775ac2"
    },
    {
        "T":"?",
        "Model":"bn999\/mistral-4.2B",
        "Average":47.25,
        "ARC":40.87,
        "HellaSwag":61.51,
        "MMLU":41.78,
        "TruthfulQA":44.82,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":4.42,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"8818646580d58ba59268e6d9bb3a43ffafe90fd2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Zangs3011\/falcon_7b_DolphinCoder",
        "Average":47.24,
        "ARC":48.72,
        "HellaSwag":78.03,
        "MMLU":27.08,
        "TruthfulQA":35.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"72558e09e54869de3d8fc9fdd42633b81a1839f2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"qblocks\/falcon_7b_DolphinCoder",
        "Average":47.24,
        "ARC":48.72,
        "HellaSwag":78.03,
        "MMLU":27.08,
        "TruthfulQA":35.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"15a07f5340cbb9b6f37db3cda7aa02169feed89f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sharathhebbar24\/ssh_1.8B",
        "Average":47.17,
        "ARC":39.08,
        "HellaSwag":62.37,
        "MMLU":44.09,
        "TruthfulQA":43.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.84,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5dc3d8d5c08c014c43adc23678b31c0ac7d615c8"
    },
    {
        "T":"\u2b55",
        "Model":"qnguyen3\/quan-1.8b-chat",
        "Average":47.17,
        "ARC":39.08,
        "HellaSwag":62.37,
        "MMLU":44.09,
        "TruthfulQA":43.15,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.53,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"3b648e8a549888292a73a21b7312d958de6e875d"
    },
    {
        "T":"?",
        "Model":"bigscience\/bloomz-7b1",
        "Average":47.14,
        "ARC":42.49,
        "HellaSwag":63.01,
        "MMLU":37.85,
        "TruthfulQA":45.2,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":7.07,
        "Hub":106,
        "Available on the hub":true,
        "Model Sha":"2f4c4f3ebcf171dbbe2bae989ea2d2f3d3486a97"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"princeton-nlp\/Sheared-LLaMA-2.7B-ShareGPT",
        "Average":47.13,
        "ARC":41.04,
        "HellaSwag":71.26,
        "MMLU":28.5,
        "TruthfulQA":47.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.62,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"802be8903ec44f49a883915882868b479ecdcc3b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-tora-code-7b-v1.0",
        "Average":47.11,
        "ARC":42.66,
        "HellaSwag":65.16,
        "MMLU":38.56,
        "TruthfulQA":42.06,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"f7b1f87a096045f1bba8f68c62e062102218717b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ziqingyang\/chinese-llama-2-7b",
        "Average":47.11,
        "ARC":44.45,
        "HellaSwag":69.5,
        "MMLU":37.47,
        "TruthfulQA":37.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.7,
        "Hub":58,
        "Available on the hub":true,
        "Model Sha":"557b5cbd48a4a4eb5a08e975c4b6e11ac1ed4cbc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Changgil\/K2S3-SOLAR-11b-v2.0",
        "Average":47.09,
        "ARC":41.64,
        "HellaSwag":61.67,
        "MMLU":37.35,
        "TruthfulQA":47.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.92,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ad171800ebf03b89cfe6d556a67ad765bb70292f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Pierre-obi\/Mistral_solar-slerp",
        "Average":47.09,
        "ARC":43.0,
        "HellaSwag":57.93,
        "MMLU":40.48,
        "TruthfulQA":46.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"874e9960000eb9abadc57755cc4251bcfe369302"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"tyson0420\/stack_codellama-7b-inst",
        "Average":47.08,
        "ARC":43.52,
        "HellaSwag":66.17,
        "MMLU":39.59,
        "TruthfulQA":39.03,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5191aaffef22c923e714c5856a91e7f5a8dcc000"
    },
    {
        "T":"\u2b55",
        "Model":"OpenAssistant\/codellama-13b-oasst-sft-v10",
        "Average":47.03,
        "ARC":45.39,
        "HellaSwag":62.36,
        "MMLU":35.36,
        "TruthfulQA":45.02,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":34,
        "Available on the hub":true,
        "Model Sha":"612dab2a8b2d77edb4fd36cfc28b3ffbbb20ffc1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wenge-research\/yayi-7b",
        "Average":47.02,
        "ARC":46.33,
        "HellaSwag":61.72,
        "MMLU":36.34,
        "TruthfulQA":43.7,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.07,
        "Hub":28,
        "Available on the hub":true,
        "Model Sha":"00be6c9e41a8367a855c6f18ebfa08f5ecdb2cc4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Senko-11B-v1",
        "Average":47.02,
        "ARC":35.67,
        "HellaSwag":40.86,
        "MMLU":56.77,
        "TruthfulQA":54.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":11.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"7d5790f235731602fd9f31eb9180e2ce81ffb780"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"tiiuae\/falcon-7b",
        "Average":47.01,
        "ARC":47.87,
        "HellaSwag":78.13,
        "MMLU":27.79,
        "TruthfulQA":34.26,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub":831,
        "Available on the hub":false,
        "Model Sha":"378337427557d1df3e742264a2901a49f25d4eb1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xiaol\/RWKV-v4-raven-14B-one-state",
        "Average":47.0,
        "ARC":45.73,
        "HellaSwag":71.48,
        "MMLU":33.47,
        "TruthfulQA":37.3,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":14.15,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"1f41a1253b47c5fa4dc71ae118d32de9178d9def"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AI-Sweden-Models\/gpt-sw3-40b",
        "Average":46.97,
        "ARC":43.0,
        "HellaSwag":72.37,
        "MMLU":34.97,
        "TruthfulQA":37.52,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":39.93,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"ed18193e7292b5a821e5271d5dac95fffdf9617c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sail\/Sailor-4B",
        "Average":46.96,
        "ARC":43.86,
        "HellaSwag":69.51,
        "MMLU":37.45,
        "TruthfulQA":37.02,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.95,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"bc4d4e338bf7e64e52dd05c69bc7e893a21d9dad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Tensoic\/Gemma-2B-Samvaad",
        "Average":46.95,
        "ARC":46.59,
        "HellaSwag":68.17,
        "MMLU":33.09,
        "TruthfulQA":39.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":2.51,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"6d8968c6515a7cc4a9ddb4aeab32a51115b6d605"
    },
    {
        "T":"\u2b55",
        "Model":"teilomillet\/MiniMerlin-3b-v0.1",
        "Average":46.93,
        "ARC":40.7,
        "HellaSwag":54.06,
        "MMLU":43.32,
        "TruthfulQA":49.65,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2addcbd985f8a7f8bb7a7c21a5ec0e2505e549c6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/RedPajama-INCITE-Instruct-7B-v0.1",
        "Average":46.93,
        "ARC":44.11,
        "HellaSwag":72.02,
        "MMLU":37.62,
        "TruthfulQA":33.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":102,
        "Available on the hub":true,
        "Model Sha":"95667a602ff2646bf67fe3a57c4eb9a1edec87fe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/RedPajama-INCITE-7B-Instruct",
        "Average":46.93,
        "ARC":44.11,
        "HellaSwag":72.02,
        "MMLU":37.62,
        "TruthfulQA":33.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":102,
        "Available on the hub":true,
        "Model Sha":"95667a602ff2646bf67fe3a57c4eb9a1edec87fe"
    },
    {
        "T":"?",
        "Model":"vicgalleorg\/TruthfulQwen1.5-1.8B",
        "Average":46.91,
        "ARC":38.74,
        "HellaSwag":61.35,
        "MMLU":46.98,
        "TruthfulQA":40.58,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.84,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"56071d920b03a77d6ea95f97023663a976216de8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/landmark-attention-llama7b-fp16",
        "Average":46.85,
        "ARC":47.35,
        "HellaSwag":65.81,
        "MMLU":31.59,
        "TruthfulQA":42.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"bf8bdcb0c30cceb0ceda33cf5fde683807e39a58"
    },
    {
        "T":"?",
        "Model":"Aryanne\/ereb-test",
        "Average":46.8,
        "ARC":40.7,
        "HellaSwag":71.04,
        "MMLU":28.06,
        "TruthfulQA":47.4,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":2.7,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b3fa34df58d0915a76c367c13a025b64bef1345d"
    },
    {
        "T":"?",
        "Model":"togethercomputer\/GPT-JT-Moderation-6B",
        "Average":46.79,
        "ARC":40.53,
        "HellaSwag":67.66,
        "MMLU":41.63,
        "TruthfulQA":37.33,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub":29,
        "Available on the hub":true,
        "Model Sha":"1297870783f6091294769014afddf94499966a78"
    },
    {
        "T":"?",
        "Model":"silvainrichou\/gemma-3b-002",
        "Average":46.74,
        "ARC":43.34,
        "HellaSwag":64.06,
        "MMLU":36.86,
        "TruthfulQA":42.68,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":3.17,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c47cbc0fce360a29ed592e1887602d916a593622"
    },
    {
        "T":"\u2b55",
        "Model":"tiiuae\/falcon-7b-instruct",
        "Average":46.73,
        "ARC":46.16,
        "HellaSwag":70.85,
        "MMLU":25.84,
        "TruthfulQA":44.08,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub":634,
        "Available on the hub":true,
        "Model Sha":"cf4b3c42ce2fdfe24f753f0f0d179202fea59c99"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"mosaicml\/mpt-7b",
        "Average":46.71,
        "ARC":47.7,
        "HellaSwag":77.53,
        "MMLU":28.07,
        "TruthfulQA":33.55,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":1068,
        "Available on the hub":true,
        "Model Sha":"0b57768f52b7775563f7cc78c4724e407b39593b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WhiteRabbitNeo\/WhiteRabbitNeo-33B-v1",
        "Average":46.71,
        "ARC":44.37,
        "HellaSwag":60.22,
        "MMLU":40.56,
        "TruthfulQA":41.68,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":33.11,
        "Hub":76,
        "Available on the hub":true,
        "Model Sha":"e508c81aaf6b8bf8d1c7cbad5c9ddaed85fbb7dd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-gm-oasst1-en-1024-20b",
        "Average":46.67,
        "ARC":48.04,
        "HellaSwag":72.76,
        "MMLU":25.96,
        "TruthfulQA":39.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"1a5b8d25587eab67d837621a6c9423e7ef6df289"
    },
    {
        "T":"\u2b55",
        "Model":"AI-Sweden-Models\/gpt-sw3-20b-instruct",
        "Average":46.65,
        "ARC":43.17,
        "HellaSwag":71.09,
        "MMLU":31.32,
        "TruthfulQA":41.02,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":20.92,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"006477ad4c4875611f20cd927f1fd76bbf5ba5ba"
    },
    {
        "T":"\u2b55",
        "Model":"mediocredev\/open-llama-3b-v2-instruct",
        "Average":46.59,
        "ARC":38.48,
        "HellaSwag":70.24,
        "MMLU":39.69,
        "TruthfulQA":37.96,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"4d50e134af1d9806cbdf6bc90795b44ae689deca"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/LongAlpaca-13B",
        "Average":46.59,
        "ARC":42.58,
        "HellaSwag":72.03,
        "MMLU":34.91,
        "TruthfulQA":36.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"e80966ae720de9a844441a4a2bbc661106969915"
    },
    {
        "T":"\u2b55",
        "Model":"tiiuae\/falcon-7b-instruct",
        "Average":46.58,
        "ARC":45.82,
        "HellaSwag":70.78,
        "MMLU":25.66,
        "TruthfulQA":44.07,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub":634,
        "Available on the hub":false,
        "Model Sha":"eb410fb6ffa9028e97adb801f0d6ec46d02f8b07"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/gpt-neox-20b-full-precision",
        "Average":46.57,
        "ARC":48.81,
        "HellaSwag":74.44,
        "MMLU":26.16,
        "TruthfulQA":36.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":20.24,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"20b347273d90da7c2c9eb4c32d4173dba862a0d2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"l3utterfly\/minima-3b-layla-v2",
        "Average":46.57,
        "ARC":44.2,
        "HellaSwag":69.93,
        "MMLU":28.53,
        "TruthfulQA":43.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":2.87,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"118b6f7cf649f829afdec715eb4720dcd2a572b9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lqtrung1998\/Codellama-7b-hf-ReFT-GSM8k",
        "Average":46.55,
        "ARC":43.52,
        "HellaSwag":64.53,
        "MMLU":40.86,
        "TruthfulQA":37.28,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a97add0e026abe7ef5c58e0af0ec79f39eb58876"
    },
    {
        "T":"?",
        "Model":"TehVenom\/Moderator-Chan_GPT-JT-6b",
        "Average":46.53,
        "ARC":43.69,
        "HellaSwag":70.77,
        "MMLU":35.61,
        "TruthfulQA":36.05,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f2b7cda25f6965c1551fa78e9e38676994bc6638"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ise-uiuc\/Magicoder-S-CL-7B",
        "Average":46.47,
        "ARC":43.34,
        "HellaSwag":67.01,
        "MMLU":36.87,
        "TruthfulQA":38.67,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":38,
        "Available on the hub":true,
        "Model Sha":"cf49bc9879266bfc0a0123aaa4ef644af1b20c04"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"frankenmerger\/gemoy-4b-instruct-scientific",
        "Average":46.43,
        "ARC":41.98,
        "HellaSwag":63.05,
        "MMLU":38.73,
        "TruthfulQA":41.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":4.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2fd6773d400afdcc4bfce6cefd32551e4087ea69"
    },
    {
        "T":"?",
        "Model":"vicgalle\/OpenHermes-Qwen1.5-1.8B",
        "Average":46.4,
        "ARC":37.8,
        "HellaSwag":59.73,
        "MMLU":45.8,
        "TruthfulQA":42.28,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.84,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"56d69d3040cd98f0958ec216f7beab75f867f6fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"llm-agents\/tora-code-13b-v1.0",
        "Average":46.38,
        "ARC":44.71,
        "HellaSwag":69.15,
        "MMLU":36.69,
        "TruthfulQA":34.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"4bf5b528d95a507b435c24a8986afe80d5951782"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Qwen\/Qwen1.5-1.8B",
        "Average":46.36,
        "ARC":37.88,
        "HellaSwag":61.42,
        "MMLU":46.71,
        "TruthfulQA":39.43,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":1.84,
        "Hub":23,
        "Available on the hub":false,
        "Model Sha":"921f88e4573192da5a10c809ed188603ea0f3937"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"arshadshk\/Mistral-Hinglish-7B-Instruct-v0.2",
        "Average":46.35,
        "ARC":40.36,
        "HellaSwag":71.98,
        "MMLU":23.12,
        "TruthfulQA":49.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"987a8027701ba1bda62ae86a57051b8b18ce7ef3"
    },
    {
        "T":"\u2b55",
        "Model":"llm-agents\/tora-code-13b-v1.0",
        "Average":46.35,
        "ARC":44.45,
        "HellaSwag":69.29,
        "MMLU":36.67,
        "TruthfulQA":34.98,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"4bf5b528d95a507b435c24a8986afe80d5951782"
    },
    {
        "T":"?",
        "Model":"aevalone\/Pengland-Merge",
        "Average":46.34,
        "ARC":40.53,
        "HellaSwag":47.06,
        "MMLU":50.72,
        "TruthfulQA":47.03,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"404bfbd322f0f5168d23a1ba8dff85e46d971db2"
    },
    {
        "T":"?",
        "Model":"Qwen\/Qwen1.5-1.8B-Chat",
        "Average":46.31,
        "ARC":38.74,
        "HellaSwag":60.02,
        "MMLU":45.87,
        "TruthfulQA":40.62,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.84,
        "Hub":31,
        "Available on the hub":false,
        "Model Sha":"3aede71902ad578aac72678f9f8b6199ca6ab53b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/oasst-gpt-neox-20b-1000-steps",
        "Average":46.29,
        "ARC":48.55,
        "HellaSwag":74.61,
        "MMLU":26.39,
        "TruthfulQA":35.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":20.24,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4aec11ef19103796fb21387ce925b63c9d61dae1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AlekseyKorshuk\/pygmalion-6b-vicuna-chatml",
        "Average":46.26,
        "ARC":40.61,
        "HellaSwag":67.73,
        "MMLU":33.92,
        "TruthfulQA":42.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"ee3ada91a69a194cedfabbfeab98f1499b75cb44"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/opt-66b",
        "Average":46.25,
        "ARC":46.33,
        "HellaSwag":76.25,
        "MMLU":26.99,
        "TruthfulQA":35.43,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.72,
        "Hub":171,
        "Available on the hub":true,
        "Model Sha":"7259969061237fe940036d22bea0fd349e4485e9"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Qwen-1_8b-EverythingLM",
        "Average":46.24,
        "ARC":38.65,
        "HellaSwag":62.66,
        "MMLU":44.94,
        "TruthfulQA":38.7,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.84,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"41d81d4bc5408e4632c967448eb8ec22851fdef5"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Salesforce\/codegen-16B-nl",
        "Average":46.23,
        "ARC":46.76,
        "HellaSwag":71.87,
        "MMLU":32.35,
        "TruthfulQA":33.95,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bsd-3-clause",
        "#Params (B)":15.72,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"b65951b0cf7c5639f73caea801a892788608ed69"
    },
    {
        "T":"\u2b55",
        "Model":"Vmware\/open-llama-7b-v2-open-instruct",
        "Average":46.19,
        "ARC":39.76,
        "HellaSwag":70.31,
        "MMLU":35.16,
        "TruthfulQA":39.53,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":6.61,
        "Hub":23,
        "Available on the hub":true,
        "Model Sha":"b8fbe09571a71603ab517fe897a1281005060b62"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mosaicml\/mpt-7b-storywriter",
        "Average":46.18,
        "ARC":45.65,
        "HellaSwag":74.14,
        "MMLU":28.8,
        "TruthfulQA":36.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":679,
        "Available on the hub":true,
        "Model Sha":"a5e85ae1941e31bb705adbcafce9b0dfd6f3a48b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"acrastt\/Marx-3B-V2",
        "Average":46.18,
        "ARC":44.03,
        "HellaSwag":72.92,
        "MMLU":27.84,
        "TruthfulQA":39.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"5fba568304f6f876f5b9e42026f986ea245b836b"
    },
    {
        "T":"?",
        "Model":"l3utterfly\/minima-3b-layla-v1",
        "Average":46.17,
        "ARC":42.32,
        "HellaSwag":67.48,
        "MMLU":28.44,
        "TruthfulQA":46.46,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":2.87,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"844bfa44b1b3cdd1c0e39c13fbb2fdaee82ff874"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Technoculture\/MT7Bi-sft",
        "Average":46.16,
        "ARC":41.81,
        "HellaSwag":56.83,
        "MMLU":41.4,
        "TruthfulQA":44.61,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"c1c15fc44948638d938d56d76b3af8b8fd516193"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"glaiveai\/glaive-coder-7b",
        "Average":46.1,
        "ARC":42.66,
        "HellaSwag":64.69,
        "MMLU":37.15,
        "TruthfulQA":39.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":52,
        "Available on the hub":true,
        "Model Sha":"72a255a58480ef0713eed988312fe82f77f94f37"
    },
    {
        "T":"?",
        "Model":"liminerity\/ultra0",
        "Average":46.09,
        "ARC":41.47,
        "HellaSwag":68.02,
        "MMLU":33.37,
        "TruthfulQA":41.49,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.83,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"046f98426c1b0da043e82a110f9690268b826b5f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"openlm-research\/open_llama_7b",
        "Average":46.08,
        "ARC":47.01,
        "HellaSwag":71.98,
        "MMLU":30.49,
        "TruthfulQA":34.85,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":99,
        "Available on the hub":true,
        "Model Sha":"6fb184ff23774c25bf84b3628e49c8b78372c7be"
    },
    {
        "T":"?",
        "Model":"h2oai\/h2o-danube-1.8b-chat",
        "Average":46.06,
        "ARC":41.13,
        "HellaSwag":68.06,
        "MMLU":33.41,
        "TruthfulQA":41.64,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.83,
        "Hub":49,
        "Available on the hub":false,
        "Model Sha":"e2a18423798fa43e6c9935073d9c24c0cd901c6d"
    },
    {
        "T":"?",
        "Model":"Fredithefish\/ReasonixPajama-3B-HF",
        "Average":46.06,
        "ARC":39.25,
        "HellaSwag":63.47,
        "MMLU":26.09,
        "TruthfulQA":55.42,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.91,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"fa87c904b5921231b9f6f94b9c537cdda8783b96"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/GPT-NeoXT-Chat-Base-20B",
        "Average":46.03,
        "ARC":45.65,
        "HellaSwag":74.03,
        "MMLU":29.92,
        "TruthfulQA":34.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub":688,
        "Available on the hub":true,
        "Model Sha":"d386708e84d862a65f7d2b4989f64750cb657227"
    },
    {
        "T":"?",
        "Model":"psmathur\/orca_mini_13b",
        "Average":46.0,
        "ARC":42.06,
        "HellaSwag":63.4,
        "MMLU":35.43,
        "TruthfulQA":43.1,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub":93,
        "Available on the hub":true,
        "Model Sha":"ca900c8f3145de40cd188c559b2901a2e4711546"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"RWKV\/rwkv-raven-14b",
        "Average":45.93,
        "ARC":44.62,
        "HellaSwag":71.25,
        "MMLU":25.92,
        "TruthfulQA":41.93,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.89,
        "Hub":43,
        "Available on the hub":true,
        "Model Sha":"359c0649b4f1d10a26ebea32908035bc00d152ee"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gmonsoon\/Qwenchana-1.8B",
        "Average":45.88,
        "ARC":38.23,
        "HellaSwag":59.92,
        "MMLU":45.78,
        "TruthfulQA":39.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":1.84,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7c793e84971c4ebd9c8ec10011f003d8063514b2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-oasst1-512-20b",
        "Average":45.86,
        "ARC":46.93,
        "HellaSwag":72.77,
        "MMLU":26.25,
        "TruthfulQA":37.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub":38,
        "Available on the hub":true,
        "Model Sha":"3bdf6f870ca14bcc5587b666fbe57488f7854d30"
    },
    {
        "T":"?",
        "Model":"Aryanne\/sheared-plus-westlake-normal",
        "Average":45.85,
        "ARC":39.76,
        "HellaSwag":70.33,
        "MMLU":26.81,
        "TruthfulQA":46.5,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.7,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"9965e14e37b22a35877eb210f28dcad60248c22b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"acrastt\/Marx-3B",
        "Average":45.85,
        "ARC":43.17,
        "HellaSwag":72.68,
        "MMLU":28.46,
        "TruthfulQA":39.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"c0dcc44989cf4e006efae31abbcef7e8be8547c0"
    },
    {
        "T":"?",
        "Model":"aloobun\/Synch-Qwen1.5-1.8B",
        "Average":45.85,
        "ARC":36.95,
        "HellaSwag":60.19,
        "MMLU":44.82,
        "TruthfulQA":41.44,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":1.84,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"5dede2620e5a01e039d3bdec9aa96b55610cd5b8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"M4-ai\/tau-1.8B",
        "Average":45.78,
        "ARC":37.2,
        "HellaSwag":60.26,
        "MMLU":45.96,
        "TruthfulQA":39.72,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":1.84,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"8ed0d61c24c9297dd35ade2716717a45db9488d8"
    },
    {
        "T":"?",
        "Model":"M4-ai\/NeuralReyna-Mini-1.8B-v0.3",
        "Average":45.73,
        "ARC":35.58,
        "HellaSwag":61.13,
        "MMLU":44.22,
        "TruthfulQA":41.99,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.84,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"38905b74c36b45f23f416d68dc2f755c81524763"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"occultml\/Helios-10.7B-v2",
        "Average":45.72,
        "ARC":39.16,
        "HellaSwag":46.63,
        "MMLU":41.57,
        "TruthfulQA":55.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"79b8aaa82a404ee79cbd724213d3c85910e4dec2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"klosax\/open_llama_13b_600bt_preview",
        "Average":45.71,
        "ARC":44.28,
        "HellaSwag":72.43,
        "MMLU":31.47,
        "TruthfulQA":34.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3465eaca4d293ccc6ce66888e6c8bd9032ae7071"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Deita-Qwen-1_8B",
        "Average":45.7,
        "ARC":36.52,
        "HellaSwag":60.63,
        "MMLU":45.62,
        "TruthfulQA":40.02,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":8.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"63c53af85aaddc99f8e981a1b07cf10c457c35e9"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Deita-1_8B",
        "Average":45.7,
        "ARC":36.52,
        "HellaSwag":60.63,
        "MMLU":45.62,
        "TruthfulQA":40.02,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.84,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"7709179d3919f48660b0bf58e5efcca2c45e2659"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aloobun\/Reyna-Mini-1.8B-v0.2",
        "Average":45.69,
        "ARC":36.6,
        "HellaSwag":60.19,
        "MMLU":44.75,
        "TruthfulQA":41.24,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":1.84,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"c754104ac85b9a598fb1f3c7b879af7f87a466ab"
    },
    {
        "T":"?",
        "Model":"abhinand\/tamil-llama-7b-instruct-v0.2",
        "Average":45.64,
        "ARC":40.44,
        "HellaSwag":68.88,
        "MMLU":23.12,
        "TruthfulQA":50.11,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.68,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"eef294818ba3fa799055e80ea28d12d2b7176070"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"qnguyen3\/quan-1.8b-base",
        "Average":45.61,
        "ARC":36.95,
        "HellaSwag":58.46,
        "MMLU":45.44,
        "TruthfulQA":41.6,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":1.53,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"ae98577d590a16cdbad681e981c5b431f9e246ea"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"occultml\/Helios-10.7B",
        "Average":45.61,
        "ARC":38.91,
        "HellaSwag":46.6,
        "MMLU":41.4,
        "TruthfulQA":55.52,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7f6e3c76304241500e010979e243d712a0dedb67"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aloobun\/Reyna-Mini-1.8B-v0.1",
        "Average":45.61,
        "ARC":35.24,
        "HellaSwag":60.42,
        "MMLU":45.37,
        "TruthfulQA":41.4,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":1.84,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"beb82e3131ebd6a9fea636b0f009adaa19a6f72d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"netcat420\/MFANN3b",
        "Average":45.59,
        "ARC":43.09,
        "HellaSwag":72.33,
        "MMLU":26.74,
        "TruthfulQA":40.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7be536931e52d73586e46782ddd2e1ba1089533f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"KnutJaegersberg\/Qwen-1_8B-Llamafied",
        "Average":45.59,
        "ARC":37.71,
        "HellaSwag":58.87,
        "MMLU":46.37,
        "TruthfulQA":39.41,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.84,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"2d58d553f3b54abbb6cc49cdb4f2b47336c3c17e"
    },
    {
        "T":"\u2b55",
        "Model":"vihangd\/shearedplats-2.7b-v2",
        "Average":45.56,
        "ARC":42.41,
        "HellaSwag":72.58,
        "MMLU":27.52,
        "TruthfulQA":39.76,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":2.62,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"2837296f28d6aa0fb6c1fe382f553e65c8e1e5f3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhinand\/tamil-llama-7b-instruct-v0.2",
        "Average":45.54,
        "ARC":40.19,
        "HellaSwag":68.83,
        "MMLU":23.12,
        "TruthfulQA":50.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.68,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"eef294818ba3fa799055e80ea28d12d2b7176070"
    },
    {
        "T":"\u2b55",
        "Model":"synapsoft\/Llama-2-7b-hf-flan2022-1.2M",
        "Average":45.52,
        "ARC":23.29,
        "HellaSwag":78.46,
        "MMLU":42.33,
        "TruthfulQA":37.97,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"792f946a1413a7c58378d7a350b7d75b9df80561"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/oasst-sft-1-pythia-12b",
        "Average":45.45,
        "ARC":46.42,
        "HellaSwag":70.0,
        "MMLU":26.19,
        "TruthfulQA":39.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub":277,
        "Available on the hub":true,
        "Model Sha":"293df535fe7711a5726987fc2f17dfc87de452a1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"frankenmerger\/gemoy-4b-instruct",
        "Average":45.45,
        "ARC":40.7,
        "HellaSwag":58.03,
        "MMLU":36.42,
        "TruthfulQA":46.64,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":4.05,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"18dae1ff443a44fa20b40b21044a6601b6544d56"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Herry443\/Mistral-7B-KNUT-ref-en",
        "Average":45.43,
        "ARC":38.99,
        "HellaSwag":70.7,
        "MMLU":23.12,
        "TruthfulQA":48.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ae87363a861afdf62bf6797b065beb8b749e9981"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Danielbrdz\/Barcenas-3b",
        "Average":45.43,
        "ARC":43.17,
        "HellaSwag":67.82,
        "MMLU":29.16,
        "TruthfulQA":41.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":2.87,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"2b6b8bfd3946c02fa4a5182ed008df8ad324a406"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vibhorag101\/llama-2-13b-chat-hf-phr_mental_therapy",
        "Average":45.41,
        "ARC":38.82,
        "HellaSwag":72.76,
        "MMLU":23.12,
        "TruthfulQA":46.92,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"0fe5a48f3d99492cb180fc6efda5b138677ca1de"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"golaxy\/gogpt-7b-bloom",
        "Average":45.4,
        "ARC":44.62,
        "HellaSwag":62.56,
        "MMLU":33.81,
        "TruthfulQA":40.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.07,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"8f9996f852db583b982efbd671465d18ad13ffae"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2o-danube-1.8b-sft",
        "Average":45.39,
        "ARC":40.19,
        "HellaSwag":67.34,
        "MMLU":33.75,
        "TruthfulQA":40.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.83,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"251a6e5b0749135c6109532734b803d15dd49b7a"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Writer\/palmyra-large",
        "Average":45.32,
        "ARC":44.97,
        "HellaSwag":71.85,
        "MMLU":28.54,
        "TruthfulQA":35.93,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.26,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"40086d791942cb28f55e679cd3fb6f6b5ba4effd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/orca_mini_7b",
        "Average":45.29,
        "ARC":43.94,
        "HellaSwag":65.22,
        "MMLU":29.97,
        "TruthfulQA":42.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.61,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"6ed0dca683685cb5b9e7df599f87d311f00ba6db"
    },
    {
        "T":"?",
        "Model":"M4-ai\/NeuralReyna-Mini-1.8B-v0.2",
        "Average":45.27,
        "ARC":37.8,
        "HellaSwag":60.51,
        "MMLU":45.04,
        "TruthfulQA":37.75,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.84,
        "Hub":10,
        "Available on the hub":false,
        "Model Sha":"a38dc9a562b52fe228636ac9099e121524187bf1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"qblocks\/codellama_7b_DolphinCoder",
        "Average":45.26,
        "ARC":41.98,
        "HellaSwag":65.5,
        "MMLU":38.11,
        "TruthfulQA":35.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"7a0aaba040ae0b122737172db4581f2d0b1064bf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Zangs3011\/codellama_7b_DolphinCoder",
        "Average":45.26,
        "ARC":41.98,
        "HellaSwag":65.5,
        "MMLU":38.11,
        "TruthfulQA":35.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"600d70148047ad1ec7cb99a596dfeb8ba6a2c42c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CobraMamba\/mamba-gpt-3b-v4",
        "Average":45.23,
        "ARC":42.58,
        "HellaSwag":71.04,
        "MMLU":30.04,
        "TruthfulQA":37.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"49cdf710c1a9178ddf616da79211fdcdb2170c3f"
    },
    {
        "T":"?",
        "Model":"dvruette\/oasst-pythia-12b-6000-steps",
        "Average":45.22,
        "ARC":45.39,
        "HellaSwag":69.68,
        "MMLU":25.97,
        "TruthfulQA":39.85,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":11.58,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e2ccc0ef8d1cc5ffc8b0e2e885f03ef50597ea8a"
    },
    {
        "T":"?",
        "Model":"Mihaiii\/dolphin-2.6-mistral-7b-dpo-5.93B",
        "Average":45.21,
        "ARC":38.99,
        "HellaSwag":61.01,
        "MMLU":27.32,
        "TruthfulQA":53.51,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.93,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"eb789a38c4c01a2f0c7130123de0e7806a9b4a8a"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"codellama\/CodeLlama-13b-hf",
        "Average":45.21,
        "ARC":40.87,
        "HellaSwag":63.35,
        "MMLU":32.81,
        "TruthfulQA":43.79,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":44,
        "Available on the hub":true,
        "Model Sha":"55876f398020b287ac845b34ca08089acf4f4bc3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/CodeLlama-13b-hf",
        "Average":45.21,
        "ARC":40.87,
        "HellaSwag":63.35,
        "MMLU":32.81,
        "TruthfulQA":43.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b7cfbbce945b966607d15ae275704922a6d04afc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vitruv\/vitruv_2",
        "Average":45.2,
        "ARC":43.34,
        "HellaSwag":68.02,
        "MMLU":32.98,
        "TruthfulQA":36.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"db9d4443473291aedc6765283d925156c0736a85"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-gm-oasst1-multilang-1024-20b",
        "Average":45.19,
        "ARC":47.44,
        "HellaSwag":72.58,
        "MMLU":26.37,
        "TruthfulQA":34.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"b3a6bf4250a037c09e451344e2a4e987011b79de"
    },
    {
        "T":"?",
        "Model":"M4-ai\/Hercules-Mini-1.8B",
        "Average":45.14,
        "ARC":37.03,
        "HellaSwag":59.53,
        "MMLU":44.77,
        "TruthfulQA":39.24,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":1.84,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"1210ddfe213da00db50df5553e1c362e337af9e9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lvxy1117\/amber_fine_tune_ori",
        "Average":45.13,
        "ARC":44.45,
        "HellaSwag":75.1,
        "MMLU":26.04,
        "TruthfulQA":34.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3d98a26f005cdace09b4ddd9c4ea67ba508946ad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"VMware\/open-llama-0.7T-7B-open-instruct-v1.1",
        "Average":45.12,
        "ARC":46.67,
        "HellaSwag":67.67,
        "MMLU":28.55,
        "TruthfulQA":37.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc",
        "#Params (B)":6.61,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"75741b55ad462330e3498d1506f438f835152177"
    },
    {
        "T":"\u2b55",
        "Model":"AI-Sweden-Models\/gpt-sw3-6.7b-v2-instruct",
        "Average":45.11,
        "ARC":40.78,
        "HellaSwag":67.77,
        "MMLU":31.57,
        "TruthfulQA":40.32,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.11,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"81ca95a4e93746240994d1e6797ffa64dc796bd9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"harborwater\/open-llama-3b-everything-v2",
        "Average":45.06,
        "ARC":42.83,
        "HellaSwag":73.28,
        "MMLU":26.87,
        "TruthfulQA":37.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"31ce2c1611d9f7d56184ceb5bff6a7e95a180c03"
    },
    {
        "T":"?",
        "Model":"dvruette\/oasst-pythia-12b-flash-attn-5000-steps",
        "Average":45.06,
        "ARC":44.97,
        "HellaSwag":69.75,
        "MMLU":26.64,
        "TruthfulQA":38.89,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":11.58,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5227ec9c9def4b0bdf6c7ad95d9f77cbf458283d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hakurei\/mommygpt-3B",
        "Average":45.06,
        "ARC":41.89,
        "HellaSwag":71.69,
        "MMLU":28.74,
        "TruthfulQA":37.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"0369335d693b753774050ae44dbaf73bac39e9eb"
    },
    {
        "T":"?",
        "Model":"axxd\/wizardllama-7b",
        "Average":45.05,
        "ARC":42.83,
        "HellaSwag":66.2,
        "MMLU":35.44,
        "TruthfulQA":35.71,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e399cd6c8855d103be1fb31c797890861db25d12"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/oasst-gpt-neox-20b-3000-steps",
        "Average":45.05,
        "ARC":46.42,
        "HellaSwag":72.08,
        "MMLU":26.16,
        "TruthfulQA":35.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":20.24,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f0462a8b7908f61202d86e6a9a2996d8339363b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"harborwater\/open-llama-3b-v2-wizard-evol-instuct-v2-196k",
        "Average":45.04,
        "ARC":41.81,
        "HellaSwag":73.01,
        "MMLU":26.36,
        "TruthfulQA":38.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"unknown",
        "#Params (B)":3.43,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4da0c661e6df1235c9997b996c8e395b87248406"
    },
    {
        "T":"?",
        "Model":"aloobun\/Cypher-Laser-Mixtral-2x1.8B-v0.1",
        "Average":45.04,
        "ARC":40.44,
        "HellaSwag":67.6,
        "MMLU":31.49,
        "TruthfulQA":40.62,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.1,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"f8a0f5a7244b5631a237efc54ae734b373de7a34"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"GeneZC\/MiniMA-3B",
        "Average":44.98,
        "ARC":43.43,
        "HellaSwag":68.06,
        "MMLU":28.69,
        "TruthfulQA":39.76,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.02,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"0a2f9d6bbb3959d68fe52e07ee6f54e8242f91ec"
    },
    {
        "T":"?",
        "Model":"aloobun\/Cypher-Mixtral-2x1.8B-v0.1",
        "Average":44.97,
        "ARC":40.44,
        "HellaSwag":67.7,
        "MMLU":31.81,
        "TruthfulQA":39.94,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.1,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"89bf138c4fef534dd049898eea3791fddb88ce49"
    },
    {
        "T":"\u2b55",
        "Model":"mwitiderrick\/shearedplats-2.7b-v2-instruct-v0.1",
        "Average":44.9,
        "ARC":40.19,
        "HellaSwag":70.08,
        "MMLU":28.12,
        "TruthfulQA":41.23,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.7,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"8eb300dc6a62166048f7ec997a0a2d8d9a5708f2"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Qwen-1_8B-Chat-llama",
        "Average":44.88,
        "ARC":36.95,
        "HellaSwag":54.34,
        "MMLU":44.55,
        "TruthfulQA":43.7,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.84,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"a84c11285875fecd9c1cc4e22543efbd4f89f5fe"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"NucleusAI\/nucleus-22B-token-500B",
        "Average":44.84,
        "ARC":40.7,
        "HellaSwag":69.39,
        "MMLU":30.11,
        "TruthfulQA":39.16,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":21.83,
        "Hub":25,
        "Available on the hub":true,
        "Model Sha":"49bb1a47c0d32b4bfa6630a4eff04a857adcd4ca"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RobbeD\/OpenLlama-Platypus-3B",
        "Average":44.8,
        "ARC":41.21,
        "HellaSwag":71.67,
        "MMLU":29.86,
        "TruthfulQA":36.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":3.43,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d3a0bf8e1181be02cc9c4c4cdfedaedacaefbfac"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/opt-30b",
        "Average":44.79,
        "ARC":43.26,
        "HellaSwag":74.07,
        "MMLU":26.66,
        "TruthfulQA":35.16,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":29.98,
        "Hub":131,
        "Available on the hub":true,
        "Model Sha":"ceea0a90ac0f6fae7c2c34bcb40477438c152546"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"acrastt\/Puma-3B",
        "Average":44.75,
        "ARC":41.3,
        "HellaSwag":71.85,
        "MMLU":27.51,
        "TruthfulQA":38.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"1159e9cdd05c03d31331f329ba58e4e3444943be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/oasst-sft-4-pythia-12b-epoch-3.5",
        "Average":44.74,
        "ARC":45.73,
        "HellaSwag":68.59,
        "MMLU":26.82,
        "TruthfulQA":37.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub":330,
        "Available on the hub":true,
        "Model Sha":"626b8c140cfdedb119dfb78c626cd772283dee33"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/oasst-pythia-12b-pretrained-sft",
        "Average":44.74,
        "ARC":45.31,
        "HellaSwag":67.67,
        "MMLU":27.81,
        "TruthfulQA":38.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":11.58,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c21fbece4253841f2d6e15f04f60fe1ba6f990dd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Devio\/test-9k-fn",
        "Average":44.73,
        "ARC":40.87,
        "HellaSwag":69.45,
        "MMLU":29.47,
        "TruthfulQA":39.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b2fc754748ee94428298de3528e549b296d51c1e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/GPT-R",
        "Average":44.71,
        "ARC":41.21,
        "HellaSwag":66.89,
        "MMLU":36.5,
        "TruthfulQA":34.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":5.84,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"92b955a3ff74aa577fa0d8517dfc314847ef60af"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"acrastt\/Griffin-3B",
        "Average":44.7,
        "ARC":41.81,
        "HellaSwag":72.3,
        "MMLU":26.36,
        "TruthfulQA":38.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"edbea6fe86d0bc2673c10269828008a1cb451919"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"namirocks\/mistral-shishya-all-hal-model-7b-ep3",
        "Average":44.68,
        "ARC":37.97,
        "HellaSwag":77.77,
        "MMLU":26.56,
        "TruthfulQA":36.43,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"582f60bf69f13025142983fa4b655049d65efd0a"
    },
    {
        "T":"?",
        "Model":"aloobun\/Cypher-Mini-1.8B",
        "Average":44.65,
        "ARC":39.59,
        "HellaSwag":67.45,
        "MMLU":31.14,
        "TruthfulQA":40.44,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.83,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"9d34981cf180b4e84bdf32e39aacb4056a72d406"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"togethercomputer\/RedPajama-INCITE-Base-7B-v0.1",
        "Average":44.65,
        "ARC":46.25,
        "HellaSwag":71.63,
        "MMLU":27.68,
        "TruthfulQA":33.03,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":85,
        "Available on the hub":true,
        "Model Sha":"78f7e482443971f4873ba3239f0ac810a367833b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"togethercomputer\/RedPajama-INCITE-7B-Base",
        "Average":44.65,
        "ARC":46.25,
        "HellaSwag":71.63,
        "MMLU":27.68,
        "TruthfulQA":33.03,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":85,
        "Available on the hub":true,
        "Model Sha":"78f7e482443971f4873ba3239f0ac810a367833b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aloobun\/open-llama-3b-v2-elmv3",
        "Average":44.62,
        "ARC":42.06,
        "HellaSwag":73.28,
        "MMLU":27.61,
        "TruthfulQA":35.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7e43b199ff51dc0e63934ba49758a8a31ff855de"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"harborwater\/open-llama-3b-v2-wizard-evol-instuct-v2-196k",
        "Average":44.59,
        "ARC":41.21,
        "HellaSwag":72.88,
        "MMLU":25.39,
        "TruthfulQA":38.87,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"unknown",
        "#Params (B)":3.43,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4da0c661e6df1235c9997b996c8e395b87248406"
    },
    {
        "T":"?",
        "Model":"Chickaboo\/ChickaQ-V2-Large-Beta",
        "Average":44.59,
        "ARC":34.3,
        "HellaSwag":57.87,
        "MMLU":42.33,
        "TruthfulQA":43.85,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":3.05,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"792fc755898baca487114a97a2fb490de3349ab6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hyunseoki\/ko-ref-llama2-7b",
        "Average":44.57,
        "ARC":42.66,
        "HellaSwag":66.58,
        "MMLU":30.41,
        "TruthfulQA":38.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"1ee08c79ae7393473754b77e82b1472ef63d5dd2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aloobun\/open-llama-3b-v2-elmv3",
        "Average":44.52,
        "ARC":42.15,
        "HellaSwag":73.26,
        "MMLU":27.16,
        "TruthfulQA":35.51,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7e43b199ff51dc0e63934ba49758a8a31ff855de"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"harborwater\/wizard-orca-3b",
        "Average":44.51,
        "ARC":41.72,
        "HellaSwag":71.78,
        "MMLU":24.49,
        "TruthfulQA":40.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"ffc81b58375342f12e38a67272d95458a72e8d09"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CobraMamba\/mamba-gpt-3b-v3",
        "Average":44.49,
        "ARC":41.72,
        "HellaSwag":71.05,
        "MMLU":27.31,
        "TruthfulQA":37.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"d860a90ef6b30c695b985dd2ff382d4bbb80e857"
    },
    {
        "T":"\u2b55",
        "Model":"deepseek-ai\/deepseek-coder-6.7b-instruct",
        "Average":44.45,
        "ARC":38.14,
        "HellaSwag":55.09,
        "MMLU":39.02,
        "TruthfulQA":45.56,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":258,
        "Available on the hub":true,
        "Model Sha":"cbb77d7448ea3168d884758817e7f895e3828d1c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kabster\/BioMistral-MedicalQA-FT",
        "Average":44.41,
        "ARC":40.02,
        "HellaSwag":67.26,
        "MMLU":23.12,
        "TruthfulQA":47.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4f58943fb487b3fe3bb467bfd69a255af18b5c37"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Danielbrdz\/CodeBarcenas-7b",
        "Average":44.41,
        "ARC":42.32,
        "HellaSwag":63.43,
        "MMLU":33.39,
        "TruthfulQA":38.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"fe7a232baac5394e821f349cb7ef31dbd4ca2078"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"l3utterfly\/open-llama-3b-v2-layla",
        "Average":44.4,
        "ARC":38.23,
        "HellaSwag":66.43,
        "MMLU":28.56,
        "TruthfulQA":44.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"465669ddafad25393ac3cfe94d3726cced112b30"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CobraMamba\/mamba-gpt-3b-v2",
        "Average":44.37,
        "ARC":42.15,
        "HellaSwag":71.5,
        "MMLU":27.1,
        "TruthfulQA":36.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"935f4d90bd0fc7117113d3c7b6b6af9dba93183d"
    },
    {
        "T":"\u2b55",
        "Model":"mediocredev\/open-llama-3b-v2-chat",
        "Average":44.37,
        "ARC":40.61,
        "HellaSwag":70.3,
        "MMLU":28.73,
        "TruthfulQA":37.84,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"0d171b62a41b2d249cd2ff235b66638e3a894c98"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/pythia-12b-sft-v8-7k-steps",
        "Average":44.35,
        "ARC":44.03,
        "HellaSwag":70.28,
        "MMLU":26.55,
        "TruthfulQA":36.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"275c9b71bfab4e271d1ed85515c61e317b6ef65e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhinand\/telugu-llama-7b-instruct-v0.1",
        "Average":44.3,
        "ARC":37.12,
        "HellaSwag":67.92,
        "MMLU":23.12,
        "TruthfulQA":49.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.68,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"5bd17c1a901cd080b0abf11a25ff9f516ad73fa1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/GPT-NeoX-20B-Erebus",
        "Average":44.3,
        "ARC":45.48,
        "HellaSwag":72.79,
        "MMLU":26.77,
        "TruthfulQA":32.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub":68,
        "Available on the hub":true,
        "Model Sha":"1a80940a290452af71caf17a8e520955eb338e0f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RobbeD\/Orca-Platypus-3B",
        "Average":44.27,
        "ARC":43.09,
        "HellaSwag":65.33,
        "MMLU":26.75,
        "TruthfulQA":41.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":3.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"243f51d75ed6d425addde839740f6fd5bcc4630f"
    },
    {
        "T":"?",
        "Model":"nomic-ai\/gpt4all-j",
        "Average":44.25,
        "ARC":41.98,
        "HellaSwag":64.06,
        "MMLU":28.2,
        "TruthfulQA":42.78,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub":241,
        "Available on the hub":true,
        "Model Sha":"73c15208cb608be2949b7c6e4ba6d88f0176c267"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"princeton-nlp\/Sheared-LLaMA-2.7B",
        "Average":44.24,
        "ARC":41.72,
        "HellaSwag":71.01,
        "MMLU":26.92,
        "TruthfulQA":37.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.62,
        "Hub":54,
        "Available on the hub":true,
        "Model Sha":"16347024c4df6cd114720958964a850fc287cac0"
    },
    {
        "T":"?",
        "Model":"abhinand\/telugu-llama-7b-instruct-v0.1",
        "Average":44.23,
        "ARC":36.95,
        "HellaSwag":67.88,
        "MMLU":23.12,
        "TruthfulQA":48.97,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.68,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"5bd17c1a901cd080b0abf11a25ff9f516ad73fa1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xaviviro\/FLAMA-0.1-3B",
        "Average":44.23,
        "ARC":41.72,
        "HellaSwag":71.41,
        "MMLU":26.59,
        "TruthfulQA":37.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"380f8c1a59a0e60e704b22720af1494801b57e85"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eren23\/finetune_test_qwen15-1-8b-sft-lora",
        "Average":44.22,
        "ARC":36.18,
        "HellaSwag":57.77,
        "MMLU":44.96,
        "TruthfulQA":38.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":8.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"07c78da7631c0e3b0f22558803de182d9255a19b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"harborwater\/open-llama-3b-claude-30k",
        "Average":44.21,
        "ARC":41.72,
        "HellaSwag":72.64,
        "MMLU":24.03,
        "TruthfulQA":38.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"049db7fda44e5ce1e8febf5c3f45e3a93aaaa859"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-gm-oasst1-en-1024-12b",
        "Average":44.18,
        "ARC":43.09,
        "HellaSwag":69.75,
        "MMLU":25.87,
        "TruthfulQA":38.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.59,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"e547fffafb382fd39ef5de35ba3b5afc1b43e74d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AtAndDev\/ShortKing-3b-v0.3",
        "Average":44.17,
        "ARC":40.96,
        "HellaSwag":70.72,
        "MMLU":26.21,
        "TruthfulQA":38.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":3.43,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"4bcf1610eb1f3959568d5acee74833c41502bf04"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/pythia-12b-sft-v8-2.5k-steps",
        "Average":44.14,
        "ARC":42.32,
        "HellaSwag":70.15,
        "MMLU":27.36,
        "TruthfulQA":36.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"142e306db8e279a07c557ea5a919ab7e7a4af17c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AlekseyKorshuk\/chatml-pyg-v1",
        "Average":44.14,
        "ARC":37.88,
        "HellaSwag":63.29,
        "MMLU":32.77,
        "TruthfulQA":42.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"79d5a4d53953ca1c26bc2155f168b7e2108f377f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"freecs\/ThetaWave-28B-v0.1",
        "Average":44.13,
        "ARC":36.6,
        "HellaSwag":35.54,
        "MMLU":54.5,
        "TruthfulQA":49.86,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":28.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9efeb3784333a072be4db0b6e413e319327d89e5"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AI-Sweden-Models\/gpt-sw3-20b",
        "Average":44.03,
        "ARC":41.81,
        "HellaSwag":68.75,
        "MMLU":28.47,
        "TruthfulQA":37.1,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":20.92,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"36797b7835a9e656af456e0006465a3af48735fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/LongAlpaca-7B",
        "Average":44.0,
        "ARC":42.66,
        "HellaSwag":65.89,
        "MMLU":27.28,
        "TruthfulQA":40.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"bebfcb894b3f5170ce54e3bb98b6e565fae7b6c0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/RedPajama-INCITE-7B-Chat",
        "Average":43.98,
        "ARC":42.06,
        "HellaSwag":70.82,
        "MMLU":26.94,
        "TruthfulQA":36.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":88,
        "Available on the hub":true,
        "Model Sha":"47b94a739e2f3164b438501c8684acc5d5acc146"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/RedPajama-INCITE-Chat-7B-v0.1",
        "Average":43.98,
        "ARC":42.06,
        "HellaSwag":70.82,
        "MMLU":26.94,
        "TruthfulQA":36.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":88,
        "Available on the hub":true,
        "Model Sha":"47b94a739e2f3164b438501c8684acc5d5acc146"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"harborwater\/open-llama-3b-everythingLM-2048",
        "Average":43.97,
        "ARC":42.75,
        "HellaSwag":71.72,
        "MMLU":27.16,
        "TruthfulQA":34.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"1f9e8d48163feb63ed190eaa982f393542a75d30"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Javelin-R",
        "Average":43.96,
        "ARC":41.64,
        "HellaSwag":69.01,
        "MMLU":30.7,
        "TruthfulQA":34.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"4c4a5caf5d9049a47f5565b72e5a53dede08ac8b"
    },
    {
        "T":"?",
        "Model":"dvruette\/oasst-pythia-12b-reference",
        "Average":43.95,
        "ARC":43.0,
        "HellaSwag":67.91,
        "MMLU":28.33,
        "TruthfulQA":36.57,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":11.58,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c5a9b7fad884e6c45ce5d2ca551aa1c03db6865f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/gpt-neox-20b",
        "Average":43.95,
        "ARC":45.73,
        "HellaSwag":73.45,
        "MMLU":25.0,
        "TruthfulQA":31.61,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.74,
        "Hub":429,
        "Available on the hub":true,
        "Model Sha":"9369f145ca7b66ef62760f9351af951b2d53b77f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/fairseq-dense-13B",
        "Average":43.94,
        "ARC":40.36,
        "HellaSwag":75.51,
        "MMLU":27.07,
        "TruthfulQA":32.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.84,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"785793f6b216afd9fc664fc63e8e6c776a016825"
    },
    {
        "T":"?",
        "Model":"OpenAssistant\/pythia-12b-sft-v8-rlhf-2k-steps",
        "Average":43.92,
        "ARC":43.43,
        "HellaSwag":70.08,
        "MMLU":26.12,
        "TruthfulQA":36.06,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a0debfed4a020d449e3d00f4e75f2c2aefb68db3"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-7b-Instruct-hf",
        "Average":43.89,
        "ARC":38.31,
        "HellaSwag":59.32,
        "MMLU":36.48,
        "TruthfulQA":41.45,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":55,
        "Available on the hub":true,
        "Model Sha":"7affc442e639b8aa1c4b3e98a10a2f45a21b8b4f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WizardLM\/WizardCoder-Python-7B-V1.0",
        "Average":43.87,
        "ARC":41.81,
        "HellaSwag":65.06,
        "MMLU":32.29,
        "TruthfulQA":36.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":69,
        "Available on the hub":true,
        "Model Sha":"e40673a27a4aefcff2c6d2b3b1e0681a38703e4e"
    },
    {
        "T":"\u2b55",
        "Model":"Rallio67\/7B-redpajama-conditional-alpha",
        "Average":43.86,
        "ARC":42.58,
        "HellaSwag":69.91,
        "MMLU":26.53,
        "TruthfulQA":36.42,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.65,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"9a3f69a1eba3618930f222d4e013d534102a2af5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Javelin-GPTJ",
        "Average":43.85,
        "ARC":42.66,
        "HellaSwag":70.45,
        "MMLU":26.2,
        "TruthfulQA":36.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"bee7068ab002784420a1a30170db3906185359f2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/GPT-NeoX-20B-Skein",
        "Average":43.82,
        "ARC":44.97,
        "HellaSwag":72.68,
        "MMLU":25.99,
        "TruthfulQA":31.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"dd98d514b5aff4e820922c88a73d6d5bf17f332e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhinand\/malayalam-llama-7b-instruct-v0.1",
        "Average":43.81,
        "ARC":37.2,
        "HellaSwag":67.81,
        "MMLU":23.12,
        "TruthfulQA":47.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.67,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"f4a9d167819eaffcfafffc3e52530d0af04efdf4"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"LLM360\/Amber",
        "Average":43.78,
        "ARC":40.96,
        "HellaSwag":73.79,
        "MMLU":26.84,
        "TruthfulQA":33.56,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":59,
        "Available on the hub":true,
        "Model Sha":"a1fb934dd7bbba8eff8c6052fa469f979803236b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Javalion-R",
        "Average":43.75,
        "ARC":41.72,
        "HellaSwag":68.02,
        "MMLU":30.81,
        "TruthfulQA":34.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"b881231ab6ea85da2a9a139f282df85d1d18b002"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-oasst1-512-12b",
        "Average":43.75,
        "ARC":42.32,
        "HellaSwag":70.24,
        "MMLU":26.01,
        "TruthfulQA":36.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.59,
        "Hub":25,
        "Available on the hub":true,
        "Model Sha":"c6bb0fe363e0105839d34ca757793b61c9606f95"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"L-R\/LLmRA-3B-v0.1",
        "Average":43.75,
        "ARC":39.42,
        "HellaSwag":59.79,
        "MMLU":25.16,
        "TruthfulQA":50.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.87,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"7d8a4ccf707de28e924653ba719a18caf8c1db05"
    },
    {
        "T":"?",
        "Model":"abhinand\/malayalam-llama-7b-instruct-v0.1",
        "Average":43.74,
        "ARC":37.03,
        "HellaSwag":67.75,
        "MMLU":23.12,
        "TruthfulQA":47.05,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.67,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"f4a9d167819eaffcfafffc3e52530d0af04efdf4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"acrastt\/Bean-3B",
        "Average":43.72,
        "ARC":40.36,
        "HellaSwag":72.0,
        "MMLU":26.43,
        "TruthfulQA":36.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"4a1ce189a3fb1d58b3fa47ebe30b3c037592670c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/GPT-J-6B-Skein",
        "Average":43.71,
        "ARC":42.58,
        "HellaSwag":68.69,
        "MMLU":24.88,
        "TruthfulQA":38.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"acfe27303f74129930fef5e6fadbc5f58c6b8590"
    },
    {
        "T":"\u2b55",
        "Model":"llm-agents\/tora-code-7b-v1.0",
        "Average":43.68,
        "ARC":40.7,
        "HellaSwag":65.86,
        "MMLU":33.34,
        "TruthfulQA":34.84,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"777501b69bb0ba2675abdcaf7b1309ab05320c2e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"databricks\/dolly-v2-12b",
        "Average":43.67,
        "ARC":42.41,
        "HellaSwag":72.53,
        "MMLU":25.92,
        "TruthfulQA":33.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":11.58,
        "Hub":1860,
        "Available on the hub":true,
        "Model Sha":"19308160448536e378e3db21a73a751579ee7fdd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xilabs\/calypso-3b-alpha-v2",
        "Average":43.64,
        "ARC":41.55,
        "HellaSwag":71.48,
        "MMLU":25.82,
        "TruthfulQA":35.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":3.32,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"933fb9db10f131f7ea54f4e6024ed2acf41c711a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/Dolly_Shygmalion-6b-Dev_V8P2",
        "Average":43.6,
        "ARC":41.38,
        "HellaSwag":67.67,
        "MMLU":28.48,
        "TruthfulQA":36.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"6413b1d9e8b58df9d3aac91a862e8d505d8c6716"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NoIdeaLand\/test-4k-fn",
        "Average":43.59,
        "ARC":39.93,
        "HellaSwag":68.13,
        "MMLU":27.44,
        "TruthfulQA":38.86,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f445760bcc12cc07a1995d178cedb5a78782662b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"databricks\/dolly-v2-7b",
        "Average":43.56,
        "ARC":44.54,
        "HellaSwag":69.64,
        "MMLU":25.18,
        "TruthfulQA":34.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.65,
        "Hub":130,
        "Available on the hub":true,
        "Model Sha":"d632f0c8b75b1ae5b26b250d25bfba4e99cb7c6f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/WizardVicuna-Uncensored-3B-instruct-PL-lora_unload",
        "Average":43.54,
        "ARC":41.98,
        "HellaSwag":66.82,
        "MMLU":25.69,
        "TruthfulQA":39.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":3.32,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"e471ec778771f29992293d1660cc108f29c9c69e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Adventien-GPTJ",
        "Average":43.51,
        "ARC":42.49,
        "HellaSwag":69.21,
        "MMLU":25.4,
        "TruthfulQA":36.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4fbfe9eae03a1d6ecf60fda8cf39c4123f0438bd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_opt13b_10e5",
        "Average":43.51,
        "ARC":42.49,
        "HellaSwag":70.31,
        "MMLU":25.45,
        "TruthfulQA":35.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"08451d85cf04c9f626ad3f8f44508602d877a873"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-tools-7b",
        "Average":43.48,
        "ARC":38.91,
        "HellaSwag":57.69,
        "MMLU":33.24,
        "TruthfulQA":44.08,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"81aefc8983d1192378c2c803f0e0d14d48561117"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"openlm-research\/open_llama_3b_v2",
        "Average":43.44,
        "ARC":40.27,
        "HellaSwag":71.6,
        "MMLU":27.12,
        "TruthfulQA":34.78,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub":49,
        "Available on the hub":true,
        "Model Sha":"bce5d60d3b0c68318862270ec4e794d83308d80a"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/openllama_3b_EvolInstruct_lora_merged",
        "Average":43.44,
        "ARC":40.27,
        "HellaSwag":71.6,
        "MMLU":27.12,
        "TruthfulQA":34.78,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":3.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c55e3e114951346f273c519d266170e4d52781e9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"RWKV\/rwkv-4-14b-pile",
        "Average":43.42,
        "ARC":44.45,
        "HellaSwag":71.07,
        "MMLU":26.12,
        "TruthfulQA":32.04,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.89,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"4effb0fa9d15c2f383a1d159f4a40df0e09eb6d5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lazycuber\/Janemalion-6B",
        "Average":43.42,
        "ARC":42.41,
        "HellaSwag":68.4,
        "MMLU":28.28,
        "TruthfulQA":34.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":5.84,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"e72ae3ec110121115b1ae6c2e5fb3995997a2d96"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/pythia-12b-pre-v8-12.5k-steps",
        "Average":43.42,
        "ARC":41.47,
        "HellaSwag":68.8,
        "MMLU":26.58,
        "TruthfulQA":36.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"37ca702e957a4b740689d67c58c284224e2fbae2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Janin-R",
        "Average":43.38,
        "ARC":40.44,
        "HellaSwag":67.36,
        "MMLU":31.24,
        "TruthfulQA":34.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"f6963f77098d8421ff4a1cf4d36f1e94c6c8f44b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kquant03\/Raiden-16x3.43B",
        "Average":43.38,
        "ARC":41.89,
        "HellaSwag":66.2,
        "MMLU":26.24,
        "TruthfulQA":39.18,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":35.78,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"5403751a298b27603b25c28b1b003cf5f8dbe186"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kfkas\/Llama-2-ko-7b-Chat",
        "Average":43.37,
        "ARC":40.44,
        "HellaSwag":67.16,
        "MMLU":30.4,
        "TruthfulQA":35.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.67,
        "Hub":38,
        "Available on the hub":true,
        "Model Sha":"3293b98cd8204371988f898dafa9b5a297555cbe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"heegyu\/WizardVicuna-Uncensored-3B-0719",
        "Average":43.36,
        "ARC":41.38,
        "HellaSwag":66.19,
        "MMLU":26.53,
        "TruthfulQA":39.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"36841c80535bc3e8403e3cc084e8e65884c75076"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigcode\/starcoder2-7b",
        "Average":43.36,
        "ARC":38.31,
        "HellaSwag":51.91,
        "MMLU":41.21,
        "TruthfulQA":41.99,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":7.17,
        "Hub":125,
        "Available on the hub":false,
        "Model Sha":"a3d33687b51284b528abeb17830776ffd24892a9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-gm-oasst1-en-1024-open-llama-7b-preview-400bt",
        "Average":43.32,
        "ARC":41.3,
        "HellaSwag":62.44,
        "MMLU":27.55,
        "TruthfulQA":42.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"29604e6e19822531b0d49d3f19abef603a97d0ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TaylorAI\/Flash-Llama-3B",
        "Average":43.32,
        "ARC":40.1,
        "HellaSwag":71.56,
        "MMLU":26.88,
        "TruthfulQA":34.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":3.32,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"b4c7bb49171ff6955cfc1f7e33143383c57f7606"
    },
    {
        "T":"\u2b55",
        "Model":"vihangd\/smartyplats-3b-v1",
        "Average":43.3,
        "ARC":40.53,
        "HellaSwag":70.85,
        "MMLU":25.31,
        "TruthfulQA":36.53,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"89272b9edb323f5ace09e097a6449554c0dcd4e7"
    },
    {
        "T":"\u2b55",
        "Model":"vihangd\/smartyplats-3b-v2",
        "Average":43.3,
        "ARC":41.04,
        "HellaSwag":71.19,
        "MMLU":24.32,
        "TruthfulQA":36.66,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"920609897049f674bc4a9678579f6869f6cbed13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kfkas\/Llama-2-ko-7b-Chat",
        "Average":43.3,
        "ARC":40.44,
        "HellaSwag":67.12,
        "MMLU":30.19,
        "TruthfulQA":35.45,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.67,
        "Hub":38,
        "Available on the hub":true,
        "Model Sha":"3293b98cd8204371988f898dafa9b5a297555cbe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Skegma-GPTJ",
        "Average":43.26,
        "ARC":43.77,
        "HellaSwag":69.22,
        "MMLU":25.37,
        "TruthfulQA":34.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4dff006b2ea7e8d9b067dfe8af8ca1a16bc44dce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Javalion-GPTJ",
        "Average":43.22,
        "ARC":41.89,
        "HellaSwag":68.69,
        "MMLU":26.85,
        "TruthfulQA":35.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"3ce176bc0f91cae416c78e99f964f54b12472de0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Pirr\/pythia-13b-deduped-green_devil",
        "Average":43.2,
        "ARC":42.32,
        "HellaSwag":68.89,
        "MMLU":26.01,
        "TruthfulQA":35.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":11.58,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"7faeb395c26189eeab9bf3a98994696687ad31a3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/PPO_Shygmalion-V8p4_Dev-6b",
        "Average":43.15,
        "ARC":40.7,
        "HellaSwag":67.04,
        "MMLU":29.31,
        "TruthfulQA":35.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"fa3d503bca50c947e7a5bbde4bdd82f699f65c02"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/oasst-pythia-6.9b-4000-steps",
        "Average":43.14,
        "ARC":41.64,
        "HellaSwag":64.24,
        "MMLU":26.26,
        "TruthfulQA":40.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.65,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0e201b6f344ac6382dda40d389e1c9144a87d027"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ise-uiuc\/Magicoder-S-DS-6.7B",
        "Average":43.13,
        "ARC":38.31,
        "HellaSwag":54.48,
        "MMLU":38.71,
        "TruthfulQA":41.0,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":189,
        "Available on the hub":true,
        "Model Sha":"cff055b1e110cbe75c0c3759bd436299c6d6bb66"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xaviviro\/FLAMA-0.5-3B",
        "Average":43.11,
        "ARC":37.97,
        "HellaSwag":67.65,
        "MMLU":25.73,
        "TruthfulQA":41.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"eeec9ee7d50953a27189ac64ee63c93a272d1a12"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"heegyu\/WizardVicuna-3B-0719",
        "Average":43.07,
        "ARC":40.7,
        "HellaSwag":65.45,
        "MMLU":25.44,
        "TruthfulQA":40.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"62d3d450b8ab2bd2fb9f82383b55d1ecae33a401"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/orca_mini_3b",
        "Average":43.07,
        "ARC":41.55,
        "HellaSwag":61.52,
        "MMLU":26.79,
        "TruthfulQA":42.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":3.32,
        "Hub":127,
        "Available on the hub":true,
        "Model Sha":"fd2754e80ce80757a3a68a840d7d287dd7def676"
    },
    {
        "T":"?",
        "Model":"OpenAssistant\/galactica-6.7b-finetuned",
        "Average":43.06,
        "ARC":41.55,
        "HellaSwag":51.01,
        "MMLU":38.03,
        "TruthfulQA":41.65,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.66,
        "Hub":34,
        "Available on the hub":true,
        "Model Sha":"d86db70e16111175ff7900f71d40806ccf4b8491"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"frank098\/orca_mini_3b_juniper",
        "Average":43.04,
        "ARC":40.87,
        "HellaSwag":61.73,
        "MMLU":26.37,
        "TruthfulQA":43.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":3.32,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"c08749034baa053834f1b709b6e7b88b914cd1fb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/PPO_Pygway-V8p4_Dev-6b",
        "Average":43.01,
        "ARC":40.36,
        "HellaSwag":67.15,
        "MMLU":29.3,
        "TruthfulQA":35.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"f30709dba36c665869f9ac8cd0cef5a8a2e7c8df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TFLai\/gpt-neox-20b-4bit-alpaca",
        "Average":43.0,
        "ARC":43.86,
        "HellaSwag":67.4,
        "MMLU":25.06,
        "TruthfulQA":35.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":20.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Corianas\/gpt-j-6B-Dolly",
        "Average":42.99,
        "ARC":41.3,
        "HellaSwag":65.97,
        "MMLU":26.78,
        "TruthfulQA":37.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"83d8c754aac12f838d7c847d4352a09396c383d0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/Dolly_Shygmalion-6b",
        "Average":42.97,
        "ARC":41.89,
        "HellaSwag":68.48,
        "MMLU":27.58,
        "TruthfulQA":33.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"108fabf8a916900525492c294c50998d7c09f10b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Janin-GPTJ",
        "Average":42.95,
        "ARC":40.87,
        "HellaSwag":67.29,
        "MMLU":27.4,
        "TruthfulQA":36.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a6773861798f2abea3849514aa6f60961518af9c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/GPT-J-Pyg_PPO-6B-Dev-V8p4",
        "Average":42.94,
        "ARC":40.19,
        "HellaSwag":66.43,
        "MMLU":30.39,
        "TruthfulQA":34.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":5.84,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"930dc82245c607ce43558a0e6c0225e77b341ea6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mwitiderrick\/open_llama_3b_glaive_assistant_v0.1",
        "Average":42.94,
        "ARC":40.7,
        "HellaSwag":67.45,
        "MMLU":27.74,
        "TruthfulQA":35.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"efa950c69b6cbe1f8629400f3a7e0ccd895551fb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mwitiderrick\/open_llama_3b_glaive_v0.1",
        "Average":42.94,
        "ARC":40.7,
        "HellaSwag":67.45,
        "MMLU":27.74,
        "TruthfulQA":35.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"efa950c69b6cbe1f8629400f3a7e0ccd895551fb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mwitiderrick\/open_llama_3b_glaive_code_v0.1",
        "Average":42.94,
        "ARC":40.7,
        "HellaSwag":67.45,
        "MMLU":27.74,
        "TruthfulQA":35.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"efa950c69b6cbe1f8629400f3a7e0ccd895551fb"
    },
    {
        "T":"\u2b55",
        "Model":"jb723\/cross_lingual_epoch2",
        "Average":42.93,
        "ARC":39.25,
        "HellaSwag":47.92,
        "MMLU":36.66,
        "TruthfulQA":47.9,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":6.67,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"86e59e85b234e6c882758724849d7a1e4fe0b30a"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/gpt-j-6B",
        "Average":42.92,
        "ARC":41.38,
        "HellaSwag":67.54,
        "MMLU":26.78,
        "TruthfulQA":35.96,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub":1364,
        "Available on the hub":true,
        "Model Sha":"47e169305d2e8376be1d31e765533382721b2cc1"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/gpt-j-6b",
        "Average":42.92,
        "ARC":41.38,
        "HellaSwag":67.54,
        "MMLU":26.78,
        "TruthfulQA":35.96,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub":1275,
        "Available on the hub":true,
        "Model Sha":"47e169305d2e8376be1d31e765533382721b2cc1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/GPT-J-6B-Shinen",
        "Average":42.89,
        "ARC":39.85,
        "HellaSwag":67.06,
        "MMLU":27.72,
        "TruthfulQA":36.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":5.84,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"afa5a11b24cb23eee708e17c83b920a788e9e07b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewqr2130\/llama2-ppo",
        "Average":42.89,
        "ARC":41.64,
        "HellaSwag":49.46,
        "MMLU":35.36,
        "TruthfulQA":45.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8619e9870ce3285bf9c2a74921b5947dd6f9e4ac"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AI-Sweden-Models\/gpt-sw3-6.7b-v2",
        "Average":42.87,
        "ARC":39.42,
        "HellaSwag":66.39,
        "MMLU":30.09,
        "TruthfulQA":35.6,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.11,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"7a7f93d4318658b354c5411cde64e9f0121f6b1f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/Dolly_Malion-6b",
        "Average":42.86,
        "ARC":42.83,
        "HellaSwag":68.43,
        "MMLU":27.13,
        "TruthfulQA":33.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"f239eb8d24fe26db3b0a9a69115dc305fc9351af"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/ChanMalion",
        "Average":42.83,
        "ARC":41.89,
        "HellaSwag":68.25,
        "MMLU":27.29,
        "TruthfulQA":33.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"2667b0e0b705ed23f81f3e2b69673d722e8f4964"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Salesforce\/codegen-6B-nl",
        "Average":42.83,
        "ARC":42.32,
        "HellaSwag":68.59,
        "MMLU":25.93,
        "TruthfulQA":34.47,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bsd-3-clause",
        "#Params (B)":6.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"dff91c0aea702edbea3528344d01d8b9aaee6e39"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ed001\/datascience-coder-6.7b",
        "Average":42.81,
        "ARC":34.64,
        "HellaSwag":53.83,
        "MMLU":37.96,
        "TruthfulQA":44.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"02c9e23ecc8d0fdcd84db006ecb608344907c5e1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/RedPajama-INCITE-Chat-3B-Instruction-Tuning-with-GPT-4",
        "Average":42.81,
        "ARC":41.64,
        "HellaSwag":66.23,
        "MMLU":27.26,
        "TruthfulQA":36.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc",
        "#Params (B)":2.91,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"c588a5924749b86a6cb36a687dafa544c189bb6f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/GPT-J-6B-Janeway",
        "Average":42.79,
        "ARC":40.87,
        "HellaSwag":67.11,
        "MMLU":27.45,
        "TruthfulQA":35.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":5.84,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"036bb03496d648ddc8cf932ad91df8ef1287116c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/RedPajama-INCITE-Chat-3B-v1",
        "Average":42.78,
        "ARC":42.83,
        "HellaSwag":67.62,
        "MMLU":26.23,
        "TruthfulQA":34.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub":112,
        "Available on the hub":true,
        "Model Sha":"f0e0995eba801096ed04cb87931d96a8316871af"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AIGym\/deepseek-coder-6.7b-chat-and-function-calling",
        "Average":42.75,
        "ARC":36.09,
        "HellaSwag":53.8,
        "MMLU":38.29,
        "TruthfulQA":42.83,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b7a2725d6576fc88278cb41fb5a35ed14cff7077"
    },
    {
        "T":"\u2b55",
        "Model":"mwitiderrick\/open_llama_3b_code_instruct_0.1",
        "Average":42.75,
        "ARC":41.21,
        "HellaSwag":66.96,
        "MMLU":27.82,
        "TruthfulQA":35.01,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"be8055f68a5d53321d98c2b3e0f153034303b96c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AIGym\/deepseek-coder-6.7b-chat",
        "Average":42.73,
        "ARC":36.01,
        "HellaSwag":53.74,
        "MMLU":38.22,
        "TruthfulQA":42.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"00f7902c69c8bc48d8289141392d41fcb7517a14"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NoIdeaLand\/test-3k-mx",
        "Average":42.7,
        "ARC":38.05,
        "HellaSwag":66.43,
        "MMLU":25.39,
        "TruthfulQA":40.93,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"659a0826532817c5f65b390a29bd7593d73d8564"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/Pythia-Chat-Base-7B",
        "Average":42.69,
        "ARC":40.02,
        "HellaSwag":68.67,
        "MMLU":27.44,
        "TruthfulQA":34.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":61,
        "Available on the hub":true,
        "Model Sha":"97aa918c383820e1a69f042801091d7deb996c20"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"harborwater\/dpo-test-hermes-open-llama-3b",
        "Average":42.68,
        "ARC":39.25,
        "HellaSwag":67.46,
        "MMLU":24.21,
        "TruthfulQA":39.81,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":3.0,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"5cd560152a364f61f92cebe18feaefc181dfd287"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AIGym\/deepseek-coder-6.7b-chat",
        "Average":42.65,
        "ARC":35.75,
        "HellaSwag":53.7,
        "MMLU":38.19,
        "TruthfulQA":42.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"00f7902c69c8bc48d8289141392d41fcb7517a14"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"heegyu\/RedTulu-Uncensored-3B-0719",
        "Average":42.63,
        "ARC":40.02,
        "HellaSwag":62.55,
        "MMLU":30.37,
        "TruthfulQA":37.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c92bf022cddc3f57b4552ec3391df487295a2f87"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/PPO_Pygway-6b-Mix",
        "Average":42.62,
        "ARC":41.81,
        "HellaSwag":67.77,
        "MMLU":28.42,
        "TruthfulQA":32.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub":19,
        "Available on the hub":true,
        "Model Sha":"b31d25819e00d5031ccdb22a9584f0850dcfe39c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-13B-Erebus",
        "Average":42.59,
        "ARC":40.02,
        "HellaSwag":70.07,
        "MMLU":25.32,
        "TruthfulQA":34.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":139,
        "Available on the hub":true,
        "Model Sha":"8a949353677d2b971910a6c4afcc70e95d838c2a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/fairseq-dense-6.7B",
        "Average":42.58,
        "ARC":39.42,
        "HellaSwag":71.26,
        "MMLU":26.91,
        "TruthfulQA":32.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.65,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"d62d83b8eb7a6ba012a762752a5b5679add3b40c"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-12b-deduped",
        "Average":42.57,
        "ARC":41.38,
        "HellaSwag":70.26,
        "MMLU":25.63,
        "TruthfulQA":33.0,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.59,
        "Hub":47,
        "Available on the hub":true,
        "Model Sha":"39c1bd94f9dbe4ebd1d191f364cb33a2e5c47707"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"julleong\/illuni-llama-2-ko-7b-test",
        "Average":42.57,
        "ARC":43.43,
        "HellaSwag":64.86,
        "MMLU":28.69,
        "TruthfulQA":33.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.86,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"104fac91a859164fd379c96814788090bbe22e76"
    },
    {
        "T":"?",
        "Model":"pszemraj\/pythia-6.9b-HC3",
        "Average":42.57,
        "ARC":36.52,
        "HellaSwag":61.76,
        "MMLU":26.94,
        "TruthfulQA":45.05,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"c5c60ea656e921e6c5415f6feaebac4dd9b2aa2a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/Guanaco-3B-Uncensored-v2",
        "Average":42.56,
        "ARC":42.15,
        "HellaSwag":66.72,
        "MMLU":26.18,
        "TruthfulQA":35.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"e07122091fd4b318dcea105b16c73144d95bc2f6"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/opt-13b",
        "Average":42.53,
        "ARC":39.93,
        "HellaSwag":71.2,
        "MMLU":24.9,
        "TruthfulQA":34.1,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":57,
        "Available on the hub":true,
        "Model Sha":"e515202d1e7750da62d245fbccb2723b9c1790f5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/GPT-J-Pyg_PPO-6B",
        "Average":42.51,
        "ARC":42.06,
        "HellaSwag":67.51,
        "MMLU":28.52,
        "TruthfulQA":31.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":5.84,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"cde5bab3ae16e1704c5fec54a6a7ff1169c935e6"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-6.9b-deduped",
        "Average":42.51,
        "ARC":41.3,
        "HellaSwag":67.05,
        "MMLU":26.48,
        "TruthfulQA":35.19,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"372b1c08d9b5b0fc18ce86bbf294930e26e66ed5"
    },
    {
        "T":"?",
        "Model":"sail\/Sailor-1.8B-Chat",
        "Average":42.47,
        "ARC":35.75,
        "HellaSwag":57.12,
        "MMLU":38.31,
        "TruthfulQA":38.71,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.84,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"2a3bbb343ffba05985f26f66e2d3ee8e695a2e94"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lqtrung1998\/galactica-6.7b-ReFT-GSM8k",
        "Average":42.47,
        "ARC":40.7,
        "HellaSwag":50.34,
        "MMLU":37.62,
        "TruthfulQA":41.21,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.66,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"db019ea6f2762330d09f28bca53a5ecee8e2819a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Devio\/test-1400",
        "Average":42.46,
        "ARC":38.14,
        "HellaSwag":66.19,
        "MMLU":28.63,
        "TruthfulQA":36.87,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"95194d494effb691edae0d596bc5df9856ee92d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/Guanaco-3B-Uncensored",
        "Average":42.44,
        "ARC":42.49,
        "HellaSwag":66.99,
        "MMLU":25.55,
        "TruthfulQA":34.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"084a12f767b31c1fde681bebb14e9a291e506ea8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"codellama\/CodeLlama-7b-hf",
        "Average":42.42,
        "ARC":39.93,
        "HellaSwag":60.8,
        "MMLU":31.12,
        "TruthfulQA":37.82,
        "Type":"pretrained",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":112,
        "Available on the hub":true,
        "Model Sha":"be52f4ad322f5a47da121c761aeb5ba20ed77b17"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"acrastt\/RedPajama-INCITE-Chat-Instruct-3B-V1",
        "Average":42.42,
        "ARC":42.58,
        "HellaSwag":67.48,
        "MMLU":25.99,
        "TruthfulQA":33.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"e19eef572d57fc734bf3ea07c7d0098b3901ec9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/ScarletPajama-3B-HF",
        "Average":42.38,
        "ARC":39.76,
        "HellaSwag":64.89,
        "MMLU":27.28,
        "TruthfulQA":37.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"9dd07308b6eb3f270c5762250b6d46abd6f87b6f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"unit-mesh\/autodev-deepseek-6.7b-finetunes-poc",
        "Average":42.37,
        "ARC":35.41,
        "HellaSwag":52.41,
        "MMLU":37.56,
        "TruthfulQA":44.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"7d853c1192a8a428dc1db2fe8608143748386fce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-13B-Nerybus-Mix",
        "Average":42.34,
        "ARC":39.85,
        "HellaSwag":70.6,
        "MMLU":24.9,
        "TruthfulQA":34.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":24,
        "Available on the hub":true,
        "Model Sha":"c27a7e2360dd313406719980851e89abf46ebb13"
    },
    {
        "T":"?",
        "Model":"YeungNLP\/firefly-bloom-7b1",
        "Average":42.33,
        "ARC":40.44,
        "HellaSwag":61.2,
        "MMLU":26.83,
        "TruthfulQA":40.83,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.07,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"6b4385dc45c47d509b6400c41a2ff3665ad1d189"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/deacon-3b",
        "Average":42.32,
        "ARC":39.68,
        "HellaSwag":66.42,
        "MMLU":27.13,
        "TruthfulQA":36.07,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.43,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"c96b846ce7bacf5ad231957630dc94d59f329339"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"DanielSc4\/RedPajama-INCITE-Chat-3B-v1-RL-LoRA-8bit-test1",
        "Average":42.31,
        "ARC":41.3,
        "HellaSwag":66.82,
        "MMLU":26.1,
        "TruthfulQA":35.04,
        "Type":"RL-tuned",
        "Precision":"8bit",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a2ee88a9fa1c9ad41e0a8c15217a4b1230ec33c8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Guanaco-3B-Uncensored-v2-GPTQ",
        "Average":42.31,
        "ARC":41.64,
        "HellaSwag":64.76,
        "MMLU":26.25,
        "TruthfulQA":36.58,
        "Type":"fine-tuned",
        "Precision":"None",
        "Hub License":"apache-2.0",
        "#Params (B)":0.6,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"c80e2f01377d551ad17c8c9bac3f52578c38d653"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"deepseek-ai\/deepseek-coder-6.7b-base",
        "Average":42.29,
        "ARC":37.03,
        "HellaSwag":53.46,
        "MMLU":38.39,
        "TruthfulQA":40.28,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":63,
        "Available on the hub":true,
        "Model Sha":"ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-coder-ds-6.7b",
        "Average":42.27,
        "ARC":36.86,
        "HellaSwag":52.46,
        "MMLU":38.08,
        "TruthfulQA":41.67,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"808ce4ef532c91bcbf826cbdc29ec5094cbd1769"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"klosax\/open_llama_7b_400bt_preview",
        "Average":42.27,
        "ARC":39.51,
        "HellaSwag":65.88,
        "MMLU":27.64,
        "TruthfulQA":36.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4cd3a97dcc9c25b44b552ab53f0ec01ec36acc8d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-13B-Nerys-v2",
        "Average":42.27,
        "ARC":39.68,
        "HellaSwag":70.53,
        "MMLU":25.36,
        "TruthfulQA":33.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"b0aa4f3630356f7801ca083c00b03d03da13b8bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/PPO_Shygmalion-6b",
        "Average":42.23,
        "ARC":40.27,
        "HellaSwag":66.88,
        "MMLU":27.53,
        "TruthfulQA":34.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"573e4546fdccc5c8a52b9d7cb23a2e10f0f2ef51"
    },
    {
        "T":"?",
        "Model":"amazon\/LightGPT",
        "Average":42.22,
        "ARC":39.93,
        "HellaSwag":63.82,
        "MMLU":28.45,
        "TruthfulQA":36.69,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub":63,
        "Available on the hub":true,
        "Model Sha":"1f6ffd8f162030396a3bc1ca2e3504896dbe6434"
    },
    {
        "T":"\u2b55",
        "Model":"matsuo-lab\/weblab-10b-instruction-sft",
        "Average":42.21,
        "ARC":40.1,
        "HellaSwag":65.3,
        "MMLU":26.66,
        "TruthfulQA":36.79,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.47,
        "Hub":63,
        "Available on the hub":true,
        "Model Sha":"112a5ad9f556078ab14a5cd93511b9db4a0d4413"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"h2oai\/h2o-danube-1.8b-base",
        "Average":42.2,
        "ARC":39.42,
        "HellaSwag":69.58,
        "MMLU":25.94,
        "TruthfulQA":33.86,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.83,
        "Hub":40,
        "Available on the hub":false,
        "Model Sha":"3201996d3a41b4a485582164db42ca58d51051aa"
    },
    {
        "T":"\u2b55",
        "Model":"mwitiderrick\/open_llama_3b_instruct_v_0.2",
        "Average":42.19,
        "ARC":38.48,
        "HellaSwag":66.77,
        "MMLU":25.34,
        "TruthfulQA":38.16,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"6ae4004fe8901c1dae19108bc37e8b744cd08539"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/black_goo_recipe_c",
        "Average":42.17,
        "ARC":38.74,
        "HellaSwag":66.83,
        "MMLU":26.57,
        "TruthfulQA":36.54,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"78c0a6432ac0a6c2e54a2c3aac4cb70f446eb18b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/CodeLlama-7b-hf",
        "Average":42.13,
        "ARC":39.85,
        "HellaSwag":59.58,
        "MMLU":30.47,
        "TruthfulQA":38.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"855c92912ea4a8eb5f0be1db4bf776ffd0815dac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/RedPajama-INCITE-Instruct-3B-v1",
        "Average":42.12,
        "ARC":41.55,
        "HellaSwag":65.48,
        "MMLU":25.03,
        "TruthfulQA":36.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub":82,
        "Available on the hub":true,
        "Model Sha":"0c66778ee09a036886741707733620b91057909a"
    },
    {
        "T":"\u2b55",
        "Model":"heegyu\/WizardVicuna-open-llama-3b-v2",
        "Average":42.09,
        "ARC":37.71,
        "HellaSwag":66.6,
        "MMLU":27.23,
        "TruthfulQA":36.8,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1c69905286171d7d3ef3f95f8e1bbc9150bad3cd"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigscience\/bloom-7b1",
        "Average":42.07,
        "ARC":41.13,
        "HellaSwag":62.0,
        "MMLU":26.25,
        "TruthfulQA":38.9,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":7.07,
        "Hub":135,
        "Available on the hub":true,
        "Model Sha":"e83e90ba86f87f74aa2731cdab25ccf33976bd66"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Devio\/test-22B",
        "Average":42.05,
        "ARC":39.42,
        "HellaSwag":64.51,
        "MMLU":27.13,
        "TruthfulQA":37.13,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"cd72f5954ab5801dd2c1b499e59265f7504f9ee6"
    },
    {
        "T":"\u2b55",
        "Model":"RWKV\/rwkv-raven-7b",
        "Average":42.02,
        "ARC":39.42,
        "HellaSwag":66.48,
        "MMLU":23.64,
        "TruthfulQA":38.56,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.19,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"a2dfc9f659be13556a25d9e38da642c6f67aeee3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hakurei\/instruct-12b",
        "Average":42.02,
        "ARC":42.58,
        "HellaSwag":66.76,
        "MMLU":26.79,
        "TruthfulQA":31.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"ff4699b502b79c716330b6f761002588a65dcba6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CobraMamba\/mamba-gpt-3b",
        "Average":41.99,
        "ARC":40.53,
        "HellaSwag":64.94,
        "MMLU":25.35,
        "TruthfulQA":37.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"21a8212e3641dd14924d6bdead0774b64dda8ce0"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/black_goo_recipe_a",
        "Average":41.98,
        "ARC":38.14,
        "HellaSwag":66.56,
        "MMLU":25.75,
        "TruthfulQA":37.46,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7067f68d4d9e7b10a1aa2c9fa97456bc04678867"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"acrastt\/OmegLLaMA-3B",
        "Average":41.95,
        "ARC":40.36,
        "HellaSwag":66.13,
        "MMLU":28.0,
        "TruthfulQA":33.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"520c5f1ceb5c90d4011887e2a8d3becf15e7e66e"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-7b-Instruct-hf",
        "Average":41.94,
        "ARC":36.52,
        "HellaSwag":55.44,
        "MMLU":34.54,
        "TruthfulQA":41.25,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":55,
        "Available on the hub":true,
        "Model Sha":"7affc442e639b8aa1c4b3e98a10a2f45a21b8b4f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-oig-oasst1-512-6_9b",
        "Average":41.9,
        "ARC":40.44,
        "HellaSwag":65.58,
        "MMLU":24.9,
        "TruthfulQA":36.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"029a787e0d98fcd3fecffbfbeb4a75a425474937"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"matsuo-lab\/weblab-10b",
        "Average":41.9,
        "ARC":39.51,
        "HellaSwag":65.76,
        "MMLU":26.29,
        "TruthfulQA":36.02,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.47,
        "Hub":54,
        "Available on the hub":true,
        "Model Sha":"d6fc432983b1633a4c1568d121c60de6b8c3e511"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-6.7B-Erebus",
        "Average":41.88,
        "ARC":39.16,
        "HellaSwag":68.66,
        "MMLU":24.58,
        "TruthfulQA":35.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.66,
        "Hub":80,
        "Available on the hub":true,
        "Model Sha":"9c4d1af96f93224e01d2f69c303fc6d6f686bdcc"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/opt-6.7b",
        "Average":41.88,
        "ARC":39.16,
        "HellaSwag":68.66,
        "MMLU":24.57,
        "TruthfulQA":35.12,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.66,
        "Hub":71,
        "Available on the hub":true,
        "Model Sha":"a45aa65bbeb77c1558bc99bedc6779195462dab0"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/black_goo_recipe_d",
        "Average":41.85,
        "ARC":37.8,
        "HellaSwag":66.5,
        "MMLU":26.64,
        "TruthfulQA":36.46,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"fdf7f93837808958f9463d3c683314e7f649a088"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/LLongMA-3b-LIMA",
        "Average":41.84,
        "ARC":39.08,
        "HellaSwag":67.15,
        "MMLU":26.43,
        "TruthfulQA":34.71,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"333b8c41e42a46a6f3aecaf8f3fa8a17c6d83990"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/black_goo_recipe_b",
        "Average":41.78,
        "ARC":37.63,
        "HellaSwag":66.72,
        "MMLU":25.68,
        "TruthfulQA":37.09,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"42faec8429cee8c9f4f5db58ffa193f6f8e0d498"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-6.7B-Nerybus-Mix",
        "Average":41.78,
        "ARC":39.16,
        "HellaSwag":68.63,
        "MMLU":24.47,
        "TruthfulQA":34.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.66,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"9afe4dca5a9dbd71cb90d1050d142837f4c739f6"
    },
    {
        "T":"\u2b55",
        "Model":"health360\/Healix-3B",
        "Average":41.77,
        "ARC":37.71,
        "HellaSwag":65.94,
        "MMLU":26.02,
        "TruthfulQA":37.4,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":3.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"52297e0b6845b3c1b26f336fd2a2c9b2f56ce6ba"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-12b",
        "Average":41.76,
        "ARC":39.59,
        "HellaSwag":68.82,
        "MMLU":26.76,
        "TruthfulQA":31.85,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.59,
        "Hub":108,
        "Available on the hub":true,
        "Model Sha":"35c9d7f32fbb108fb8b5bdd574eb03369d1eed49"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/RedPajama-INCITE-Chat-3B-ShareGPT-11K",
        "Average":41.75,
        "ARC":40.61,
        "HellaSwag":64.84,
        "MMLU":26.13,
        "TruthfulQA":35.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"ec33d12d08d61ed821e67b1a55ad404dc3457ebf"
    },
    {
        "T":"?",
        "Model":"GeorgiaTechResearchInstitute\/galactica-6.7b-evol-instruct-70k",
        "Average":41.74,
        "ARC":42.58,
        "HellaSwag":49.3,
        "MMLU":32.96,
        "TruthfulQA":42.1,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.66,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"14fa470051d0bc38fd871643186a9edfd3a8a9aa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xaviviro\/FLOR-6.3B-xat",
        "Average":41.73,
        "ARC":38.65,
        "HellaSwag":63.76,
        "MMLU":26.54,
        "TruthfulQA":37.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.25,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"edd1cbf53f584c6bc7b38a31a0b7beed8e942e8f"
    },
    {
        "T":"?",
        "Model":"Technoculture\/MT7Bi-wizard-3-alpha-dpo",
        "Average":41.73,
        "ARC":41.21,
        "HellaSwag":59.34,
        "MMLU":27.31,
        "TruthfulQA":39.06,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"097011308950f819d70277b5a35ddf2e09fc9122"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-oig-oasst1-256-6_9b",
        "Average":41.69,
        "ARC":39.93,
        "HellaSwag":65.42,
        "MMLU":26.39,
        "TruthfulQA":35.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"f1c9bac89b74d3487cb092788ce828fb9520c1a7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-nl2sql-ds-6.7b",
        "Average":41.63,
        "ARC":36.35,
        "HellaSwag":52.83,
        "MMLU":36.8,
        "TruthfulQA":40.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"e55ace80c04ed4ace1876ba192e6ecb4ef0353b8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RESMPDEV\/Gemma-Wukong1.1-2b",
        "Average":41.62,
        "ARC":33.45,
        "HellaSwag":42.42,
        "MMLU":42.94,
        "TruthfulQA":47.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"0a04dcf420eb4b1c8ed5eb58afc9cf813b639d0d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ikala\/bloom-zh-3b-chat",
        "Average":41.6,
        "ARC":38.82,
        "HellaSwag":54.71,
        "MMLU":31.62,
        "TruthfulQA":41.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":3.0,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"4ea0ad223a2623fc15e8824c1c4f8e6539bc40b0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Taekyoon\/llama2-ko-7b-test",
        "Average":41.6,
        "ARC":37.8,
        "HellaSwag":63.04,
        "MMLU":29.55,
        "TruthfulQA":36.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1d9b52cc5832ae0ea37514330d38193b737e1d07"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/pygmalion-6b",
        "Average":41.57,
        "ARC":40.53,
        "HellaSwag":67.47,
        "MMLU":25.73,
        "TruthfulQA":32.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub":663,
        "Available on the hub":true,
        "Model Sha":"30e2405100eac6bd53f75964cc7345eeafd19f7d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"anhnv125\/pygmalion-6b-roleplay",
        "Average":41.57,
        "ARC":40.53,
        "HellaSwag":67.47,
        "MMLU":25.73,
        "TruthfulQA":32.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"e49ed0bde45de0a436bff678ec4872069e8f230c"
    },
    {
        "T":"?",
        "Model":"TehVenom\/DiffMerge_Pygmalion_Main-onto-V8P4",
        "Average":41.56,
        "ARC":40.53,
        "HellaSwag":67.48,
        "MMLU":25.68,
        "TruthfulQA":32.55,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"f855780745aa34c3bdbe020e4c51253d538cb21e"
    },
    {
        "T":"?",
        "Model":"RESMPDEV\/Gemma-Wukong1.1-2b",
        "Average":41.52,
        "ARC":33.45,
        "HellaSwag":42.39,
        "MMLU":42.52,
        "TruthfulQA":47.73,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":2.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"0a04dcf420eb4b1c8ed5eb58afc9cf813b639d0d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-6B-nerys-v2",
        "Average":41.51,
        "ARC":38.4,
        "HellaSwag":68.57,
        "MMLU":24.34,
        "TruthfulQA":34.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.66,
        "Hub":18,
        "Available on the hub":true,
        "Model Sha":"9e1f1498391df2c28ce35a9290a5a24b8022a43b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aihub-app\/zyte-1.1B",
        "Average":41.51,
        "ARC":37.88,
        "HellaSwag":61.37,
        "MMLU":24.62,
        "TruthfulQA":42.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"4537b28d9b2e9958c53b6d4aa6e16f46f85c1867"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"aihub-app\/zyte-1B",
        "Average":41.5,
        "ARC":37.88,
        "HellaSwag":61.37,
        "MMLU":24.61,
        "TruthfulQA":42.14,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"6c2b31ee038f8df37547c013d73b91c4a07e41a5"
    },
    {
        "T":"?",
        "Model":"InnerI\/I-Code-NousLlama7B-slerp",
        "Average":41.49,
        "ARC":40.36,
        "HellaSwag":61.05,
        "MMLU":28.37,
        "TruthfulQA":36.17,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1b54f03cd80453df7d36065af89660d878dd52a1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"princeton-nlp\/Sheared-LLaMA-1.3B-ShareGPT",
        "Average":41.49,
        "ARC":33.96,
        "HellaSwag":62.55,
        "MMLU":26.42,
        "TruthfulQA":43.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.28,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"d2f3cfae7746c4ff07353b39828985ea0f36b07d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NoIdeaLand\/test-2048-1500ck",
        "Average":41.48,
        "ARC":36.69,
        "HellaSwag":62.56,
        "MMLU":25.72,
        "TruthfulQA":40.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"dae2a6d32b71fb5f88856a324e594f4f5be2f283"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lazycuber\/pyg-instruct-wizardlm",
        "Average":41.48,
        "ARC":40.96,
        "HellaSwag":66.71,
        "MMLU":26.33,
        "TruthfulQA":31.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"f00ef7a7b0cc6f02af2a11ac764270dfd61b9e2f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Devio\/testC",
        "Average":41.47,
        "ARC":39.59,
        "HellaSwag":62.88,
        "MMLU":27.76,
        "TruthfulQA":35.67,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"318159010931164dcacb5dc2a7a54d48990fb969"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"venkycs\/zyte-v1-1.1B",
        "Average":41.47,
        "ARC":37.29,
        "HellaSwag":61.41,
        "MMLU":24.6,
        "TruthfulQA":42.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"b75c703a236c6f0394f7f8641c4ecee016c2e43f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-30B-Erebus",
        "Average":41.46,
        "ARC":36.69,
        "HellaSwag":65.6,
        "MMLU":24.8,
        "TruthfulQA":38.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":29.97,
        "Hub":38,
        "Available on the hub":true,
        "Model Sha":"a1041efcf9599c962822274e92040710579a5bf2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/CrimsonPajama",
        "Average":41.35,
        "ARC":40.19,
        "HellaSwag":65.47,
        "MMLU":25.95,
        "TruthfulQA":33.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"ff054eeff9e3541464383d40b36d182057d01113"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_opt6.7b_10e5",
        "Average":41.32,
        "ARC":37.03,
        "HellaSwag":65.65,
        "MMLU":25.0,
        "TruthfulQA":37.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.7,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"43cd61a5ed4caeedf335aa8f2ccde8c7457ced73"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"togethercomputer\/RedPajama-INCITE-Base-3B-v1",
        "Average":41.3,
        "ARC":40.19,
        "HellaSwag":64.77,
        "MMLU":27.03,
        "TruthfulQA":33.23,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub":79,
        "Available on the hub":true,
        "Model Sha":"094fbdd0c911feb485ce55de1952ab2e75277e1e"
    },
    {
        "T":"?",
        "Model":"bigscience\/bloomz-3b",
        "Average":41.27,
        "ARC":36.86,
        "HellaSwag":54.95,
        "MMLU":32.91,
        "TruthfulQA":40.34,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":3.0,
        "Hub":68,
        "Available on the hub":true,
        "Model Sha":"31eefcb2bcd69632925adf07e090debafe95436d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vihangd\/DopeyTinyLlama-1.1B-v1",
        "Average":41.25,
        "ARC":38.4,
        "HellaSwag":63.49,
        "MMLU":25.76,
        "TruthfulQA":37.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"34b3b15e9c37be1a89745f06904c1e89ef98d417"
    },
    {
        "T":"?",
        "Model":"appvoid\/palmer-002.5",
        "Average":41.2,
        "ARC":37.54,
        "HellaSwag":61.84,
        "MMLU":25.21,
        "TruthfulQA":40.22,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"5a987c226e4935167dbbec5565d16c66853a3932"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"conceptofmind\/Open-LLongMA-3b",
        "Average":41.17,
        "ARC":39.76,
        "HellaSwag":65.46,
        "MMLU":24.95,
        "TruthfulQA":34.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":3.32,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"397d45bba893f6ad2b85a11f273f34289557edae"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"RWKV\/rwkv-4-7b-pile",
        "Average":41.15,
        "ARC":39.68,
        "HellaSwag":66.31,
        "MMLU":24.96,
        "TruthfulQA":33.65,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.19,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"922e22a761427e50d7be457b31a76b1126021b8b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/Galactica-6.7B-EssayWriter",
        "Average":41.14,
        "ARC":40.1,
        "HellaSwag":50.29,
        "MMLU":33.88,
        "TruthfulQA":40.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.66,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"ac74fdd938de1ffd34832d66a25db20b0230983e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"openlm-research\/open_llama_3b",
        "Average":41.1,
        "ARC":39.85,
        "HellaSwag":62.65,
        "MMLU":26.94,
        "TruthfulQA":34.97,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub":107,
        "Available on the hub":true,
        "Model Sha":"141067009124b9c0aea62c76b3eb952174864057"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andrijdavid\/tinyllama-dare",
        "Average":41.07,
        "ARC":37.29,
        "HellaSwag":62.78,
        "MMLU":25.2,
        "TruthfulQA":39.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"f3c5e1369064d3167377b6965a74637d26102e6b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lqtrung1998\/galactica-6.7b-ReFT-Rerank-GSM8k",
        "Average":40.99,
        "ARC":41.13,
        "HellaSwag":48.78,
        "MMLU":32.86,
        "TruthfulQA":41.2,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.66,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"13f88bef7068492879a32eeee42597cc37fc727e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Code-290k-6.7B-Instruct",
        "Average":40.93,
        "ARC":34.9,
        "HellaSwag":51.99,
        "MMLU":34.89,
        "TruthfulQA":41.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"4ef569814773fac1700bfb8c563118d497af7b76"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewof\/koishi-instruct-3b",
        "Average":40.93,
        "ARC":40.96,
        "HellaSwag":64.54,
        "MMLU":26.58,
        "TruthfulQA":31.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":2.91,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"2bb7f3842398b048efa4ae2d1aafb9e2f18a8586"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"OEvortex\/vortex-3b-v2",
        "Average":40.9,
        "ARC":39.68,
        "HellaSwag":65.04,
        "MMLU":25.09,
        "TruthfulQA":33.8,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.78,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"fff79b81d767be1830b8f9887f1c084b8636711a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ddyuudd\/dolly-v2-3b",
        "Average":40.9,
        "ARC":39.68,
        "HellaSwag":65.04,
        "MMLU":25.09,
        "TruthfulQA":33.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"84519fd8b688e7e1c122b5d9338c225a7ff2cadc"
    },
    {
        "T":"?",
        "Model":"ddyuudd\/merge_dolly-v2-3b_dpo_test",
        "Average":40.86,
        "ARC":40.02,
        "HellaSwag":65.14,
        "MMLU":24.99,
        "TruthfulQA":33.3,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.65,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e624fdbb538362206af1b98400bf44bbc0247226"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DanielSc4\/RedPajama-INCITE-Chat-3B-v1-FT-LoRA-8bit-test1",
        "Average":40.85,
        "ARC":38.65,
        "HellaSwag":63.53,
        "MMLU":25.16,
        "TruthfulQA":36.07,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f477d24b00e05fe4c5f8d5f933080994cfd90e4e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cerebras\/Cerebras-GPT-13B",
        "Average":40.81,
        "ARC":38.14,
        "HellaSwag":60.01,
        "MMLU":25.92,
        "TruthfulQA":39.19,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":632,
        "Available on the hub":true,
        "Model Sha":"7e97fa4b15edd955094c4395d62e6f4290e365b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"euclaise\/falcon_1b_stage2",
        "Average":40.8,
        "ARC":35.49,
        "HellaSwag":65.56,
        "MMLU":23.83,
        "TruthfulQA":38.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"c3ef73a8c9dc06fae4bfe4460d2f293147aecbb0"
    },
    {
        "T":"?",
        "Model":"maywell\/TinyLlama-MoE-Chat",
        "Average":40.78,
        "ARC":34.64,
        "HellaSwag":59.22,
        "MMLU":29.9,
        "TruthfulQA":39.37,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"2d786c9077b949d7ee3f5201813d7edccc7bd2da"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/TinyLlama-MoE-Chat",
        "Average":40.77,
        "ARC":34.73,
        "HellaSwag":59.29,
        "MMLU":29.71,
        "TruthfulQA":39.35,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"2d786c9077b949d7ee3f5201813d7edccc7bd2da"
    },
    {
        "T":"?",
        "Model":"Chickaboo\/ChickaQ",
        "Average":40.71,
        "ARC":29.44,
        "HellaSwag":49.15,
        "MMLU":37.05,
        "TruthfulQA":47.22,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.62,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"2f6bc46231a35c15a0343ef9f09899381116091b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"gemmathon\/gemma-2b-ko-dev-pbmt192",
        "Average":40.69,
        "ARC":38.57,
        "HellaSwag":52.95,
        "MMLU":28.71,
        "TruthfulQA":42.54,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.51,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"856bd740a9c014e7dfc7233dbe6b9083f44ac8a5"
    },
    {
        "T":"\u2b55",
        "Model":"HiTZ\/GoLLIE-7B",
        "Average":40.67,
        "ARC":36.09,
        "HellaSwag":57.93,
        "MMLU":29.38,
        "TruthfulQA":39.27,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"d3e41fef45f6a7d438c46ba7d9fce5d0d486c7a9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-6.7b",
        "Average":40.65,
        "ARC":40.1,
        "HellaSwag":65.0,
        "MMLU":24.64,
        "TruthfulQA":32.85,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"b666a6e46eeade607c73ed1334ecda3b9345e4bf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/chopt-2_7b",
        "Average":40.63,
        "ARC":36.01,
        "HellaSwag":63.38,
        "MMLU":25.44,
        "TruthfulQA":37.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"45f57352c10a1fb1ec13c4bf387a15552ca1fe65"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ndavidson\/cisco-iNAM-1.1B",
        "Average":40.61,
        "ARC":36.01,
        "HellaSwag":60.74,
        "MMLU":26.39,
        "TruthfulQA":39.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c1949425f65e6160efec2a0237397f85d9e94fb0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aihub-app\/zyte-1.1b",
        "Average":40.6,
        "ARC":37.54,
        "HellaSwag":60.82,
        "MMLU":24.57,
        "TruthfulQA":39.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"3d4e61bc3c090a28355cceba8da106c31e3bbb84"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cmarkea\/bloomz-3b-sft-chat",
        "Average":40.6,
        "ARC":36.86,
        "HellaSwag":54.34,
        "MMLU":31.49,
        "TruthfulQA":39.69,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":3.0,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"a35b6ae6809891e253b45fb5795979c33992e548"
    },
    {
        "T":"\u2b55",
        "Model":"vihangd\/dopeyshearedplats-1.3b-v1",
        "Average":40.58,
        "ARC":34.39,
        "HellaSwag":64.31,
        "MMLU":25.4,
        "TruthfulQA":38.21,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":1.28,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"45aa5d406bb6975deb801e5fffa27ca23e5724a5"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AI-Sweden-Models\/gpt-sw3-6.7b",
        "Average":40.53,
        "ARC":36.35,
        "HellaSwag":60.75,
        "MMLU":26.0,
        "TruthfulQA":39.04,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.11,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7b20cb87e793e1b73b6a73da5261c6010f2b5410"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azure99\/blossom-v1-3b",
        "Average":40.53,
        "ARC":36.86,
        "HellaSwag":55.1,
        "MMLU":26.7,
        "TruthfulQA":43.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3235ee41e3793c98749b7bbd2bb80882a12ac889"
    },
    {
        "T":"?",
        "Model":"alnrg2arg\/blockchainlabs_tinyllama_fusion_LHK_yunkong_v2",
        "Average":40.52,
        "ARC":34.9,
        "HellaSwag":63.11,
        "MMLU":26.75,
        "TruthfulQA":37.33,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"374cfd08ccc027f695f045cb4b31794e142769b0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"euclaise\/falcon_1b_stage1",
        "Average":40.5,
        "ARC":35.15,
        "HellaSwag":62.4,
        "MMLU":24.47,
        "TruthfulQA":40.0,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f85d91ff3f6cadc93f7222a19b9c4930c8842366"
    },
    {
        "T":"?",
        "Model":"Evaloric\/Evaloric-1.1B-test",
        "Average":40.49,
        "ARC":36.6,
        "HellaSwag":60.97,
        "MMLU":26.12,
        "TruthfulQA":38.28,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"44d7b186e9725d7eab1b5b79dc0d088176bb4496"
    },
    {
        "T":"?",
        "Model":"frankenmerger\/MiniLlama-1.8b-Chat-v0.1",
        "Average":40.44,
        "ARC":34.73,
        "HellaSwag":62.38,
        "MMLU":25.69,
        "TruthfulQA":38.97,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.89,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a880960de7a6340e68ebd92004430eaee3a6890b"
    },
    {
        "T":"\u2b55",
        "Model":"ericzzz\/falcon-rw-1b-instruct-openorca",
        "Average":40.42,
        "ARC":34.56,
        "HellaSwag":60.93,
        "MMLU":28.77,
        "TruthfulQA":37.42,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"bb5f86170d8d01aa850bb216bb2797899570c13e"
    },
    {
        "T":"\u2b55",
        "Model":"PSanni\/Deer-3b",
        "Average":40.38,
        "ARC":38.48,
        "HellaSwag":57.41,
        "MMLU":25.64,
        "TruthfulQA":39.98,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"53ea8f8862fc1820f0cd31f62953b7290fd79867"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aloobun\/d-Qwen1.5-1.8B",
        "Average":40.36,
        "ARC":30.89,
        "HellaSwag":49.73,
        "MMLU":37.92,
        "TruthfulQA":42.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":1.8,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"12eb58c7ddc17a6eddf5c52a3d9793e1e35464b8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bertin-project\/bertin-gpt-j-6B-alpaca",
        "Average":40.34,
        "ARC":36.01,
        "HellaSwag":54.3,
        "MMLU":27.66,
        "TruthfulQA":43.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":5.84,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"636b17d6044189343475d1889f076aba73036905"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigcode\/starcoder2-3b",
        "Average":40.33,
        "ARC":34.56,
        "HellaSwag":47.62,
        "MMLU":38.65,
        "TruthfulQA":40.49,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":3.03,
        "Hub":99,
        "Available on the hub":false,
        "Model Sha":"733247c55e3f73af49ce8e9c7949bf14af205928"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Changgil\/K2S3-SOLAR-11b-v1.0",
        "Average":40.28,
        "ARC":33.7,
        "HellaSwag":51.39,
        "MMLU":30.05,
        "TruthfulQA":45.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.92,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3c5ff9c8a00dfb6cf8619ce08c2f06a22e650e0c"
    },
    {
        "T":"?",
        "Model":"Aryanne\/sheared-plus-westlake-50_75p",
        "Average":40.24,
        "ARC":34.04,
        "HellaSwag":58.05,
        "MMLU":26.24,
        "TruthfulQA":42.64,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.7,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a0b63698693a3f86d6ba817999d51de850a3fd33"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ShieldX\/manovyadh-1.1B-v1-chat",
        "Average":40.24,
        "ARC":35.92,
        "HellaSwag":60.03,
        "MMLU":25.82,
        "TruthfulQA":39.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"32dbae5401c03792f275d7f079be568002eafd9a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sbawa\/elysa_model",
        "Average":40.21,
        "ARC":37.54,
        "HellaSwag":60.37,
        "MMLU":25.58,
        "TruthfulQA":37.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f57eba56111fcea5f1438d31d05bc84ccb4fc51c"
    },
    {
        "T":"\u2b55",
        "Model":"ericzzz\/falcon-rw-1b-chat",
        "Average":40.21,
        "ARC":35.58,
        "HellaSwag":61.12,
        "MMLU":24.51,
        "TruthfulQA":39.62,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"61c2b3f27c8d32912d0b9ff47ebf687af2eb9e86"
    },
    {
        "T":"?",
        "Model":"gardner\/TinyLlama-1.1B-SlimOrca-Function-Calling-3T",
        "Average":40.18,
        "ARC":36.09,
        "HellaSwag":59.66,
        "MMLU":28.21,
        "TruthfulQA":36.74,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"df79003585fb9af2ac4644a9029c70d5a998a95e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"venkycs\/ZySec-1B",
        "Average":40.16,
        "ARC":38.4,
        "HellaSwag":61.53,
        "MMLU":25.05,
        "TruthfulQA":35.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4a0f6267424e62319a8c0516af377bf576d9e210"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-openllama-3b-v10-bf16",
        "Average":40.14,
        "ARC":36.26,
        "HellaSwag":58.38,
        "MMLU":23.89,
        "TruthfulQA":42.04,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.34,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"7f24d32de53aa4bc150f04ca2418604475173921"
    },
    {
        "T":"?",
        "Model":"Trelis\/TinyLlama-chat-SFT",
        "Average":40.14,
        "ARC":34.47,
        "HellaSwag":61.03,
        "MMLU":25.77,
        "TruthfulQA":39.29,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"cad981584d4be97aa49353082af8a771e20e2c3b"
    },
    {
        "T":"?",
        "Model":"l3utterfly\/tinyllama-1.1b-layla-v4",
        "Average":40.14,
        "ARC":34.81,
        "HellaSwag":61.25,
        "MMLU":25.53,
        "TruthfulQA":38.97,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.03,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3d3e70c293cb0ad0ff271f8b959e5da6005aafa8"
    },
    {
        "T":"\u2b55",
        "Model":"42dot\/42dot_LLM-SFT-1.3B",
        "Average":40.14,
        "ARC":36.09,
        "HellaSwag":58.96,
        "MMLU":25.51,
        "TruthfulQA":39.98,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.44,
        "Hub":30,
        "Available on the hub":true,
        "Model Sha":"7474cafe5dc60549c19f89f7c49392a8a32b9199"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/fairseq-dense-2.7B",
        "Average":40.13,
        "ARC":33.79,
        "HellaSwag":65.74,
        "MMLU":26.44,
        "TruthfulQA":34.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":2.78,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"4201f4b101bad2992efc8452009317a354ec52d2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aabbhishekk\/TinyLlama-1.1B-miniguanaco",
        "Average":40.13,
        "ARC":35.15,
        "HellaSwag":60.26,
        "MMLU":26.26,
        "TruthfulQA":38.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.03,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e2495b16f1d812485842b199a026438e037f15f5"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/CodeLlama-34b-Instruct-hf",
        "Average":40.12,
        "ARC":40.78,
        "HellaSwag":35.68,
        "MMLU":39.75,
        "TruthfulQA":44.29,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"50ac374da09ab585b9cf7625a2ea3554ef97f18a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eren23\/DistiLabelOrca-TinyLLama-1.1B",
        "Average":40.12,
        "ARC":36.18,
        "HellaSwag":61.15,
        "MMLU":25.09,
        "TruthfulQA":38.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"c8fa674a68fe45f8f87a6aef87c8e2cbcee62b81"
    },
    {
        "T":"\u2b55",
        "Model":"TheBloke\/CodeLlama-34B-Instruct-fp16",
        "Average":40.11,
        "ARC":40.78,
        "HellaSwag":35.66,
        "MMLU":39.72,
        "TruthfulQA":44.29,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"a4d0ce949de4d5b5f74691641efb5b70736a32a8"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-34b-Instruct-hf",
        "Average":40.11,
        "ARC":40.78,
        "HellaSwag":35.66,
        "MMLU":39.72,
        "TruthfulQA":44.29,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub":104,
        "Available on the hub":true,
        "Model Sha":"c109b9dde086b31725fa09ff7effdc04c03c033d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2",
        "Average":40.11,
        "ARC":36.43,
        "HellaSwag":61.41,
        "MMLU":25.01,
        "TruthfulQA":37.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"fdc6ff469295d0aaabec8948525b70d6688728ac"
    },
    {
        "T":"?",
        "Model":"dball\/zephyr-tiny-dpo-qlora",
        "Average":40.11,
        "ARC":36.6,
        "HellaSwag":61.66,
        "MMLU":25.78,
        "TruthfulQA":36.4,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.03,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6c6f2246d4db07605b714f956f5e48878049b7b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Deathsquad10\/TinyLlama-repeat",
        "Average":40.08,
        "ARC":35.24,
        "HellaSwag":60.25,
        "MMLU":26.07,
        "TruthfulQA":38.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"14728ff7c51471faec92a4c86261951cc4175f61"
    },
    {
        "T":"?",
        "Model":"Aryanne\/sheared-plus-westlake-nearest-50_75p",
        "Average":40.08,
        "ARC":36.18,
        "HellaSwag":57.54,
        "MMLU":24.2,
        "TruthfulQA":42.39,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.7,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b3322cdef5234ba6a20928d8aaeee7a9e7c79e2c"
    },
    {
        "T":"?",
        "Model":"Nekochu\/Confluence-Renegade-7B",
        "Average":40.06,
        "ARC":31.91,
        "HellaSwag":45.38,
        "MMLU":31.48,
        "TruthfulQA":51.47,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"35a69deab8523329176e87313d77c19593f9b89d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"openlm-research\/open_llama_7b_700bt_preview",
        "Average":40.02,
        "ARC":35.07,
        "HellaSwag":61.79,
        "MMLU":25.45,
        "TruthfulQA":37.77,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":99,
        "Available on the hub":false,
        "Model Sha":"501f6dfa3b5ae9d3b5b9af5f6c6de1ce8bd44a91"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TinyLlama\/TinyLlama-1.1B-Chat-v1.0",
        "Average":40.02,
        "ARC":36.09,
        "HellaSwag":61.1,
        "MMLU":25.39,
        "TruthfulQA":37.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":881,
        "Available on the hub":true,
        "Model Sha":"de253fa9783f8bd558c9ed398c8ffbe3c55cedb3"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Dampish\/StellarX-4B-V0",
        "Average":40.0,
        "ARC":36.95,
        "HellaSwag":61.9,
        "MMLU":26.85,
        "TruthfulQA":34.3,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":3.83,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"0a79832bd57a8cdadc61626fb77bdc26c85b9fa4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"l3utterfly\/tinyllama-1.1b-layla-v1",
        "Average":39.99,
        "ARC":34.39,
        "HellaSwag":59.86,
        "MMLU":24.7,
        "TruthfulQA":41.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.03,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a12aba8771fb310578d07a76c8666972e3ca21df"
    },
    {
        "T":"\u2b55",
        "Model":"Rallio67\/3B-redpajama-conditional-alpha",
        "Average":39.97,
        "ARC":36.26,
        "HellaSwag":61.9,
        "MMLU":25.42,
        "TruthfulQA":36.31,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":2.65,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"7e2156c14b4b7981a4cd6db7b878888a98144df0"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"tiiuae\/falcon-rw-1b",
        "Average":39.97,
        "ARC":35.07,
        "HellaSwag":63.56,
        "MMLU":25.28,
        "TruthfulQA":35.96,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":48,
        "Available on the hub":true,
        "Model Sha":"e4b9872bb803165eb22f0a867d4e6a64d34fce19"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kevin009\/lamatama",
        "Average":39.97,
        "ARC":36.35,
        "HellaSwag":61.12,
        "MMLU":24.72,
        "TruthfulQA":37.67,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"bb8349cd64652df9a62bc46c12c24f3226662a5c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NbAiLab\/nb-gpt-j-6B-alpaca",
        "Average":39.96,
        "ARC":36.86,
        "HellaSwag":57.46,
        "MMLU":27.53,
        "TruthfulQA":38.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.0,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"05c9b894b7b5e222cc4d33fa33f59c7b40c3337c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-2.7B-Erebus",
        "Average":39.95,
        "ARC":34.39,
        "HellaSwag":60.91,
        "MMLU":26.7,
        "TruthfulQA":37.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub":32,
        "Available on the hub":true,
        "Model Sha":"39ca914ceb82f7f14a38484023bc04f0cd5d0a8d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Danielbrdz\/Barcenas-Tiny-1.1b-DPO",
        "Average":39.94,
        "ARC":36.26,
        "HellaSwag":61.2,
        "MMLU":24.83,
        "TruthfulQA":37.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"871b3e79f0fe988e2dc601c5e062d612ce17b129"
    },
    {
        "T":"?",
        "Model":"OEvortex\/HelpingAI-Lite-2x1B",
        "Average":39.92,
        "ARC":36.09,
        "HellaSwag":61.11,
        "MMLU":25.1,
        "TruthfulQA":37.39,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":1.86,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"d7795c76cf4591e0360afbfe956a4146a5834bfc"
    },
    {
        "T":"?",
        "Model":"kevin009\/TinyNaughtyLlama-v1.0",
        "Average":39.89,
        "ARC":35.92,
        "HellaSwag":61.04,
        "MMLU":25.82,
        "TruthfulQA":36.77,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"0ed70fbd1075c6f62cf9227023a67dd832e2b710"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"davanstrien\/TinyLlama-1.1B-Chat-v1.0-intel-dpo",
        "Average":39.89,
        "ARC":35.84,
        "HellaSwag":61.29,
        "MMLU":25.05,
        "TruthfulQA":37.38,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"da2b792b7edf3d30b6e8ed05ebc8e9bbde442b5c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Writer\/camel-5b-hf",
        "Average":39.87,
        "ARC":35.15,
        "HellaSwag":57.62,
        "MMLU":26.07,
        "TruthfulQA":40.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.05,
        "Hub":100,
        "Available on the hub":true,
        "Model Sha":"d1438e22a33b9115af0e47ab3a0fe844cbf588a6"
    },
    {
        "T":"?",
        "Model":"Abhaykoul\/HelpingAI-Lite-4x1b",
        "Average":39.87,
        "ARC":35.84,
        "HellaSwag":61.0,
        "MMLU":25.24,
        "TruthfulQA":37.39,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.38,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e55836d41118de827f19cef1efc73f1cf1dd3abd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"saberai\/Zro1.5_3B",
        "Average":39.87,
        "ARC":35.92,
        "HellaSwag":61.11,
        "MMLU":25.55,
        "TruthfulQA":36.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"434e3ac9bb300779d677486d5e04d774fc514169"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AIGym\/TinyLlama-1.1B-2.5T-chat",
        "Average":39.86,
        "ARC":34.47,
        "HellaSwag":59.71,
        "MMLU":26.45,
        "TruthfulQA":38.8,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"17ea96f8da6f61eee63fa430607e3974825a3218"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TinyLlama\/TinyLlama-1.1B-Chat-v1.0",
        "Average":39.85,
        "ARC":35.92,
        "HellaSwag":61.11,
        "MMLU":25.0,
        "TruthfulQA":37.38,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":881,
        "Available on the hub":true,
        "Model Sha":"de253fa9783f8bd558c9ed398c8ffbe3c55cedb3"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-2.7b",
        "Average":39.84,
        "ARC":37.37,
        "HellaSwag":60.74,
        "MMLU":25.86,
        "TruthfulQA":35.4,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.91,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"b9d8cace80b1a97f5ed380711aea31f2d1b24310"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/xglm-7.5B",
        "Average":39.84,
        "ARC":34.13,
        "HellaSwag":60.77,
        "MMLU":27.79,
        "TruthfulQA":36.66,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.49,
        "Hub":39,
        "Available on the hub":true,
        "Model Sha":"732d59308a844004bd9a4def972cc7c3896a38e0"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"sreeramajay\/TinyLlama-1.1B-orca-v1.0",
        "Average":39.84,
        "ARC":36.35,
        "HellaSwag":61.23,
        "MMLU":25.18,
        "TruthfulQA":36.58,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7dbbc8ccc85c1c3f1ce7cffbb62b97ca6d2ca046"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"indischepartij\/TinyUltra-4x1.1B-Base-Alpha",
        "Average":39.83,
        "ARC":34.9,
        "HellaSwag":61.42,
        "MMLU":25.42,
        "TruthfulQA":37.59,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.38,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c705462832bc69b3ab71cf6e5ebc46388b4145a6"
    },
    {
        "T":"?",
        "Model":"gmonsoon\/TinyUltra-4x1.1B-Base-Alpha",
        "Average":39.83,
        "ARC":34.9,
        "HellaSwag":61.42,
        "MMLU":25.42,
        "TruthfulQA":37.59,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.38,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c705462832bc69b3ab71cf6e5ebc46388b4145a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xaviviro\/OpenHermes-2.5-FLOR-6.3B",
        "Average":39.82,
        "ARC":33.45,
        "HellaSwag":54.53,
        "MMLU":25.18,
        "TruthfulQA":46.12,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.25,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"1ac3215a61b2e11010230d52a6153635def819e6"
    },
    {
        "T":"?",
        "Model":"kalisai\/Nusantara-1.8b-Indo-Chat",
        "Average":39.82,
        "ARC":35.32,
        "HellaSwag":56.32,
        "MMLU":30.37,
        "TruthfulQA":37.27,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.84,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9b92b6cc3f4b3769886aac151edeef9990dcec4b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-2.8b-deduped",
        "Average":39.82,
        "ARC":36.26,
        "HellaSwag":60.66,
        "MMLU":26.78,
        "TruthfulQA":35.56,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.91,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"7d977fed8c4ce9649816af8cd5fe36a639cbe5b2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jan-hq\/LlamaCorn-1.1B",
        "Average":39.81,
        "ARC":34.13,
        "HellaSwag":59.33,
        "MMLU":29.01,
        "TruthfulQA":36.78,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"309e861eb3291666e9bd4e899fc95c8513beda4d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AIGym\/TinyLlama-1.1B-2.5T-chat-and-function-calling",
        "Average":39.81,
        "ARC":34.39,
        "HellaSwag":59.61,
        "MMLU":26.32,
        "TruthfulQA":38.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5dc9334f86c4d5eaa916edd02262416b54343fa8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AIChenKai\/TinyLlama-1.1B-Chat-v1.0-x2-MoE",
        "Average":39.81,
        "ARC":36.01,
        "HellaSwag":61.04,
        "MMLU":24.81,
        "TruthfulQA":37.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.86,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"fe49be7cae7eb8362e176e4d371fb9dd8c68422d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"klosax\/open_llama_3b_350bt_preview",
        "Average":39.8,
        "ARC":36.52,
        "HellaSwag":60.86,
        "MMLU":26.78,
        "TruthfulQA":35.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":3.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4ed354d8f9537fe0a7400772eece1a93f2bd1366"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"klosax\/openllama-3b-350bt",
        "Average":39.8,
        "ARC":36.52,
        "HellaSwag":60.86,
        "MMLU":26.78,
        "TruthfulQA":35.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":3.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4ed354d8f9537fe0a7400772eece1a93f2bd1366"
    },
    {
        "T":"?",
        "Model":"Evaloric\/Evaloric-1.1B",
        "Average":39.79,
        "ARC":35.07,
        "HellaSwag":60.93,
        "MMLU":25.36,
        "TruthfulQA":37.78,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b3953afa698098e06e03aa14713015b11b4ab421"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-2.7B-Nerybus-Mix",
        "Average":39.77,
        "ARC":33.7,
        "HellaSwag":61.21,
        "MMLU":26.6,
        "TruthfulQA":37.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"b4131723cfff1fa42f6cbab546c5b4bb0d19fd83"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Deacon-1_8b",
        "Average":39.76,
        "ARC":33.7,
        "HellaSwag":52.33,
        "MMLU":33.97,
        "TruthfulQA":39.05,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.84,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"77056bdfc4f05eb933a9e9af3af6fe68f89eb0b7"
    },
    {
        "T":"?",
        "Model":"Deathsquad10\/TakeTwo",
        "Average":39.76,
        "ARC":37.2,
        "HellaSwag":62.01,
        "MMLU":23.8,
        "TruthfulQA":36.02,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"50248bfc7243ea02712ca694f1f50f1760a378d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"euclaise\/falcon_1b_stage2",
        "Average":39.73,
        "ARC":33.11,
        "HellaSwag":63.19,
        "MMLU":24.22,
        "TruthfulQA":38.4,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"025c77e9ee457c6771c5a36dbacd064c269642a5"
    },
    {
        "T":"?",
        "Model":"jan-hq\/LlamaCorn-1.1B-Chat",
        "Average":39.72,
        "ARC":33.79,
        "HellaSwag":59.24,
        "MMLU":29.01,
        "TruthfulQA":36.86,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"c2b9512df2001f9ead2ebd3d0286cf47be73ad68"
    },
    {
        "T":"?",
        "Model":"kevin009\/babyllama-v0.6",
        "Average":39.72,
        "ARC":36.09,
        "HellaSwag":61.59,
        "MMLU":25.37,
        "TruthfulQA":35.84,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":1.1,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"ea9e3e3d79df9769679c5a617ec755359338a425"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/GPT-J-6B-Adventure",
        "Average":39.72,
        "ARC":37.12,
        "HellaSwag":61.26,
        "MMLU":25.94,
        "TruthfulQA":34.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"e2c00dc99f986f2430f5d34c0214969cee786755"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"alexredna\/TinyLlama-1.1B-Chat-v1.0-reasoning-v2-dpo",
        "Average":39.68,
        "ARC":34.39,
        "HellaSwag":61.87,
        "MMLU":26.34,
        "TruthfulQA":36.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"f61da97b0c79b404f3dbe88f9379d1c918777338"
    },
    {
        "T":"?",
        "Model":"HuggingFaceTB\/cosmo-1b",
        "Average":39.64,
        "ARC":38.57,
        "HellaSwag":55.13,
        "MMLU":26.69,
        "TruthfulQA":38.15,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.74,
        "Hub":104,
        "Available on the hub":true,
        "Model Sha":"ffb89c545ae24f7f164e121cf68723a18f5b28c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/Tinyllama-Cinder-1.3B-Reason-Test",
        "Average":39.63,
        "ARC":34.56,
        "HellaSwag":58.24,
        "MMLU":25.79,
        "TruthfulQA":39.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.28,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"c7f891765a4d43ac972302a24ed67158cec8dc18"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h4rz3rk4s3\/TinyNewsLlama-1.1B",
        "Average":39.62,
        "ARC":32.94,
        "HellaSwag":59.43,
        "MMLU":25.18,
        "TruthfulQA":40.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a4e7c60302a70746c6bfc4a79d85f040c27c675d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"L-R\/LLmRa-2.7B",
        "Average":39.62,
        "ARC":37.03,
        "HellaSwag":60.65,
        "MMLU":25.58,
        "TruthfulQA":35.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"93201b7d778272fb3252481c1cbd56f726d43e6b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"sail\/Sailor-1.8B",
        "Average":39.6,
        "ARC":33.11,
        "HellaSwag":57.06,
        "MMLU":30.44,
        "TruthfulQA":37.81,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.84,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"c2d5776ce22949330793ddcc4f5b19f61f0dcf8d"
    },
    {
        "T":"?",
        "Model":"HuggingFaceTB\/cosmo-1b",
        "Average":39.6,
        "ARC":38.57,
        "HellaSwag":55.08,
        "MMLU":26.5,
        "TruthfulQA":38.26,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.74,
        "Hub":104,
        "Available on the hub":true,
        "Model Sha":"ffb89c545ae24f7f164e121cf68723a18f5b28c9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cerebras\/Cerebras-GPT-6.7B",
        "Average":39.6,
        "ARC":35.07,
        "HellaSwag":59.36,
        "MMLU":25.93,
        "TruthfulQA":38.02,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.66,
        "Hub":61,
        "Available on the hub":true,
        "Model Sha":"4f56c6e28f9a2a1c470626f1a064238806f19f09"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TFLai\/pythia-2.8b-4bit-alpaca",
        "Average":39.59,
        "ARC":34.73,
        "HellaSwag":58.96,
        "MMLU":25.53,
        "TruthfulQA":39.14,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":2.8,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"40e84b6d38aac92a0302c2a682498794ef0fd901"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/TinyLlama-MoE-Chat-0.1",
        "Average":39.57,
        "ARC":34.39,
        "HellaSwag":56.72,
        "MMLU":29.36,
        "TruthfulQA":37.82,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub":16,
        "Available on the hub":false,
        "Model Sha":"2ebc34217cafbff7812e85fd59c682550bbeb4f8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/opt-2.7b",
        "Average":39.56,
        "ARC":33.96,
        "HellaSwag":61.43,
        "MMLU":25.43,
        "TruthfulQA":37.43,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub":44,
        "Available on the hub":true,
        "Model Sha":"397f71a473a150c00f0fe3fc4a2f78ff3ccaf82d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Qwen\/Qwen1.5-0.5B",
        "Average":39.55,
        "ARC":31.48,
        "HellaSwag":49.05,
        "MMLU":39.35,
        "TruthfulQA":38.3,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.62,
        "Hub":98,
        "Available on the hub":false,
        "Model Sha":"fedce23ef6393499effdf4958f9b3256f299cc7d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-2.7B-Nerys-v2",
        "Average":39.54,
        "ARC":33.28,
        "HellaSwag":61.23,
        "MMLU":26.44,
        "TruthfulQA":37.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"91d7afd6dbf3bbd1e4ccc6b9a2618d632a8cbb92"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Evaloric\/Evaloric-1.1B-V.0.1",
        "Average":39.53,
        "ARC":36.86,
        "HellaSwag":61.9,
        "MMLU":23.96,
        "TruthfulQA":35.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"4cfcbf3aabd60fdeff47e013c40c0dc211f68ddb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BEE-spoke-data\/TinyLlama-3T-1.1bee",
        "Average":39.52,
        "ARC":33.79,
        "HellaSwag":60.29,
        "MMLU":25.86,
        "TruthfulQA":38.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"eca8e79df61b9872b84df24f61f0d8f0573d383e"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/platypus-1_8b",
        "Average":39.5,
        "ARC":33.28,
        "HellaSwag":50.76,
        "MMLU":33.25,
        "TruthfulQA":40.73,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.84,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"688223a26ae6c3f6102bc3f524594cf21ebb752a"
    },
    {
        "T":"?",
        "Model":"invalid-coder\/TinyLlama-1.1B-intermediate-step-1431k-3T-laser-dpo",
        "Average":39.5,
        "ARC":33.02,
        "HellaSwag":60.0,
        "MMLU":26.88,
        "TruthfulQA":38.08,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5781cace9e0c56c090f981375000d5f33512acaa"
    },
    {
        "T":"?",
        "Model":"Aryanne\/sheared-silicon10p",
        "Average":39.43,
        "ARC":36.18,
        "HellaSwag":51.12,
        "MMLU":25.56,
        "TruthfulQA":44.85,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":2.7,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"17494d892ed5d6346ebfaf999af697f1310757cc"
    },
    {
        "T":"?",
        "Model":"ozayezerceli\/TinyLlamax2-1.1b",
        "Average":39.39,
        "ARC":33.87,
        "HellaSwag":60.31,
        "MMLU":26.04,
        "TruthfulQA":37.32,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"89e5c84a590d840c41ba0ac2147cd6aa517f5320"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TinyLlama\/TinyLlama-1.1B-intermediate-step-1431k-3T",
        "Average":39.39,
        "ARC":33.87,
        "HellaSwag":60.31,
        "MMLU":26.04,
        "TruthfulQA":37.32,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":127,
        "Available on the hub":true,
        "Model Sha":"df4c1907f152969ce2850c097e414d79c3a1665a"
    },
    {
        "T":"?",
        "Model":"alnrg2arg\/blockchainlabs_tinyllama_fusion_LHK_yunkong",
        "Average":39.39,
        "ARC":34.73,
        "HellaSwag":60.41,
        "MMLU":24.96,
        "TruthfulQA":37.45,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"90efaf10c2374914063031791059afd1125f1293"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigscience\/bloom-3b",
        "Average":39.32,
        "ARC":35.75,
        "HellaSwag":54.37,
        "MMLU":26.59,
        "TruthfulQA":40.57,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":3.0,
        "Hub":67,
        "Available on the hub":true,
        "Model Sha":"52bc5b43010b4844513826b8be3f78c7344c37d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Devio\/test100",
        "Average":39.3,
        "ARC":37.37,
        "HellaSwag":58.54,
        "MMLU":27.29,
        "TruthfulQA":34.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6bd139260f60232328b05b2cd973c3d8f07c0c02"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Abhaykoul\/qwen1.5-vortex",
        "Average":39.29,
        "ARC":31.83,
        "HellaSwag":47.71,
        "MMLU":38.66,
        "TruthfulQA":38.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.46,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"52cd90c24411e9474a32c70e78b549e98108206e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"alexredna\/Tukan-1.1B-Chat-reasoning-sft-COLA",
        "Average":39.26,
        "ARC":34.13,
        "HellaSwag":59.78,
        "MMLU":24.86,
        "TruthfulQA":38.25,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"fa129eb7563bc1f8234dc372d6255bec3c3b4143"
    },
    {
        "T":"\u2b55",
        "Model":"RWKV\/rwkv-raven-3b",
        "Average":39.23,
        "ARC":36.69,
        "HellaSwag":59.78,
        "MMLU":24.87,
        "TruthfulQA":35.6,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":2.86,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"1ddeea6a7313c8ba8824645d7aa88d5449458f67"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"appvoid\/palmer-002",
        "Average":39.22,
        "ARC":34.47,
        "HellaSwag":59.41,
        "MMLU":25.94,
        "TruthfulQA":37.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.03,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"8b79b8c2126483baeb3a503c51cd4ffa9d7c11a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Abhaykoul\/Qwen1.5-0.5B-vortex",
        "Average":39.22,
        "ARC":31.74,
        "HellaSwag":47.78,
        "MMLU":38.44,
        "TruthfulQA":38.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.46,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"dd2364092537b736e87bafb4b2fd65ea460beb32"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RESMPDEV\/Qwen1.5-Wukong-0.5B",
        "Average":39.22,
        "ARC":31.74,
        "HellaSwag":47.78,
        "MMLU":38.44,
        "TruthfulQA":38.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.46,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"cb46afdc603e018e2ec4a672769b0e8a473aa88b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Abhaykoul\/qwen1.5-vortex",
        "Average":39.22,
        "ARC":31.74,
        "HellaSwag":47.78,
        "MMLU":38.44,
        "TruthfulQA":38.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.46,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"52cd90c24411e9474a32c70e78b549e98108206e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/WoolyHermes-1.1B",
        "Average":39.21,
        "ARC":34.3,
        "HellaSwag":59.37,
        "MMLU":25.59,
        "TruthfulQA":37.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":1.1,
        "Hub":28,
        "Available on the hub":false,
        "Model Sha":"cf2c67039648176ffe45e3ffb9892557a95d3405"
    },
    {
        "T":"\u2b55",
        "Model":"vihangd\/shearedplats-1.3b-v1",
        "Average":39.21,
        "ARC":35.41,
        "HellaSwag":62.75,
        "MMLU":24.75,
        "TruthfulQA":33.93,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":1.28,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7ac93152e1807ec1d732500255a747e27922fb1a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Abhaykoul\/Qwen1.5-0.5B-vortex-v2",
        "Average":39.19,
        "ARC":30.63,
        "HellaSwag":45.54,
        "MMLU":36.29,
        "TruthfulQA":44.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.46,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"029222d7fbad83a11b2b721aae4dbed6c8f848d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"euclaise\/falcon_1b_stage3_2",
        "Average":39.17,
        "ARC":34.56,
        "HellaSwag":58.37,
        "MMLU":23.87,
        "TruthfulQA":39.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"aec2f59879ea6dfa5233611c4cf83cf3cb974d40"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gmonsoon\/TinyWombat-1.8b-Chat-v.1",
        "Average":39.17,
        "ARC":32.94,
        "HellaSwag":58.88,
        "MMLU":25.12,
        "TruthfulQA":39.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.8,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"149003fdf86efe3d8d28145ef31f2baeb01b019a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"princeton-nlp\/Sheared-LLaMA-1.3B",
        "Average":39.15,
        "ARC":32.85,
        "HellaSwag":60.91,
        "MMLU":25.71,
        "TruthfulQA":37.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.28,
        "Hub":81,
        "Available on the hub":true,
        "Model Sha":"b1c3f74c8495e27b3963d64af0781d4a611794f3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azure99\/blossom-v2-3b",
        "Average":39.13,
        "ARC":35.32,
        "HellaSwag":54.1,
        "MMLU":23.99,
        "TruthfulQA":43.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1a403344de52ddb7f18548a526a927714adfe4d4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AtAndDev\/ShortKingv0.1",
        "Average":39.06,
        "ARC":34.22,
        "HellaSwag":54.59,
        "MMLU":25.78,
        "TruthfulQA":41.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.42,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"6cd9b5bc13ee15b5e7e7cfb46477bc6a7c0b5d47"
    },
    {
        "T":"?",
        "Model":"ToastyPigeon\/smolphin-test-bottomheavy",
        "Average":39.04,
        "ARC":32.68,
        "HellaSwag":59.17,
        "MMLU":25.84,
        "TruthfulQA":38.49,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.54,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c8226313c06f0d749be5a29db75a4e9467921d87"
    },
    {
        "T":"?",
        "Model":"ToastyPigeon\/SmolPlatypus-1.5B-Sorted",
        "Average":39.04,
        "ARC":33.62,
        "HellaSwag":59.06,
        "MMLU":25.61,
        "TruthfulQA":37.88,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.54,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"508179f70e68b6213c5f2f02ff76cfc2796441b4"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-34b-Python-hf",
        "Average":39.02,
        "ARC":40.19,
        "HellaSwag":36.82,
        "MMLU":34.79,
        "TruthfulQA":44.28,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub":51,
        "Available on the hub":true,
        "Model Sha":"3dd8ab05bbd273b9f77088b1d4015b7f1848793d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/TinyLlama-Cinder-1.3B-Test.2",
        "Average":39.01,
        "ARC":33.7,
        "HellaSwag":58.66,
        "MMLU":25.69,
        "TruthfulQA":37.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.28,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8ef75bb502bcafe83a03fe7154e302ea6de185df"
    },
    {
        "T":"\u2b55",
        "Model":"Ba2han\/TinyOpenHermes-1.1B-4k",
        "Average":38.98,
        "ARC":33.62,
        "HellaSwag":58.53,
        "MMLU":26.45,
        "TruthfulQA":37.33,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b496ad5f3462828778aa9ec40ec78157f84240e3"
    },
    {
        "T":"?",
        "Model":"TinyLlama\/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T",
        "Average":38.98,
        "ARC":33.53,
        "HellaSwag":59.38,
        "MMLU":26.22,
        "TruthfulQA":36.79,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":44,
        "Available on the hub":true,
        "Model Sha":"03978af6c0997cda809de070e056ee5ddb7e7188"
    },
    {
        "T":"?",
        "Model":"cognitivecomputations\/TinyDolphin-2.8.1-1.1b",
        "Average":38.98,
        "ARC":34.98,
        "HellaSwag":60.11,
        "MMLU":25.31,
        "TruthfulQA":35.51,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"5117b2287bfce3549fc534e16f427cacf521fc7d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bhenrym14\/airoboros-33b-gpt4-1.4.1-PI-8192-fp16",
        "Average":38.98,
        "ARC":32.0,
        "HellaSwag":53.88,
        "MMLU":31.43,
        "TruthfulQA":38.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.53,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1dd7804dbbb547c1be852652ce74568ba41d4e73"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/gpt-neo-2.7B",
        "Average":38.96,
        "ARC":33.36,
        "HellaSwag":56.24,
        "MMLU":26.45,
        "TruthfulQA":39.78,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.72,
        "Hub":358,
        "Available on the hub":true,
        "Model Sha":"e24fa291132763e59f4a5422741b424fb5d59056"
    },
    {
        "T":"?",
        "Model":"cognitivecomputations\/TinyDolphin-2.8-1.1b",
        "Average":38.96,
        "ARC":34.3,
        "HellaSwag":59.44,
        "MMLU":25.59,
        "TruthfulQA":36.51,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":28,
        "Available on the hub":true,
        "Model Sha":"fc3e646d484cf1e48d210b69d7f142f104b996da"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Ba2han\/Tinypus-1.5B",
        "Average":38.92,
        "ARC":33.45,
        "HellaSwag":57.35,
        "MMLU":25.53,
        "TruthfulQA":39.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":1.45,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5bfbd6b5920189dad68217576e0e23be4d2265d4"
    },
    {
        "T":"?",
        "Model":"Josephgflowers\/TinyLlama-3T-Cinder-v1.3",
        "Average":38.91,
        "ARC":33.96,
        "HellaSwag":58.14,
        "MMLU":25.41,
        "TruthfulQA":38.13,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.1,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"4cd25191268b231cb584c85ce55285902c9fa31a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"danielhanchen\/open_llama_3b_600bt_preview",
        "Average":38.9,
        "ARC":36.86,
        "HellaSwag":59.96,
        "MMLU":25.97,
        "TruthfulQA":32.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d8fddf7651dfcae5aefda59d9e868c9111d8bdb3"
    },
    {
        "T":"?",
        "Model":"ToastyPigeon\/SmolPlatypus-1.5B",
        "Average":38.89,
        "ARC":33.96,
        "HellaSwag":60.05,
        "MMLU":24.73,
        "TruthfulQA":36.82,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.54,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"98d850be9b5536120bb4591381854c1acfa20b12"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aloobun\/d-Qwen1.5-0.5B",
        "Average":38.89,
        "ARC":30.29,
        "HellaSwag":47.75,
        "MMLU":38.21,
        "TruthfulQA":39.29,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.46,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"c845a4ca140e3abfb58793731c4addd4533102b3"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Dampish\/StellarX-4B-V0.2",
        "Average":38.87,
        "ARC":34.64,
        "HellaSwag":56.74,
        "MMLU":25.55,
        "TruthfulQA":38.55,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":2.65,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"605b6812956400dbde24ad7b8649a744a2ddfc8e"
    },
    {
        "T":"?",
        "Model":"HuggingFaceH4\/starchat-alpha",
        "Average":38.85,
        "ARC":31.57,
        "HellaSwag":49.43,
        "MMLU":30.76,
        "TruthfulQA":43.66,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":15.52,
        "Hub":220,
        "Available on the hub":true,
        "Model Sha":"b693a7a7d52bed1cd7cc0fe00399db838b09c74f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Dans-DiscountModels\/ShearedLlama-1.3b-FFT-Test1",
        "Average":38.83,
        "ARC":32.68,
        "HellaSwag":59.99,
        "MMLU":25.69,
        "TruthfulQA":36.97,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.28,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"68e43c006a01764d3ff2bcaeaec5289f2ddad36a"
    },
    {
        "T":"?",
        "Model":"ToastyPigeon\/smolphin-test-stack-sorted",
        "Average":38.83,
        "ARC":32.34,
        "HellaSwag":59.07,
        "MMLU":26.44,
        "TruthfulQA":37.48,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.54,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"41c13b6301a2e03272c5a2ebaa57a0a4d048d1cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"habanoz\/tinyllama-oasst1-top1-instruct-full-lr1-5-v0.1",
        "Average":38.83,
        "ARC":32.85,
        "HellaSwag":58.16,
        "MMLU":25.96,
        "TruthfulQA":38.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"e55b262cbd0ee52f7a4cbda136dbf1a027987c47"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Walter-Llama-1B",
        "Average":38.82,
        "ARC":32.85,
        "HellaSwag":61.05,
        "MMLU":27.46,
        "TruthfulQA":33.93,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"ae782b5a37bc961d0860e6a8edb10547bb5285d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nnpy\/Nape-0",
        "Average":38.81,
        "ARC":32.68,
        "HellaSwag":58.68,
        "MMLU":24.88,
        "TruthfulQA":38.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.1,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"47e07bd518b989890a7f694d39e2772e703384c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h4rz3rk4s3\/TinyPoliticaLlama-1.1B",
        "Average":38.78,
        "ARC":33.79,
        "HellaSwag":57.83,
        "MMLU":25.45,
        "TruthfulQA":38.06,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8838d8f094dee1078572cf127f835cdb32117d6f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewqr2130\/TinyLamma-SFT",
        "Average":38.75,
        "ARC":34.39,
        "HellaSwag":59.14,
        "MMLU":24.26,
        "TruthfulQA":37.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4dbfdc67f096a0a801d95c4f4c74cd6dd0c52e1c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PY007\/TinyLlama-1.1B-Chat-v0.3",
        "Average":38.74,
        "ARC":35.07,
        "HellaSwag":57.7,
        "MMLU":25.53,
        "TruthfulQA":36.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":39,
        "Available on the hub":false,
        "Model Sha":"20dd44d78aa09480bf15ca0ecc0c0780951d49a9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pythainlp\/wangchanglm-7.5B-sft-en-sharded",
        "Average":38.7,
        "ARC":34.47,
        "HellaSwag":59.81,
        "MMLU":26.37,
        "TruthfulQA":34.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.49,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"dd22eaea8be3fcb8c28f61b513a89d1adac00ffd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beaugogh\/pythia-1.4b-deduped-sharegpt",
        "Average":38.65,
        "ARC":34.3,
        "HellaSwag":54.49,
        "MMLU":24.0,
        "TruthfulQA":41.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.42,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"03dfdc25c111a6a4a16d3da12190697611936426"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HWERI\/pythia-1.4b-deduped-sharegpt",
        "Average":38.65,
        "ARC":34.3,
        "HellaSwag":54.49,
        "MMLU":24.0,
        "TruthfulQA":41.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.42,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5b50336208840f557ef3301d841e7994caaa63bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"42dot\/42dot_LLM-PLM-1.3B",
        "Average":38.65,
        "ARC":32.42,
        "HellaSwag":56.39,
        "MMLU":27.09,
        "TruthfulQA":38.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.44,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"a72bf57eb02cd4ea4388a344b4a5893aa95698da"
    },
    {
        "T":"?",
        "Model":"ToastyPigeon\/smolphin-test-stack",
        "Average":38.61,
        "ARC":32.68,
        "HellaSwag":59.94,
        "MMLU":25.16,
        "TruthfulQA":36.64,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.54,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0ac7c8b73ca8a9602c777481367fca2c4528c17e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/Tinyllama-Cinder-1.3B-Reason-Test.2",
        "Average":38.6,
        "ARC":32.76,
        "HellaSwag":58.27,
        "MMLU":24.39,
        "TruthfulQA":39.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.28,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4c049680e43aa4fdab117c7a440b83efb4560ef5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"remyxai\/localmentor_25K_3epochs_tinyllama",
        "Average":38.56,
        "ARC":34.22,
        "HellaSwag":59.01,
        "MMLU":24.93,
        "TruthfulQA":36.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"670b3f3be7ee5fd09922c033d0fa2d539f98344a"
    },
    {
        "T":"?",
        "Model":"cognitivecomputations\/TinyDolphin-2.8.2-1.1b-laser",
        "Average":38.54,
        "ARC":33.36,
        "HellaSwag":58.53,
        "MMLU":25.93,
        "TruthfulQA":36.33,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"471c1ff16a8a78afa702a69f16df98dc14464bf6"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"maywell\/TinyWand-DPO",
        "Average":38.52,
        "ARC":31.66,
        "HellaSwag":50.42,
        "MMLU":26.22,
        "TruthfulQA":45.8,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.63,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"7bf42524d664785d92243576b1f7d3b3ed463819"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Dans-DiscountModels\/TinyLlama-1.1B-FFT-Test2",
        "Average":38.51,
        "ARC":34.22,
        "HellaSwag":57.96,
        "MMLU":25.54,
        "TruthfulQA":36.32,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.03,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"dfedea2fbf66c27c88cd4b2eeb0ff0f5041e3b59"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"frankenmerger\/cosmo-3b-test",
        "Average":38.49,
        "ARC":35.32,
        "HellaSwag":52.36,
        "MMLU":27.25,
        "TruthfulQA":39.02,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.95,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"bb3e1b70079ea2d17c23171d01189e09fe6712c5"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/falcon-1b-t-sft",
        "Average":38.48,
        "ARC":32.94,
        "HellaSwag":57.24,
        "MMLU":25.26,
        "TruthfulQA":38.49,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.31,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3b891a0c37f8fa98301c85fcf34baae876e4cac1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/stablelm-7b-sft-v7-epoch-3",
        "Average":38.46,
        "ARC":36.01,
        "HellaSwag":55.81,
        "MMLU":25.01,
        "TruthfulQA":37.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.56,
        "Hub":65,
        "Available on the hub":true,
        "Model Sha":"4c454bfc0e3618b3d574e28ba71369607e637e91"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/TinyLlama-3T-Cinder-v1.2",
        "Average":38.45,
        "ARC":34.39,
        "HellaSwag":56.51,
        "MMLU":26.14,
        "TruthfulQA":36.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.1,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"15c3d37d6d0a6ec7294ce9b5c84851b739f47508"
    },
    {
        "T":"?",
        "Model":"ToastyPigeon\/SmolLlama-1.5B-Bottomheavy",
        "Average":38.43,
        "ARC":34.22,
        "HellaSwag":59.54,
        "MMLU":24.96,
        "TruthfulQA":35.0,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.54,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b383556d08f411258991be602297b00208753c87"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-7b-Python-hf",
        "Average":38.43,
        "ARC":31.31,
        "HellaSwag":52.86,
        "MMLU":27.32,
        "TruthfulQA":42.21,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":49,
        "Available on the hub":true,
        "Model Sha":"ec4dd26f30674fdee00ef161b55f464ce28f9c20"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"habanoz\/TinyLlama-1.1B-intermediate-step-715k-1.5T-lr-5-2.2epochs-oasst1-top1-instruct-V1",
        "Average":38.42,
        "ARC":31.48,
        "HellaSwag":54.4,
        "MMLU":25.47,
        "TruthfulQA":42.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"74cd9eba94e77832b3081689fc5c99c37c063790"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/chopt-1_3b",
        "Average":38.41,
        "ARC":31.48,
        "HellaSwag":56.63,
        "MMLU":25.35,
        "TruthfulQA":40.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"fdd3691978f557baf9d1c20d4ede900c47f7e135"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"M4-ai\/tau-0.5B",
        "Average":38.4,
        "ARC":29.27,
        "HellaSwag":47.43,
        "MMLU":37.53,
        "TruthfulQA":39.39,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.46,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"dee82e4f55c393354b33ed3f1d448aa520ba8e26"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"freecs\/Tiny-Llama-3-7b",
        "Average":38.39,
        "ARC":34.64,
        "HellaSwag":56.39,
        "MMLU":24.51,
        "TruthfulQA":38.03,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"778db38d13be6ed3384fa049114a95d56cf420d3"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"freecs\/Llama-3-7b",
        "Average":38.39,
        "ARC":34.64,
        "HellaSwag":56.39,
        "MMLU":24.51,
        "TruthfulQA":38.03,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"778db38d13be6ed3384fa049114a95d56cf420d3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OEvortex\/vortex-3b",
        "Average":38.38,
        "ARC":31.91,
        "HellaSwag":56.89,
        "MMLU":27.32,
        "TruthfulQA":37.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.78,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"033a96fd948b6375247465c72be51a6cb6b46c50"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"habanoz\/TinyLlama-1.1B-intermediate-step-715k-1.5T-lr-5-3epochs-oasst1-top1-instruct-V1",
        "Average":38.37,
        "ARC":31.4,
        "HellaSwag":54.24,
        "MMLU":25.36,
        "TruthfulQA":42.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b1ec2a1e08eb790b9a32a43053316650921af943"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"VAIBHAV22334455\/JARVIS",
        "Average":38.36,
        "ARC":32.08,
        "HellaSwag":56.86,
        "MMLU":27.15,
        "TruthfulQA":37.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"28091aa912d17a231f59a18a286f289928c098fb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/Tinyllama-1.3B-Cinder-Reason-Test-2",
        "Average":38.34,
        "ARC":32.76,
        "HellaSwag":57.92,
        "MMLU":25.42,
        "TruthfulQA":37.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.28,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3afb28f36d110d6520bb2f08baf40283babf1e9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"M4-ai\/tau-0.5B",
        "Average":38.32,
        "ARC":29.01,
        "HellaSwag":47.45,
        "MMLU":37.44,
        "TruthfulQA":39.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.46,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"dee82e4f55c393354b33ed3f1d448aa520ba8e26"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhinand\/TinyLlama-1.1B-OpenHermes-2.5-Chat-v0.1-sft",
        "Average":38.31,
        "ARC":33.79,
        "HellaSwag":58.72,
        "MMLU":24.52,
        "TruthfulQA":36.22,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f91c61253e1f80e7a04ee3a002ef6c7681379d42"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"frankenmerger\/cosmo-3b-test-v0.2",
        "Average":38.29,
        "ARC":35.32,
        "HellaSwag":51.7,
        "MMLU":27.33,
        "TruthfulQA":38.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.95,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"544e8b53e20aa379415ba12ecd1616d2a894672d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/metharme-1.3b",
        "Average":38.27,
        "ARC":34.39,
        "HellaSwag":55.94,
        "MMLU":25.07,
        "TruthfulQA":37.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.52,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"62ec4ff53042f692ef0661e54f371747214707a4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Linly-AI\/Chinese-LLaMA-2-13B-hf",
        "Average":38.22,
        "ARC":33.62,
        "HellaSwag":39.59,
        "MMLU":33.97,
        "TruthfulQA":45.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.89,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"7b01e95769d61960dbd8ad52045852aebcdf92b0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chargoddard\/llama-2-34b-uncode",
        "Average":38.21,
        "ARC":39.51,
        "HellaSwag":33.9,
        "MMLU":38.49,
        "TruthfulQA":40.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"d434d06249feb6ca511b0a09162130bcc59d84e3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dball\/zephyr-tiny-sft-qlora-quantized-2",
        "Average":38.2,
        "ARC":33.19,
        "HellaSwag":58.58,
        "MMLU":25.21,
        "TruthfulQA":35.82,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":1.03,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"effd86f3284e6472f1a865a208ee68900e9f7318"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"22h\/open-cabrita3b",
        "Average":38.2,
        "ARC":33.79,
        "HellaSwag":55.35,
        "MMLU":25.16,
        "TruthfulQA":38.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.39,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"fc2a2de94a3b31de54aaace695537c4d1c3e456d"
    },
    {
        "T":"?",
        "Model":"jeff31415\/TinyLlama-1.1B-1.5T-OpenOrca-Alpha",
        "Average":38.2,
        "ARC":32.76,
        "HellaSwag":53.77,
        "MMLU":25.73,
        "TruthfulQA":40.52,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"c337449718ad228fcf205e9c963ad31043e027ad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"habanoz\/TinyLlama-1.1B-intermediate-step-715k-1.5T-lr-5-4epochs-oasst1-top1-instruct-V1",
        "Average":38.15,
        "ARC":31.14,
        "HellaSwag":54.31,
        "MMLU":25.42,
        "TruthfulQA":41.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"7cd6d5ad10180127771e4326772eae3d40fa8445"
    },
    {
        "T":"?",
        "Model":"kalisai\/Nusantara-2.7b-Indo-Chat",
        "Average":38.14,
        "ARC":34.22,
        "HellaSwag":56.1,
        "MMLU":24.83,
        "TruthfulQA":37.41,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"452bc6935b408166abe65a7966afa03cbd78ce02"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"RWKV\/rwkv-4-3b-pile",
        "Average":38.12,
        "ARC":36.01,
        "HellaSwag":59.66,
        "MMLU":24.67,
        "TruthfulQA":32.14,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":2.86,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"7fdda3c5570d4a9711f8f02cc3a20941a5623cd3"
    },
    {
        "T":"?",
        "Model":"ToastyPigeon\/smolphin-test1",
        "Average":38.1,
        "ARC":32.25,
        "HellaSwag":59.73,
        "MMLU":24.61,
        "TruthfulQA":35.81,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4e83efe15c074ba19df21f64d6aa989b096dab01"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zyh3826\/20231206094523-pretrain-Llama-2-13b-hf-76000",
        "Average":38.06,
        "ARC":31.06,
        "HellaSwag":52.03,
        "MMLU":24.43,
        "TruthfulQA":44.71,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.25,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"28b3ae089b5610053f2294d24667fe248405f031"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pythainlp\/wangchanglm-7.5B-sft-enth",
        "Average":38.05,
        "ARC":33.79,
        "HellaSwag":58.99,
        "MMLU":24.52,
        "TruthfulQA":34.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.49,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"eeee33ea6778a5e66184eeb4bf4294d4316b1933"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MBZUAI\/LaMini-GPT-1.5B",
        "Average":38.04,
        "ARC":31.4,
        "HellaSwag":48.38,
        "MMLU":29.92,
        "TruthfulQA":42.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.56,
        "Hub":32,
        "Available on the hub":true,
        "Model Sha":"88ca6f5abe2335bac317e82684e574afdd6046b5"
    },
    {
        "T":"?",
        "Model":"Corianas\/DPO-miniguanaco-1.5T",
        "Average":38.04,
        "ARC":30.63,
        "HellaSwag":54.05,
        "MMLU":24.79,
        "TruthfulQA":42.69,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b9be4cc848fc5c7047b32a42451b1631a14ee00e"
    },
    {
        "T":"?",
        "Model":"ToastyPigeon\/SmolLlama-1.5B",
        "Average":38.03,
        "ARC":32.76,
        "HellaSwag":56.74,
        "MMLU":24.53,
        "TruthfulQA":38.08,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.54,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1fdc734ee4063929f4b79aeea78fb849904a83ee"
    },
    {
        "T":"?",
        "Model":"Thytu\/phi-2-audio-super",
        "Average":38.01,
        "ARC":35.92,
        "HellaSwag":45.33,
        "MMLU":24.58,
        "TruthfulQA":46.21,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2e2e3c678cfd3d9a61ab669c354b479a30d0cdeb"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Writer\/palmyra-base",
        "Average":38.01,
        "ARC":31.91,
        "HellaSwag":55.39,
        "MMLU":27.15,
        "TruthfulQA":37.57,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.05,
        "Hub":33,
        "Available on the hub":true,
        "Model Sha":"df2f3bdb7cbe4295d69cf0cbc35f3ceaf451de82"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/fairseq-dense-1.3B",
        "Average":37.99,
        "ARC":31.14,
        "HellaSwag":58.39,
        "MMLU":24.98,
        "TruthfulQA":37.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.41,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"20bf1732212ea81adb45b782a25ce69e65a01ad2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PY007\/TinyLlama-1.1B-Chat-v0.1",
        "Average":37.99,
        "ARC":32.0,
        "HellaSwag":54.21,
        "MMLU":26.71,
        "TruthfulQA":39.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":38,
        "Available on the hub":true,
        "Model Sha":"7abc14e7779eabc3a028bc695342869d0410dea2"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-1.4b-deduped",
        "Average":37.96,
        "ARC":32.68,
        "HellaSwag":54.96,
        "MMLU":25.56,
        "TruthfulQA":38.66,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"77f320b24ccae4aa85a5890dbb9514bd11267bb3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MBZUAI\/lamini-neo-1.3b",
        "Average":37.93,
        "ARC":32.76,
        "HellaSwag":49.13,
        "MMLU":28.79,
        "TruthfulQA":41.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.32,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"a5c7ecc4d908e7a9469d080308af64ae775c733d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/Cinder-1.3B-Test",
        "Average":37.91,
        "ARC":33.19,
        "HellaSwag":55.48,
        "MMLU":26.37,
        "TruthfulQA":36.62,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":1.28,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"cf20c861e6ed630c5391640a049b9c4f92748a2b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Minami-su\/Qwen1.5-0.5B-Chat_llamafy",
        "Average":37.89,
        "ARC":30.63,
        "HellaSwag":44.11,
        "MMLU":33.82,
        "TruthfulQA":42.97,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.46,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"23d87e6b371b3bb929600b690ec4f8160a387eaa"
    },
    {
        "T":"?",
        "Model":"habanoz\/TinyLlama-1.1B-intermediate-step-715k-1.5T-lr-5-1epch-airoboros3.1-1k-instruct-V1",
        "Average":37.87,
        "ARC":30.72,
        "HellaSwag":54.32,
        "MMLU":24.78,
        "TruthfulQA":41.67,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2b961bacab9fcd4bf9a0d6979b024fe23f61555e"
    },
    {
        "T":"?",
        "Model":"Qwen\/Qwen1.5-0.5B-Chat",
        "Average":37.85,
        "ARC":30.55,
        "HellaSwag":44.07,
        "MMLU":33.82,
        "TruthfulQA":42.95,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.62,
        "Hub":39,
        "Available on the hub":false,
        "Model Sha":"6c705984bb8b5591dd4e1a9e66e1a127965fd08d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"juhwanlee\/gemma-7B-alpaca-case-3-2",
        "Average":37.84,
        "ARC":33.28,
        "HellaSwag":49.24,
        "MMLU":27.86,
        "TruthfulQA":40.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3e66bf986384a611f1316b660e1b0a6072d77866"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"vihangd\/dopeyplats-1.1b-2T-v1",
        "Average":37.81,
        "ARC":33.11,
        "HellaSwag":54.31,
        "MMLU":24.55,
        "TruthfulQA":39.26,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.03,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"4ca47b470296de0e7bf3261e377aabaff9ad5c06"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h4rz3rk4s3\/TinyParlaMintLlama-1.1B",
        "Average":37.79,
        "ARC":31.66,
        "HellaSwag":55.87,
        "MMLU":24.84,
        "TruthfulQA":38.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"0c9aa196c68732bf1b563dcfb4d9c6f835087e9e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt",
        "Average":37.75,
        "ARC":34.04,
        "HellaSwag":50.51,
        "MMLU":24.66,
        "TruthfulQA":41.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"754e0c90ed5d9241fdfd5a188572b3ea2152eaa7"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Deacon-1b",
        "Average":37.75,
        "ARC":32.42,
        "HellaSwag":58.62,
        "MMLU":24.89,
        "TruthfulQA":35.05,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.1,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"77f16fd4c605fe043033d4335024fb887cedef69"
    },
    {
        "T":"?",
        "Model":"tyson0420\/mixtral_stack_llama",
        "Average":37.75,
        "ARC":34.56,
        "HellaSwag":50.24,
        "MMLU":27.97,
        "TruthfulQA":38.22,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.07,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b231bdee0dea526b0d7b6df2a182ab1cd224f8eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"L-R\/LLmRa-1.3B",
        "Average":37.72,
        "ARC":32.68,
        "HellaSwag":58.77,
        "MMLU":23.23,
        "TruthfulQA":36.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8d5e8bb336cb886e20a7570bc00c2381792338a5"
    },
    {
        "T":"\u2b55",
        "Model":"sartmis1\/starcoder-finetune-openapi",
        "Average":37.72,
        "ARC":30.63,
        "HellaSwag":48.09,
        "MMLU":30.4,
        "TruthfulQA":41.77,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"fed87393fd749e46c0c82da09d433deb9b7cf9ee"
    },
    {
        "T":"\u2b55",
        "Model":"MayaPH\/opt-flan-iml-6.7b",
        "Average":37.7,
        "ARC":30.12,
        "HellaSwag":58.82,
        "MMLU":25.12,
        "TruthfulQA":36.74,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":6.66,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"cbe8d60db6f3c52e653ca73e23a1c34c08127d02"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/xglm-4.5B",
        "Average":37.68,
        "ARC":31.48,
        "HellaSwag":57.95,
        "MMLU":25.43,
        "TruthfulQA":35.84,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":5.08,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"dc6a67fac06c8bca7860b84656a0cb736293a7a8"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/tinyllama-1.1b-chat-v0.3_platypus",
        "Average":37.67,
        "ARC":30.29,
        "HellaSwag":55.12,
        "MMLU":26.13,
        "TruthfulQA":39.15,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.03,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"0bb6ebe1d41d394bae0ed9107ec8d776d9d76a68"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beberik\/TinyExperts-v0-4x1B",
        "Average":37.67,
        "ARC":31.4,
        "HellaSwag":52.29,
        "MMLU":25.87,
        "TruthfulQA":41.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":2.62,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"cf8144d3b97b9f0154d0a84be2ee758cc60ca33c"
    },
    {
        "T":"\u2b55",
        "Model":"w95\/megachat",
        "Average":37.64,
        "ARC":30.8,
        "HellaSwag":54.35,
        "MMLU":25.55,
        "TruthfulQA":39.85,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.03,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"789b259a18ca7b168ced4995138ad6195cd2e8e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/Tinyllama-1.3B-Cinder-Reason-Test",
        "Average":37.64,
        "ARC":32.51,
        "HellaSwag":55.85,
        "MMLU":26.61,
        "TruthfulQA":35.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.28,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c747bd4aebbcbe59cd5d9bf5a1b4825a74626f43"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WizardLM\/WizardCoder-15B-V1.0",
        "Average":37.63,
        "ARC":32.34,
        "HellaSwag":47.2,
        "MMLU":29.43,
        "TruthfulQA":41.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":15.52,
        "Hub":601,
        "Available on the hub":true,
        "Model Sha":"926ca1b215c4631bc5f8c3e47173381452c23e5c"
    },
    {
        "T":"?",
        "Model":"facebook\/opt-iml-max-1.3b",
        "Average":37.62,
        "ARC":30.72,
        "HellaSwag":53.81,
        "MMLU":27.61,
        "TruthfulQA":38.34,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.32,
        "Hub":34,
        "Available on the hub":true,
        "Model Sha":"d60fa58f50def19751da2075791da359ca19d273"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Deathsquad10\/TinyLlama-1.1B-Remix-V.2",
        "Average":37.61,
        "ARC":33.19,
        "HellaSwag":56.62,
        "MMLU":25.99,
        "TruthfulQA":34.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d60a0c010610de653e55fe498585a44a7202c8b1"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/TinyWand-SFT",
        "Average":37.6,
        "ARC":31.4,
        "HellaSwag":49.96,
        "MMLU":25.98,
        "TruthfulQA":43.08,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.63,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"ac1dffae8e8a8324fdac7a266a8ce82e6d033577"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"stabilityai\/stablelm-tuned-alpha-7b",
        "Average":37.57,
        "ARC":31.91,
        "HellaSwag":53.59,
        "MMLU":24.41,
        "TruthfulQA":40.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":7.56,
        "Hub":350,
        "Available on the hub":true,
        "Model Sha":"25071b093c15c0d1cb2b2876c6deb621b764fcf5"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"croissantllm\/CroissantCool-v0.2",
        "Average":37.57,
        "ARC":31.83,
        "HellaSwag":54.58,
        "MMLU":24.54,
        "TruthfulQA":39.34,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":1.34,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c74ac4b38311306a7b49d99a1f32431e4508e4f3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v2-1_5b",
        "Average":37.57,
        "ARC":32.59,
        "HellaSwag":53.98,
        "MMLU":24.93,
        "TruthfulQA":38.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.56,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"97440ff1b6ef749423758e3495cdce1b5e68ee92"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"euclaise\/falcon_1b_stage3",
        "Average":37.55,
        "ARC":33.11,
        "HellaSwag":54.08,
        "MMLU":25.11,
        "TruthfulQA":37.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"593e48197e91537b203ba288260f6580b9cbcbe6"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"stabilityai\/stablelm-base-alpha-7b",
        "Average":37.54,
        "ARC":32.0,
        "HellaSwag":51.78,
        "MMLU":26.21,
        "TruthfulQA":40.19,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.56,
        "Hub":209,
        "Available on the hub":true,
        "Model Sha":"38366357b5a45e002af2d254ff3d559444ec2147"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sartmis1\/starcoder-finetune-selfinstruct",
        "Average":37.51,
        "ARC":31.23,
        "HellaSwag":47.66,
        "MMLU":29.52,
        "TruthfulQA":41.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":15.52,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b21bd307ea7417185e7dc59557c399a3e4e0092b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lizhuang144\/starcoder_mirror",
        "Average":37.45,
        "ARC":31.31,
        "HellaSwag":45.82,
        "MMLU":29.29,
        "TruthfulQA":43.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":15.52,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"eb5f39bac15ccab9463001aa203e33d49f4ff7cb"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigcode\/starcoderbase",
        "Average":37.41,
        "ARC":30.29,
        "HellaSwag":47.21,
        "MMLU":32.12,
        "TruthfulQA":40.02,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":0.0,
        "Hub":379,
        "Available on the hub":false,
        "Model Sha":"88ec5781ad071a9d9e925cd28f327dea22eb5188"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt1.3b_10e4",
        "Average":37.4,
        "ARC":30.55,
        "HellaSwag":53.52,
        "MMLU":26.89,
        "TruthfulQA":38.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":1.3,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"722619c9735f29fab37c181bc9d2f6178391dc82"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigcode\/starcoder",
        "Average":37.38,
        "ARC":30.29,
        "HellaSwag":47.97,
        "MMLU":30.0,
        "TruthfulQA":41.28,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":15.52,
        "Hub":2309,
        "Available on the hub":false,
        "Model Sha":"7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1"
    },
    {
        "T":"\u2b55",
        "Model":"TheBloke\/CodeLlama-34B-Python-fp16",
        "Average":37.36,
        "ARC":38.14,
        "HellaSwag":34.8,
        "MMLU":32.95,
        "TruthfulQA":43.57,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"875f9d97fb6c9619d8867887dd1d80918ff0f593"
    },
    {
        "T":"?",
        "Model":"Deathsquad10\/TinyMix",
        "Average":37.35,
        "ARC":32.0,
        "HellaSwag":53.69,
        "MMLU":24.27,
        "TruthfulQA":39.42,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"be8089b91dc36c42ffac1101d8a386bf4e5e765d"
    },
    {
        "T":"?",
        "Model":"open-llm-leaderboard\/bloomz-1b7-4bit-alpaca-auto-eval-adapter-applied",
        "Average":37.34,
        "ARC":29.1,
        "HellaSwag":47.42,
        "MMLU":31.8,
        "TruthfulQA":41.06,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":1.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7c46d9e7aa05a8f711a93603199f9476742fe9d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/CodeLlama-34b-Python-hf",
        "Average":37.34,
        "ARC":38.05,
        "HellaSwag":34.79,
        "MMLU":32.96,
        "TruthfulQA":43.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"45f38e53a579a2b39298cc57ab04078722bebec0"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-7b-Python-hf",
        "Average":37.34,
        "ARC":29.27,
        "HellaSwag":50.12,
        "MMLU":28.37,
        "TruthfulQA":41.61,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub":49,
        "Available on the hub":true,
        "Model Sha":"ec4dd26f30674fdee00ef161b55f464ce28f9c20"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GeorgiaTechResearchInstitute\/starcoder-gpteacher-code-instruct",
        "Average":37.33,
        "ARC":32.68,
        "HellaSwag":47.6,
        "MMLU":28.63,
        "TruthfulQA":40.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":15.52,
        "Hub":72,
        "Available on the hub":true,
        "Model Sha":"d866b68daa719239dc44979dbf39a608ed6f7bce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"amu\/orpo-phi2",
        "Average":37.32,
        "ARC":31.23,
        "HellaSwag":41.52,
        "MMLU":28.9,
        "TruthfulQA":47.62,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"97dae7d50dd98ed0eeb63afcd218a11b1e172ecb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LoupGarou\/WizardCoder-Guanaco-15B-V1.0",
        "Average":37.31,
        "ARC":30.46,
        "HellaSwag":45.59,
        "MMLU":26.79,
        "TruthfulQA":46.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "apache-2.0"
        ],
        "#Params (B)":15.52,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"ab5ea678d63eb2324658dcc8cfae267eabc366ef"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"golaxy\/gogpt-3b-bloom",
        "Average":37.3,
        "ARC":31.91,
        "HellaSwag":50.32,
        "MMLU":25.2,
        "TruthfulQA":41.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"fe942d5d0faca8156eaf456ecdf569993eab8062"
    },
    {
        "T":"?",
        "Model":"M4-ai\/tau-0.5B-instruct-DPOP",
        "Average":37.3,
        "ARC":28.92,
        "HellaSwag":43.63,
        "MMLU":33.92,
        "TruthfulQA":42.73,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.46,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"30cbeff9365d6141ddebab7562b26e31409c5e51"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Walter-Falcon-1B",
        "Average":37.26,
        "ARC":31.06,
        "HellaSwag":54.92,
        "MMLU":24.58,
        "TruthfulQA":38.47,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"9cc302810282152eea488e8649e45dbc332313e3"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigcode\/starcoder",
        "Average":37.24,
        "ARC":30.29,
        "HellaSwag":47.88,
        "MMLU":29.47,
        "TruthfulQA":41.3,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":15.52,
        "Hub":2309,
        "Available on the hub":false,
        "Model Sha":"e117ab3b3d0769fd962bd48b099de711757a3d60"
    },
    {
        "T":"\u2b55",
        "Model":"AI-Sweden-Models\/gpt-sw3-1.3b-instruct",
        "Average":37.22,
        "ARC":30.97,
        "HellaSwag":51.42,
        "MMLU":26.17,
        "TruthfulQA":40.31,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":1.44,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"5f2f03167dedc59192ee02694e07424a890d9206"
    },
    {
        "T":"\u2b55",
        "Model":"Jiayi-Pan\/Tiny-Vicuna-1B",
        "Average":37.16,
        "ARC":33.45,
        "HellaSwag":55.92,
        "MMLU":25.45,
        "TruthfulQA":33.82,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"175336a0000f36b508575ef1a2da05755faf48c3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nnheui\/pythia-1.4b-sft-full",
        "Average":37.16,
        "ARC":32.68,
        "HellaSwag":52.08,
        "MMLU":25.44,
        "TruthfulQA":38.42,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.42,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5a15673e313e80a70f5d71396e612a8088bde650"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Deathsquad10\/TinyLlama-Remix",
        "Average":37.13,
        "ARC":31.14,
        "HellaSwag":49.5,
        "MMLU":27.34,
        "TruthfulQA":40.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e5ba81a66f14d23a72053b2d6bdcd31c111d81ac"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"PY007\/TinyLlama-1.1B-intermediate-step-480k-1T",
        "Average":37.1,
        "ARC":30.89,
        "HellaSwag":52.97,
        "MMLU":25.0,
        "TruthfulQA":39.55,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":27,
        "Available on the hub":true,
        "Model Sha":"098830e58452a0a08f90eb0189ec5925803fd48b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-1.3b",
        "Average":37.09,
        "ARC":31.14,
        "HellaSwag":51.43,
        "MMLU":26.55,
        "TruthfulQA":39.24,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"34b668ff0acfe56f2d541aa46b385557ee39eb3f"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-13b-Python-hf",
        "Average":37.09,
        "ARC":32.59,
        "HellaSwag":43.94,
        "MMLU":27.23,
        "TruthfulQA":44.59,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"ea1b775799b477fe22e64f8ac9107f28950b5c87"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"croissantllm\/CroissantLLMBase",
        "Average":37.04,
        "ARC":31.57,
        "HellaSwag":54.03,
        "MMLU":25.07,
        "TruthfulQA":37.49,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":1.28,
        "Hub":25,
        "Available on the hub":true,
        "Model Sha":"b22c20207a5a1b2b4bb3f2b511096c1f0cc95b81"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OEvortex\/HelpingAI-Lite-1.5T",
        "Average":37.04,
        "ARC":31.23,
        "HellaSwag":52.39,
        "MMLU":25.93,
        "TruthfulQA":38.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.1,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"25333435dd8d5605ac4f3d6bc9cd1cb886f15b77"
    },
    {
        "T":"?",
        "Model":"TinyLlama\/TinyLlama-1.1B-Chat-v0.6",
        "Average":37.04,
        "ARC":31.66,
        "HellaSwag":55.79,
        "MMLU":25.98,
        "TruthfulQA":34.72,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":67,
        "Available on the hub":true,
        "Model Sha":"bf9ae1c8bf026667e6f810768de259bb4a7f4777"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hakurei\/lotus-12B",
        "Average":37.02,
        "ARC":30.72,
        "HellaSwag":52.71,
        "MMLU":24.55,
        "TruthfulQA":40.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":11.59,
        "Hub":26,
        "Available on the hub":true,
        "Model Sha":"f212b695aabf5dafb5dccf5013ddb765ba1e47d7"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"NYTK\/PULI-GPTrio",
        "Average":36.99,
        "ARC":30.72,
        "HellaSwag":53.49,
        "MMLU":24.73,
        "TruthfulQA":39.03,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.06,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"c85efce322a0f6d93d64f7b9096525753da6913e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"croissantllm\/CroissantLLMBase",
        "Average":36.98,
        "ARC":30.63,
        "HellaSwag":54.18,
        "MMLU":25.72,
        "TruthfulQA":37.39,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.28,
        "Hub":25,
        "Available on the hub":true,
        "Model Sha":"b22c20207a5a1b2b4bb3f2b511096c1f0cc95b81"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/TinyLlama-3T-Cinder-v1.1",
        "Average":36.94,
        "ARC":34.04,
        "HellaSwag":50.4,
        "MMLU":25.75,
        "TruthfulQA":37.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.1,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"1f1bc965140150b7c7a5012abe1e0e0fcce93d68"
    },
    {
        "T":"?",
        "Model":"facebook\/opt-1.3b",
        "Average":36.93,
        "ARC":29.52,
        "HellaSwag":54.53,
        "MMLU":24.96,
        "TruthfulQA":38.71,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.32,
        "Hub":112,
        "Available on the hub":true,
        "Model Sha":"8c7b10754972749675d22364c25c428b29face51"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/RWKV-pileplus-1B5-evol_instruct_v2",
        "Average":36.92,
        "ARC":31.83,
        "HellaSwag":55.51,
        "MMLU":25.13,
        "TruthfulQA":35.21,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.41,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"cb5582403f78f09973291980da56d7636516545a"
    },
    {
        "T":"?",
        "Model":"TinyLlama\/TinyLlama-1.1B-intermediate-step-955k-token-2T",
        "Average":36.92,
        "ARC":30.29,
        "HellaSwag":54.84,
        "MMLU":26.47,
        "TruthfulQA":36.07,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":28,
        "Available on the hub":true,
        "Model Sha":"f62ecb34ea0d4acea9d896040a4616a9538e2f36"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-13b-Python-hf",
        "Average":36.91,
        "ARC":33.19,
        "HellaSwag":44.5,
        "MMLU":25.94,
        "TruthfulQA":43.99,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"ea1b775799b477fe22e64f8ac9107f28950b5c87"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/CodeLlama-13B-Python-fp16",
        "Average":36.91,
        "ARC":33.19,
        "HellaSwag":44.5,
        "MMLU":25.94,
        "TruthfulQA":43.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":25,
        "Available on the hub":true,
        "Model Sha":"442282f4207442b828953a72c51a919c332cba5c"
    },
    {
        "T":"?",
        "Model":"habanoz\/TinyLlama-1.1B-step-2T-lr-5-5ep-oasst1-top1-instruct-V1",
        "Average":36.89,
        "ARC":31.06,
        "HellaSwag":55.02,
        "MMLU":26.41,
        "TruthfulQA":35.08,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"586c223b539e05fd8a63733c6a540f292460e639"
    },
    {
        "T":"?",
        "Model":"jeff31415\/TinyLlama-1.1B-1T-OpenOrca",
        "Average":36.88,
        "ARC":31.31,
        "HellaSwag":52.34,
        "MMLU":25.31,
        "TruthfulQA":38.58,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"605c2a8b2324a25ca0513c4c862bfa9c937b3514"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bn22\/tinyllama_frankenmerge",
        "Average":36.87,
        "ARC":30.2,
        "HellaSwag":51.01,
        "MMLU":26.11,
        "TruthfulQA":40.18,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.54,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"086cd453c6d72be4960b6ff15fa5c97dc47993cc"
    },
    {
        "T":"\u2b55",
        "Model":"RWKV\/rwkv-raven-1b5",
        "Average":36.87,
        "ARC":31.83,
        "HellaSwag":52.6,
        "MMLU":25.96,
        "TruthfulQA":37.09,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.41,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"571a3bd891ce33f2ee3fc6de09218178edb0dae2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"winglian\/llama-2-4b",
        "Average":36.86,
        "ARC":31.23,
        "HellaSwag":53.29,
        "MMLU":24.22,
        "TruthfulQA":38.72,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":3.37,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"fbba77f9894cf738ad8d7d08fc6874856fb42507"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/pygmalion-2.7b",
        "Average":36.84,
        "ARC":32.76,
        "HellaSwag":54.13,
        "MMLU":23.28,
        "TruthfulQA":37.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":2.65,
        "Hub":46,
        "Available on the hub":true,
        "Model Sha":"9533805293bc48e8ddfe9dc1940d8cbc5662113e"
    },
    {
        "T":"?",
        "Model":"bigscience\/bloom-1b7",
        "Average":36.75,
        "ARC":30.63,
        "HellaSwag":47.6,
        "MMLU":27.48,
        "TruthfulQA":41.31,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":1.72,
        "Hub":98,
        "Available on the hub":true,
        "Model Sha":"cc72a88036c2fb937d65efeacc57a0c2ef5d6fe5"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AI-Sweden-Models\/gpt-sw3-1.3b",
        "Average":36.72,
        "ARC":30.38,
        "HellaSwag":50.4,
        "MMLU":26.14,
        "TruthfulQA":39.97,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.44,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"b0d9545a27cfaf9a937adac72ed6953f2dc597de"
    },
    {
        "T":"\u2b55",
        "Model":"0x7194633\/fialka-13B-v3",
        "Average":36.69,
        "ARC":30.97,
        "HellaSwag":48.83,
        "MMLU":26.36,
        "TruthfulQA":40.58,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"81bde04594320c0e8174644be352a98c7b073a88"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"gpt2-xl",
        "Average":36.68,
        "ARC":30.29,
        "HellaSwag":51.36,
        "MMLU":26.54,
        "TruthfulQA":38.54,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.56,
        "Hub":182,
        "Available on the hub":true,
        "Model Sha":"33cdb5c0db5423c1879b1b9f16c352988e8754a8"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2-xl_lima",
        "Average":36.65,
        "ARC":31.14,
        "HellaSwag":51.28,
        "MMLU":25.43,
        "TruthfulQA":38.74,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.56,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f7db5b1db521abd7578b95138e737637e0037ca5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FINGU-AI\/FinguAI-Chat-v1",
        "Average":36.61,
        "ARC":29.18,
        "HellaSwag":44.08,
        "MMLU":30.39,
        "TruthfulQA":42.79,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.46,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"3557829049749742bdb0bfaf23de2d07ecf928f2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LoupGarou\/WizardCoder-Guanaco-15B-V1.1",
        "Average":36.56,
        "ARC":32.59,
        "HellaSwag":45.42,
        "MMLU":25.88,
        "TruthfulQA":42.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "apache-2.0"
        ],
        "#Params (B)":15.52,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"979531c84ec0b4e1712d6a5cec6907126a21e605"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"habanoz\/TinyLlama-1.1B-2T-lr-2e-4-3ep-dolly-15k-instruct-v1",
        "Average":36.54,
        "ARC":30.55,
        "HellaSwag":53.7,
        "MMLU":26.07,
        "TruthfulQA":35.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"152436a0dd6ca1603b3993bbf08a227ea131f85d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt1.3b_10e5",
        "Average":36.53,
        "ARC":29.52,
        "HellaSwag":52.81,
        "MMLU":25.61,
        "TruthfulQA":38.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":1.3,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"58d5814c99392194b9d7a5ef7c2c4023eb75934e"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt-2-xl_camel-ai-physics",
        "Average":36.51,
        "ARC":29.52,
        "HellaSwag":50.62,
        "MMLU":26.79,
        "TruthfulQA":39.12,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.56,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e20cf5a8c89441f4dc15fd2af12dbe72b7df8e60"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"L-R\/LLmRa-1.3B_V2",
        "Average":36.5,
        "ARC":30.46,
        "HellaSwag":53.03,
        "MMLU":26.06,
        "TruthfulQA":36.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a760ebda8f736988eafea879173c5be468ea68d0"
    },
    {
        "T":"?",
        "Model":"0x7194633\/fialka-13B-v4",
        "Average":36.45,
        "ARC":29.69,
        "HellaSwag":47.37,
        "MMLU":25.09,
        "TruthfulQA":43.65,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"ca8208bc78cdce0be47f8726926b242961fd0c07"
    },
    {
        "T":"\u2b55",
        "Model":"0x7194633\/fialka-13B-v3.1",
        "Average":36.42,
        "ARC":29.95,
        "HellaSwag":47.28,
        "MMLU":25.41,
        "TruthfulQA":43.03,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"5d7ce7a375b6641a133485c47542d522d7096f2e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"RWKV\/rwkv-4-1b5-pile",
        "Average":36.41,
        "ARC":31.83,
        "HellaSwag":52.25,
        "MMLU":25.77,
        "TruthfulQA":35.8,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.41,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"643585471eaf5821d94dfcb498ab5b94a36b42cf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BEE-spoke-data\/TinyLlama-1.1bee",
        "Average":36.4,
        "ARC":30.55,
        "HellaSwag":51.8,
        "MMLU":24.25,
        "TruthfulQA":39.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"5889ec467cf80a83c4092b55686f8121e81bf001"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/CodeLlama-34b-hf",
        "Average":36.37,
        "ARC":37.54,
        "HellaSwag":31.84,
        "MMLU":37.2,
        "TruthfulQA":38.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":33.48,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"4e61ec70eb258047f5bc689fa6a66f7753da52b8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"jylee420\/gemma-2b-data-std",
        "Average":36.36,
        "ARC":37.54,
        "HellaSwag":32.49,
        "MMLU":35.82,
        "TruthfulQA":39.56,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.51,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"8514b865649969a5e1acdbff5d098694269c69ab"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"PY007\/TinyLlama-1.1B-intermediate-step-240k-503b",
        "Average":36.35,
        "ARC":29.27,
        "HellaSwag":49.71,
        "MMLU":26.26,
        "TruthfulQA":40.17,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":18,
        "Available on the hub":true,
        "Model Sha":"213ebf60d7fdd3258fa5574840b06c97a7e8cf5d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"rinna\/bilingual-gpt-neox-4b-8k",
        "Average":36.34,
        "ARC":28.58,
        "HellaSwag":43.94,
        "MMLU":25.38,
        "TruthfulQA":47.48,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":3.95,
        "Hub":22,
        "Available on the hub":true,
        "Model Sha":"ad56d7fc86db4ad5a7036bc9f80e11cd6f435a60"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lxe\/Cerebras-GPT-2.7B-Alpaca-SP",
        "Average":36.26,
        "ARC":30.8,
        "HellaSwag":48.88,
        "MMLU":25.12,
        "TruthfulQA":40.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"ae7f22e90cb968b0a73355aa2001d6bc7df28477"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cerebras\/Cerebras-GPT-2.7B",
        "Average":36.23,
        "ARC":29.1,
        "HellaSwag":49.29,
        "MMLU":25.17,
        "TruthfulQA":41.37,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub":41,
        "Available on the hub":true,
        "Model Sha":"4383dfd80aafdbcfd0876419d246de51e6cbf7c1"
    },
    {
        "T":"?",
        "Model":"ToastyPigeon\/SmolLlama-1.5B-Sorted",
        "Average":36.23,
        "ARC":31.91,
        "HellaSwag":56.39,
        "MMLU":24.48,
        "TruthfulQA":32.11,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.54,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6e3ee804d739faa38cb008f5cbdc94670e5f3191"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"postbot\/gpt-neo-1.3B-emailgen",
        "Average":36.14,
        "ARC":29.95,
        "HellaSwag":47.95,
        "MMLU":24.11,
        "TruthfulQA":42.55,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.32,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"accdf0e43c0d1b313bc6d1fb307d67f1921ef3ca"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jzjiao\/opt-1.3b-rlhf",
        "Average":36.13,
        "ARC":28.92,
        "HellaSwag":52.77,
        "MMLU":25.39,
        "TruthfulQA":37.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5b12df71b21b6b7d76ca9d56de6751f25022e854"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/TinyLlama-3T-Cinder-v1",
        "Average":36.06,
        "ARC":33.53,
        "HellaSwag":46.36,
        "MMLU":26.03,
        "TruthfulQA":38.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.1,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"52ccb7253aaa88f675ff117917d541ec7e49d56d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/gpt-neo-1.3B",
        "Average":36.04,
        "ARC":31.23,
        "HellaSwag":48.47,
        "MMLU":24.82,
        "TruthfulQA":39.63,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.37,
        "Hub":201,
        "Available on the hub":true,
        "Model Sha":"8282180b53cba30a1575e49de1530019e5931739"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v1-1_5b",
        "Average":36.01,
        "ARC":31.66,
        "HellaSwag":49.69,
        "MMLU":25.62,
        "TruthfulQA":37.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.56,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"4ac21faec255e3544e96aeb3591c27bdee5ebf45"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"RaoFoundation\/774M-03_09_2024",
        "Average":35.99,
        "ARC":30.29,
        "HellaSwag":53.88,
        "MMLU":25.33,
        "TruthfulQA":34.44,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.77,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"27d2412db12d6bedf3b3f26ffa4045a6ba7d0e48"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Corianas\/Quokka_2.7b",
        "Average":35.93,
        "ARC":31.06,
        "HellaSwag":47.72,
        "MMLU":24.8,
        "TruthfulQA":40.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.79,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"abe5e0f574d32f3234035b6e8c5d68bbb201e03c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MrNJK\/gpt2-xl-sft",
        "Average":35.89,
        "ARC":30.03,
        "HellaSwag":49.17,
        "MMLU":25.56,
        "TruthfulQA":38.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.56,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"53250831436460254b7ee9afc4014d4d3156b372"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/galactica-1.3b",
        "Average":35.88,
        "ARC":34.13,
        "HellaSwag":40.91,
        "MMLU":27.09,
        "TruthfulQA":41.4,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.32,
        "Hub":50,
        "Available on the hub":true,
        "Model Sha":"f711c69357d598defb703ddce93c5d7f7bc6e6da"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_opt1.3b_10e5",
        "Average":35.85,
        "ARC":29.44,
        "HellaSwag":51.7,
        "MMLU":25.38,
        "TruthfulQA":36.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":1.3,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"9aeaed5981224761a1cf0840da1761948881f8cb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/RWKV-4-PilePlus-1B5-20230520-2942-486Gtokens-ctx4096",
        "Average":35.81,
        "ARC":30.63,
        "HellaSwag":52.63,
        "MMLU":25.04,
        "TruthfulQA":34.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.41,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"657e40fe890c2baa1705b45084a93a70b98842eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v2-774m",
        "Average":35.79,
        "ARC":30.12,
        "HellaSwag":47.68,
        "MMLU":25.37,
        "TruthfulQA":40.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"0ea894a33e491912cd1a65dde47b4af03f03c4f2"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigcode\/starcoderbase-7b",
        "Average":35.66,
        "ARC":29.86,
        "HellaSwag":43.87,
        "MMLU":28.45,
        "TruthfulQA":40.46,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":7.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"4ab631381edb607557cbb04b6e9a225bad16807c"
    },
    {
        "T":"?",
        "Model":"l3utterfly\/llama2-3b-distilled-layla-v1",
        "Average":35.64,
        "ARC":30.46,
        "HellaSwag":46.05,
        "MMLU":23.91,
        "TruthfulQA":42.14,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a1ba0a65e5262bc134dbc562a9faf80865b0a72f"
    },
    {
        "T":"?",
        "Model":"sail\/Sailor-0.5B-Chat",
        "Average":35.62,
        "ARC":30.38,
        "HellaSwag":45.51,
        "MMLU":26.73,
        "TruthfulQA":39.85,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.62,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"3d72ea8def9a4aa40d7536f5d568fcc4a16218b2"
    },
    {
        "T":"\u2b55",
        "Model":"rinna\/bilingual-gpt-neox-4b-instruction-sft",
        "Average":35.61,
        "ARC":28.07,
        "HellaSwag":47.5,
        "MMLU":23.12,
        "TruthfulQA":43.76,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":3.8,
        "Hub":18,
        "Available on the hub":true,
        "Model Sha":"c20e42bd49a3b1b0d0a07151899a322c4760e871"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wandb\/pruned_mistral",
        "Average":35.59,
        "ARC":28.33,
        "HellaSwag":46.35,
        "MMLU":26.62,
        "TruthfulQA":41.09,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.88,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"dc597f8e6661e96ed0c50056c38eefb857315112"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gmonsoon\/Qwenchana-0.5B-restart",
        "Average":35.56,
        "ARC":30.46,
        "HellaSwag":45.89,
        "MMLU":25.39,
        "TruthfulQA":40.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.5,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4793a45028bdf35b26438799eb8090a3077beba6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gmonsoon\/Qwenchana-0.5B-restart",
        "Average":35.52,
        "ARC":30.03,
        "HellaSwag":45.95,
        "MMLU":25.61,
        "TruthfulQA":40.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.5,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4793a45028bdf35b26438799eb8090a3077beba6"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-1b-deduped",
        "Average":35.49,
        "ARC":29.1,
        "HellaSwag":49.65,
        "MMLU":24.27,
        "TruthfulQA":38.94,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.08,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"7199d8fc61a6d565cd1f3c62bf11525b563e13b2"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"sail\/Sailor-0.5B",
        "Average":35.47,
        "ARC":29.69,
        "HellaSwag":45.82,
        "MMLU":25.62,
        "TruthfulQA":40.76,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.62,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"69e509f56254ae8bba6fdb9d2e35b9df03b96b7b"
    },
    {
        "T":"?",
        "Model":"health360\/Healix-1.1B-V1-Chat-dDPO",
        "Average":35.38,
        "ARC":30.55,
        "HellaSwag":44.78,
        "MMLU":24.64,
        "TruthfulQA":41.55,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"07dd0532fda09df289f6617e1135b09fb705080d"
    },
    {
        "T":"?",
        "Model":"kalisai\/Nusantara-0.8b-Indo-Chat",
        "Average":35.35,
        "ARC":30.38,
        "HellaSwag":44.61,
        "MMLU":26.89,
        "TruthfulQA":39.54,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.82,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"0d5d402467e6d72883bc1d64695a450497d5925c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sail\/Sailor-0.5B",
        "Average":35.35,
        "ARC":29.69,
        "HellaSwag":45.82,
        "MMLU":25.13,
        "TruthfulQA":40.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.62,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"69e509f56254ae8bba6fdb9d2e35b9df03b96b7b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Kunhao\/pile-7b-250b-tokens",
        "Average":35.32,
        "ARC":29.27,
        "HellaSwag":46.29,
        "MMLU":25.25,
        "TruthfulQA":40.49,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":5.87,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"caefdf7a7c177905b0b16fbe9d4c7ba08def97c2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DatPySci\/pythia-1b-sft-50k",
        "Average":35.3,
        "ARC":30.29,
        "HellaSwag":49.21,
        "MMLU":24.64,
        "TruthfulQA":37.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0fc973e8a9960f21d057681be7d2af7c8c10f43d"
    },
    {
        "T":"?",
        "Model":"DatPySci\/pythia-1b-spin-iter1",
        "Average":35.29,
        "ARC":30.55,
        "HellaSwag":49.26,
        "MMLU":24.46,
        "TruthfulQA":36.89,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4c80730b5c7fea5e02941c1845f172dc1f022623"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"w601sxs\/b1ade-1b",
        "Average":35.28,
        "ARC":28.58,
        "HellaSwag":46.08,
        "MMLU":25.11,
        "TruthfulQA":41.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.91,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b4b0fd71589e6590089e1ec14a840ecab10894ae"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"rinna\/bilingual-gpt-neox-4b",
        "Average":35.25,
        "ARC":29.18,
        "HellaSwag":43.73,
        "MMLU":23.1,
        "TruthfulQA":45.0,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":3.95,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"f02f6f3c8da0093f3c1ce59220409bc2fa9fbb17"
    },
    {
        "T":"\u2b55",
        "Model":"deepseek-ai\/deepseek-coder-1.3b-instruct",
        "Average":35.24,
        "ARC":28.58,
        "HellaSwag":39.87,
        "MMLU":28.47,
        "TruthfulQA":44.02,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.35,
        "Hub":73,
        "Available on the hub":true,
        "Model Sha":"e04e04028d6345ab3225644cd615e2573ffb9b8c"
    },
    {
        "T":"?",
        "Model":"DatPySci\/pythia-1b-dpo",
        "Average":35.2,
        "ARC":30.12,
        "HellaSwag":49.24,
        "MMLU":24.24,
        "TruthfulQA":37.2,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"65412d0e910fadcb54513624759417f8f805f75e"
    },
    {
        "T":"?",
        "Model":"shaohang\/SparseOPT-1.3B",
        "Average":35.13,
        "ARC":27.13,
        "HellaSwag":48.69,
        "MMLU":25.6,
        "TruthfulQA":39.11,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"06249d582b0cfefac537dd6bee2e578002ffff00"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shaohang\/Sparse0.5_OPT-1.3",
        "Average":35.13,
        "ARC":27.13,
        "HellaSwag":48.69,
        "MMLU":25.6,
        "TruthfulQA":39.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"06249d582b0cfefac537dd6bee2e578002ffff00"
    },
    {
        "T":"\u2b55",
        "Model":"nicholasKluge\/Aira-1B5",
        "Average":35.12,
        "ARC":28.92,
        "HellaSwag":43.11,
        "MMLU":27.29,
        "TruthfulQA":41.16,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"4bca81e6a8fbe73956b9e3cda47fb017fd147973"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DatPySci\/pythia-1b-sft-50k",
        "Average":35.04,
        "ARC":30.03,
        "HellaSwag":49.1,
        "MMLU":24.03,
        "TruthfulQA":37.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0fc973e8a9960f21d057681be7d2af7c8c10f43d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/polyglot-ko-12.8b",
        "Average":35.01,
        "ARC":27.05,
        "HellaSwag":51.68,
        "MMLU":26.64,
        "TruthfulQA":34.69,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.06,
        "Hub":55,
        "Available on the hub":true,
        "Model Sha":"09dfc839067bf44e7f52976eca8adbc17f04e1b0"
    },
    {
        "T":"\u2b55",
        "Model":"llm-jp\/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0",
        "Average":34.99,
        "ARC":26.88,
        "HellaSwag":44.78,
        "MMLU":23.12,
        "TruthfulQA":45.19,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.86,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"68282fe744c69ea2e4420a4a6833c0b9168215eb"
    },
    {
        "T":"?",
        "Model":"DatPySci\/pythia-1b-dpo-full",
        "Average":34.97,
        "ARC":29.44,
        "HellaSwag":49.03,
        "MMLU":24.13,
        "TruthfulQA":37.27,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"fe1ac6dd06014f44404f2007103414b21d5dc2f5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DatPySci\/pythia-1b-kto-iter0",
        "Average":34.97,
        "ARC":30.12,
        "HellaSwag":48.95,
        "MMLU":24.39,
        "TruthfulQA":36.4,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.01,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a188b55a7680083715f5adde3994cf5620e0a978"
    },
    {
        "T":"\u2b55",
        "Model":"llm-jp\/llm-jp-13b-instruct-full-jaster-v1.0",
        "Average":34.93,
        "ARC":27.22,
        "HellaSwag":44.7,
        "MMLU":23.12,
        "TruthfulQA":44.69,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.86,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"b44eac954eac7ddbceba4f510325fd710c977eab"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DatPySci\/pythia-1b-self-kto-iter0",
        "Average":34.93,
        "ARC":30.2,
        "HellaSwag":49.06,
        "MMLU":24.11,
        "TruthfulQA":36.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.01,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"47f394e1df169f2264859757f6a092de4172c15e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Salesforce\/codegen-6B-multi",
        "Average":34.92,
        "ARC":27.22,
        "HellaSwag":41.11,
        "MMLU":25.71,
        "TruthfulQA":45.65,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bsd-3-clause",
        "#Params (B)":6.85,
        "Hub":18,
        "Available on the hub":true,
        "Model Sha":"2d58b1e73791e8f0be7ea59c2720dccb6f4d0f06"
    },
    {
        "T":"?",
        "Model":"bigscience\/bloom-1b1",
        "Average":34.9,
        "ARC":28.33,
        "HellaSwag":42.78,
        "MMLU":26.7,
        "TruthfulQA":41.8,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":1.06,
        "Hub":40,
        "Available on the hub":true,
        "Model Sha":"6f4195539db0eef1c9d010289f32e0645d9a2354"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LordNoah\/Alpaca_spin_tuned_gpt2_large",
        "Average":34.88,
        "ARC":27.9,
        "HellaSwag":45.12,
        "MMLU":27.08,
        "TruthfulQA":39.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"044c10b7d54fbf685e0cd0ac958b6d8cad67f18d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DatPySci\/pythia-1b-sft-full",
        "Average":34.87,
        "ARC":29.52,
        "HellaSwag":48.91,
        "MMLU":23.95,
        "TruthfulQA":37.08,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"83ef084c876204aa4e3f5f33e23056f551fc58cf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LordNoah\/Alpaca_spin_gpt2_e1_se0",
        "Average":34.86,
        "ARC":27.99,
        "HellaSwag":45.74,
        "MMLU":26.68,
        "TruthfulQA":39.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2fb0e1fbba1275c78915cbe6c293c7ed67af9245"
    },
    {
        "T":"\u2b55",
        "Model":"xaviviro\/FLOR-1.3B-xat",
        "Average":34.86,
        "ARC":26.79,
        "HellaSwag":41.63,
        "MMLU":26.65,
        "TruthfulQA":44.38,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"35cdda0d2b7ade43fd39f3fb4ffad25f0c2730ea"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TurkuNLP\/gpt3-finnish-13B",
        "Average":34.84,
        "ARC":24.66,
        "HellaSwag":46.76,
        "MMLU":23.49,
        "TruthfulQA":44.47,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.26,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"ade35fd78ac2c29f7a56ffd3087321d297bb97a9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LordNoah\/Alpaca_refine_gpt2_e0_se1",
        "Average":34.83,
        "ARC":29.18,
        "HellaSwag":45.35,
        "MMLU":26.91,
        "TruthfulQA":37.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"fbb41cbdfc6662cfff26e0aec950df6e8d9dc8c2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LordNoah\/Alpaca_spin_gpt2_e0_se1",
        "Average":34.79,
        "ARC":27.99,
        "HellaSwag":45.84,
        "MMLU":26.44,
        "TruthfulQA":38.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"60f08e79339d6c6c02521fd8d3cd5fc16a0fd108"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TFLai\/gpt-neo-1.3B-4bit-alpaca",
        "Average":34.76,
        "ARC":28.24,
        "HellaSwag":46.35,
        "MMLU":25.19,
        "TruthfulQA":39.26,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":1.3,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"137d483d1dc757c81c59bd190016f7c5df01f978"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"daekeun-ml\/phi-2-ko-v0.1",
        "Average":34.74,
        "ARC":30.72,
        "HellaSwag":37.26,
        "MMLU":27.34,
        "TruthfulQA":43.64,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":2.86,
        "Hub":18,
        "Available on the hub":false,
        "Model Sha":"5c366413d429ce7c72ca34fac94db6651d9e8b80"
    },
    {
        "T":"?",
        "Model":"FabbriSimo01\/Bloom_1b_Quantized",
        "Average":34.67,
        "ARC":27.73,
        "HellaSwag":42.83,
        "MMLU":26.28,
        "TruthfulQA":41.82,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":1.06,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f31188966c6735bd894edacfee8371a6eaf7dbc7"
    },
    {
        "T":"?",
        "Model":"Aryanne\/TinyllamaMix-1.1B",
        "Average":34.59,
        "ARC":31.48,
        "HellaSwag":48.39,
        "MMLU":25.05,
        "TruthfulQA":33.45,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"0a00b7bb4f046e98cc833f5303522afc057e1058"
    },
    {
        "T":"?",
        "Model":"MBZUAI\/LaMini-GPT-774M",
        "Average":34.5,
        "ARC":27.65,
        "HellaSwag":43.81,
        "MMLU":26.3,
        "TruthfulQA":40.26,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.77,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"4f3bd4b37d249e6aa335be677afd39f417e05b5d"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Locutusque\/gpt2-large-conversational",
        "Average":34.47,
        "ARC":26.96,
        "HellaSwag":44.98,
        "MMLU":26.33,
        "TruthfulQA":39.6,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"openrail",
        "#Params (B)":0.77,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"6674ad1ed9f518054561b866172eb88b7a769413"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hyunjae\/polyglot-ko-3.8b-total",
        "Average":34.47,
        "ARC":25.34,
        "HellaSwag":39.69,
        "MMLU":29.16,
        "TruthfulQA":43.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":3.72,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"658a043415467ca5286f3348493db10aa8b94f2c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sharathhebbar24\/SSH_355M",
        "Average":34.42,
        "ARC":26.96,
        "HellaSwag":38.98,
        "MMLU":27.59,
        "TruthfulQA":44.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.36,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"11bdb293dc0bfd2afc406fc26c765aac7f06cbb7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LordNoah\/latent_gpt2_medium_alpaca_e3",
        "Average":34.41,
        "ARC":28.75,
        "HellaSwag":37.78,
        "MMLU":26.35,
        "TruthfulQA":44.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.36,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f8eee4693c4efb77cc3694484ca8af68e64938a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LordNoah\/Alpaca_refine_tuned_gpt2_large",
        "Average":34.37,
        "ARC":27.56,
        "HellaSwag":45.09,
        "MMLU":26.91,
        "TruthfulQA":37.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"312b55480d2c551b92edc66054d3bb7acf96876f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Devio\/test-3b",
        "Average":34.35,
        "ARC":27.65,
        "HellaSwag":44.79,
        "MMLU":23.53,
        "TruthfulQA":41.42,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":3.5,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b81c038ee2fa2addd285acde08b1a7ca3cb2854d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fhai50032\/Mistral-4B-FT-2",
        "Average":34.34,
        "ARC":25.94,
        "HellaSwag":39.63,
        "MMLU":25.46,
        "TruthfulQA":46.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.75,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"8784a1ae1bd0f5f986ab0dd2cd27b514c7698251"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"stabilityai\/stablelm-tuned-alpha-3b",
        "Average":34.32,
        "ARC":27.82,
        "HellaSwag":44.06,
        "MMLU":23.08,
        "TruthfulQA":42.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "cc-by-nc-sa-4.0"
        ],
        "#Params (B)":3.43,
        "Hub":104,
        "Available on the hub":true,
        "Model Sha":"d1c03d2114451d562416b9efe4281d319ceff99e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BarraHome\/PequeLLaMa-1B-Instruct-v0.1-16bit",
        "Average":34.21,
        "ARC":27.99,
        "HellaSwag":43.03,
        "MMLU":24.73,
        "TruthfulQA":41.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3ab3fcbf9b4a057c38bb4e50290e23a0fb23e049"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/pygmalion-1.3b",
        "Average":34.2,
        "ARC":28.07,
        "HellaSwag":46.96,
        "MMLU":24.12,
        "TruthfulQA":37.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":1.52,
        "Hub":50,
        "Available on the hub":true,
        "Model Sha":"bef2c90128c00ff6f16c0f397463423b7d988e17"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/fairseq-dense-355M",
        "Average":34.15,
        "ARC":25.43,
        "HellaSwag":46.67,
        "MMLU":25.3,
        "TruthfulQA":39.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.4,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"24da1ea670f0638c2df911596e95c764bcd5fb44"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sharathhebbar24\/SSH_300M",
        "Average":34.13,
        "ARC":28.24,
        "HellaSwag":38.74,
        "MMLU":27.03,
        "TruthfulQA":42.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.36,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d531d193cfb1e645e8afb89203983450b6655967"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LordNoah\/Alpaca_refine_gpt2_e1_se0",
        "Average":34.12,
        "ARC":27.3,
        "HellaSwag":45.39,
        "MMLU":26.51,
        "TruthfulQA":37.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d1c9d3e02d5eed70032df54898ea11e51a7b41b2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LordNoah\/spin_gpt2_medium_alpaca_e2",
        "Average":34.12,
        "ARC":28.07,
        "HellaSwag":39.88,
        "MMLU":26.99,
        "TruthfulQA":41.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.36,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f269152bdb88c649e38afa72677cc810cdd46c07"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mohammed-Altaf\/Medical-ChatBot",
        "Average":34.1,
        "ARC":30.55,
        "HellaSwag":38.63,
        "MMLU":25.98,
        "TruthfulQA":41.25,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"mit",
        "#Params (B)":2.59,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"9e2d5d7a6189762164690a2fe714b00ce497b253"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Rachneet\/gpt2-xl-alpaca",
        "Average":34.09,
        "ARC":26.79,
        "HellaSwag":43.85,
        "MMLU":26.31,
        "TruthfulQA":39.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.56,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a1a19acc0ef161bfa35f460c15ed3015595714d8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"gpt2-large",
        "Average":34.08,
        "ARC":25.94,
        "HellaSwag":45.6,
        "MMLU":26.08,
        "TruthfulQA":38.71,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.81,
        "Hub":158,
        "Available on the hub":true,
        "Model Sha":"97935fc1a406f447320c3db70fe9e9875dca2595"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"PY007\/TinyLlama-1.1B-step-50K-105b",
        "Average":34.06,
        "ARC":25.85,
        "HellaSwag":44.1,
        "MMLU":26.78,
        "TruthfulQA":39.51,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":95,
        "Available on the hub":true,
        "Model Sha":"c1f1ef67c12e4bb85fe0bdf1747c645a202cc118"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LordNoah\/Alpaca-tuned-gpt2",
        "Average":34.05,
        "ARC":26.54,
        "HellaSwag":44.79,
        "MMLU":27.22,
        "TruthfulQA":37.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d34098965369d0ddb41c44d19671429440490859"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"openai-community\/gpt2-large",
        "Average":34.04,
        "ARC":25.77,
        "HellaSwag":45.62,
        "MMLU":26.07,
        "TruthfulQA":38.72,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.81,
        "Hub":214,
        "Available on the hub":true,
        "Model Sha":"32b71b12589c2f8d625668d2335a01cac3249519"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mohammed-Altaf\/Medical-ChatBot",
        "Average":34.02,
        "ARC":30.46,
        "HellaSwag":38.6,
        "MMLU":25.96,
        "TruthfulQA":41.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.59,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"9e2d5d7a6189762164690a2fe714b00ce497b253"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt1.3b_10e6",
        "Average":34.02,
        "ARC":25.77,
        "HellaSwag":41.67,
        "MMLU":25.9,
        "TruthfulQA":42.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":1.3,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"d467863c8401c4ccb740b7c05fc8d5d6bbed0e0c"
    },
    {
        "T":"\u2b55",
        "Model":"nicholasKluge\/Aira-2-774M",
        "Average":34.0,
        "ARC":28.75,
        "HellaSwag":40.8,
        "MMLU":25.1,
        "TruthfulQA":41.33,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"f43044cfe7bf0827a176f0d319c63251c2b29373"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mohammed-Altaf\/Medical-ChatBot",
        "Average":33.99,
        "ARC":30.46,
        "HellaSwag":38.55,
        "MMLU":25.91,
        "TruthfulQA":41.02,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":2.59,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"9e2d5d7a6189762164690a2fe714b00ce497b253"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AIGym\/deepseek-coder-1.3b-chat-and-function-calling",
        "Average":33.96,
        "ARC":26.28,
        "HellaSwag":39.27,
        "MMLU":26.92,
        "TruthfulQA":43.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.35,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c0a37346bb1c3fac3b345106b3b691f3460e445e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mikivis\/gpt2-large-lora-sft",
        "Average":33.96,
        "ARC":26.79,
        "HellaSwag":44.15,
        "MMLU":25.82,
        "TruthfulQA":39.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.77,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1c0c5a686f3c83692e033416197155557e4d3a0d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sharathhebbar24\/Instruct_GPT_v1",
        "Average":33.96,
        "ARC":28.07,
        "HellaSwag":38.98,
        "MMLU":26.55,
        "TruthfulQA":42.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.36,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1eea8e13be8b2616cc4a4bedb796f61ea894751c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LordNoah\/spin_gpt2_medium_alpaca_e3",
        "Average":33.94,
        "ARC":27.82,
        "HellaSwag":38.82,
        "MMLU":26.92,
        "TruthfulQA":42.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.36,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3e1df48210c5ef275174a7dc0d7f27e3436a90d5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AIGym\/deepseek-coder-1.3b-chat",
        "Average":33.93,
        "ARC":25.85,
        "HellaSwag":39.59,
        "MMLU":26.36,
        "TruthfulQA":43.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.35,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"1e167cd95fc142008b7ea37a1d59a12f972b8c96"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-410m",
        "Average":33.88,
        "ARC":26.19,
        "HellaSwag":40.85,
        "MMLU":27.25,
        "TruthfulQA":41.22,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.51,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"9879c9b5f8bea9051dcb0e68dff21493d67e9d4f"
    },
    {
        "T":"\u2b55",
        "Model":"SummerSigh\/GPTNeo350M-Instruct-SFT",
        "Average":33.87,
        "ARC":25.94,
        "HellaSwag":38.55,
        "MMLU":25.76,
        "TruthfulQA":45.25,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.46,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"5e41660ced3edf13c47e933112efd280b710b977"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mikivis\/gpt2-large-lora-stf4",
        "Average":33.86,
        "ARC":26.88,
        "HellaSwag":42.17,
        "MMLU":25.53,
        "TruthfulQA":40.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.77,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"82eff3a62116fd589ad7319c9d75ff6b12f42f72"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigcode\/starcoderbase-3b",
        "Average":33.84,
        "ARC":25.85,
        "HellaSwag":39.11,
        "MMLU":27.35,
        "TruthfulQA":43.05,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":3.0,
        "Hub":17,
        "Available on the hub":false,
        "Model Sha":"e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AIGym\/deepseek-coder-1.3b-chat",
        "Average":33.69,
        "ARC":25.6,
        "HellaSwag":39.69,
        "MMLU":25.54,
        "TruthfulQA":43.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.35,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"1e167cd95fc142008b7ea37a1d59a12f972b8c96"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LordNoah\/latent_gpt2_medium_alpaca_e2",
        "Average":33.66,
        "ARC":26.96,
        "HellaSwag":39.72,
        "MMLU":26.93,
        "TruthfulQA":41.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.36,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d2d04c16c047a048c3addcd2480bd61ac04e359d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"stabilityai\/stablelm-base-alpha-3b",
        "Average":33.66,
        "ARC":26.45,
        "HellaSwag":42.24,
        "MMLU":25.43,
        "TruthfulQA":40.5,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":[
            "cc-by-sa-4.0"
        ],
        "#Params (B)":3.43,
        "Hub":80,
        "Available on the hub":true,
        "Model Sha":"99567ccfe45fabe467c71393aa6716106edb83c2"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"gpt2-medium",
        "Average":33.64,
        "ARC":27.05,
        "HellaSwag":40.17,
        "MMLU":26.6,
        "TruthfulQA":40.76,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.38,
        "Hub":75,
        "Available on the hub":true,
        "Model Sha":"f65d4965d1221eff2bcf34f53a2ba12120e18f24"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Kunhao\/pile-7b",
        "Average":33.63,
        "ARC":26.79,
        "HellaSwag":38.76,
        "MMLU":26.55,
        "TruthfulQA":42.41,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"aa1c2fff615235b007e15ce191b35816959ace99"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v1-774m",
        "Average":33.61,
        "ARC":28.07,
        "HellaSwag":44.35,
        "MMLU":25.91,
        "TruthfulQA":36.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d3f5401d07965fb13c2cb8b458ffaed9a5a79c2d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v2-355m",
        "Average":33.6,
        "ARC":28.33,
        "HellaSwag":40.54,
        "MMLU":26.77,
        "TruthfulQA":38.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.36,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"f51d310aebc16a9fe0d999d2a437b5faff635716"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-bloom-2b6-v2",
        "Average":33.6,
        "ARC":27.65,
        "HellaSwag":39.23,
        "MMLU":25.24,
        "TruthfulQA":42.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":2.48,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"8334b22c39937c0404e09dd22a867e2e2a6fc9e0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mikivis\/gpt2-large-lora-sft2",
        "Average":33.58,
        "ARC":26.62,
        "HellaSwag":42.68,
        "MMLU":24.72,
        "TruthfulQA":40.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.77,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1244efb5d20765beb54f6b4a4e1426cf6d5daf44"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/gpt-2-xl-EvolInstruct",
        "Average":33.57,
        "ARC":27.39,
        "HellaSwag":38.46,
        "MMLU":25.67,
        "TruthfulQA":42.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3e68735b9bfbca5c2e6a8e4367f003ab3d3c1512"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sharathhebbar24\/Instruct_GPT",
        "Average":33.54,
        "ARC":28.24,
        "HellaSwag":39.33,
        "MMLU":26.84,
        "TruthfulQA":39.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.36,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"fa52bd3a8909f0b69844280d3bb5da1070d49979"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/galactica-orca-wizardlm-1.3b",
        "Average":33.53,
        "ARC":30.89,
        "HellaSwag":36.02,
        "MMLU":25.94,
        "TruthfulQA":41.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.32,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"4c0294934ecafb9ee6ec120b17f7ef81c2e1240b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cerebras\/Cerebras-GPT-1.3B",
        "Average":33.53,
        "ARC":26.28,
        "HellaSwag":38.54,
        "MMLU":26.59,
        "TruthfulQA":42.7,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.32,
        "Hub":43,
        "Available on the hub":true,
        "Model Sha":"5b95400ee8d1e3cc9f79f0dec7182ed9c1009c34"
    },
    {
        "T":"?",
        "Model":"FabbriSimo01\/Cerebras_1.3b_Quantized",
        "Average":33.49,
        "ARC":25.94,
        "HellaSwag":38.56,
        "MMLU":26.79,
        "TruthfulQA":42.67,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e2126a42a1c8a938553dd513e4adafec41cb793e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Technoculture\/Mediquad-4x7b",
        "Average":33.48,
        "ARC":27.47,
        "HellaSwag":28.21,
        "MMLU":28.66,
        "TruthfulQA":49.56,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":19.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"91cd7ebc2a1ec9f88073842ce9cbd92a6943fd55"
    },
    {
        "T":"?",
        "Model":"facebook\/xglm-1.7B",
        "Average":33.46,
        "ARC":25.85,
        "HellaSwag":45.68,
        "MMLU":25.1,
        "TruthfulQA":37.21,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.73,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"d23a5e8e2164af31a84a26756b9b17f925143050"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"winglian\/basilisk-4b",
        "Average":33.45,
        "ARC":25.85,
        "HellaSwag":39.6,
        "MMLU":24.61,
        "TruthfulQA":43.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":3.37,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"b91c2e5389f4f0ce2d6042fdce5927343d8dcb06"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"postbot\/emailgen-pythia-410m-deduped",
        "Average":33.37,
        "ARC":27.9,
        "HellaSwag":40.04,
        "MMLU":27.35,
        "TruthfulQA":38.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.51,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e0208b02990c49138350da791f0b6fcb8a65e738"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-410m-deduped",
        "Average":33.26,
        "ARC":24.83,
        "HellaSwag":41.29,
        "MMLU":25.99,
        "TruthfulQA":40.95,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.51,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"c4fc8d586d62df497f1f9b69d66d3ca419992d3e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/GPlatty-30B-SuperHOT-8K-fp16",
        "Average":33.25,
        "ARC":28.33,
        "HellaSwag":33.48,
        "MMLU":24.92,
        "TruthfulQA":46.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"e2103a424c1700756df1c0c0b334195f37efe17b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-coder-ds-1.3b",
        "Average":33.25,
        "ARC":26.54,
        "HellaSwag":39.49,
        "MMLU":24.85,
        "TruthfulQA":42.12,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.28,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8045ddf0d93e582dd6ed80c9f62fd0b6c7d8f806"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DataLinguistic\/DataLinguistic-34B-V1.0",
        "Average":33.11,
        "ARC":27.65,
        "HellaSwag":32.96,
        "MMLU":23.12,
        "TruthfulQA":48.73,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"6744f1442d8ec2716d091cfddbf5766a1ec8d533"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Corianas\/Quokka_1.3b",
        "Average":33.11,
        "ARC":27.73,
        "HellaSwag":37.91,
        "MMLU":26.66,
        "TruthfulQA":40.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.42,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8a8d738e841a524d658897d89b9e39e7b9272ed8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/bloom-1b1_10e6",
        "Average":33.1,
        "ARC":25.43,
        "HellaSwag":37.12,
        "MMLU":25.43,
        "TruthfulQA":44.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":1.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"37d0d3582e88a382e22c7958dd908081553babb6"
    },
    {
        "T":"\u2b55",
        "Model":"nicholasKluge\/Aira-2-355M",
        "Average":33.07,
        "ARC":27.56,
        "HellaSwag":38.92,
        "MMLU":27.26,
        "TruthfulQA":38.53,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.36,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2479f5b1bb62251ec88e60182ba81390a4c19cf9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nnheui\/pythia-410m-sft-full",
        "Average":33.06,
        "ARC":26.54,
        "HellaSwag":40.0,
        "MMLU":25.49,
        "TruthfulQA":40.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.4,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4fd9c56aba82a5a7e93369d3a9e894e277d24841"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vicgalle\/Miqu-6B-truthy",
        "Average":33.01,
        "ARC":27.65,
        "HellaSwag":26.71,
        "MMLU":27.04,
        "TruthfulQA":50.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.66,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"179b531fff0959893bb486df30f1f374a2c42b90"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewqr2130\/mistral-inst-v02-dpo",
        "Average":32.95,
        "ARC":27.9,
        "HellaSwag":26.08,
        "MMLU":27.02,
        "TruthfulQA":50.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d907e70ac8d48e22b85f57b4fb715dfef9f4cfc8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mikivis\/gpt2-large-lora-sft1",
        "Average":32.9,
        "ARC":24.66,
        "HellaSwag":42.67,
        "MMLU":24.89,
        "TruthfulQA":39.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.77,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8e26a8d2dc1661d87a8652c75f00b805d63e7330"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"TheTravellingEngineer\/bloom-560m-RLHF-v2",
        "Average":32.89,
        "ARC":26.45,
        "HellaSwag":37.67,
        "MMLU":23.95,
        "TruthfulQA":43.51,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.56,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"7128cbfcdaf67f1eff27e45d875c35e7b47618db"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeen214\/test_llama2_ko_7b",
        "Average":32.88,
        "ARC":29.95,
        "HellaSwag":26.94,
        "MMLU":25.62,
        "TruthfulQA":49.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.67,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"45901e1d6ccb22f5ed8aec3f9dd366823fdd1c33"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nnheui\/pythia-410m-sft-full",
        "Average":32.86,
        "ARC":26.11,
        "HellaSwag":39.92,
        "MMLU":25.28,
        "TruthfulQA":40.11,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.4,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4fd9c56aba82a5a7e93369d3a9e894e277d24841"
    },
    {
        "T":"?",
        "Model":"Corianas\/1.3b",
        "Average":32.85,
        "ARC":27.3,
        "HellaSwag":38.3,
        "MMLU":26.77,
        "TruthfulQA":39.02,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.42,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"9831f95df82155ef95ff46a505506bf6194b131a"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AkiGogikar\/KnowledgeNinja-LiteLlama-460Mx6MoE-1T",
        "Average":32.84,
        "ARC":25.17,
        "HellaSwag":38.45,
        "MMLU":26.16,
        "TruthfulQA":41.57,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.97,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"04c91b4a7759f67dc236e8d61846f0cf756da9fa"
    },
    {
        "T":"\u2b55",
        "Model":"AI-Sweden-Models\/gpt-sw3-356m-instruct",
        "Average":32.81,
        "ARC":26.96,
        "HellaSwag":38.01,
        "MMLU":25.53,
        "TruthfulQA":40.74,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.47,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"85615b7c700ca7f38c32db8c7efabfa97668f1c2"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"RWKV\/rwkv-4-430m-pile",
        "Average":32.79,
        "ARC":26.71,
        "HellaSwag":40.01,
        "MMLU":24.85,
        "TruthfulQA":39.58,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.38,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"a4f6ec80438d4262d1bbc8f385feb2ef1a4a9d6b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"ahxt\/llama2_xs_460M_experimental",
        "Average":32.79,
        "ARC":24.91,
        "HellaSwag":38.47,
        "MMLU":26.17,
        "TruthfulQA":41.59,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.41,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"c8db281477559f5c969a9be794ce236f8a99e1a0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/Tinyllama-616M-Cinder",
        "Average":32.78,
        "ARC":26.45,
        "HellaSwag":36.4,
        "MMLU":24.86,
        "TruthfulQA":43.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.62,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9556943a1a039272f052231e1626ba606994f43e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"porkorbeef\/Llama-2-13b-sf",
        "Average":32.74,
        "ARC":29.52,
        "HellaSwag":26.49,
        "MMLU":25.98,
        "TruthfulQA":48.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"06253ee259e6b205c4734ab6ec3fa850737b2110"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xhyi\/PT_GPTNEO350_ATG",
        "Average":32.71,
        "ARC":25.43,
        "HellaSwag":37.59,
        "MMLU":24.79,
        "TruthfulQA":43.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.36,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"56ab08aaa6802d0f830d42c352d5d536be72811d"
    },
    {
        "T":"?",
        "Model":"kevin009\/flyingllama-v2",
        "Average":32.71,
        "ARC":24.74,
        "HellaSwag":38.44,
        "MMLU":26.37,
        "TruthfulQA":41.3,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.46,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"a1c0d260967efd02b197d525ce2802d42a3fb694"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kevin009\/flyingllama",
        "Average":32.71,
        "ARC":24.74,
        "HellaSwag":38.35,
        "MMLU":26.14,
        "TruthfulQA":41.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.46,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"57297d80cdbd91415b76b2ef58d272262a627a98"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cmarkea\/bloomz-560m-sft-chat",
        "Average":32.7,
        "ARC":27.47,
        "HellaSwag":37.05,
        "MMLU":23.93,
        "TruthfulQA":42.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.56,
        "Hub":10,
        "Available on the hub":true,
        "Model Sha":"e2bbcbdd534c7d75b7d2f9408e74f6682cf3a05e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"ahxt\/LiteLlama-460M-1T",
        "Average":32.69,
        "ARC":24.83,
        "HellaSwag":38.39,
        "MMLU":25.96,
        "TruthfulQA":41.59,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.41,
        "Hub":152,
        "Available on the hub":true,
        "Model Sha":"77b8a976440e7d1ea5a890eaf1e0175b1cac0078"
    },
    {
        "T":"?",
        "Model":"TheBloke\/Wizard-Vicuna-13B-Uncensored-GPTQ",
        "Average":32.67,
        "ARC":29.61,
        "HellaSwag":25.47,
        "MMLU":25.34,
        "TruthfulQA":50.25,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.03,
        "Hub":230,
        "Available on the hub":true,
        "Model Sha":"d9b00ec47ae3546398432f0693fe2d5d92bf143b"
    },
    {
        "T":"?",
        "Model":"bigscience\/bloomz-560m",
        "Average":32.66,
        "ARC":23.55,
        "HellaSwag":36.31,
        "MMLU":25.1,
        "TruthfulQA":45.69,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.56,
        "Hub":73,
        "Available on the hub":true,
        "Model Sha":"a2845d7e13dd12efae154a9f1c63fcc2e0cc4b05"
    },
    {
        "T":"?",
        "Model":"TheBloke\/medalpaca-13B-GPTQ-4bit",
        "Average":32.66,
        "ARC":29.35,
        "HellaSwag":26.32,
        "MMLU":25.44,
        "TruthfulQA":49.51,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc",
        "#Params (B)":2.03,
        "Hub":26,
        "Available on the hub":true,
        "Model Sha":"12190f743a19e91dfe1f5c77abc0c1bf486073dd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Vicuna-33B-1-3-SuperHOT-8K-fp16",
        "Average":32.65,
        "ARC":25.43,
        "HellaSwag":34.61,
        "MMLU":23.62,
        "TruthfulQA":46.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"0b6484697d5cca5baa534b882dcad8101add8cda"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v1-355m",
        "Average":32.61,
        "ARC":27.13,
        "HellaSwag":39.07,
        "MMLU":27.12,
        "TruthfulQA":37.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.36,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"c5f4b5a61e6a66a5c7613164d99a70db5bf7e9a2"
    },
    {
        "T":"?",
        "Model":"Felladrin\/Llama-160M-Chat-v1",
        "Average":32.59,
        "ARC":24.74,
        "HellaSwag":35.32,
        "MMLU":26.14,
        "TruthfulQA":44.16,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.16,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"06b255f112080b26c62e72404331421ffcb95293"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-orca-airoboros-13b-0.10e",
        "Average":32.58,
        "ARC":29.27,
        "HellaSwag":25.74,
        "MMLU":25.69,
        "TruthfulQA":49.61,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"dbd1d1f7ad7b6b359f8246141650b25ca0bb8cbb"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"uukuguy\/Orca-2-7b-f16",
        "Average":32.57,
        "ARC":29.61,
        "HellaSwag":25.62,
        "MMLU":26.7,
        "TruthfulQA":48.36,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f6b2f717467dc12b2b19cad90ed4362153863ad9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NEU-HAI\/mental-alpaca",
        "Average":32.56,
        "ARC":28.58,
        "HellaSwag":26.02,
        "MMLU":27.04,
        "TruthfulQA":48.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"f5f24d4a11ed52b4a224f365b6a694cf4e27c1bc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yhyhy3\/med-orca-instruct-33b",
        "Average":32.56,
        "ARC":28.84,
        "HellaSwag":25.63,
        "MMLU":26.5,
        "TruthfulQA":49.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1d636881854338e571825226c712180da06be72c"
    },
    {
        "T":"\u2b55",
        "Model":"uukuguy\/speechless-codellama-orca-airoboros-13b-0.10e",
        "Average":32.55,
        "ARC":29.44,
        "HellaSwag":25.71,
        "MMLU":25.43,
        "TruthfulQA":49.64,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"dbd1d1f7ad7b6b359f8246141650b25ca0bb8cbb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Wizard-Vicuna-30B-Superhot-8K-fp16",
        "Average":32.52,
        "ARC":26.19,
        "HellaSwag":32.96,
        "MMLU":23.46,
        "TruthfulQA":47.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"062fe5409861d7386279fb534b435be39c88ceaf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lqtrung1998\/Codellama-7b-hf-ReFT-Rerank-GSM8k",
        "Average":32.5,
        "ARC":29.27,
        "HellaSwag":26.13,
        "MMLU":24.64,
        "TruthfulQA":49.97,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"b863eff60d154ed4d68349f75550377f9ff7fefc"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"TheTravellingEngineer\/bloom-1b1-RLHF",
        "Average":32.48,
        "ARC":27.99,
        "HellaSwag":26.19,
        "MMLU":26.86,
        "TruthfulQA":48.88,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"65bd72580520a1d4a0c19fcb23f68c1f28464e1b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MBZUAI\/lamini-cerebras-1.3b",
        "Average":32.43,
        "ARC":26.88,
        "HellaSwag":37.96,
        "MMLU":28.43,
        "TruthfulQA":36.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.32,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"502e70081df53edc8a9156acf5a26a11a9dad8fb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"player1537\/dolphinette",
        "Average":32.42,
        "ARC":24.91,
        "HellaSwag":37.33,
        "MMLU":25.37,
        "TruthfulQA":42.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.56,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"20529d47b0a82343014727edd1639a9a6a6b09e6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LordNoah\/latent_gpt2_medium_alpaca_e4",
        "Average":32.41,
        "ARC":29.1,
        "HellaSwag":39.8,
        "MMLU":25.52,
        "TruthfulQA":35.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.36,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a2da5e4fbd6a50110a0106ef4f046deb56e5d7a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"IDEA-CCNL\/Ziya-LLaMA-13B-Pretrain-v1",
        "Average":32.4,
        "ARC":27.99,
        "HellaSwag":26.0,
        "MMLU":27.04,
        "TruthfulQA":48.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":12.89,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"826e83e411df32f358893ab21f5eae680499ae9a"
    },
    {
        "T":"?",
        "Model":"TheBloke\/WizardLM-7B-uncensored-GPTQ",
        "Average":32.4,
        "ARC":28.5,
        "HellaSwag":25.37,
        "MMLU":24.85,
        "TruthfulQA":50.86,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.13,
        "Hub":147,
        "Available on the hub":true,
        "Model Sha":"cc30c031fd795ee3d3a50312ab4549415bfbdb46"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/megatron-gpt2-345m-evol_instruct_v2",
        "Average":32.39,
        "ARC":26.37,
        "HellaSwag":38.39,
        "MMLU":23.6,
        "TruthfulQA":41.19,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.36,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2866eeaaf62014a7a6e939d18b6e27f44df48428"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/EverythingLM-13B-16K-GPTQ",
        "Average":32.37,
        "ARC":29.27,
        "HellaSwag":26.24,
        "MMLU":25.4,
        "TruthfulQA":48.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":2.03,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"f14d3df05577f3e1ac35e2c4ec32ce0d39b97508"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"quantumaikr\/open_llama_7b_hf",
        "Average":32.37,
        "ARC":26.45,
        "HellaSwag":26.95,
        "MMLU":26.54,
        "TruthfulQA":49.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"41441cea58f963cfc4827da12ae5759e943151cb"
    },
    {
        "T":"?",
        "Model":"TheBloke\/chronos-wizardlm-uc-scot-st-13B-GPTQ",
        "Average":32.37,
        "ARC":27.99,
        "HellaSwag":26.1,
        "MMLU":25.72,
        "TruthfulQA":49.68,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.03,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"c4246e4b8d3fc77b9fe4ebb1ead61cda4b83575b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigcode\/starcoderbase-1b",
        "Average":32.37,
        "ARC":22.7,
        "HellaSwag":34.31,
        "MMLU":26.67,
        "TruthfulQA":45.79,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":1.0,
        "Hub":125,
        "Available on the hub":false,
        "Model Sha":"182f0165fdf8da9c9935901eec65c94337f01c11"
    },
    {
        "T":"?",
        "Model":"TheBloke\/WizardLM-30B-Uncensored-GPTQ",
        "Average":32.35,
        "ARC":29.44,
        "HellaSwag":26.47,
        "MMLU":24.35,
        "TruthfulQA":49.15,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":4.45,
        "Hub":99,
        "Available on the hub":true,
        "Model Sha":"43c701ddbe0bceac26c860307e06763cc5203500"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"IDEA-CCNL\/Ziya-LLaMA-13B-v1",
        "Average":32.34,
        "ARC":27.73,
        "HellaSwag":25.96,
        "MMLU":27.04,
        "TruthfulQA":48.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":12.89,
        "Hub":250,
        "Available on the hub":true,
        "Model Sha":"fccf34387d2c9f2f95ff59ae380e6de3718e41ff"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Minami-su\/roleplay_alpaca_llama_lora",
        "Average":32.33,
        "ARC":27.65,
        "HellaSwag":25.99,
        "MMLU":27.04,
        "TruthfulQA":48.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"bc79883dd53a993dbe1c100ae6f40811179a382d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Phind\/Phind-CodeLlama-34B-v1",
        "Average":32.32,
        "ARC":27.13,
        "HellaSwag":28.28,
        "MMLU":28.94,
        "TruthfulQA":44.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":314,
        "Available on the hub":true,
        "Model Sha":"b073c9bb418ae52ca76b4ab48ac2dfbc8622f434"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Minami-su\/Qwen1.5-7B-Chat_mistral",
        "Average":32.32,
        "ARC":24.49,
        "HellaSwag":26.69,
        "MMLU":25.78,
        "TruthfulQA":52.33,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"b159c4dc7f9d9fa6c5b799325df6964b653e30cc"
    },
    {
        "T":"\u2b55",
        "Model":"uukuguy\/speechless-codellama-orca-platypus-13b-0.10e",
        "Average":32.32,
        "ARC":28.75,
        "HellaSwag":25.88,
        "MMLU":25.36,
        "TruthfulQA":49.27,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"119abfc73f9ce541a40779f167fe21e95faed4e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"marcchew\/LaMini-40k-Platypus2-7B",
        "Average":32.31,
        "ARC":28.5,
        "HellaSwag":26.32,
        "MMLU":27.04,
        "TruthfulQA":47.39,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e8c03e43eab479a216b5f4f182a711c3624f38bd"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"robowaifudev\/megatron-gpt2-345m",
        "Average":32.31,
        "ARC":24.23,
        "HellaSwag":39.18,
        "MMLU":24.32,
        "TruthfulQA":41.51,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.38,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"b39f8d00fb9f33da4271be2035da848da896a23b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MayaPH\/GodziLLa-30B-instruct",
        "Average":32.31,
        "ARC":29.01,
        "HellaSwag":26.49,
        "MMLU":24.9,
        "TruthfulQA":48.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":30.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"642bf3683801e20e4b7cf28d94374d5e6054c007"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/CAMEL-33B-Combined-Data-SuperHOT-8K-fp16",
        "Average":32.31,
        "ARC":25.85,
        "HellaSwag":31.57,
        "MMLU":23.69,
        "TruthfulQA":48.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"14744d11eab7028c5c845f89db2edc9c6fe2becb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-orca-platypus-13b-0.10e",
        "Average":32.3,
        "ARC":28.92,
        "HellaSwag":25.76,
        "MMLU":25.28,
        "TruthfulQA":49.22,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"119abfc73f9ce541a40779f167fe21e95faed4e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/GPT2-774M-CINDER-SHOW-MULTI-CHAT",
        "Average":32.3,
        "ARC":26.54,
        "HellaSwag":39.69,
        "MMLU":25.8,
        "TruthfulQA":37.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.77,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"4262c2b9647c19a2b1dbf876f9e93e57643eb7d2"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AI-Sweden-Models\/gpt-sw3-356m",
        "Average":32.29,
        "ARC":23.63,
        "HellaSwag":37.05,
        "MMLU":25.93,
        "TruthfulQA":42.55,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.47,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"15ba8a812d3eb265342f62cb0ee9ab6a45fdbd89"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shitshow123\/TinyLlama-1.1B-ChatStrong-DPO-PPO",
        "Average":32.29,
        "ARC":30.38,
        "HellaSwag":25.75,
        "MMLU":24.17,
        "TruthfulQA":48.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.03,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"8bf7ba0c5552fd7377c75e0ad8e6030a16234f86"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fionazhang\/mistral-environment-adapter",
        "Average":32.28,
        "ARC":29.18,
        "HellaSwag":25.81,
        "MMLU":25.38,
        "TruthfulQA":48.75,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"28910193dcfc67b615e918c6cd90162b9ef12446"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hoskinson-center\/proofGPT-v0.1",
        "Average":32.28,
        "ARC":22.87,
        "HellaSwag":28.66,
        "MMLU":25.96,
        "TruthfulQA":51.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.31,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"1e4dd330ca90c0ef6d77ca71bd49cbe3d71f26b8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"doas\/test2",
        "Average":32.27,
        "ARC":29.61,
        "HellaSwag":26.65,
        "MMLU":24.34,
        "TruthfulQA":48.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f08d224deae510ebf1408ce38bc2610b1e4c77eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"marcchew\/Marcoroni-7B-LaMini-80K",
        "Average":32.27,
        "ARC":28.75,
        "HellaSwag":26.13,
        "MMLU":24.46,
        "TruthfulQA":49.71,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ea7a283403ec1a40570bfc25f2c4b8fcb089b6bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BreadAi\/PM_modelV2",
        "Average":32.26,
        "ARC":25.09,
        "HellaSwag":26.45,
        "MMLU":26.14,
        "TruthfulQA":51.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4edde209eea33af491206f8651c0c47e70e08289"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bigcode\/santacoder",
        "Average":32.25,
        "ARC":26.28,
        "HellaSwag":25.6,
        "MMLU":25.89,
        "TruthfulQA":51.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":1.31,
        "Hub":295,
        "Available on the hub":true,
        "Model Sha":"132eb6b6cedaf579c2f333f1ecd78a16d7e45978"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Abe13\/juniper-certificate-Llama-2-7b-chat-hf",
        "Average":32.25,
        "ARC":29.1,
        "HellaSwag":27.63,
        "MMLU":24.02,
        "TruthfulQA":48.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"90ed388e5503c02f5e6ba8dbc7286687a85ce1c1"
    },
    {
        "T":"?",
        "Model":"TheBloke\/wizard-vicuna-13B-GPTQ",
        "Average":32.25,
        "ARC":28.67,
        "HellaSwag":25.94,
        "MMLU":25.84,
        "TruthfulQA":48.53,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":2.03,
        "Hub":99,
        "Available on the hub":true,
        "Model Sha":"936a51c0219744d7a9598d0c65a7d18e01660601"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"porkorbeef\/Llama-2-13b",
        "Average":32.24,
        "ARC":29.35,
        "HellaSwag":26.35,
        "MMLU":24.94,
        "TruthfulQA":48.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"06253ee259e6b205c4734ab6ec3fa850737b2110"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vikash06\/doctorMistralLLM10k",
        "Average":32.23,
        "ARC":27.22,
        "HellaSwag":27.45,
        "MMLU":25.95,
        "TruthfulQA":48.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a0af93b8550a5eb5424cda986e6c91b603cebfe9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/DiffMerge-DollyGPT-Pygmalion",
        "Average":32.22,
        "ARC":23.63,
        "HellaSwag":34.38,
        "MMLU":24.41,
        "TruthfulQA":46.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"6a00b371146d4bd2903890814485ee1b775162e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/pygmalion-350m",
        "Average":32.22,
        "ARC":25.0,
        "HellaSwag":37.8,
        "MMLU":25.68,
        "TruthfulQA":40.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.33,
        "Hub":47,
        "Available on the hub":true,
        "Model Sha":"d65832d913f6b396e2ffb64c373d9383c9da9303"
    },
    {
        "T":"\u2b55",
        "Model":"TheBloke\/orca_mini_v3_7B-GPTQ",
        "Average":32.22,
        "ARC":30.12,
        "HellaSwag":25.99,
        "MMLU":24.31,
        "TruthfulQA":48.44,
        "Type":"instruction-tuned",
        "Precision":"8bit",
        "Hub License":"llama2",
        "#Params (B)":1.13,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"06ddd48cd904907e3c73d2dfe47d28626053598b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/santa1.1b_10e6",
        "Average":32.21,
        "ARC":27.65,
        "HellaSwag":26.39,
        "MMLU":25.42,
        "TruthfulQA":49.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":1.1,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"a27e0cf68c590772c74b981c8bd69ce6a559e776"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt350m_10e5",
        "Average":32.21,
        "ARC":24.15,
        "HellaSwag":36.53,
        "MMLU":26.0,
        "TruthfulQA":42.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.35,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"9351bd3b7ded60bcf170d81fd3a6040ea431a8de"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"postbot\/gpt2-medium-emailgen",
        "Average":32.2,
        "ARC":26.45,
        "HellaSwag":34.31,
        "MMLU":24.1,
        "TruthfulQA":43.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":[
            "apache-2.0"
        ],
        "#Params (B)":0.38,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"1b9b03d00b2b300d3c04c37fe3782c180ef51a27"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"222gate\/TinyMistral-248Mx4-MOE",
        "Average":32.18,
        "ARC":29.52,
        "HellaSwag":25.71,
        "MMLU":24.82,
        "TruthfulQA":48.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.7,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4610e3fb0c4f541835bedf9be5a8fd6955827e3b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"senseable\/moe-x33",
        "Average":32.18,
        "ARC":26.19,
        "HellaSwag":26.44,
        "MMLU":24.93,
        "TruthfulQA":51.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":58.94,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2ce4ba7ce76392721be10c3c05b63853be98b686"
    },
    {
        "T":"?",
        "Model":"TheBloke\/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ",
        "Average":32.18,
        "ARC":28.41,
        "HellaSwag":26.05,
        "MMLU":24.71,
        "TruthfulQA":49.54,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":4.45,
        "Hub":66,
        "Available on the hub":true,
        "Model Sha":"cd07cc7c55b46524f61214012653c25226d24c0d"
    },
    {
        "T":"?",
        "Model":"TheBloke\/WizardLM-30B-GPTQ",
        "Average":32.17,
        "ARC":28.84,
        "HellaSwag":26.08,
        "MMLU":24.62,
        "TruthfulQA":49.14,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":4.45,
        "Hub":19,
        "Available on the hub":true,
        "Model Sha":"e2e97475a9775d2fe7afba098aee37e694b9220f"
    },
    {
        "T":"?",
        "Model":"bigscience\/bloom-560m",
        "Average":32.14,
        "ARC":24.74,
        "HellaSwag":37.15,
        "MMLU":24.22,
        "TruthfulQA":42.44,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.56,
        "Hub":250,
        "Available on the hub":true,
        "Model Sha":"4f42c91d806a19ae1a46af6c3fb5f4990d884cd6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vihangd\/neuralfalcon-1b-v1",
        "Average":32.13,
        "ARC":26.79,
        "HellaSwag":26.56,
        "MMLU":26.22,
        "TruthfulQA":48.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f788af66f22a933ad60e732ebaede3dfb5679bd4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"danielpark\/gorani-100k-llama2-13b-instruct",
        "Average":32.12,
        "ARC":28.07,
        "HellaSwag":26.3,
        "MMLU":25.17,
        "TruthfulQA":48.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"f7d38ee654e505ad7a454f192d5e3d85cb60b3b8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TFLai\/gpt2-turkish-uncased",
        "Average":32.12,
        "ARC":24.49,
        "HellaSwag":25.08,
        "MMLU":26.59,
        "TruthfulQA":52.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"4807e7df1dfb9d60c6d98e3cfeff62cb6b9a1579"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/RWKV-4-PilePlus-430M-20230520-6162-1018Gtokens-ctx4098",
        "Average":32.11,
        "ARC":26.02,
        "HellaSwag":40.39,
        "MMLU":24.45,
        "TruthfulQA":37.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.38,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e31777c9d3b8c5c9f803b23f49550c009cbdcf6d"
    },
    {
        "T":"?",
        "Model":"TheBloke\/guanaco-33B-GPTQ",
        "Average":32.11,
        "ARC":28.16,
        "HellaSwag":26.34,
        "MMLU":24.94,
        "TruthfulQA":48.98,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":4.45,
        "Hub":70,
        "Available on the hub":true,
        "Model Sha":"8e42e031bfc8be3bbf31dc546d7c51fb991ff6e0"
    },
    {
        "T":"?",
        "Model":"TheBloke\/openchat_v2_openorca_preview-GPTQ",
        "Average":32.09,
        "ARC":27.99,
        "HellaSwag":26.06,
        "MMLU":24.24,
        "TruthfulQA":50.08,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.03,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"5a4c2ea612b71d7c00118f796db7189bc1a0c930"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"porkorbeef\/Llama-2-13b-public",
        "Average":32.09,
        "ARC":29.95,
        "HellaSwag":26.65,
        "MMLU":22.74,
        "TruthfulQA":49.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e1b32a8fcfc0f37fd5f50cf765151897574c73c7"
    },
    {
        "T":"?",
        "Model":"TheBloke\/LongChat-13B-GPTQ",
        "Average":32.07,
        "ARC":28.33,
        "HellaSwag":26.12,
        "MMLU":25.56,
        "TruthfulQA":48.27,
        "Type":"Unknown",
        "Precision":"4bit",
        "Hub License":"other",
        "#Params (B)":2.03,
        "Hub":25,
        "Available on the hub":true,
        "Model Sha":"8ec25a29033b7be5daeafa26f08e1ea7cf232b98"
    },
    {
        "T":"?",
        "Model":"yhyhy3\/med-orca-instruct-33b",
        "Average":32.06,
        "ARC":27.39,
        "HellaSwag":25.89,
        "MMLU":25.37,
        "TruthfulQA":49.6,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1d636881854338e571825226c712180da06be72c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"alnrg2arg\/test2",
        "Average":32.06,
        "ARC":27.22,
        "HellaSwag":26.25,
        "MMLU":24.64,
        "TruthfulQA":50.14,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6310110a31918d27d42116942bc2ba3941784ae9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"marcchew\/Marcoroni-7B-LaMini-40K",
        "Average":32.05,
        "ARC":27.65,
        "HellaSwag":26.23,
        "MMLU":26.92,
        "TruthfulQA":47.4,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"27868e4faed5d68d059c8c57dbd3e24e4933ca28"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/airoboros-33B-gpt4-1-4-SuperHOT-8K-fp16",
        "Average":32.04,
        "ARC":26.02,
        "HellaSwag":30.65,
        "MMLU":23.57,
        "TruthfulQA":47.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"53fdac1cdb8a37647e5dbe4199bc3fb70e617fce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Chinese-Alpaca-33B-SuperHOT-8K-fp16",
        "Average":32.04,
        "ARC":26.79,
        "HellaSwag":29.56,
        "MMLU":24.07,
        "TruthfulQA":47.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.44,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"a55ce761bace8be6d17c357c57ef927751afd40c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MayaPH\/FinOPT-Franklin",
        "Average":32.04,
        "ARC":27.73,
        "HellaSwag":24.91,
        "MMLU":23.12,
        "TruthfulQA":52.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":1.32,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"1b13331834190bfe49a176f1661ba4d8309a5051"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openbmb\/UltraRM-13b",
        "Average":32.04,
        "ARC":28.16,
        "HellaSwag":26.13,
        "MMLU":25.96,
        "TruthfulQA":47.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub":46,
        "Available on the hub":true,
        "Model Sha":"4b231ae58c15244e6e15f0d2f4e26ec37b846229"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/WizardLM-33B-V1.0-Uncensored-GPTQ",
        "Average":32.03,
        "ARC":27.39,
        "HellaSwag":26.03,
        "MMLU":25.81,
        "TruthfulQA":48.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":4.45,
        "Hub":35,
        "Available on the hub":true,
        "Model Sha":"1c65902c620fcdf6b9c8e36ce17f21360e186a1e"
    },
    {
        "T":"?",
        "Model":"TheBloke\/Project-Baize-v2-13B-GPTQ",
        "Average":32.03,
        "ARC":27.56,
        "HellaSwag":26.42,
        "MMLU":25.91,
        "TruthfulQA":48.22,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.03,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"8dee7c7129aaad1ded245fce712ff5dbb2845258"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"malhajar\/Platypus2-70B-instruct-4bit-gptq",
        "Average":32.01,
        "ARC":29.01,
        "HellaSwag":25.95,
        "MMLU":23.53,
        "TruthfulQA":49.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2aa2f5646e496b3cd9b510681ba2c5081bde821f"
    },
    {
        "T":"?",
        "Model":"NobodyExistsOnTheInternet\/code-llama-70b-python-instruct",
        "Average":32.01,
        "ARC":29.61,
        "HellaSwag":25.66,
        "MMLU":23.5,
        "TruthfulQA":49.26,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":68.98,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"b11a209df4f27f9db7464677dbb14fba4baf1c3c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt350m_10e6",
        "Average":32.0,
        "ARC":23.98,
        "HellaSwag":32.36,
        "MMLU":24.96,
        "TruthfulQA":46.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.35,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"d1cba6a82e52f551953e1d47bdaca262a2989f9d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-350M-Erebus",
        "Average":31.99,
        "ARC":23.81,
        "HellaSwag":34.35,
        "MMLU":26.23,
        "TruthfulQA":43.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.33,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"83ce2f4e78d308968cf7ecd03d86a1f64aea8336"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KaeriJenti\/Kaori-34b-v2",
        "Average":31.98,
        "ARC":23.89,
        "HellaSwag":28.97,
        "MMLU":25.59,
        "TruthfulQA":49.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e28a7b27201045a0ca9b1504c5bae53428f2c0ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KaeriJenti\/kaori-34b-v4",
        "Average":31.98,
        "ARC":23.89,
        "HellaSwag":28.97,
        "MMLU":25.59,
        "TruthfulQA":49.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":34.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"94628cc31b1acac36a464edbfea09949bca139b7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vihangd\/neuralfalcon-1b-v1",
        "Average":31.97,
        "ARC":26.37,
        "HellaSwag":26.56,
        "MMLU":25.93,
        "TruthfulQA":49.03,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f788af66f22a933ad60e732ebaede3dfb5679bd4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"janhq\/supermario-v1",
        "Average":31.97,
        "ARC":27.73,
        "HellaSwag":25.83,
        "MMLU":27.04,
        "TruthfulQA":47.27,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"22a88e62529dc2cc95991478cd87e6c588237258"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Cartinoe5930\/iDUS",
        "Average":31.97,
        "ARC":27.73,
        "HellaSwag":26.65,
        "MMLU":24.91,
        "TruthfulQA":48.58,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"680101b4b43110627f526cd4d05856cf624a6ce2"
    },
    {
        "T":"?",
        "Model":"rishiraj\/bloom-560m-guanaco",
        "Average":31.96,
        "ARC":27.9,
        "HellaSwag":26.11,
        "MMLU":24.46,
        "TruthfulQA":49.37,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.02,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"17b886fe53bdb4cea75a7f40da1e8e987124edef"
    },
    {
        "T":"?",
        "Model":"Panchovix\/WizardLM-33B-V1.0-Uncensored-SuperHOT-8k",
        "Average":31.96,
        "ARC":25.43,
        "HellaSwag":31.97,
        "MMLU":23.43,
        "TruthfulQA":47.0,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":6,
        "Available on the hub":true,
        "Model Sha":"b6d0002b10d43ab48aa14e365d9e7b40655ec160"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"0x7o\/BulgakovLM-3B",
        "Average":31.95,
        "ARC":28.33,
        "HellaSwag":26.57,
        "MMLU":24.99,
        "TruthfulQA":47.93,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.84,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4e0c6683dd5d2aa9bb306d2292c2a0f91f36e636"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Technoculture\/Medorca-4x7b",
        "Average":31.94,
        "ARC":29.35,
        "HellaSwag":25.72,
        "MMLU":24.28,
        "TruthfulQA":48.42,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":19.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"70fa312cca9f7d966c37ccb52f0ce6a2aa2fd3a0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Technoculture\/Mediquad-orca-20B",
        "Average":31.94,
        "ARC":29.35,
        "HellaSwag":25.72,
        "MMLU":24.28,
        "TruthfulQA":48.42,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":19.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"6a5a811206e5c255dff8128334c06924347ae324"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chihoonlee10\/T3Q-MSlerp-13B",
        "Average":31.94,
        "ARC":27.65,
        "HellaSwag":25.85,
        "MMLU":26.26,
        "TruthfulQA":48.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"e0e967ea95b34436f54dd00340b5fd4da51e7d10"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"doas\/test5",
        "Average":31.93,
        "ARC":28.41,
        "HellaSwag":26.63,
        "MMLU":25.36,
        "TruthfulQA":47.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b0dae937b7137790d8946794375e1affd51c760a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vicgalle\/alpaca-7b",
        "Average":31.93,
        "ARC":28.07,
        "HellaSwag":25.83,
        "MMLU":25.31,
        "TruthfulQA":48.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"7f22882125208d1f54765c21abf84fd162aa454a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beomi\/KoAlpaca-Polyglot-5.8B",
        "Average":31.92,
        "ARC":27.65,
        "HellaSwag":35.58,
        "MMLU":24.72,
        "TruthfulQA":39.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.0,
        "Hub":47,
        "Available on the hub":true,
        "Model Sha":"1051dacf82ca9fba0ba4a4ff67f1d98a81ef7a2e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Phind\/Phind-CodeLlama-34B-Python-v1",
        "Average":31.91,
        "ARC":24.66,
        "HellaSwag":29.77,
        "MMLU":27.95,
        "TruthfulQA":45.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":224,
        "Available on the hub":true,
        "Model Sha":"3aabef8c9bc1b3ec2fffed053645bc1e2d829b6c"
    },
    {
        "T":"?",
        "Model":"timdettmers\/guanaco-65b-merged",
        "Average":31.91,
        "ARC":27.47,
        "HellaSwag":26.6,
        "MMLU":25.17,
        "TruthfulQA":48.41,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":65.02,
        "Hub":55,
        "Available on the hub":true,
        "Model Sha":"98c803bb6e70efe9f2aefb12cba36a96f2959d4d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"winglian\/Llama-2-3b-hf",
        "Average":31.88,
        "ARC":26.96,
        "HellaSwag":26.52,
        "MMLU":23.33,
        "TruthfulQA":50.71,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":3.37,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"293f071b223efd7959f9e1fac66285369aaa959d"
    },
    {
        "T":"?",
        "Model":"Technoculture\/mtor",
        "Average":31.87,
        "ARC":27.3,
        "HellaSwag":26.22,
        "MMLU":24.28,
        "TruthfulQA":49.68,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2164456a751d3ba578b17df0a4b097d4e3ad8df1"
    },
    {
        "T":"?",
        "Model":"TheBloke\/wizard-mega-13B-GPTQ",
        "Average":31.85,
        "ARC":27.73,
        "HellaSwag":26.01,
        "MMLU":24.97,
        "TruthfulQA":48.69,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.03,
        "Hub":101,
        "Available on the hub":true,
        "Model Sha":"848bf2514f804799dd28c188e5428d497dc983fb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abideen\/Mistral-v0.2-orpo",
        "Average":31.84,
        "ARC":27.99,
        "HellaSwag":26.41,
        "MMLU":23.12,
        "TruthfulQA":49.85,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"1dbaad225a1568e48abf33dad4365dd9a51ca27f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"yec019\/fbopt-350m-8bit",
        "Average":31.83,
        "ARC":23.55,
        "HellaSwag":36.6,
        "MMLU":26.22,
        "TruthfulQA":40.97,
        "Type":"pretrained",
        "Precision":"8bit",
        "Hub License":"unknown",
        "#Params (B)":0.33,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"305f804054d75a406a85a568ea99dca17cfc998d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MayaPH\/GodziLLa-30B-plus",
        "Average":31.83,
        "ARC":29.18,
        "HellaSwag":25.35,
        "MMLU":24.23,
        "TruthfulQA":48.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":30.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"a66b1860d11ebf8aed07237cf636fdd2b3a07f06"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Llama-2-7b-Chat-AWQ",
        "Average":31.83,
        "ARC":27.22,
        "HellaSwag":25.48,
        "MMLU":24.67,
        "TruthfulQA":49.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":1.13,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"a065961fd627aa3b3e6dde21e77fd5e20f712189"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Platypus-30B-SuperHOT-8K-fp16",
        "Average":31.81,
        "ARC":25.68,
        "HellaSwag":30.82,
        "MMLU":23.61,
        "TruthfulQA":47.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"e8ac508308911475125252dcf2677fe355dd3700"
    },
    {
        "T":"?",
        "Model":"monology\/mixtral-soup",
        "Average":31.81,
        "ARC":23.98,
        "HellaSwag":27.08,
        "MMLU":26.25,
        "TruthfulQA":49.94,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9d8a942dace21a8104f0e8ff6b3d85aee82e4cd2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vikp\/phi2",
        "Average":31.8,
        "ARC":22.87,
        "HellaSwag":30.7,
        "MMLU":27.55,
        "TruthfulQA":46.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"9fd01ce09da870fc66af88616d43e53db642ef46"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hoskinson-center\/proofGPT-v0.1-6.7B",
        "Average":31.8,
        "ARC":23.29,
        "HellaSwag":28.45,
        "MMLU":24.57,
        "TruthfulQA":50.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.65,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"02f405f08ca0e5b1aaa90a7c3b11303b5f245102"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openbmb\/UltraLM-13b",
        "Average":31.79,
        "ARC":29.44,
        "HellaSwag":25.99,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":70,
        "Available on the hub":true,
        "Model Sha":"2c732c2899fc329036d97e5c6f0a61eaff19d97d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/opt-350m",
        "Average":31.78,
        "ARC":23.55,
        "HellaSwag":36.73,
        "MMLU":26.02,
        "TruthfulQA":40.83,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.33,
        "Hub":70,
        "Available on the hub":true,
        "Model Sha":"cb32f77e905cccbca1d970436fb0f5e6b58ee3c5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-350M-Nerys-v2",
        "Average":31.78,
        "ARC":23.63,
        "HellaSwag":35.49,
        "MMLU":25.91,
        "TruthfulQA":42.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.33,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"59b1019c35ab17a7d77ea1ad32b45a8375ba6e89"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"marcchew\/Platypus-2-7B-LaMini-14K",
        "Average":31.77,
        "ARC":29.52,
        "HellaSwag":26.15,
        "MMLU":23.13,
        "TruthfulQA":48.29,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"50199ba51c4d002cc86cf3fb2ac921ec52bf4828"
    },
    {
        "T":"?",
        "Model":"TheBloke\/robin-33B-v2-GPTQ",
        "Average":31.77,
        "ARC":27.73,
        "HellaSwag":26.29,
        "MMLU":23.53,
        "TruthfulQA":49.54,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":4.45,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"4c2588d65302e9ca634548ed81e8650fb2975686"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chlee10\/T3Q-MSlerp-7Bx2",
        "Average":31.77,
        "ARC":28.41,
        "HellaSwag":25.46,
        "MMLU":25.91,
        "TruthfulQA":47.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.88,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"aa3ac6b1f0739b40674cb32d30e8b1d196dfdbdc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/TinyLlama-748M-Reason-With-Cinder-Test-2",
        "Average":31.77,
        "ARC":24.66,
        "HellaSwag":34.5,
        "MMLU":25.15,
        "TruthfulQA":42.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.75,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ed18e6755a8d925d18a5d23d0005c600b7edf326"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"max-2022\/test_mistral2",
        "Average":31.76,
        "ARC":27.9,
        "HellaSwag":25.32,
        "MMLU":24.74,
        "TruthfulQA":49.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3c74c0654e9de8e19356c5c70eebf15dddd8840e"
    },
    {
        "T":"?",
        "Model":"jaspercatapang\/Echidna-30B",
        "Average":31.76,
        "ARC":28.5,
        "HellaSwag":25.5,
        "MMLU":24.89,
        "TruthfulQA":48.14,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":30.0,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"20b13b6676d54b555ae2b9b2b4b6fc8a0c7c2e89"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt125m_10e5_30ep",
        "Average":31.75,
        "ARC":25.6,
        "HellaSwag":30.3,
        "MMLU":23.9,
        "TruthfulQA":47.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.12,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"6b007a3eaf31ad1c1186b937704986882f473dbe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/llama-30b-supercot-SuperHOT-8K-fp16",
        "Average":31.73,
        "ARC":25.85,
        "HellaSwag":30.53,
        "MMLU":23.5,
        "TruthfulQA":47.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"7efdff78a90132c1c66e1d27518ad7cbadffa139"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt125m_10e5_40ep",
        "Average":31.73,
        "ARC":24.23,
        "HellaSwag":29.9,
        "MMLU":23.75,
        "TruthfulQA":49.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.12,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"ba153179184a0d951ce8a9434d84b3a6a091f644"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt125m_10e6_run1",
        "Average":31.71,
        "ARC":23.98,
        "HellaSwag":29.79,
        "MMLU":24.49,
        "TruthfulQA":48.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.12,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"452e86748d96bab00fa5b7a576c49e3bc66fca6c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shitshow123\/mistral7b_sft_dpo",
        "Average":31.71,
        "ARC":27.56,
        "HellaSwag":25.53,
        "MMLU":24.05,
        "TruthfulQA":49.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0191ecaf158b047b4c2f87edfcbe5c144c509d38"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Technoculture\/Medtulu-4x7B",
        "Average":31.7,
        "ARC":28.75,
        "HellaSwag":25.74,
        "MMLU":24.41,
        "TruthfulQA":47.91,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":19.73,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b9b38d1b9039038d7d4e5177884bb35300f2fdf1"
    },
    {
        "T":"?",
        "Model":"G-reen\/EXPERIMENT-DPO-m7b2-3-merged",
        "Average":31.7,
        "ARC":29.52,
        "HellaSwag":25.9,
        "MMLU":23.12,
        "TruthfulQA":48.27,
        "Type":"Unknown",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":3.86,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c1f34fad9a39b6b3e5dfad6898c8654a99ea8e8b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"marcchew\/test1",
        "Average":31.67,
        "ARC":27.65,
        "HellaSwag":26.17,
        "MMLU":24.55,
        "TruthfulQA":48.33,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7444355ad764584ef05805f58ccf174bb03e0f46"
    },
    {
        "T":"\u2b55",
        "Model":"bsp-albz\/llama2-13b-platypus-ckpt-1000",
        "Average":31.67,
        "ARC":28.16,
        "HellaSwag":26.55,
        "MMLU":23.17,
        "TruthfulQA":48.79,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"d9f3e490df2134784afc3a86f5c617a9bab8db4d"
    },
    {
        "T":"?",
        "Model":"MatthieuJ\/ING_Triomphant_M2_SLERP",
        "Average":31.67,
        "ARC":27.22,
        "HellaSwag":26.45,
        "MMLU":24.21,
        "TruthfulQA":48.79,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4962111e068a187dfe8ba76f4b612281c6496f87"
    },
    {
        "T":"\u2b55",
        "Model":"kz919\/mistral-7b-dpo-open-orca-flan-50k-synthetic-5-models",
        "Average":31.67,
        "ARC":25.51,
        "HellaSwag":25.52,
        "MMLU":26.82,
        "TruthfulQA":48.81,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"abe276881262a4571412e6b1bf545c3d61c9e49e"
    },
    {
        "T":"\u2b55",
        "Model":"Harshvir\/LaMini-Neo-1.3B-Mental-Health_lora",
        "Average":31.66,
        "ARC":25.77,
        "HellaSwag":25.67,
        "MMLU":27.0,
        "TruthfulQA":48.21,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9f1c45d5ce88a8eaf7ec03b760a4adfb5fda07eb"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"KnutJaegersberg\/internlm-20b-llamafied",
        "Average":31.66,
        "ARC":26.79,
        "HellaSwag":26.4,
        "MMLU":25.4,
        "TruthfulQA":48.06,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":20.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"f859dfb710431ad6cd7d4e8389297d0f0b196278"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"golaxy\/gogpt-560m",
        "Average":31.66,
        "ARC":26.37,
        "HellaSwag":31.86,
        "MMLU":25.29,
        "TruthfulQA":43.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.56,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"82bd8b88b95068eee614a35b790388c5d2415705"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SilverCoder66\/Mistral-7B-Instruct-adapt-vbh",
        "Average":31.66,
        "ARC":27.56,
        "HellaSwag":25.73,
        "MMLU":25.38,
        "TruthfulQA":47.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"76f61fe15cb9a1ac129d3e2980a91c9c7aaeec61"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"rishiraj\/cutie",
        "Average":31.64,
        "ARC":26.96,
        "HellaSwag":27.02,
        "MMLU":24.17,
        "TruthfulQA":48.42,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"eab22794d6cf39c945f7dc326c9785a5abf88ddd"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Hemanth-thunder\/Tamil-Mistral-7B-v0.1",
        "Average":31.64,
        "ARC":28.75,
        "HellaSwag":26.52,
        "MMLU":24.28,
        "TruthfulQA":46.99,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.39,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"35c013932c92b186eeb8597b9a5261304846f029"
    },
    {
        "T":"?",
        "Model":"TheBloke\/orca_mini_13B-GPTQ",
        "Average":31.63,
        "ARC":27.3,
        "HellaSwag":25.85,
        "MMLU":25.31,
        "TruthfulQA":48.06,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.03,
        "Hub":40,
        "Available on the hub":true,
        "Model Sha":"8ec18e5c597da86fa123c08b6e6bef7da6ec7440"
    },
    {
        "T":"?",
        "Model":"Panchovix\/airoboros-33b-gpt4-1.2-SuperHOT-8k",
        "Average":31.61,
        "ARC":24.66,
        "HellaSwag":31.23,
        "MMLU":23.13,
        "TruthfulQA":47.44,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"47c14f699cbbc9bd24458edd86eb70d87552b623"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"health360\/Healix-410M",
        "Average":31.61,
        "ARC":25.09,
        "HellaSwag":32.02,
        "MMLU":24.94,
        "TruthfulQA":44.42,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":0.35,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"df5a3cec54a0bdd22e1644bfe576c7b58eca6bfd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/tulu-7b-instruct-pl-lora_unload",
        "Average":31.61,
        "ARC":28.67,
        "HellaSwag":26.05,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"962d4e5d8da5a4ec0ec047b6f8f08f1bb9e509fe"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"codeparrot\/codeparrot",
        "Average":31.61,
        "ARC":21.67,
        "HellaSwag":28.34,
        "MMLU":25.55,
        "TruthfulQA":50.87,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.53,
        "Hub":96,
        "Available on the hub":true,
        "Model Sha":"065248a99f051da363b1c2cbf05da943c8b6211b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"JackFram\/llama-160m",
        "Average":31.6,
        "ARC":24.83,
        "HellaSwag":35.23,
        "MMLU":24.26,
        "TruthfulQA":42.08,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.16,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"aca9b687d1425f863dcf5de9a4c96e3fe36266dd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fionazhang\/mistral-environment-all",
        "Average":31.59,
        "ARC":29.44,
        "HellaSwag":25.89,
        "MMLU":23.12,
        "TruthfulQA":47.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ba2832b0dbd70860408d7786026549407c951a8a"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"team-lucid\/mptk-1b",
        "Average":31.58,
        "ARC":24.06,
        "HellaSwag":35.61,
        "MMLU":26.95,
        "TruthfulQA":39.71,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"382a746dfb0745bab2b2e63a1e6a28ba1aa3f306"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"juhwanlee\/gemma-7B-alpaca-case-2-3",
        "Average":31.58,
        "ARC":25.6,
        "HellaSwag":25.66,
        "MMLU":27.07,
        "TruthfulQA":47.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"0e6c655f2ab8185961d7babc0dbf79f6091e89e8"
    },
    {
        "T":"\u2b55",
        "Model":"Phind\/Phind-CodeLlama-34B-v2",
        "Average":31.58,
        "ARC":24.57,
        "HellaSwag":27.6,
        "MMLU":25.76,
        "TruthfulQA":48.37,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub":252,
        "Available on the hub":true,
        "Model Sha":"949f61e203f91b412efe8f679c798f09f0ff4b0c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BreadAi\/MusePy-1-2",
        "Average":31.56,
        "ARC":25.77,
        "HellaSwag":25.94,
        "MMLU":25.22,
        "TruthfulQA":49.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.04,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6c1725158a74a41a10f21696a48510d45b4b425b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt125m_10e5",
        "Average":31.56,
        "ARC":24.66,
        "HellaSwag":31.23,
        "MMLU":26.45,
        "TruthfulQA":43.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.12,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"b1cc44b598222a3657b96be755cc35c1d541549f"
    },
    {
        "T":"?",
        "Model":"cerebras\/Cerebras-GPT-590M",
        "Average":31.56,
        "ARC":23.72,
        "HellaSwag":32.4,
        "MMLU":25.97,
        "TruthfulQA":44.15,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.59,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"67a653304fd782a34906d59f3795a37f9e053397"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/tinyllama-730M-test",
        "Average":31.56,
        "ARC":25.09,
        "HellaSwag":33.82,
        "MMLU":24.43,
        "TruthfulQA":42.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.75,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"29b2c22fd13f0fb6e903f33998ba0866750854f8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Hemanth-thunder\/Tamil-Mistral-7B-Instruct-v0.1",
        "Average":31.56,
        "ARC":27.39,
        "HellaSwag":27.16,
        "MMLU":24.42,
        "TruthfulQA":47.27,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.39,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"504c4964130f0696e387e94ee6073aa565b082e9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/Tinyllama-320M-Cinder-v1",
        "Average":31.56,
        "ARC":27.73,
        "HellaSwag":29.68,
        "MMLU":24.52,
        "TruthfulQA":44.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.34,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"89290f10ce5b9fcc2a27d2e297eb244cc866da2b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"microsoft\/CodeGPT-small-py",
        "Average":31.56,
        "ARC":22.7,
        "HellaSwag":27.26,
        "MMLU":25.05,
        "TruthfulQA":51.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub":16,
        "Available on the hub":true,
        "Model Sha":"e5f31df92bfb7b7a808ea8d1c7557488e1bdff7f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt125m_10e5_10ep",
        "Average":31.56,
        "ARC":23.98,
        "HellaSwag":31.24,
        "MMLU":24.79,
        "TruthfulQA":46.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.12,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"af26f29ed2520cfe4bbb213457b956491ec68d6a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"victor123\/WizardLM-13B-1.0",
        "Average":31.55,
        "ARC":28.5,
        "HellaSwag":25.97,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":63,
        "Available on the hub":true,
        "Model Sha":"2ea86d3c02ca0c2abb086a2145e1e85eaea4a23e"
    },
    {
        "T":"?",
        "Model":"OptimalScale\/robin-65b-v2-delta",
        "Average":31.55,
        "ARC":28.5,
        "HellaSwag":25.97,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":65.02,
        "Hub":12,
        "Available on the hub":true,
        "Model Sha":"cde761c8c5e956a4d981d396f993f46971ea2cd4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/Llama-2-7b-longlora-32k-ft",
        "Average":31.54,
        "ARC":27.9,
        "HellaSwag":25.61,
        "MMLU":23.08,
        "TruthfulQA":49.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"ab48674ffc55568ffe2a1207ef0e711c2febbaaf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt125m_10e5_20ep",
        "Average":31.54,
        "ARC":25.43,
        "HellaSwag":30.84,
        "MMLU":23.39,
        "TruthfulQA":46.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.12,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"5c4954bb2595cce1256f496d2374d2dee4f79e93"
    },
    {
        "T":"\u2b55",
        "Model":"voidful\/changpt-bart",
        "Average":31.53,
        "ARC":28.67,
        "HellaSwag":26.41,
        "MMLU":23.12,
        "TruthfulQA":47.94,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.18,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e3d26f736b8b47d5275421be6133b81bef84db7d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/Llama-2-7b-longlora-100k-ft",
        "Average":31.53,
        "ARC":28.16,
        "HellaSwag":25.43,
        "MMLU":23.48,
        "TruthfulQA":49.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":51,
        "Available on the hub":true,
        "Model Sha":"242c6469cab41b41d30826e850afa4687e422f24"
    },
    {
        "T":"?",
        "Model":"NobodyExistsOnTheInternet\/clown-SUV-4x70b",
        "Average":31.51,
        "ARC":24.74,
        "HellaSwag":28.29,
        "MMLU":24.2,
        "TruthfulQA":48.81,
        "Type":"Unknown",
        "Precision":"4bit",
        "Hub License":"mit",
        "#Params (B)":238.09,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"323a18e2bc3c2fada3daefe71befe616354fd6eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Hemanth-thunder\/Tamil-Mistral-7B-Instruct-v0.1",
        "Average":31.5,
        "ARC":27.13,
        "HellaSwag":27.09,
        "MMLU":24.5,
        "TruthfulQA":47.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.39,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"504c4964130f0696e387e94ee6073aa565b082e9"
    },
    {
        "T":"?",
        "Model":"Technoculture\/PMCorca-2x13b",
        "Average":31.5,
        "ARC":27.22,
        "HellaSwag":25.94,
        "MMLU":23.12,
        "TruthfulQA":49.72,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3e32f163db0a5d25d00f9d1c8aff0a3666f2b25e"
    },
    {
        "T":"?",
        "Model":"FabbriSimo01\/GPT_Large_Quantized",
        "Average":31.48,
        "ARC":27.05,
        "HellaSwag":26.29,
        "MMLU":24.12,
        "TruthfulQA":48.46,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"unknown",
        "#Params (B)":0.77,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c2df1904aa18de22d03ba0fee925e831d8468898"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MayaPH\/FinOPT-Lincoln",
        "Average":31.47,
        "ARC":26.71,
        "HellaSwag":25.6,
        "MMLU":23.0,
        "TruthfulQA":50.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":0.33,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"7ddc381fa3968df22f72acb6cf03b75d3ac49661"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/fairseq-dense-125M",
        "Average":31.47,
        "ARC":24.06,
        "HellaSwag":34.14,
        "MMLU":23.98,
        "TruthfulQA":43.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"c8fb975220512b34e7b4a9fc570ca333ddcaf9b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SebastianSchramm\/Cerebras-GPT-111M-instruction",
        "Average":31.44,
        "ARC":24.4,
        "HellaSwag":26.05,
        "MMLU":25.87,
        "TruthfulQA":49.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.11,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"09f1ec782ae2243fc605b24eb13ec8d5e4fd2734"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"TheTravellingEngineer\/bloom-560m-RLHF",
        "Average":31.44,
        "ARC":24.4,
        "HellaSwag":36.96,
        "MMLU":23.63,
        "TruthfulQA":40.76,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.56,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b1769e92f325d8a28e7db1c21f133e6c85b84e78"
    },
    {
        "T":"\u2b55",
        "Model":"nicholasKluge\/Aira-2-1B1",
        "Average":31.42,
        "ARC":23.21,
        "HellaSwag":26.97,
        "MMLU":24.86,
        "TruthfulQA":50.63,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"a53eb20b72ae86441566f99acc204d9bb527bf32"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zelus82\/Asterix-B7",
        "Average":31.41,
        "ARC":28.16,
        "HellaSwag":25.65,
        "MMLU":24.59,
        "TruthfulQA":47.24,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2885da230c05730dff0501b95aa18c533b63017e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mavihsrr\/GetCode-slerp",
        "Average":31.41,
        "ARC":26.54,
        "HellaSwag":26.2,
        "MMLU":23.12,
        "TruthfulQA":49.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"c4e9a5c09be34872e7a1db125d851ae1210d15ff"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bit-dny\/MindLLM",
        "Average":31.38,
        "ARC":22.44,
        "HellaSwag":34.11,
        "MMLU":25.5,
        "TruthfulQA":43.48,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.37,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"b3554c83555a098c94b626c3ab67247bfd024fb5"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Dans-DiscountModels\/TinyMistral-v2.5-MiniPile-Guidelines-E1",
        "Average":31.38,
        "ARC":26.54,
        "HellaSwag":25.65,
        "MMLU":23.44,
        "TruthfulQA":49.9,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"11a6744bc71fa05bc14e0944001c7a5c318440f0"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Dans-DiscountModels\/TinyMistral-v2.5-MiniPile-Guidelines-E1",
        "Average":31.38,
        "ARC":26.45,
        "HellaSwag":25.68,
        "MMLU":23.53,
        "TruthfulQA":49.85,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"11a6744bc71fa05bc14e0944001c7a5c318440f0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"juhwanlee\/gemma-7B-alpaca-case-1-3",
        "Average":31.38,
        "ARC":25.0,
        "HellaSwag":27.42,
        "MMLU":24.03,
        "TruthfulQA":49.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"22010813444e2fe8244ae5e5313489b61a5b12ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/Llama-2-13b-longlora-16k-ft",
        "Average":31.36,
        "ARC":25.85,
        "HellaSwag":27.6,
        "MMLU":23.1,
        "TruthfulQA":48.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"5f0cfdef590fc9bd7642042fb5f1ed9679260b93"
    },
    {
        "T":"?",
        "Model":"monology\/mixtral-ties",
        "Average":31.36,
        "ARC":26.45,
        "HellaSwag":26.19,
        "MMLU":24.05,
        "TruthfulQA":48.75,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"921017cf0f80cdaf03434dc90be294d05d1ad7cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HWERI\/pythia-70m-deduped-cleansharegpt",
        "Average":31.34,
        "ARC":25.68,
        "HellaSwag":25.4,
        "MMLU":23.12,
        "TruthfulQA":51.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.07,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6ea42abd94cb0017918f6fe5e71d78bcb7c75548"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zelus82\/JuliusCesar-72B-BeyonderV.0",
        "Average":31.32,
        "ARC":26.02,
        "HellaSwag":26.24,
        "MMLU":23.12,
        "TruthfulQA":49.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":37.39,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"07854414580231d4048de6cd7e1723425c1961db"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"postbot\/pythia-160m-hq-emails",
        "Average":31.31,
        "ARC":23.12,
        "HellaSwag":30.05,
        "MMLU":26.58,
        "TruthfulQA":45.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.21,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"6eeded627780b47b5221ed72ebea436514621964"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"breadlicker45\/dough-instruct-base-001",
        "Average":31.29,
        "ARC":23.89,
        "HellaSwag":24.76,
        "MMLU":23.13,
        "TruthfulQA":53.4,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.19,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3e1b0bf0a887feeb342982eee4f6d8041772a7dd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"breadlicker45\/dough-base-001",
        "Average":31.29,
        "ARC":23.89,
        "HellaSwag":24.76,
        "MMLU":23.13,
        "TruthfulQA":53.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.15,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e42b65191f97d786eadaba450f1d34baea470734"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WizardLM\/WizardLM-30B-V1.0",
        "Average":31.27,
        "ARC":27.39,
        "HellaSwag":25.94,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":68,
        "Available on the hub":true,
        "Model Sha":"815e2dd7daabe446c429f3c9f70ef01582528f81"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardLM-30B-V1.0",
        "Average":31.27,
        "ARC":27.39,
        "HellaSwag":25.94,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":68,
        "Available on the hub":true,
        "Model Sha":"815e2dd7daabe446c429f3c9f70ef01582528f81"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WizardLM\/WizardLM-30B-V1.0",
        "Average":31.27,
        "ARC":27.39,
        "HellaSwag":25.94,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub":68,
        "Available on the hub":true,
        "Model Sha":"815e2dd7daabe446c429f3c9f70ef01582528f81"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"yyjjtt\/test-model",
        "Average":31.26,
        "ARC":24.4,
        "HellaSwag":30.17,
        "MMLU":25.88,
        "TruthfulQA":44.59,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":1.04,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3ea8330f61a47f16861415359f09ff0c6a210f27"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TaylorAI\/Flash-Llama-30M-20001",
        "Average":31.26,
        "ARC":23.89,
        "HellaSwag":25.76,
        "MMLU":24.09,
        "TruthfulQA":51.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.02,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6ff84442217565875450bd7a0457121dcedf6b0b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"BEE-spoke-data\/smol_llama-220M-GQA",
        "Average":31.25,
        "ARC":24.83,
        "HellaSwag":29.76,
        "MMLU":25.85,
        "TruthfulQA":44.55,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.22,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"2d144b9a69b3620110e8a14790d383076ac87925"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"porkorbeef\/Llama-2-13b-12_153950",
        "Average":31.25,
        "ARC":28.58,
        "HellaSwag":26.58,
        "MMLU":20.79,
        "TruthfulQA":49.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ee9b0cf26f521b5cb2322d743880e8b6bfadb0b7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shahzebnaveed\/codeparrot-ds",
        "Average":31.24,
        "ARC":25.26,
        "HellaSwag":25.75,
        "MMLU":23.11,
        "TruthfulQA":50.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8b2a61c278488b60d12f574e3086d895c3635df6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"huggingtweets\/bladeecity-jerma985",
        "Average":31.24,
        "ARC":22.87,
        "HellaSwag":30.53,
        "MMLU":26.56,
        "TruthfulQA":44.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9bf3a0db7f6bc960c51f2c0dc6fb66ed982b0180"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/megatron-GPT-2-345m-EvolInstruct",
        "Average":31.23,
        "ARC":24.06,
        "HellaSwag":35.12,
        "MMLU":24.48,
        "TruthfulQA":41.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.38,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"dc95fda9f1e51d94870e28751e35410c66563d18"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/Llama-2-13b-chat-longlora-32k-sft",
        "Average":31.23,
        "ARC":26.54,
        "HellaSwag":26.1,
        "MMLU":23.12,
        "TruthfulQA":49.16,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":22,
        "Available on the hub":true,
        "Model Sha":"6f2924e354c3ab035aa2ff7c7e28d0e5327e2667"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt125m_10e5_50ep",
        "Average":31.23,
        "ARC":23.89,
        "HellaSwag":28.98,
        "MMLU":23.74,
        "TruthfulQA":48.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.12,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"93dec43baa7693b669be5ddb24ce8909ffcad21d"
    },
    {
        "T":"\u2b55",
        "Model":"BEE-spoke-data\/smol_llama-220M-open_instruct",
        "Average":31.22,
        "ARC":25.0,
        "HellaSwag":29.71,
        "MMLU":26.11,
        "TruthfulQA":44.06,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.22,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"6d4735f86c74c881857659efb7d981c5f50bee77"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bhenrym14\/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16",
        "Average":31.22,
        "ARC":25.34,
        "HellaSwag":26.66,
        "MMLU":23.36,
        "TruthfulQA":49.51,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":32.53,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"468225a547a8cb0a62758d813cf9606b58506ab4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"microsoft\/DialoGPT-small",
        "Average":31.21,
        "ARC":25.77,
        "HellaSwag":25.79,
        "MMLU":25.81,
        "TruthfulQA":47.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.18,
        "Hub":54,
        "Available on the hub":true,
        "Model Sha":"97d0fec744c2cb4d48f5db51d17e3258e185858e"
    },
    {
        "T":"?",
        "Model":"Corianas\/590m",
        "Average":31.21,
        "ARC":24.15,
        "HellaSwag":31.91,
        "MMLU":26.61,
        "TruthfulQA":42.19,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.67,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ec721c97ef0e6ebfc578ab98b3ff6e2bd19b3e27"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/xglm-564M",
        "Average":31.21,
        "ARC":24.57,
        "HellaSwag":34.64,
        "MMLU":25.18,
        "TruthfulQA":40.43,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.56,
        "Hub":26,
        "Available on the hub":true,
        "Model Sha":"f3059f01b98ccc877c673149e0178c0e957660f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishek\/hepu-o4zf-ravz-7-0",
        "Average":31.2,
        "ARC":24.49,
        "HellaSwag":25.36,
        "MMLU":23.27,
        "TruthfulQA":51.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b73d869edfc259dea27c15d06cf65ee08ec3c2c7"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/gpt-neo-125m",
        "Average":31.19,
        "ARC":22.95,
        "HellaSwag":30.26,
        "MMLU":25.97,
        "TruthfulQA":45.58,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.15,
        "Hub":124,
        "Available on the hub":true,
        "Model Sha":"6cb0d322a3a484e99667e7cb240e22f1ac036b99"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-160m-deduped",
        "Average":31.16,
        "ARC":24.06,
        "HellaSwag":31.39,
        "MMLU":24.86,
        "TruthfulQA":44.34,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.21,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"582159a2dfe3e712a8d47ae83dec95ae3bde8e7e"
    },
    {
        "T":"?",
        "Model":"TW3PartnersLLM\/TW3-v1-AlpacaSmaug-30B",
        "Average":31.16,
        "ARC":26.96,
        "HellaSwag":26.11,
        "MMLU":23.11,
        "TruthfulQA":48.45,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":30.0,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"789045861027df1adf1c4d5ae5cde11f534de35e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"abhinand\/mistral7b-test001",
        "Average":31.16,
        "ARC":24.66,
        "HellaSwag":26.78,
        "MMLU":23.12,
        "TruthfulQA":50.07,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"52d285a1d9bdd52e50a4cd10b9de43f2f4332517"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"klosax\/pythia-160m-deduped-step92k-193bt",
        "Average":31.15,
        "ARC":24.23,
        "HellaSwag":32.33,
        "MMLU":24.54,
        "TruthfulQA":43.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9eac24dad1bd7194e38ce8083a0197cee456456c"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"amazingvince\/zephyr-smol_llama-100m-dpo-full",
        "Average":31.12,
        "ARC":25.0,
        "HellaSwag":28.54,
        "MMLU":25.18,
        "TruthfulQA":45.75,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.1,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"be3400c89d66ed66f0aa96f1b8131604c118b67b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/Llama-2-13b-chat-longlora-32k-sft",
        "Average":31.12,
        "ARC":26.11,
        "HellaSwag":26.17,
        "MMLU":23.12,
        "TruthfulQA":49.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub":22,
        "Available on the hub":true,
        "Model Sha":"6f2924e354c3ab035aa2ff7c7e28d0e5327e2667"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"euclaise\/crow-1b",
        "Average":31.12,
        "ARC":25.51,
        "HellaSwag":25.87,
        "MMLU":24.8,
        "TruthfulQA":48.28,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"dbbcb8892474ce1571297eb68b6c1ef971fa0cf8"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"BEE-spoke-data\/zephyr-220m-dpo-full",
        "Average":31.11,
        "ARC":25.43,
        "HellaSwag":29.15,
        "MMLU":26.43,
        "TruthfulQA":43.44,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.22,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"58b624e62557ea56b525ead061b6bd92dae37970"
    },
    {
        "T":"?",
        "Model":"Aratako\/Qwen1.5-MoE-2x7B",
        "Average":31.11,
        "ARC":26.02,
        "HellaSwag":25.69,
        "MMLU":24.24,
        "TruthfulQA":48.5,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":12.05,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"ba7ff76107af47b4c458e7dac4b9c2e77845f546"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/lora_opt125m_10e5",
        "Average":31.11,
        "ARC":22.78,
        "HellaSwag":31.22,
        "MMLU":25.18,
        "TruthfulQA":45.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.12,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"b9b45183b73c2ce10092d55e710e1e31b8463620"
    },
    {
        "T":"?",
        "Model":"ByteWave\/Yi-8B-Llama",
        "Average":31.1,
        "ARC":25.68,
        "HellaSwag":26.79,
        "MMLU":24.14,
        "TruthfulQA":47.79,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-2.0",
        "#Params (B)":8.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4f3f4d73ff3962487d1c51702b02d795bf1f33a4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MBZUAI\/lamini-neo-125m",
        "Average":31.1,
        "ARC":24.57,
        "HellaSwag":30.22,
        "MMLU":26.74,
        "TruthfulQA":42.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.12,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"f01e73ba67da96f6645be3067158cc493b0cbbcb"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"princeton-nlp\/Sheared-Pythia-160m",
        "Average":31.1,
        "ARC":22.44,
        "HellaSwag":32.07,
        "MMLU":26.65,
        "TruthfulQA":43.22,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"c8889f496254bae7b6196dfd64521e1581eb5567"
    },
    {
        "T":"?",
        "Model":"Felladrin\/TinyMistral-248M-SFT-v3",
        "Average":31.07,
        "ARC":25.68,
        "HellaSwag":25.31,
        "MMLU":24.41,
        "TruthfulQA":48.87,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.25,
        "Hub":25,
        "Available on the hub":false,
        "Model Sha":"7a4787dfed21a432924d24575e6c65a97e1dd98a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/Llama-2-7b-longlora-16k-ft",
        "Average":31.06,
        "ARC":26.37,
        "HellaSwag":26.37,
        "MMLU":23.75,
        "TruthfulQA":47.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c86de31b80866d047e680e08dbd3572e2965d4c5"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigcode\/tiny_starcoder_py",
        "Average":31.06,
        "ARC":20.99,
        "HellaSwag":28.77,
        "MMLU":26.79,
        "TruthfulQA":47.68,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":0.16,
        "Hub":55,
        "Available on the hub":true,
        "Model Sha":"8547527bef0bc927268c1653cce6948c5c242dd1"
    },
    {
        "T":"?",
        "Model":"Isotonic\/smol_llama-4x220M-MoE",
        "Average":31.03,
        "ARC":25.09,
        "HellaSwag":29.24,
        "MMLU":25.88,
        "TruthfulQA":43.92,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.6,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"5d2a0bcaa25ae455a8111a385c95b3827c972e26"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MaziyarPanahi\/UNA-34Beagles-32K-bf16-v1-GPTQ",
        "Average":31.02,
        "ARC":26.11,
        "HellaSwag":26.29,
        "MMLU":24.43,
        "TruthfulQA":47.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.4,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"7094ef1fc4e032cf2c03a2f43f3db5e814bce318"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"buildingthemoon\/testfinetunedmodel",
        "Average":31.02,
        "ARC":25.85,
        "HellaSwag":31.4,
        "MMLU":26.07,
        "TruthfulQA":40.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9efeae0561a9af68ea7f9b26c5184838760372bc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Minami-su\/Qwen1.5-0.5B-Chat_mistral",
        "Average":31.02,
        "ARC":25.51,
        "HellaSwag":26.41,
        "MMLU":23.08,
        "TruthfulQA":49.06,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.5,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"09a21a368d0bf4a81b22772948b74f13fa066c26"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"concedo\/OPT-19M-ChatSalad",
        "Average":31.01,
        "ARC":24.4,
        "HellaSwag":25.15,
        "MMLU":23.12,
        "TruthfulQA":51.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.02,
        "Hub":14,
        "Available on the hub":true,
        "Model Sha":"3930ca6bf3976e9b603815403cb373398ae509e5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"anton-l\/gpt-j-tiny-random",
        "Average":31.01,
        "ARC":26.37,
        "HellaSwag":25.76,
        "MMLU":24.46,
        "TruthfulQA":47.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.05,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"feea91564dac0081f73aeb6744979c6cfe553fff"
    },
    {
        "T":"\u2b55",
        "Model":"Felladrin\/Llama-68M-Chat-v1",
        "Average":31.0,
        "ARC":23.29,
        "HellaSwag":28.27,
        "MMLU":25.18,
        "TruthfulQA":47.27,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.07,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"f60895b5cf4e4f2c9387c6c851a4f6691c40ce95"
    },
    {
        "T":"?",
        "Model":"grantprice\/Cerebras-GPT-590M-finetuned-DND",
        "Average":30.99,
        "ARC":24.74,
        "HellaSwag":27.84,
        "MMLU":23.12,
        "TruthfulQA":48.26,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.59,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a0a2fbe342cdc86433913ba5f96978e4703ff672"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"BEE-spoke-data\/zephyr-220m-sft-full",
        "Average":30.99,
        "ARC":25.26,
        "HellaSwag":29.03,
        "MMLU":26.45,
        "TruthfulQA":43.23,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.22,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"49f3c45163e7eb65b9b9deb971f1f69424d5d261"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"deepnight-research\/zsc-text",
        "Average":30.98,
        "ARC":26.71,
        "HellaSwag":25.76,
        "MMLU":23.12,
        "TruthfulQA":48.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9b1c704ac76968dbd61597c22610084b975ef576"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"anas-awadalla\/mpt-1b-redpajama-200b",
        "Average":30.98,
        "ARC":25.77,
        "HellaSwag":26.08,
        "MMLU":24.5,
        "TruthfulQA":47.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.0,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"fc98636655efb7c091bbe5d8014eb138ddfc5471"
    },
    {
        "T":"?",
        "Model":"WangZeJun\/bloom-820m-chat",
        "Average":30.96,
        "ARC":23.38,
        "HellaSwag":34.16,
        "MMLU":25.98,
        "TruthfulQA":40.32,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.75,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"f98b1f9c1bd358dd837d05d443d992c495497606"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cerebras\/Cerebras-GPT-256M",
        "Average":30.95,
        "ARC":22.01,
        "HellaSwag":28.99,
        "MMLU":26.83,
        "TruthfulQA":45.98,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.26,
        "Hub":20,
        "Available on the hub":true,
        "Model Sha":"d77812ac95aece1f1edef6745ae2a1b325ad01a4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yash21\/TinyYi-7b-Test",
        "Average":30.94,
        "ARC":26.88,
        "HellaSwag":26.14,
        "MMLU":24.41,
        "TruthfulQA":46.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.06,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e680a6b8244e9a4871aa419e2faca079d4f42381"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yash21\/TinyYi-7B-Test",
        "Average":30.94,
        "ARC":26.88,
        "HellaSwag":26.14,
        "MMLU":24.41,
        "TruthfulQA":46.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.06,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"e680a6b8244e9a4871aa419e2faca079d4f42381"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt125m_10e2",
        "Average":30.94,
        "ARC":23.21,
        "HellaSwag":31.41,
        "MMLU":26.55,
        "TruthfulQA":42.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.12,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"8b738dcf7a2e38fc2e014dfc1c50f5ca30acccbf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ai-forever\/rugpt3large_based_on_gpt2",
        "Average":30.93,
        "ARC":22.61,
        "HellaSwag":32.84,
        "MMLU":24.9,
        "TruthfulQA":43.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.76,
        "Hub":54,
        "Available on the hub":true,
        "Model Sha":"8201db0de8deb68f25e7309db04d163b71970494"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"alibidaran\/medical_transcription_generator",
        "Average":30.93,
        "ARC":22.78,
        "HellaSwag":30.6,
        "MMLU":23.84,
        "TruthfulQA":46.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.14,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"f622239151c89c2db0f1cef495d1b42afd16ce64"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Deci\/DeciCoder-1b",
        "Average":30.91,
        "ARC":21.16,
        "HellaSwag":31.09,
        "MMLU":24.34,
        "TruthfulQA":47.05,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.11,
        "Hub":203,
        "Available on the hub":true,
        "Model Sha":"af2ef45ef8cbe82eb7eb4074f260412bc14c7b11"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt125m_10e5_1ep",
        "Average":30.91,
        "ARC":23.46,
        "HellaSwag":30.9,
        "MMLU":26.73,
        "TruthfulQA":42.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.12,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"fc31db73f21f8ca07e1a72ffab8684a00f99cfc8"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2-dolly",
        "Average":30.91,
        "ARC":22.7,
        "HellaSwag":30.15,
        "MMLU":25.81,
        "TruthfulQA":44.97,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7e75e6f4626437305e4d3e7b2aa36f617c517247"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"microsoft\/DialoGPT-medium",
        "Average":30.9,
        "ARC":24.49,
        "HellaSwag":26.21,
        "MMLU":25.84,
        "TruthfulQA":47.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.36,
        "Hub":227,
        "Available on the hub":true,
        "Model Sha":"9d5c5fadcc072b693fb5a5e29416bbf3f503c26c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ogimgio\/gpt-neo-125m-neurallinguisticpioneers",
        "Average":30.9,
        "ARC":22.44,
        "HellaSwag":30.36,
        "MMLU":25.14,
        "TruthfulQA":45.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"515fd7753c5fecbf4a2951f7cebb2846d91324b3"
    },
    {
        "T":"?",
        "Model":"open-llm-leaderboard\/bloom-560m-4bit-alpaca-auto-eval-adapter-applied",
        "Average":30.89,
        "ARC":23.98,
        "HellaSwag":29.21,
        "MMLU":25.2,
        "TruthfulQA":45.18,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.56,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"61e0b861d59319a96bba5af8c246e69d82e8e6e6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BreadAi\/gpt-YA-1-1_160M",
        "Average":30.88,
        "ARC":22.95,
        "HellaSwag":27.29,
        "MMLU":26.25,
        "TruthfulQA":47.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b9b3577df726f7984721e4d73741296db50fa782"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pankajmathur\/Mistral-7B-model_45k6e2e4",
        "Average":30.86,
        "ARC":24.32,
        "HellaSwag":25.08,
        "MMLU":23.19,
        "TruthfulQA":50.85,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"71d9cfd1aae4dfaebc74cab19b1d7a1da77ff390"
    },
    {
        "T":"\u2b55",
        "Model":"BEE-spoke-data\/smol_llama-220M-openhermes",
        "Average":30.85,
        "ARC":25.17,
        "HellaSwag":28.98,
        "MMLU":26.17,
        "TruthfulQA":43.08,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.22,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"356848c3ced75332f875abf0896e0157a33abd8e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt125m_10e4",
        "Average":30.85,
        "ARC":22.95,
        "HellaSwag":30.9,
        "MMLU":26.66,
        "TruthfulQA":42.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.12,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"502d29fa88d682b5cc0bbd44e5815c2d0c955f3f"
    },
    {
        "T":"?",
        "Model":"dfurman\/MoMoMerge-72B-v0.1",
        "Average":30.84,
        "ARC":26.28,
        "HellaSwag":25.27,
        "MMLU":23.08,
        "TruthfulQA":48.73,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":72.0,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"43aa890e6e85a40a1d0a967bca6f7f0c7fd409f9"
    },
    {
        "T":"?",
        "Model":"Cartinoe5930\/DARE-Merging",
        "Average":30.84,
        "ARC":25.26,
        "HellaSwag":26.11,
        "MMLU":23.68,
        "TruthfulQA":48.31,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"bd5072000c2d7db7c72ec8286d79e00671188605"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"chargoddard\/SmolLlamix-8x101M-take2",
        "Average":30.84,
        "ARC":23.98,
        "HellaSwag":28.43,
        "MMLU":25.07,
        "TruthfulQA":45.87,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.4,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"c9f73e5f63546ca506bbae944ab546a8d8e42d24"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"BEE-spoke-data\/NanoLlama-GQA-L10-A32_KV8-v13-KI",
        "Average":30.83,
        "ARC":23.81,
        "HellaSwag":29.39,
        "MMLU":25.37,
        "TruthfulQA":44.77,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.22,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"29fc3a802ee639be914d2a54fa6d9f595036ecf2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"juhwanlee\/gemma-7B-alpaca-case-0-3",
        "Average":30.83,
        "ARC":27.3,
        "HellaSwag":27.59,
        "MMLU":24.7,
        "TruthfulQA":43.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":8.54,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"3e57e848c73f812b324109ffea29e60443979d4f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FINDA-FIT\/llama-r",
        "Average":30.82,
        "ARC":21.59,
        "HellaSwag":30.18,
        "MMLU":26.13,
        "TruthfulQA":45.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.69,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6bdde9a227da60c2db803024d5b2e3a53a41cf0b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"microsoft\/DialoGPT-large",
        "Average":30.81,
        "ARC":23.38,
        "HellaSwag":25.77,
        "MMLU":23.81,
        "TruthfulQA":50.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.77,
        "Hub":200,
        "Available on the hub":true,
        "Model Sha":"04e3e47b52dadbcf7688aa61a7ed0438ecf9184c"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/opt-125m",
        "Average":30.8,
        "ARC":22.87,
        "HellaSwag":31.47,
        "MMLU":26.02,
        "TruthfulQA":42.87,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.12,
        "Hub":73,
        "Available on the hub":true,
        "Model Sha":"3d2b5f275bdf882b8775f902e1bfdb790e2cfc32"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"pszemraj\/pythia-31m-goodwiki-deduped-2048-scratch",
        "Average":30.8,
        "ARC":23.12,
        "HellaSwag":25.66,
        "MMLU":23.11,
        "TruthfulQA":51.32,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"01a3cd918dd7c233bc0c3c0c948a9a462a5359d1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Facebook\/OPT-125M",
        "Average":30.8,
        "ARC":22.87,
        "HellaSwag":31.44,
        "MMLU":26.01,
        "TruthfulQA":42.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.12,
        "Hub":109,
        "Available on the hub":true,
        "Model Sha":"27dcfa74d334bc871f3234de431e71c6eeba5dd6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kodonho\/Momo-70b-DPO-mixed",
        "Average":30.79,
        "ARC":26.28,
        "HellaSwag":24.98,
        "MMLU":23.06,
        "TruthfulQA":48.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":72.29,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7d93ee866df83b1924289512cc6c07c3d8800b43"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"huggingtweets\/jerma985",
        "Average":30.79,
        "ARC":21.67,
        "HellaSwag":30.91,
        "MMLU":26.57,
        "TruthfulQA":44.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"816206ad02a397161be78dcb70eeda67e0c53132"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"pszemraj\/pythia-31m-KI_v1-2048-scratch",
        "Average":30.79,
        "ARC":23.12,
        "HellaSwag":25.23,
        "MMLU":23.12,
        "TruthfulQA":51.67,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b29a3229f8d5317adeabafeb20677ec7bea9d703"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TurkuNLP\/gpt3-finnish-large",
        "Average":30.78,
        "ARC":21.76,
        "HellaSwag":32.88,
        "MMLU":24.11,
        "TruthfulQA":44.35,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.88,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"b9a3dd97387fc70d07010d469888a918842d3449"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BFauber\/opt125m_10e3",
        "Average":30.76,
        "ARC":22.87,
        "HellaSwag":31.01,
        "MMLU":26.66,
        "TruthfulQA":42.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.12,
        "Hub":30,
        "Available on the hub":false,
        "Model Sha":"1b7a1e2cce264be61f67360011b4a85824e27caa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cloudyu\/Qwen-72Bx2-MoE-120B",
        "Average":30.76,
        "ARC":25.94,
        "HellaSwag":24.91,
        "MMLU":23.27,
        "TruthfulQA":48.91,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":72.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"8074fa7f9d97775efe3bcb8b11c04cdcbf3a9810"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"pszemraj\/pythia-31m-simplepile-lite-2048-scratch-2e",
        "Average":30.75,
        "ARC":21.59,
        "HellaSwag":25.79,
        "MMLU":24.99,
        "TruthfulQA":50.62,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"91f011eb99502e667ebc2803f354ce5f5209ccf1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"binbi\/SF-72B-V1.8.6-V1.2",
        "Average":30.74,
        "ARC":26.28,
        "HellaSwag":24.87,
        "MMLU":23.03,
        "TruthfulQA":48.78,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":72.29,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f894446c80611e3fc174e4cf3af0e149a316b9bb"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"binbi\/SF-72B-V1",
        "Average":30.74,
        "ARC":26.28,
        "HellaSwag":24.87,
        "MMLU":23.03,
        "TruthfulQA":48.78,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":72.29,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"39e00bb5cbebecb7b62f3b696423127e6ca5283b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mzio\/hedgehog-mistral_7b-alpaca_clean-smd_lora_1e_3",
        "Average":30.73,
        "ARC":23.29,
        "HellaSwag":25.47,
        "MMLU":23.5,
        "TruthfulQA":50.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.28,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"43f7dc636fbd7811c93d6123c5637db9701a6bb5"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"ethzanalytics\/pythia-31m",
        "Average":30.73,
        "ARC":21.84,
        "HellaSwag":27.0,
        "MMLU":24.97,
        "TruthfulQA":49.1,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"eeea0b6b80603d162fe7de4e80a5bf4a8e9c6207"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"playdev7\/theseed-v0.3",
        "Average":30.72,
        "ARC":25.94,
        "HellaSwag":26.05,
        "MMLU":24.55,
        "TruthfulQA":46.33,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"mit",
        "#Params (B)":24.37,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"545fd9e47d92b243c42b521a64596f114c961b3f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Locutusque\/gpt2-conversational-or-qa",
        "Average":30.71,
        "ARC":21.42,
        "HellaSwag":27.61,
        "MMLU":26.51,
        "TruthfulQA":47.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":0.14,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"f881c740c82ee9bc3191b886ad53f18d741960ea"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"concedo\/Pythia-70M-ChatSalad",
        "Average":30.7,
        "ARC":20.99,
        "HellaSwag":27.28,
        "MMLU":24.78,
        "TruthfulQA":49.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.1,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"692289413c47c219cf83b1596783a8e9223541eb"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"crumb\/nano-mistral",
        "Average":30.69,
        "ARC":21.67,
        "HellaSwag":28.52,
        "MMLU":25.16,
        "TruthfulQA":47.42,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.17,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c988b8c8f84d863f9155e924884169081fbde2b8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibivibiv\/orthorus-125b-moe-v2",
        "Average":30.68,
        "ARC":26.28,
        "HellaSwag":25.17,
        "MMLU":22.79,
        "TruthfulQA":48.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":125.0,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"4e6706454e0db6b216ab81c7a9a918834e289f19"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"OEvortex\/HelpingAI-110M",
        "Average":30.68,
        "ARC":22.78,
        "HellaSwag":28.02,
        "MMLU":23.66,
        "TruthfulQA":48.25,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.11,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"197c0107aed8e7e14d300ff4d72478b377929323"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MysteriousAI\/Mia-001",
        "Average":30.68,
        "ARC":22.78,
        "HellaSwag":28.02,
        "MMLU":23.66,
        "TruthfulQA":48.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.11,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"4d1cce84c2d161c634ad83548ce7fb2570d5d12e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Thytu\/phi-2-audio-super",
        "Average":30.67,
        "ARC":23.46,
        "HellaSwag":26.58,
        "MMLU":23.12,
        "TruthfulQA":49.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.78,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"28fffd749751b67637a5fe22288ffe9cedf5610f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"roneneldan\/TinyStories-1M",
        "Average":30.67,
        "ARC":23.46,
        "HellaSwag":25.23,
        "MMLU":24.57,
        "TruthfulQA":49.4,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub":15,
        "Available on the hub":true,
        "Model Sha":"8cd14d5339178f1b285f55baee14a0deff7103ac"
    },
    {
        "T":"?",
        "Model":"TW3PartnersLLM\/TW3-v2-AlpacaSmaug-72B",
        "Average":30.66,
        "ARC":25.77,
        "HellaSwag":25.23,
        "MMLU":23.0,
        "TruthfulQA":48.65,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":72.0,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"d251bc926a483153b466beeced045dbdad699a2c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BreadAi\/DiscordPy",
        "Average":30.66,
        "ARC":23.29,
        "HellaSwag":26.15,
        "MMLU":25.04,
        "TruthfulQA":48.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.26,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a5405585aec0b60c5de7d942ccd58421fe9239be"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigcode\/gpt_bigcode-santacoder",
        "Average":30.65,
        "ARC":21.16,
        "HellaSwag":30.84,
        "MMLU":24.97,
        "TruthfulQA":45.64,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":1.12,
        "Hub":21,
        "Available on the hub":true,
        "Model Sha":"291931872cae83498cf984b16319f47f5e9e7a07"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Tincando\/fiction_story_generator",
        "Average":30.62,
        "ARC":23.29,
        "HellaSwag":28.68,
        "MMLU":26.72,
        "TruthfulQA":43.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"377b080cf96e10d50289aa3e1fd79c330265f45a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"klosax\/pythia-70m-deduped-step44k-92bt",
        "Average":30.61,
        "ARC":22.1,
        "HellaSwag":28.21,
        "MMLU":26.03,
        "TruthfulQA":46.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.04,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"aac86fff08965d84d8bfc3e7c14559d48b8c4c99"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Quake24\/easyTermsSummerizer",
        "Average":30.6,
        "ARC":25.77,
        "HellaSwag":25.81,
        "MMLU":23.12,
        "TruthfulQA":47.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.41,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8df9f96cc14be8f681c40bd1672b3f3540b70e31"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BreadAi\/gpt-YA-1-1_70M",
        "Average":30.59,
        "ARC":22.53,
        "HellaSwag":27.37,
        "MMLU":25.38,
        "TruthfulQA":47.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.04,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"218e8da522cf6fb5566314f37624f27412ae2259"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"wtang06\/mpt-125m-c4",
        "Average":30.59,
        "ARC":22.18,
        "HellaSwag":26.41,
        "MMLU":24.68,
        "TruthfulQA":49.08,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"f13efec5c8498cb52998eb9ed347207f077b5f9d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BEE-spoke-data\/smol_llama-101M-GQA",
        "Average":30.58,
        "ARC":23.46,
        "HellaSwag":28.73,
        "MMLU":24.35,
        "TruthfulQA":45.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.1,
        "Hub":19,
        "Available on the hub":true,
        "Model Sha":"cac68b3377fd0a1eb1aca92a2e661d81f59d8b08"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-160m",
        "Average":30.58,
        "ARC":22.78,
        "HellaSwag":30.34,
        "MMLU":24.95,
        "TruthfulQA":44.26,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.21,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"50f5173d932e8e61f858120bcb800b97af589f46"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"BEE-spoke-data\/smol_llama-101M-GQA",
        "Average":30.58,
        "ARC":23.55,
        "HellaSwag":28.77,
        "MMLU":24.24,
        "TruthfulQA":45.76,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.1,
        "Hub":19,
        "Available on the hub":true,
        "Model Sha":"cac68b3377fd0a1eb1aca92a2e661d81f59d8b08"
    },
    {
        "T":"?",
        "Model":"Felladrin\/Smol-Llama-101M-Chat-v1",
        "Average":30.57,
        "ARC":22.87,
        "HellaSwag":28.71,
        "MMLU":24.93,
        "TruthfulQA":45.76,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.1,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"b7c10b0e04ef6f9811ac7f57b3a947546d288eea"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"euclaise\/gpt-neox-122m-minipile-digits",
        "Average":30.56,
        "ARC":20.73,
        "HellaSwag":27.03,
        "MMLU":25.31,
        "TruthfulQA":49.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc0-1.0",
        "#Params (B)":0.17,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"3e9187385d31234b04021ddc8b03cbd5cfef9fb4"
    },
    {
        "T":"?",
        "Model":"MBZUAI\/lamini-cerebras-590m",
        "Average":30.55,
        "ARC":24.32,
        "HellaSwag":31.58,
        "MMLU":25.57,
        "TruthfulQA":40.72,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.59,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"bab37eb7ba63f6ff9f0eb36a85727146b82ae5ed"
    },
    {
        "T":"\u2b55",
        "Model":"nicholasKluge\/Aira-124M",
        "Average":30.54,
        "ARC":24.57,
        "HellaSwag":31.29,
        "MMLU":25.29,
        "TruthfulQA":41.02,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"0c0d509ec9ce057e7b506e15c868eecf79cc8ae5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HWERI\/pythia-70m-deduped-cleansharegpt-en",
        "Average":30.53,
        "ARC":21.16,
        "HellaSwag":27.16,
        "MMLU":25.24,
        "TruthfulQA":48.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.04,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"a97ff56bc68a81a9f6147f1590e53511246d1040"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MayaPH\/FinOPT-Washington",
        "Average":30.51,
        "ARC":25.17,
        "HellaSwag":26.25,
        "MMLU":24.83,
        "TruthfulQA":45.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":0.12,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"cdd8a6cde7902de39757cf31d73af1f51df0d8e8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cyberagent\/open-calm-large",
        "Average":30.51,
        "ARC":20.73,
        "HellaSwag":29.56,
        "MMLU":25.23,
        "TruthfulQA":46.52,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":0.76,
        "Hub":9,
        "Available on the hub":true,
        "Model Sha":"f9b7a3222967b15169a09bcc86b118ac68a1ad62"
    },
    {
        "T":"?",
        "Model":"Felladrin\/Minueza-32M-Deita",
        "Average":30.51,
        "ARC":20.73,
        "HellaSwag":26.72,
        "MMLU":26.84,
        "TruthfulQA":47.75,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"2523b34a5ede2ef8534521080b92380ccaace340"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TomGrc\/FN-OpenLLM_2x72B_MoE",
        "Average":30.51,
        "ARC":25.51,
        "HellaSwag":25.23,
        "MMLU":22.8,
        "TruthfulQA":48.47,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":72.0,
        "Hub":9,
        "Available on the hub":false,
        "Model Sha":"e1bf8cd6594a6ae363f07ab2ed3fe8ea2e718c81"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BreadAi\/StoryPy",
        "Average":30.5,
        "ARC":22.35,
        "HellaSwag":26.19,
        "MMLU":24.37,
        "TruthfulQA":49.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.1,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"5c32081bd3bc1404c2f5b8dbb6f888048bcb7cd7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/160M-TinyLLama-Mini-Cinder",
        "Average":30.5,
        "ARC":24.66,
        "HellaSwag":28.16,
        "MMLU":25.09,
        "TruthfulQA":44.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"a82c33fd5feadf6034e1df1e7dad4fc9993bf065"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"chargoddard\/SmolLlamix-8x101M",
        "Average":30.49,
        "ARC":22.7,
        "HellaSwag":28.5,
        "MMLU":24.69,
        "TruthfulQA":46.09,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.4,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"02909f5f76561cc02059b0802d4b894f4a8f9b5a"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Dans-DiscountModels\/TinyMistral-v2-Test1",
        "Average":30.49,
        "ARC":21.5,
        "HellaSwag":26.79,
        "MMLU":23.36,
        "TruthfulQA":50.3,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"c7f5b9b6915cb9942d47ab8bde32093bcc4a4374"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Locutusque\/TinyMistral-248M-v2.5",
        "Average":30.48,
        "ARC":24.57,
        "HellaSwag":27.49,
        "MMLU":23.15,
        "TruthfulQA":46.72,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.25,
        "Hub":23,
        "Available on the hub":false,
        "Model Sha":"99c8efd7bc4aba7939e20b7e9e3a46b542cae713"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/RWKV-4-PilePlus-169M-20230520-done-ctx4096",
        "Average":30.47,
        "ARC":23.98,
        "HellaSwag":32.25,
        "MMLU":23.37,
        "TruthfulQA":42.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.13,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"1134d31db1aee9fc970d3e9dc4e7314fb8bba500"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"xformAI\/opt-125m-gqa-ub-6-best-for-KV-cache",
        "Average":30.47,
        "ARC":24.23,
        "HellaSwag":25.0,
        "MMLU":23.12,
        "TruthfulQA":49.53,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"078bcddaf1abf77d8c44b05b8f2e1d1c0a82855d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"crumb\/model-a-48.5m",
        "Average":30.47,
        "ARC":22.18,
        "HellaSwag":27.85,
        "MMLU":25.08,
        "TruthfulQA":46.75,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.05,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"5aba9fe63195029ceddc2c54751be160b635586a"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-70m",
        "Average":30.46,
        "ARC":21.59,
        "HellaSwag":27.29,
        "MMLU":25.9,
        "TruthfulQA":47.06,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.1,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"2ab25ed47af79376eed2baaf8bbb7a192a0c73ff"
    },
    {
        "T":"?",
        "Model":"Felladrin\/Minueza-32M-UltraChat",
        "Average":30.45,
        "ARC":21.08,
        "HellaSwag":26.95,
        "MMLU":26.08,
        "TruthfulQA":47.7,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"512baa3b64b2a88d1e35bcd6ab4164124ba184d1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mzio\/hedgehog-alpaca_clean_mistral-mistral_7b_lk_esn_tqk_lora-lk_untied_head-lsc_1",
        "Average":30.45,
        "ARC":21.25,
        "HellaSwag":28.74,
        "MMLU":25.15,
        "TruthfulQA":46.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.28,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"d9ce8cc144f015d0b968beaec11d1ea6ba00e5b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BreadAi\/gpt-Youtube",
        "Average":30.45,
        "ARC":23.29,
        "HellaSwag":26.34,
        "MMLU":23.54,
        "TruthfulQA":48.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"de88554a0212c16fdfeda030afb58f831ebcd895"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeen214\/llama2_7b_small_tuning_v1",
        "Average":30.41,
        "ARC":22.44,
        "HellaSwag":25.0,
        "MMLU":25.51,
        "TruthfulQA":48.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3f9b43b4db2da4fe3785071dd52c9fc92aa0801d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"namanpundir\/theus_concepttagger",
        "Average":30.36,
        "ARC":24.57,
        "HellaSwag":25.5,
        "MMLU":23.12,
        "TruthfulQA":48.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.51,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"54f816e4cc09d5e3615da5a0eedb67b2be529cd9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"postbot\/distilgpt2-emailgen",
        "Average":30.36,
        "ARC":21.76,
        "HellaSwag":27.52,
        "MMLU":25.97,
        "TruthfulQA":46.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.09,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"fe96d63cc2edcbd1ae444ada293cc59d1e01a6ad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sharathhebbar24\/chat_gpt2_dpo",
        "Average":30.35,
        "ARC":23.98,
        "HellaSwag":31.22,
        "MMLU":24.95,
        "TruthfulQA":41.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2c35dac9f97e3756137c175b9d49d72fdcf2d059"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"roneneldan\/TinyStories-33M",
        "Average":30.35,
        "ARC":24.23,
        "HellaSwag":25.69,
        "MMLU":23.82,
        "TruthfulQA":47.64,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.07,
        "Hub":50,
        "Available on the hub":true,
        "Model Sha":"190d22e37cba4b12ddae57d6738a0c65f6ab1aa5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nisten\/smaugzilla-77b",
        "Average":30.32,
        "ARC":24.83,
        "HellaSwag":25.16,
        "MMLU":23.05,
        "TruthfulQA":48.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":76.65,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"12c4d57705c4f7a1a2b9fd94e79ddb1cd357aa9a"
    },
    {
        "T":"?",
        "Model":"Felladrin\/Pythia-31M-Chat-v1",
        "Average":30.31,
        "ARC":21.84,
        "HellaSwag":26.81,
        "MMLU":24.55,
        "TruthfulQA":48.04,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"e6a52e4ac98e20c7f9e39aaba9368dd6faacdad9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"pszemraj\/pythia-31m-simplewiki-scratch-bf16",
        "Average":30.29,
        "ARC":22.78,
        "HellaSwag":25.61,
        "MMLU":23.12,
        "TruthfulQA":49.65,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4eaec0542e7609fd3f364cb34491f05d7c61a3d0"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Cheng98\/llama-39m",
        "Average":30.28,
        "ARC":24.06,
        "HellaSwag":25.57,
        "MMLU":24.31,
        "TruthfulQA":47.19,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":0.04,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"7add242066017116840350cd1f6415b071faac6d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"kenhktsui\/nano-phi-115M-v0.1",
        "Average":30.28,
        "ARC":21.93,
        "HellaSwag":27.86,
        "MMLU":25.34,
        "TruthfulQA":46.0,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"888a0d660e498daed51fdf69da70b075452b4bf9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"saarvajanik\/facebook-opt-6.7b-gqa-ub-16-best-for-KV-cache",
        "Average":30.27,
        "ARC":23.04,
        "HellaSwag":25.94,
        "MMLU":23.12,
        "TruthfulQA":48.99,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.66,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4b01e3b68eadfeffec10ea017e6c1249c58a8d46"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"xformAI\/facebook-opt-125m-qcqa-ub-6-best-for-q-loss",
        "Average":30.26,
        "ARC":23.29,
        "HellaSwag":25.57,
        "MMLU":23.15,
        "TruthfulQA":49.03,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9e7a1dd66f1fe7b0808dcdb12a8ad5d166c67576"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-70m-deduped",
        "Average":30.25,
        "ARC":21.08,
        "HellaSwag":27.17,
        "MMLU":25.26,
        "TruthfulQA":47.51,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.1,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"e93a9faa9c77e5d09219f6c868bfc7a1bd65593c"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2_guanaco-dolly-platypus",
        "Average":30.25,
        "ARC":23.55,
        "HellaSwag":31.03,
        "MMLU":26.4,
        "TruthfulQA":40.02,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"6bf0a8146cf255c829ec2ad83926c8b80945b431"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aloobun\/falcon-1b-cot-t2",
        "Average":30.25,
        "ARC":24.74,
        "HellaSwag":24.75,
        "MMLU":23.12,
        "TruthfulQA":48.38,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"fed367016c8adcd499f18eab5e8a9eda71c5e647"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Corianas\/Quokka_590m",
        "Average":30.24,
        "ARC":24.4,
        "HellaSwag":31.61,
        "MMLU":25.36,
        "TruthfulQA":39.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.67,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ae0ac41e9be016f6dceac06821fbf6ebacc7edb9"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2_camel_physics-platypus",
        "Average":30.21,
        "ARC":23.04,
        "HellaSwag":31.32,
        "MMLU":26.91,
        "TruthfulQA":39.56,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"66165ff32ed8de6c39f3524a810f5e97ba6d3347"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2_platypus-camel_physics",
        "Average":30.21,
        "ARC":23.04,
        "HellaSwag":31.32,
        "MMLU":26.91,
        "TruthfulQA":39.56,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"66165ff32ed8de6c39f3524a810f5e97ba6d3347"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yash21\/SuperChat-7B",
        "Average":30.21,
        "ARC":23.98,
        "HellaSwag":26.4,
        "MMLU":23.24,
        "TruthfulQA":47.21,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b416e3a17d1954d488c29bcc50841dd735527b52"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Locutusque\/TinyMistral-248M-v2",
        "Average":30.2,
        "ARC":21.25,
        "HellaSwag":26.56,
        "MMLU":23.39,
        "TruthfulQA":49.6,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.25,
        "Hub":14,
        "Available on the hub":false,
        "Model Sha":"937ed7abdec98b7a9868b95e3b8a0d757b902325"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"xformAI\/facebook-opt-125m-qcqa-ub-6-best-for-KV-cache",
        "Average":30.19,
        "ARC":24.23,
        "HellaSwag":25.0,
        "MMLU":23.12,
        "TruthfulQA":48.41,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9bfbe90e1b638fe96534bf5085442ecde45f854d"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2_platypus-dolly-guanaco",
        "Average":30.18,
        "ARC":23.21,
        "HellaSwag":31.04,
        "MMLU":26.16,
        "TruthfulQA":40.31,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"bfa144d3eb087e54f1798fd2e2fb17e894cc39d3"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"ethzanalytics\/pythia-31m",
        "Average":30.17,
        "ARC":19.97,
        "HellaSwag":26.34,
        "MMLU":24.27,
        "TruthfulQA":50.12,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"8a3c2f1555de8a3c53d67d73b5d0d53a66a6c6c2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Josephgflowers\/distillgpt2Cinder",
        "Average":30.16,
        "ARC":24.49,
        "HellaSwag":27.24,
        "MMLU":24.97,
        "TruthfulQA":43.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.08,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"bc2bb342a2a239258e4862272ba3993c955e8289"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"saarvajanik\/facebook-opt-6.7b-qcqa-ub-16-best-for-KV-cache",
        "Average":30.16,
        "ARC":23.81,
        "HellaSwag":27.05,
        "MMLU":23.12,
        "TruthfulQA":46.69,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.66,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"42a63c805a12dda777f145ef3650202a55183a9f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"BEE-spoke-data\/Mixtral-GQA-400m-v2",
        "Average":30.16,
        "ARC":20.22,
        "HellaSwag":27.78,
        "MMLU":26.1,
        "TruthfulQA":46.55,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.01,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"6f8c51d1bf60da6f8e64ba7fb75fb747d9b124cf"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"vilm\/Mixsmol-4x400M-v0.1-epoch2",
        "Average":30.16,
        "ARC":23.55,
        "HellaSwag":32.6,
        "MMLU":25.26,
        "TruthfulQA":39.24,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.77,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"3741dbdbb179f58d07ac9f2d082fb7a6cffe7613"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"instructkr\/ko-wand-136M",
        "Average":30.15,
        "ARC":21.33,
        "HellaSwag":25.0,
        "MMLU":23.58,
        "TruthfulQA":50.68,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":[
            "apache-2.0"
        ],
        "#Params (B)":0.14,
        "Hub":1,
        "Available on the hub":false,
        "Model Sha":"86cc9bf25c45c60cc16ea6002609121fdcd83609"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"mosaicml\/mpt-7b-storywriter",
        "Average":30.13,
        "ARC":20.39,
        "HellaSwag":27.45,
        "MMLU":24.25,
        "TruthfulQA":48.44,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub":679,
        "Available on the hub":true,
        "Model Sha":"a5e85ae1941e31bb705adbcafce9b0dfd6f3a48b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cyberagent\/open-calm-7b",
        "Average":30.13,
        "ARC":20.48,
        "HellaSwag":30.65,
        "MMLU":25.22,
        "TruthfulQA":44.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":6.66,
        "Hub":187,
        "Available on the hub":true,
        "Model Sha":"276a5fb67510554e11ef191a2da44c919acccdf5"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"RWKV\/rwkv-4-169m-pile",
        "Average":30.12,
        "ARC":23.63,
        "HellaSwag":31.74,
        "MMLU":23.18,
        "TruthfulQA":41.92,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.13,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"46bdc280eb97b6141d5d51a935e0c4870ecaefcc"
    },
    {
        "T":"\u2b55",
        "Model":"0x7194633\/nanoFialka-v1",
        "Average":30.11,
        "ARC":22.01,
        "HellaSwag":28.12,
        "MMLU":25.03,
        "TruthfulQA":45.26,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"96023dad08cf1f9a300c95c8834e28631ca7167b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"BEE-spoke-data\/verysmol_llama-v11-KIx2",
        "Average":30.08,
        "ARC":22.7,
        "HellaSwag":27.6,
        "MMLU":25.28,
        "TruthfulQA":44.75,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.06,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"1cd271d3d62a9e1dc4b7c2978e54806d74705439"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"roneneldan\/TinyStories-28M",
        "Average":30.06,
        "ARC":22.78,
        "HellaSwag":25.83,
        "MMLU":23.53,
        "TruthfulQA":48.08,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.05,
        "Hub":5,
        "Available on the hub":true,
        "Model Sha":"52dabea9997faf578489d619249616926e54ed18"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"pszemraj\/pythia-31m-simplewiki-2048",
        "Average":30.06,
        "ARC":22.18,
        "HellaSwag":25.55,
        "MMLU":23.12,
        "TruthfulQA":49.37,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"95d47818055661250b55144c7d9beaf05dc126d8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AI-Sweden-Models\/gpt-sw3-126m",
        "Average":30.05,
        "ARC":22.18,
        "HellaSwag":29.54,
        "MMLU":24.43,
        "TruthfulQA":44.03,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.19,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"9272f5a996cf785b8ab706a27d1e7dff1228dc70"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"qblocks\/gpt2_137m_DolphinCoder",
        "Average":30.04,
        "ARC":21.84,
        "HellaSwag":31.35,
        "MMLU":25.4,
        "TruthfulQA":41.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.14,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"906d8a02bdb444159b189a153f1f5589071ed74e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Zangs3011\/gpt2_137m_DolphinCoder",
        "Average":30.04,
        "ARC":21.84,
        "HellaSwag":31.35,
        "MMLU":25.4,
        "TruthfulQA":41.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.14,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"a558223f774bbd315d1a3890d93ab80dc720fbb1"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AI-Sweden-Models\/gpt-sw3-126m",
        "Average":30.04,
        "ARC":22.01,
        "HellaSwag":29.56,
        "MMLU":24.53,
        "TruthfulQA":44.07,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.19,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"c9d5a2f3fe905557cf0acba496a903255a11907c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"qiyinmiss\/My_GPT2",
        "Average":30.02,
        "ARC":21.93,
        "HellaSwag":31.59,
        "MMLU":25.84,
        "TruthfulQA":40.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"4145e280b85ec619906dfc5a624e17cde8ffbea6"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"gpt2",
        "Average":30.02,
        "ARC":22.01,
        "HellaSwag":31.53,
        "MMLU":25.83,
        "TruthfulQA":40.69,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub":1372,
        "Available on the hub":true,
        "Model Sha":"11c5a3d5811f50298f278a704980280950aedb10"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"openai-community\/gpt2",
        "Average":30.02,
        "ARC":22.01,
        "HellaSwag":31.53,
        "MMLU":25.83,
        "TruthfulQA":40.69,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub":1788,
        "Available on the hub":true,
        "Model Sha":"11c5a3d5811f50298f278a704980280950aedb10"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2_open-platypus",
        "Average":30.01,
        "ARC":22.18,
        "HellaSwag":31.29,
        "MMLU":26.19,
        "TruthfulQA":40.35,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"745c1864b752525789cad2b75166c519a327325e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"gpt2",
        "Average":29.99,
        "ARC":21.84,
        "HellaSwag":31.6,
        "MMLU":25.86,
        "TruthfulQA":40.67,
        "Type":"pretrained",
        "Precision":"torch.4bit",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub":1372,
        "Available on the hub":true,
        "Model Sha":"11c5a3d5811f50298f278a704980280950aedb10"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"SaylorTwift\/gpt2_test",
        "Average":29.99,
        "ARC":21.84,
        "HellaSwag":31.6,
        "MMLU":25.86,
        "TruthfulQA":40.67,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ef61310a16ffda93bf8f6132e02658482ffc2bcc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dpv\/finetuned-gpt2-tiny",
        "Average":29.99,
        "ARC":21.84,
        "HellaSwag":31.6,
        "MMLU":25.86,
        "TruthfulQA":40.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"379e02101b4dccba48e7ae792708d2fe7f0bbca2"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Felladrin\/Minueza-32M-Base",
        "Average":29.99,
        "ARC":21.33,
        "HellaSwag":26.39,
        "MMLU":24.8,
        "TruthfulQA":47.45,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub":7,
        "Available on the hub":false,
        "Model Sha":"d05607502380476adee68810778b8752846b0bcc"
    },
    {
        "T":"?",
        "Model":"Felladrin\/Minueza-32M-Chat",
        "Average":29.99,
        "ARC":20.39,
        "HellaSwag":26.54,
        "MMLU":25.75,
        "TruthfulQA":47.27,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub":5,
        "Available on the hub":false,
        "Model Sha":"9722ba30871f0479ac340b1656ad31c49e330536"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Tensoic\/Qwixtral-4x1.8B-SFT",
        "Average":29.99,
        "ARC":21.42,
        "HellaSwag":24.96,
        "MMLU":23.42,
        "TruthfulQA":50.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":1.8,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"17b2e9b0d8fa62575e5192299dd3d9f05eb42765"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"roneneldan\/TinyStories-3M",
        "Average":29.98,
        "ARC":22.01,
        "HellaSwag":25.58,
        "MMLU":24.99,
        "TruthfulQA":47.33,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.01,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"cfaf26ec85ecdfc1bd7c2638104cce55cb67f894"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sharathhebbar24\/code_gpt2",
        "Average":29.98,
        "ARC":23.29,
        "HellaSwag":30.99,
        "MMLU":25.03,
        "TruthfulQA":40.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"fd12ea93faeecbe4d1f4bc2b1d1c3bce0521d182"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nthngdy\/pythia-owt2-70m-50k",
        "Average":29.97,
        "ARC":21.5,
        "HellaSwag":28.15,
        "MMLU":25.7,
        "TruthfulQA":44.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.04,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"9fce9b8252f7891dbd50299a8c3bd71cd25454db"
    },
    {
        "T":"\u2b55",
        "Model":"beomi\/KoAlpaca-KoRWKV-6B",
        "Average":29.96,
        "ARC":23.46,
        "HellaSwag":31.65,
        "MMLU":24.89,
        "TruthfulQA":39.83,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.53,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"427ee72c4350f26de1b287a0c07b842e7d168dbc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"postbot\/distilgpt2-emailgen-V2",
        "Average":29.95,
        "ARC":20.99,
        "HellaSwag":26.78,
        "MMLU":25.53,
        "TruthfulQA":46.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.09,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"9750ba00e79a02e1bf98d3faa3d49b8ae0f8ae63"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sharathhebbar24\/code_gpt2_mini_model",
        "Average":29.95,
        "ARC":23.72,
        "HellaSwag":31.25,
        "MMLU":24.96,
        "TruthfulQA":39.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"97d249d37896a4c20485830862541373edeca49c"
    },
    {
        "T":"?",
        "Model":"Sayan01\/Llama-Flan-XL2base",
        "Average":29.94,
        "ARC":20.65,
        "HellaSwag":25.33,
        "MMLU":23.19,
        "TruthfulQA":50.58,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.2,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"5ffcaeaf5645d96c3f04ed632a820590d3f87c6c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sharathhebbar24\/math_gpt2",
        "Average":29.93,
        "ARC":24.23,
        "HellaSwag":30.88,
        "MMLU":25.38,
        "TruthfulQA":39.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"57b7106f661d4874578a2ef48784d1afc0cccd8f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"gpt2",
        "Average":29.93,
        "ARC":21.59,
        "HellaSwag":31.58,
        "MMLU":25.4,
        "TruthfulQA":41.15,
        "Type":"pretrained",
        "Precision":"8bit",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub":1372,
        "Available on the hub":true,
        "Model Sha":"11c5a3d5811f50298f278a704980280950aedb10"
    },
    {
        "T":"\u2b55",
        "Model":"AI-Sweden-Models\/gpt-sw3-126m-instruct",
        "Average":29.92,
        "ARC":23.38,
        "HellaSwag":29.88,
        "MMLU":23.78,
        "TruthfulQA":42.65,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.19,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"5f353e1eb1b579ef62e10302b7c0bb843ee8eba9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Corianas\/256_5epoch",
        "Average":29.9,
        "ARC":22.27,
        "HellaSwag":28.99,
        "MMLU":26.62,
        "TruthfulQA":41.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":0.32,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"b1fe75844a07832acd405a4d989a26f6ab7b1c00"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"roneneldan\/TinyStories-8M",
        "Average":29.89,
        "ARC":24.66,
        "HellaSwag":25.03,
        "MMLU":23.33,
        "TruthfulQA":46.54,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.02,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"8612e3b15c66ffa94eaa6ee0de5c96edd2d630af"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"BEE-spoke-data\/smol_llama-81M-tied",
        "Average":29.89,
        "ARC":22.18,
        "HellaSwag":29.33,
        "MMLU":24.06,
        "TruthfulQA":43.97,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.08,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"096e543bd36d067a819ea867c66f14d946849053"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TurkuNLP\/gpt3-finnish-small",
        "Average":29.88,
        "ARC":20.48,
        "HellaSwag":28.09,
        "MMLU":24.47,
        "TruthfulQA":46.47,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.19,
        "Hub":8,
        "Available on the hub":true,
        "Model Sha":"20a19af481bf59f38610a2977b2b513e9df51e3a"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2-dolly",
        "Average":29.85,
        "ARC":21.76,
        "HellaSwag":30.77,
        "MMLU":24.66,
        "TruthfulQA":42.22,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"52fcf61a8eef255a981be6efde187481086e1a48"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nthngdy\/pythia-owt2-70m-100k",
        "Average":29.85,
        "ARC":20.9,
        "HellaSwag":28.34,
        "MMLU":25.02,
        "TruthfulQA":45.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.04,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"b288893319b6cdce499148f4482043c350116560"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"daekeun-ml\/phi-2-upscaled-4B-instruct-v0.1",
        "Average":29.84,
        "ARC":22.95,
        "HellaSwag":28.68,
        "MMLU":26.8,
        "TruthfulQA":40.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":4.04,
        "Hub":3,
        "Available on the hub":false,
        "Model Sha":"7647fcf7c3aa98a04a86a65e2f774ec670994b07"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v2-124m",
        "Average":29.84,
        "ARC":23.98,
        "HellaSwag":31.1,
        "MMLU":25.29,
        "TruthfulQA":38.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub":4,
        "Available on the hub":true,
        "Model Sha":"bc719f990748ea72be4b6c270df34fc3d37291dc"
    },
    {
        "T":"?",
        "Model":"mncai\/SGPT-1.3B-insurance-epoch10",
        "Average":29.82,
        "ARC":24.57,
        "HellaSwag":24.25,
        "MMLU":25.23,
        "TruthfulQA":45.24,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.27,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"df685c0bbf838f0627383c28f48e577ee901ba68"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"budecosystem\/boomer-1b",
        "Average":29.8,
        "ARC":22.78,
        "HellaSwag":31.58,
        "MMLU":25.66,
        "TruthfulQA":39.17,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.94,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"f8f24b5480fa43f23d858f0eb8d1af1b7ad0af59"
    },
    {
        "T":"?",
        "Model":"distilgpt2",
        "Average":29.79,
        "ARC":22.27,
        "HellaSwag":27.58,
        "MMLU":24.81,
        "TruthfulQA":44.49,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.09,
        "Hub":340,
        "Available on the hub":true,
        "Model Sha":"38cc92ec43315abd5136313225e95acc5986876c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"huggingtweets\/gladosystem",
        "Average":29.77,
        "ARC":24.4,
        "HellaSwag":29.71,
        "MMLU":23.18,
        "TruthfulQA":41.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"02a1bbcee7b584ace743b2fe4885cc0eaf2179ac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhiramtirumala\/DialoGPT-sarcastic-medium",
        "Average":29.76,
        "ARC":23.29,
        "HellaSwag":25.93,
        "MMLU":23.76,
        "TruthfulQA":46.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.14,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"292596e120591887383011c4520bc5b57e7e8993"
    },
    {
        "T":"\u2b55",
        "Model":"Locutusque\/TinyMistral-248M-Instruct",
        "Average":29.74,
        "ARC":24.32,
        "HellaSwag":27.52,
        "MMLU":25.18,
        "TruthfulQA":41.94,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.25,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"32a9317176bd8562bbb6497eef43a95f2c0261c3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MBZUAI\/lamini-cerebras-256m",
        "Average":29.73,
        "ARC":21.76,
        "HellaSwag":28.7,
        "MMLU":26.66,
        "TruthfulQA":41.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.26,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"72df0b6d62d64002575687ea2edbb0df05712678"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"blueapple8259\/TinyStories-Alpaca",
        "Average":29.73,
        "ARC":23.98,
        "HellaSwag":24.92,
        "MMLU":23.35,
        "TruthfulQA":46.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.07,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"18e0bde7e72e477757832f0624a0410efc066216"
    },
    {
        "T":"\u2b55",
        "Model":"behnamsh\/gpt2_platypus-camel_physics",
        "Average":29.71,
        "ARC":22.78,
        "HellaSwag":31.24,
        "MMLU":25.87,
        "TruthfulQA":38.95,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"cd4d700d13b3bc9371bf45616ef74ac20d165c3d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"crumb\/gpt2023",
        "Average":29.7,
        "ARC":21.93,
        "HellaSwag":31.11,
        "MMLU":25.05,
        "TruthfulQA":40.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub":17,
        "Available on the hub":true,
        "Model Sha":"e3620b53d164529575db66d9d4f4382311dd713c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Locutusque\/LocutusqueXFelladrin-TinyMistral248M-Instruct",
        "Average":29.69,
        "ARC":24.74,
        "HellaSwag":27.79,
        "MMLU":26.12,
        "TruthfulQA":40.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.25,
        "Hub":6,
        "Available on the hub":false,
        "Model Sha":"646fc1eaf46fcd7f1f9141da8a259715ff7528be"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cerebras\/Cerebras-GPT-111M",
        "Average":29.69,
        "ARC":20.22,
        "HellaSwag":26.73,
        "MMLU":25.51,
        "TruthfulQA":46.31,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.11,
        "Hub":61,
        "Available on the hub":true,
        "Model Sha":"d2b54d7af419055f204690fe0385959616a1723e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pszemraj\/distilgpt2-HC3",
        "Average":29.67,
        "ARC":24.66,
        "HellaSwag":27.99,
        "MMLU":23.95,
        "TruthfulQA":42.1,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.09,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"6f9ad473a3793d0271df34a55882ad30846a6788"
    },
    {
        "T":"\u2b55",
        "Model":"Felladrin\/TinyMistral-248M-SFT-v4",
        "Average":29.67,
        "ARC":24.91,
        "HellaSwag":28.15,
        "MMLU":26.04,
        "TruthfulQA":39.56,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.25,
        "Hub":25,
        "Available on the hub":false,
        "Model Sha":"ec0ff201527cd9b50eb9b4fc754d6c08f1242ea1"
    },
    {
        "T":"\u2b55",
        "Model":"xzuyn\/GPT-2-SlimOrcaDeduped-airoboros-3.1-MetaMathQA-SFT-124M",
        "Average":29.66,
        "ARC":24.57,
        "HellaSwag":29.43,
        "MMLU":25.82,
        "TruthfulQA":38.84,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub":4,
        "Available on the hub":false,
        "Model Sha":"e12dbd27ee148ce4af6faf742aa936d38c26536f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uberkie\/metharme-1.3b-finetuned",
        "Average":29.66,
        "ARC":20.56,
        "HellaSwag":28.02,
        "MMLU":25.26,
        "TruthfulQA":44.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.3,
        "Hub":0,
        "Available on the hub":false,
        "Model Sha":"7335669475711806eb04f8850e4eef91a9d2677d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MBZUAI\/lamini-cerebras-111m",
        "Average":29.63,
        "ARC":22.1,
        "HellaSwag":27.12,
        "MMLU":25.51,
        "TruthfulQA":43.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.11,
        "Hub":3,
        "Available on the hub":true,
        "Model Sha":"e8e347b02f9305e4bc144eb9be2821c518d43183"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"saarvajanik\/facebook-opt-6.7b-qcqa-ub-16-best-for-q-loss",
        "Average":29.57,
        "ARC":21.67,
        "HellaSwag":26.65,
        "MMLU":23.15,
        "TruthfulQA":46.81,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.66,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"2e3897dc86f781538f316fbf27072ae45d0779ee"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sharathhebbar24\/convo_bot_gpt_v1",
        "Average":29.57,
        "ARC":22.35,
        "HellaSwag":31.07,
        "MMLU":26.12,
        "TruthfulQA":38.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"29f955906543788e2f1de656637c9e068cf177f7"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"beomi\/KoRWKV-6B",
        "Average":29.51,
        "ARC":22.1,
        "HellaSwag":32.18,
        "MMLU":24.69,
        "TruthfulQA":39.05,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.53,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"541600070459baf0f1be9560181d5ceb77794085"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sharathhebbar24\/chat_gpt2",
        "Average":29.5,
        "ARC":23.04,
        "HellaSwag":30.76,
        "MMLU":24.39,
        "TruthfulQA":39.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"4acb2db0b75f98219114bcd96d1e6be8c6b86010"
    },
    {
        "T":"?",
        "Model":"Locutusque\/TinyMistral-248M-v2.5-Instruct",
        "Average":29.49,
        "ARC":22.27,
        "HellaSwag":27.6,
        "MMLU":23.9,
        "TruthfulQA":44.21,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.25,
        "Hub":11,
        "Available on the hub":false,
        "Model Sha":"0490a521f39dfdf3e50a500773cd1772322b66a9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psyche\/kogpt",
        "Average":29.47,
        "ARC":21.16,
        "HellaSwag":28.11,
        "MMLU":26.56,
        "TruthfulQA":42.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.39,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"4c02d48f548103ba53a5e481b8aa81bf7a259287"
    },
    {
        "T":"?",
        "Model":"vilm\/Mixsmol-4x400M-v0.1-epoch1",
        "Average":29.44,
        "ARC":22.87,
        "HellaSwag":30.57,
        "MMLU":25.28,
        "TruthfulQA":39.03,
        "Type":"Unknown",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.77,
        "Hub":12,
        "Available on the hub":false,
        "Model Sha":"0389e88c0309b95c885bdfd7fd1a4d5a39b4bcc4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Corianas\/Quokka_256m",
        "Average":29.41,
        "ARC":22.87,
        "HellaSwag":28.84,
        "MMLU":26.48,
        "TruthfulQA":39.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.32,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"d4e69f714d360d39979eb7b8cbc9decdb7190c88"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vicgalle\/gpt2-alpaca-gpt4",
        "Average":29.39,
        "ARC":22.61,
        "HellaSwag":31.17,
        "MMLU":25.76,
        "TruthfulQA":38.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub":11,
        "Available on the hub":true,
        "Model Sha":"282e9bd56f0cab5d48e6954793647eecaa0871d9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sharathhebbar24\/Med_GPT2",
        "Average":29.33,
        "ARC":23.38,
        "HellaSwag":30.99,
        "MMLU":24.0,
        "TruthfulQA":38.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"46fa8704ddb8c2b84a66f9ec0adbd84f3f0f0337"
    },
    {
        "T":"?",
        "Model":"Felladrin\/Minueza-32Mx2-Chat",
        "Average":29.28,
        "ARC":20.14,
        "HellaSwag":26.36,
        "MMLU":26.07,
        "TruthfulQA":44.56,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.04,
        "Hub":2,
        "Available on the hub":false,
        "Model Sha":"79ede1f05e0f7090ca2f002cdaf2b3bbfb1b5ee9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v1-124m",
        "Average":29.23,
        "ARC":24.32,
        "HellaSwag":31.16,
        "MMLU":25.08,
        "TruthfulQA":36.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"f6fd5f3960f31881e6cee23f5a872ecc80b40283"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mikivis\/xuanxuan",
        "Average":29.21,
        "ARC":23.46,
        "HellaSwag":31.12,
        "MMLU":26.27,
        "TruthfulQA":35.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"ba6ae2b347bc613ae38980e059ec8c5ec8b26038"
    },
    {
        "T":"?",
        "Model":"M4-ai\/TinyMistral-6x248M-Instruct",
        "Average":29.19,
        "ARC":22.44,
        "HellaSwag":27.02,
        "MMLU":24.13,
        "TruthfulQA":43.16,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.0,
        "Hub":8,
        "Available on the hub":false,
        "Model Sha":"6004505aa44f0101f69c6e1bf29722c863858c7b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MBZUAI\/LaMini-GPT-124M",
        "Average":29.18,
        "ARC":24.32,
        "HellaSwag":30.82,
        "MMLU":24.99,
        "TruthfulQA":36.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.12,
        "Hub":13,
        "Available on the hub":true,
        "Model Sha":"5c67c8c03c08e82d6138ce2a1eddf5317fac3a6b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Locutusque\/TinyMistral-248m",
        "Average":29.14,
        "ARC":22.87,
        "HellaSwag":28.02,
        "MMLU":23.15,
        "TruthfulQA":42.52,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.25,
        "Hub":28,
        "Available on the hub":false,
        "Model Sha":"8f03f72bca0542aa164c29ba41f02cba6f9d7748"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shitshow123\/tinylamma-20000",
        "Average":29.13,
        "ARC":23.81,
        "HellaSwag":32.45,
        "MMLU":25.37,
        "TruthfulQA":34.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"51b5eea5679f69d00571d94fb87ee12496cb8104"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vicgalle\/gpt2-alpaca",
        "Average":29.12,
        "ARC":22.87,
        "HellaSwag":31.14,
        "MMLU":26.26,
        "TruthfulQA":36.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub":7,
        "Available on the hub":true,
        "Model Sha":"e06875a588f7b3386c18a6efdc8cc7583d95b21b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sharathhebbar24\/math_gpt2_sft",
        "Average":28.99,
        "ARC":22.87,
        "HellaSwag":30.41,
        "MMLU":25.06,
        "TruthfulQA":37.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub":0,
        "Available on the hub":true,
        "Model Sha":"3ed7a2e8ff3b47cca5428d8870434251a0353a53"
    },
    {
        "T":"?",
        "Model":"Felladrin\/TinyMistral-248M-Chat-v2",
        "Average":28.88,
        "ARC":23.29,
        "HellaSwag":27.39,
        "MMLU":23.52,
        "TruthfulQA":41.32,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.25,
        "Hub":25,
        "Available on the hub":false,
        "Model Sha":"010d90c3ae5f1a03ec76e42512f421921d3966ce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Corianas\/111m",
        "Average":28.85,
        "ARC":19.71,
        "HellaSwag":26.68,
        "MMLU":25.28,
        "TruthfulQA":43.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":0.15,
        "Hub":2,
        "Available on the hub":true,
        "Model Sha":"ee58d79e27f8b9e3984aab29235c5851d2be01d4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"huashiyiqike\/testmodel",
        "Average":28.85,
        "ARC":19.71,
        "HellaSwag":26.68,
        "MMLU":25.28,
        "TruthfulQA":43.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":0.15,
        "Hub":1,
        "Available on the hub":true,
        "Model Sha":"1ac5d244402e2433b6abfcff1fe65e84af15766b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"ai-forever\/mGPT",
        "Average":28.74,
        "ARC":23.81,
        "HellaSwag":26.37,
        "MMLU":25.17,
        "TruthfulQA":39.62,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.42,
        "Hub":220,
        "Available on the hub":true,
        "Model Sha":"40897bd7c8b47a76802c411108ca6220438b8b40"
    },
    {
        "T":"?",
        "Model":"Felladrin\/TinyMistral-248M-Chat-v1",
        "Average":28.26,
        "ARC":21.59,
        "HellaSwag":27.45,
        "MMLU":23.08,
        "TruthfulQA":40.91,
        "Type":"Unknown",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.25,
        "Hub":25,
        "Available on the hub":false,
        "Model Sha":"0a9bb94974cbc12c049fc16b0a5b3755532df78f"
    },
    {
        "T":"?",
        "Model":"baseline",
        "Average":25.0,
        "ARC":25.0,
        "HellaSwag":25.0,
        "MMLU":25.0,
        "TruthfulQA":25.0,
        "Type":"Unknown",
        "Precision":null,
        "Hub License":"llama2",
        "#Params (B)":0.0,
        "Hub":1,
        "Available on the hub":null,
        "Model Sha":"N\/A"
    }
]