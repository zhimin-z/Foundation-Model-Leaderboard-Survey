[
    {
        "table_id":2492,
        "row_id":86971,
        "rank":1,
        "method":"TubeVit-H",
        "mlmodel":{

        },
        "method_short":"TubeVit-H",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Top-1 Accuracy":"91.8",
            "Top-5 Accuracy":"98.9",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":91.8,
            "Top-5 Accuracy":98.9,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124229,
            "title":"Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning",
            "url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for\/review\/?hl=86971"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":86970,
        "rank":2,
        "method":"TubeVit-L",
        "mlmodel":{

        },
        "method_short":"TubeVit-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Top-1 Accuracy":"91.5",
            "Top-5 Accuracy":"98.7",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":91.5,
            "Top-5 Accuracy":98.7,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124229,
            "title":"Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning",
            "url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for\/review\/?hl=86970"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":86792,
        "rank":3,
        "method":"InternVideo-T",
        "mlmodel":{

        },
        "method_short":"InternVideo-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Top-1 Accuracy":"91.3",
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":91.3,
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124231,
            "title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
            "url":"\/paper\/internvideo-general-video-foundation-models",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internvideo-general-video-foundation-models\/review\/?hl=86792"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":49165,
        "rank":4,
        "method":"\ud83c\udf77MerlotReserve-Large (+Audio)",
        "mlmodel":{

        },
        "method_short":"\ud83c\udf77MerlotReserve-Large ",
        "method_details":"+Audio",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-14",
        "metrics":{
            "Top-1 Accuracy":"91.1",
            "Top-5 Accuracy":"97.1",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":91.1,
            "Top-5 Accuracy":97.1,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":942729,
            "title":"MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound",
            "url":"\/paper\/merlot-reserve-neural-script-knowledge",
            "published":"2022-01-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/merlot-reserve-neural-script-knowledge\/review\/?hl=49165"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":86969,
        "rank":5,
        "method":"TubeVit-B",
        "mlmodel":{

        },
        "method_short":"TubeVit-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Top-1 Accuracy":"90.9",
            "Top-5 Accuracy":"97.3",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":90.9,
            "Top-5 Accuracy":97.3,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124229,
            "title":"Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning",
            "url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for\/review\/?hl=86969"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":100381,
        "rank":6,
        "method":"UMT-L (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"UMT-L ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "Top-1 Accuracy":"90.5",
            "Top-5 Accuracy":"98.8",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":90.5,
            "Top-5 Accuracy":98.8,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1181934,
            "title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
            "url":"\/paper\/unmasked-teacher-towards-training-efficient",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":145,
                "name":"ViT",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":45377,
        "rank":7,
        "method":"MTV-H (WTS 60M)",
        "mlmodel":{

        },
        "method_short":"MTV-H ",
        "method_details":"WTS 60M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-12",
        "metrics":{
            "Top-1 Accuracy":"90.3",
            "Top-5 Accuracy":"98.5",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":90.3,
            "Top-5 Accuracy":98.5,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":944453,
            "title":"Multiview Transformers for Video Recognition",
            "url":"\/paper\/multiview-transformers-for-video-recognition",
            "published":"2022-01-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiview-transformers-for-video-recognition\/review\/?hl=45377"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":172,
                "name":"MTV",
                "color":"#27b6d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":77319,
        "rank":8,
        "method":"UniFormerV2-L",
        "mlmodel":{

        },
        "method_short":"UniFormerV2-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-22",
        "metrics":{
            "Top-1 Accuracy":"90.1",
            "Top-5 Accuracy":"98.5",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":90.1,
            "Top-5 Accuracy":98.5,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1087805,
            "title":"UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer",
            "url":"\/paper\/uniformerv2-spatiotemporal-learning-by-arming",
            "published":"2022-09-22T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":100525,
        "rank":9,
        "method":"VideoMAE V2-g (64x266x266)",
        "mlmodel":{

        },
        "method_short":"VideoMAE V2-g ",
        "method_details":"64x266x266",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-29",
        "metrics":{
            "Top-1 Accuracy":"89.9",
            "Top-5 Accuracy":"98.5",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":89.9,
            "Top-5 Accuracy":98.5,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1182705,
            "title":"VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
            "url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders",
            "published":"2023-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders\/review\/?hl=100525"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":96434,
        "rank":10,
        "method":"mPLUG-2",
        "mlmodel":{

        },
        "method_short":"mPLUG-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-01",
        "metrics":{
            "Top-1 Accuracy":"89.8",
            "Top-5 Accuracy":"98.3",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":89.8,
            "Top-5 Accuracy":98.3,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1151002,
            "title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",
            "url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation",
            "published":"2023-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation\/review\/?hl=96434"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":78932,
        "rank":11,
        "method":"EVA",
        "mlmodel":{

        },
        "method_short":"EVA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-14",
        "metrics":{
            "Top-1 Accuracy":"89.8%",
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":89.8,
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1110592,
            "title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
            "url":"\/paper\/eva-exploring-the-limits-of-masked-visual",
            "published":"2022-11-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/eva-exploring-the-limits-of-masked-visual\/review\/?hl=78932"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":49167,
        "rank":12,
        "method":"\ud83c\udf77MerlotReserve-Base (+Audio)",
        "mlmodel":{

        },
        "method_short":"\ud83c\udf77MerlotReserve-Base ",
        "method_details":"+Audio",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-14",
        "metrics":{
            "Top-1 Accuracy":"89.7",
            "Top-5 Accuracy":"96.6",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":89.7,
            "Top-5 Accuracy":96.6,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":942729,
            "title":"MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound",
            "url":"\/paper\/merlot-reserve-neural-script-knowledge",
            "published":"2022-01-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/merlot-reserve-neural-script-knowledge\/review\/?hl=49167"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":49166,
        "rank":13,
        "method":"\ud83c\udf77MerlotReserve-Large (no Audio)",
        "mlmodel":{

        },
        "method_short":"\ud83c\udf77MerlotReserve-Large ",
        "method_details":"no Audio",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-14",
        "metrics":{
            "Top-1 Accuracy":"89.4",
            "Top-5 Accuracy":"96.3",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":89.4,
            "Top-5 Accuracy":96.3,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":942729,
            "title":"MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound",
            "url":"\/paper\/merlot-reserve-neural-script-knowledge",
            "published":"2022-01-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/merlot-reserve-neural-script-knowledge\/review\/?hl=49166"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":54002,
        "rank":14,
        "method":"CoCa (finetuned)",
        "mlmodel":{

        },
        "method_short":"CoCa ",
        "method_details":"finetuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Top-1 Accuracy":"89.4",
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":89.4,
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=54002"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":100526,
        "rank":15,
        "method":"VideoMAE V2-g",
        "mlmodel":{

        },
        "method_short":"VideoMAE V2-g",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-29",
        "metrics":{
            "Top-1 Accuracy":"88.8",
            "Top-5 Accuracy":"98.2",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":88.8,
            "Top-5 Accuracy":98.2,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1182705,
            "title":"VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
            "url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders",
            "published":"2023-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders\/review\/?hl=100526"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":104384,
        "rank":16,
        "method":"Hiera-H (no extra data)",
        "mlmodel":{

        },
        "method_short":"Hiera-H ",
        "method_details":"no extra data",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-01",
        "metrics":{
            "Top-1 Accuracy":"88.8",
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":88.8,
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1221231,
            "title":"Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
            "url":"\/paper\/hiera-a-hierarchical-vision-transformer",
            "published":"2023-06-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hiera-a-hierarchical-vision-transformer\/review\/?hl=104384"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":53998,
        "rank":17,
        "method":"CoCa (frozen)",
        "mlmodel":{

        },
        "method_short":"CoCa ",
        "method_details":"frozen",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Top-1 Accuracy":"88.5",
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":88.5,
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=53998"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":44629,
        "rank":18,
        "method":"MaskFeat (no extra data, MViT-L)",
        "mlmodel":{

        },
        "method_short":"MaskFeat ",
        "method_details":"no extra data, MViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-16",
        "metrics":{
            "Top-1 Accuracy":"88.3",
            "Top-5 Accuracy":"98.0",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":88.3,
            "Top-5 Accuracy":98.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932132,
            "title":"Masked Feature Prediction for Self-Supervised Visual Pre-Training",
            "url":"\/paper\/masked-feature-prediction-for-self-supervised",
            "published":"2021-12-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-feature-prediction-for-self-supervised\/review\/?hl=44629"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            },
            {
                "id":154,
                "name":"MViT",
                "color":"#d327c5"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":61369,
        "rank":19,
        "method":"X-CLIP(ViT-L\/14, CLIP)",
        "mlmodel":{

        },
        "method_short":"X-CLIP",
        "method_details":"ViT-L\/14, CLIP",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-04",
        "metrics":{
            "Top-1 Accuracy":"88.3",
            "Top-5 Accuracy":"97.7",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":88.3,
            "Top-5 Accuracy":97.7,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1054742,
            "title":"Expanding Language-Image Pretrained Models for General Video Recognition",
            "url":"\/paper\/expanding-language-image-pretrained-models",
            "published":"2022-08-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/expanding-language-image-pretrained-models\/review\/?hl=61369"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":156,
                "name":"Vision Language",
                "color":"#86ba17"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":49168,
        "rank":20,
        "method":"\ud83c\udf77MerlotReserve-Base (no Audio)",
        "mlmodel":{

        },
        "method_short":"\ud83c\udf77MerlotReserve-Base ",
        "method_details":"no Audio",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-14",
        "metrics":{
            "Top-1 Accuracy":"88.1",
            "Top-5 Accuracy":"95.8",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":88.1,
            "Top-5 Accuracy":95.8,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":942729,
            "title":"MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound",
            "url":"\/paper\/merlot-reserve-neural-script-knowledge",
            "published":"2022-01-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/merlot-reserve-neural-script-knowledge\/review\/?hl=49168"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":53601,
        "rank":21,
        "method":"MViTv2-L (ImageNet-21k pretrain)",
        "mlmodel":{

        },
        "method_short":"MViTv2-L ",
        "method_details":"ImageNet-21k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top-1 Accuracy":"87.9",
            "Top-5 Accuracy":"97.9",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":87.9,
            "Top-5 Accuracy":97.9,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=53601"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":44410,
        "rank":22,
        "method":"CoVeR (JFT-3B)",
        "mlmodel":{

        },
        "method_short":"CoVeR ",
        "method_details":"JFT-3B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-14",
        "metrics":{
            "Top-1 Accuracy":"87.9",
            "Top-5 Accuracy":"97.8",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":87.9,
            "Top-5 Accuracy":97.8,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":931113,
            "title":"Co-training Transformer with Videos and Images Improves Action Recognition",
            "url":"\/paper\/co-training-transformer-with-videos-and",
            "published":"2021-12-14T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":43874,
        "rank":23,
        "method":"Florence (curated FLD-900M pretrain)",
        "mlmodel":{

        },
        "method_short":"Florence ",
        "method_details":"curated FLD-900M pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-22",
        "metrics":{
            "Top-1 Accuracy":"87.8",
            "Top-5 Accuracy":"97.9",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":87.8,
            "Top-5 Accuracy":97.9,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":914419,
            "title":"Florence: A New Foundation Model for Computer Vision",
            "url":"\/paper\/florence-a-new-foundation-model-for-computer",
            "published":"2021-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/florence-a-new-foundation-model-for-computer\/review\/?hl=43874"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":44413,
        "rank":24,
        "method":"CoVeR (JFT-300M)",
        "mlmodel":{

        },
        "method_short":"CoVeR ",
        "method_details":"JFT-300M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-14",
        "metrics":{
            "Top-1 Accuracy":"86.8",
            "Top-5 Accuracy":"97.3",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":86.8,
            "Top-5 Accuracy":97.3,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":931113,
            "title":"Co-training Transformer with Videos and Images Improves Action Recognition",
            "url":"\/paper\/co-training-transformer-with-videos-and",
            "published":"2021-12-14T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":40304,
        "rank":25,
        "method":"TokenLearner 16at18 w. Fuser (L\/10)",
        "mlmodel":{

        },
        "method_short":"TokenLearner 16at18 w. Fuser ",
        "method_details":"L\/10",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-21",
        "metrics":{
            "Top-1 Accuracy":"86.3",
            "Top-5 Accuracy":"97.0",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":86.3,
            "Top-5 Accuracy":97.0,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":821783,
            "title":"TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?",
            "url":"\/paper\/tokenlearner-what-can-8-learned-tokens-do-for",
            "published":"2021-06-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tokenlearner-what-can-8-learned-tokens-do-for\/review\/?hl=40304"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":35526,
        "rank":26,
        "method":"Swin-L (384x384, ImageNet-21k pretrain)",
        "mlmodel":{

        },
        "method_short":"Swin-L ",
        "method_details":"384x384, ImageNet-21k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Top-1 Accuracy":"86.1",
            "Top-5 Accuracy":"97.3",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":86.1,
            "Top-5 Accuracy":97.3,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":824241,
            "title":"Video Swin Transformer",
            "url":"\/paper\/video-swin-transformer",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-swin-transformer\/review\/?hl=35526"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":29018,
        "rank":27,
        "method":"ViViT-H\/16x2 (JFT)",
        "mlmodel":{

        },
        "method_short":"ViViT-H\/16x2 ",
        "method_details":"JFT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top-1 Accuracy":"85.8",
            "Top-5 Accuracy":"96.5",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":85.8,
            "Top-5 Accuracy":96.5,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":758440,
            "title":"ViViT: A Video Vision Transformer",
            "url":"\/paper\/2103-15691",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-15691\/review\/?hl=29018"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":53613,
        "rank":28,
        "method":"MViTv2-L (train from scratch)",
        "mlmodel":{

        },
        "method_short":"MViTv2-L ",
        "method_details":"train from scratch",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top-1 Accuracy":"85.5",
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":85.5,
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=53613"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":45422,
        "rank":29,
        "method":"UniFormer-B (ImageNet-1K)",
        "mlmodel":{

        },
        "method_short":"UniFormer-B ",
        "method_details":"ImageNet-1K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top-1 Accuracy":"84.8",
            "Top-5 Accuracy":"96.7",
            "GFLOPs":"259x4"
        },
        "raw_metrics":{
            "Top-1 Accuracy":84.8,
            "Top-5 Accuracy":96.7,
            "GFLOPs":259.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":883979,
            "title":"UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning",
            "url":"\/paper\/uniformer-unified-transformer-for-efficient",
            "published":"2021-09-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":34829,
        "rank":30,
        "method":"XViT (x16)",
        "mlmodel":{

        },
        "method_short":"XViT ",
        "method_details":"x16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-10",
        "metrics":{
            "Top-1 Accuracy":"84.5",
            "Top-5 Accuracy":"96.3",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":84.5,
            "Top-5 Accuracy":96.3,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":815239,
            "title":"Space-time Mixing Attention for Video Transformer",
            "url":"\/paper\/space-time-mixing-attention-for-video",
            "published":"2021-06-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/space-time-mixing-attention-for-video\/review\/?hl=34829"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":30504,
        "rank":31,
        "method":"MoViNet-A5 (AutoAugment)",
        "mlmodel":{

        },
        "method_short":"MoViNet-A5 ",
        "method_details":"AutoAugment",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"84.3",
            "Top-5 Accuracy":"96.4",
            "GFLOPs":"281x1"
        },
        "raw_metrics":{
            "Top-1 Accuracy":84.3,
            "Top-5 Accuracy":96.4,
            "GFLOPs":281.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=30504"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":29015,
        "rank":32,
        "method":"ViViT-L\/16x2",
        "mlmodel":{

        },
        "method_short":"ViViT-L\/16x2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top-1 Accuracy":"84.3",
            "Top-5 Accuracy":"95.6",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":84.3,
            "Top-5 Accuracy":95.6,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758440,
            "title":"ViViT: A Video Vision Transformer",
            "url":"\/paper\/2103-15691",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-15691\/review\/?hl=29015"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":35527,
        "rank":33,
        "method":"Swin-B (ImageNet-21k pretrain)",
        "mlmodel":{

        },
        "method_short":"Swin-B ",
        "method_details":"ImageNet-21k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Top-1 Accuracy":"84.0",
            "Top-5 Accuracy":"96.5",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":84.0,
            "Top-5 Accuracy":96.5,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":824241,
            "title":"Video Swin Transformer",
            "url":"\/paper\/video-swin-transformer",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-swin-transformer\/review\/?hl=35527"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":30545,
        "rank":34,
        "method":"MViT-B-24, 32x3",
        "mlmodel":{

        },
        "method_short":"MViT-B-24, 32x3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Top-1 Accuracy":"83.8",
            "Top-5 Accuracy":"96.3",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":83.8,
            "Top-5 Accuracy":96.3,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30545"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":30517,
        "rank":35,
        "method":"VATT-Large",
        "mlmodel":{

        },
        "method_short":"VATT-Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Top-1 Accuracy":"83.6",
            "Top-5 Accuracy":"96.6",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":83.6,
            "Top-5 Accuracy":96.6,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":787028,
            "title":"VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",
            "url":"\/paper\/vatt-transformers-for-multimodal-self",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vatt-transformers-for-multimodal-self\/review\/?hl=30517"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":28452,
        "rank":36,
        "method":"MoViNet-A6",
        "mlmodel":{

        },
        "method_short":"MoViNet-A6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"83.5",
            "Top-5 Accuracy":"96.5",
            "GFLOPs":"386x1"
        },
        "raw_metrics":{
            "Top-1 Accuracy":83.5,
            "Top-5 Accuracy":96.5,
            "GFLOPs":386.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28452"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":30544,
        "rank":37,
        "method":"MViT-B, 32x3",
        "mlmodel":{

        },
        "method_short":"MViT-B, 32x3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Top-1 Accuracy":"83.4",
            "Top-5 Accuracy":"96.3",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":83.4,
            "Top-5 Accuracy":96.3,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30544"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":27869,
        "rank":38,
        "method":"LGD-3D Two-stream",
        "mlmodel":{

        },
        "method_short":"LGD-3D Two-stream",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-13",
        "metrics":{
            "Top-1 Accuracy":"83.1",
            "Top-5 Accuracy":"96.2",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":83.1,
            "Top-5 Accuracy":96.2,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142601,
            "title":"Learning Spatio-Temporal Representation with Local and Global Diffusion",
            "url":"\/paper\/learning-spatio-temporal-representation-with-3",
            "published":"2019-06-13T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/learning-spatio-temporal-representation-with-3\/review\/?hl=27869"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":39073,
        "rank":39,
        "method":"R3D-RS-200",
        "mlmodel":{

        },
        "method_short":"R3D-RS-200",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-03",
        "metrics":{
            "Top-1 Accuracy":"83.1",
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":83.1,
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":862026,
            "title":"Revisiting 3D ResNets for Video Recognition",
            "url":"\/paper\/revisiting-3d-resnets-for-video-recognition",
            "published":"2021-09-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revisiting-3d-resnets-for-video-recognition\/review\/?hl=39073"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":29016,
        "rank":40,
        "method":"ViViT-L\/16x2 (320x320)",
        "mlmodel":{

        },
        "method_short":"ViViT-L\/16x2 ",
        "method_details":"320x320",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top-1 Accuracy":"83.0",
            "Top-5 Accuracy":"95.7",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":83.0,
            "Top-5 Accuracy":95.7,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758440,
            "title":"ViViT: A Video Vision Transformer",
            "url":"\/paper\/2103-15691",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-15691\/review\/?hl=29016"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":28453,
        "rank":41,
        "method":"MoViNet-A5",
        "mlmodel":{

        },
        "method_short":"MoViNet-A5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"82.7",
            "Top-5 Accuracy":"95.7",
            "GFLOPs":"281x1"
        },
        "raw_metrics":{
            "Top-1 Accuracy":82.7,
            "Top-5 Accuracy":95.7,
            "GFLOPs":281.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28453"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":30543,
        "rank":42,
        "method":"MViT-B, 16x4",
        "mlmodel":{

        },
        "method_short":"MViT-B, 16x4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Top-1 Accuracy":"82.1",
            "Top-5 Accuracy":"95.7",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":82.1,
            "Top-5 Accuracy":95.7,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30543"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":23857,
        "rank":43,
        "method":"PERF-Net (distilled ResNet50-G)",
        "mlmodel":{

        },
        "method_short":"PERF-Net ",
        "method_details":"distilled ResNet50-G",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-09-28",
        "metrics":{
            "Top-1 Accuracy":"82.0",
            "Top-5 Accuracy":"95.7",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":82.0,
            "Top-5 Accuracy":95.7,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":219654,
            "title":"PERF-Net: Pose Empowered RGB-Flow Net",
            "url":"\/paper\/perf-net-pose-empowered-rgb-flow-net",
            "published":"2020-09-28T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/perf-net-pose-empowered-rgb-flow-net\/review\/?hl=23857"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":27804,
        "rank":44,
        "method":"SlowFast 16x8 (ResNet-101 + NL)",
        "mlmodel":{

        },
        "method_short":"SlowFast 16x8 ",
        "method_details":"ResNet-101 + NL",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "Top-1 Accuracy":"81.8",
            "Top-5 Accuracy":"95.1",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":81.8,
            "Top-5 Accuracy":95.1,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=27804"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":27867,
        "rank":45,
        "method":"LGD-3D RGB",
        "mlmodel":{

        },
        "method_short":"LGD-3D RGB",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-13",
        "metrics":{
            "Top-1 Accuracy":"81.5",
            "Top-5 Accuracy":"95.6",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":81.5,
            "Top-5 Accuracy":95.6,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142601,
            "title":"Learning Spatio-Temporal Representation with Local and Global Diffusion",
            "url":"\/paper\/learning-spatio-temporal-representation-with-3",
            "published":"2019-06-13T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/learning-spatio-temporal-representation-with-3\/review\/?hl=27867"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":28455,
        "rank":46,
        "method":"MoViNet-A4",
        "mlmodel":{

        },
        "method_short":"MoViNet-A4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"81.2",
            "Top-5 Accuracy":"94.9",
            "GFLOPs":"105x1"
        },
        "raw_metrics":{
            "Top-1 Accuracy":81.2,
            "Top-5 Accuracy":94.9,
            "GFLOPs":105.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28455"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":27803,
        "rank":47,
        "method":"SlowFast 16x8 (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"SlowFast 16x8 ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "Top-1 Accuracy":"81.1",
            "Top-5 Accuracy":"95.1",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":81.1,
            "Top-5 Accuracy":95.1,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=27803"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":28456,
        "rank":48,
        "method":"MoViNet-A3",
        "mlmodel":{

        },
        "method_short":"MoViNet-A3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"80.8",
            "Top-5 Accuracy":"80.8",
            "GFLOPs":"56.9x1"
        },
        "raw_metrics":{
            "Top-1 Accuracy":80.8,
            "Top-5 Accuracy":80.8,
            "GFLOPs":56.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28456"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":27802,
        "rank":49,
        "method":"SlowFast 8x8 (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"SlowFast 8x8 ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "Top-1 Accuracy":"80.4",
            "Top-5 Accuracy":"94.8",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":80.4,
            "Top-5 Accuracy":94.8,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=27802"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":27801,
        "rank":50,
        "method":"SlowFast 8x8 (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"SlowFast 8x8 ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "Top-1 Accuracy":"79.9",
            "Top-5 Accuracy":"94.5",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":79.9,
            "Top-5 Accuracy":94.5,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=27801"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":27815,
        "rank":51,
        "method":"D3D+S3D-G",
        "mlmodel":{

        },
        "method_short":"D3D+S3D-G",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-19",
        "metrics":{
            "Top-1 Accuracy":"79.1",
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":79.1,
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":65765,
            "title":"D3D: Distilled 3D Networks for Video Action Recognition",
            "url":"\/paper\/d3d-distilled-3d-networks-for-video-action",
            "published":"2018-12-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/d3d-distilled-3d-networks-for-video-action\/review\/?hl=27815"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":27800,
        "rank":52,
        "method":"SlowFast 4x16 (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"SlowFast 4x16 ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "Top-1 Accuracy":"78.8",
            "Top-5 Accuracy":"94",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":78.8,
            "Top-5 Accuracy":94.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=27800"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":27827,
        "rank":53,
        "method":"S3D-G (RGB+Flow)",
        "mlmodel":{

        },
        "method_short":"S3D-G ",
        "method_details":"RGB+Flow",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-12-13",
        "metrics":{
            "Top-1 Accuracy":"78.6",
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":78.6,
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":13041,
            "title":"Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
            "url":"\/paper\/rethinking-spatiotemporal-feature-learning",
            "published":"2017-12-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-spatiotemporal-feature-learning\/review\/?hl=27827"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":27814,
        "rank":54,
        "method":"D3D",
        "mlmodel":{

        },
        "method_short":"D3D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-19",
        "metrics":{
            "Top-1 Accuracy":"77.9",
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":77.9,
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":65765,
            "title":"D3D: Distilled 3D Networks for Video Action Recognition",
            "url":"\/paper\/d3d-distilled-3d-networks-for-video-action",
            "published":"2018-12-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/d3d-distilled-3d-networks-for-video-action\/review\/?hl=27814"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":28457,
        "rank":55,
        "method":"MoViNet-A2",
        "mlmodel":{

        },
        "method_short":"MoViNet-A2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"77.5",
            "Top-5 Accuracy":"93.4",
            "GFLOPs":"10.3x1"
        },
        "raw_metrics":{
            "Top-1 Accuracy":77.5,
            "Top-5 Accuracy":93.4,
            "GFLOPs":10.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28457"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":27825,
        "rank":56,
        "method":"S3D-G (RGB)",
        "mlmodel":{

        },
        "method_short":"S3D-G ",
        "method_details":"RGB",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-12-13",
        "metrics":{
            "Top-1 Accuracy":"76.6",
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":76.6,
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":13041,
            "title":"Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
            "url":"\/paper\/rethinking-spatiotemporal-feature-learning",
            "published":"2017-12-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-spatiotemporal-feature-learning\/review\/?hl=27825"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":28458,
        "rank":57,
        "method":"MoViNet-A1",
        "mlmodel":{

        },
        "method_short":"MoViNet-A1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"76.0",
            "Top-5 Accuracy":"92.6",
            "GFLOPs":"6.0x1"
        },
        "raw_metrics":{
            "Top-1 Accuracy":76.0,
            "Top-5 Accuracy":92.6,
            "GFLOPs":6.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28458"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":27868,
        "rank":58,
        "method":"LGD-3D Flow",
        "mlmodel":{

        },
        "method_short":"LGD-3D Flow",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-13",
        "metrics":{
            "Top-1 Accuracy":"75",
            "Top-5 Accuracy":"92.4",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":75.0,
            "Top-5 Accuracy":92.4,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142601,
            "title":"Learning Spatio-Temporal Representation with Local and Global Diffusion",
            "url":"\/paper\/learning-spatio-temporal-representation-with-3",
            "published":"2019-06-13T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/learning-spatio-temporal-representation-with-3\/review\/?hl=27868"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":27824,
        "rank":59,
        "method":"I3D (RGB)",
        "mlmodel":{

        },
        "method_short":"I3D ",
        "method_details":"RGB",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-08-03",
        "metrics":{
            "Top-1 Accuracy":"73.6",
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":73.6,
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":54424,
            "title":"A Short Note about Kinetics-600",
            "url":"\/paper\/a-short-note-about-kinetics-600",
            "published":"2018-08-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-short-note-about-kinetics-600\/review\/?hl=27824"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":28459,
        "rank":60,
        "method":"MoViNet-A0",
        "mlmodel":{

        },
        "method_short":"MoViNet-A0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"71.5",
            "Top-5 Accuracy":"90.4",
            "GFLOPs":"2.7x1"
        },
        "raw_metrics":{
            "Top-1 Accuracy":71.5,
            "Top-5 Accuracy":90.4,
            "GFLOPs":2.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28459"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":27826,
        "rank":61,
        "method":"S3D-G (Flow)",
        "mlmodel":{

        },
        "method_short":"S3D-G ",
        "method_details":"Flow",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-12-13",
        "metrics":{
            "Top-1 Accuracy":"69.7",
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":69.7,
            "Top-5 Accuracy":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":13041,
            "title":"Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
            "url":"\/paper\/rethinking-spatiotemporal-feature-learning",
            "published":"2017-12-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-spatiotemporal-feature-learning\/review\/?hl=27826"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":53600,
        "rank":62,
        "method":"MViTv2-B (train from scratch)",
        "mlmodel":{

        },
        "method_short":"MViTv2-B ",
        "method_details":"train from scratch",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top-1 Accuracy":null,
            "Top-5 Accuracy":"97.2",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":null,
            "Top-5 Accuracy":97.2,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=53600"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2492,
        "row_id":44013,
        "rank":63,
        "method":"MViT-L (train from scratch)",
        "mlmodel":{

        },
        "method_short":"MViT-L ",
        "method_details":"train from scratch",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top-1 Accuracy":null,
            "Top-5 Accuracy":null,
            "GFLOPs":"206x5"
        },
        "raw_metrics":{
            "Top-1 Accuracy":null,
            "Top-5 Accuracy":null,
            "GFLOPs":206.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=44013"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":154,
                "name":"MViT",
                "color":"#d327c5"
            }
        ],
        "reports":[

        ]
    }
]