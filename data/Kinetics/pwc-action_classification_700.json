[
    {
        "table_id":3378,
        "row_id":86793,
        "rank":1,
        "Model":"InternVideo-T",
        "mlmodel":{

        },
        "method_short":"InternVideo-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Top-1 Accuracy":"84.0",
            "Top-5 Accuracy":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":84.0,
            "Top-5 Accuracy":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124231,
            "title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
            "url":"\/paper\/internvideo-general-video-foundation-models",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internvideo-general-video-foundation-models\/review\/?hl=86793"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":86975,
        "rank":2,
        "Model":"TubeViT-L",
        "mlmodel":{

        },
        "method_short":"TubeViT-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Top-1 Accuracy":"83.8",
            "Top-5 Accuracy":"96.6"
        },
        "raw_metrics":{
            "Top-1 Accuracy":83.8,
            "Top-5 Accuracy":96.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1124229,
            "title":"Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning",
            "url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for\/review\/?hl=86975"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":100382,
        "rank":3,
        "Model":"UMT-L (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"UMT-L ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "Top-1 Accuracy":"83.6",
            "Top-5 Accuracy":"96.7"
        },
        "raw_metrics":{
            "Top-1 Accuracy":83.6,
            "Top-5 Accuracy":96.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":1181934,
            "title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
            "url":"\/paper\/unmasked-teacher-towards-training-efficient",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":145,
                "name":"ViT",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":45378,
        "rank":4,
        "Model":"MTV-H (WTS 60M)",
        "mlmodel":{

        },
        "method_short":"MTV-H ",
        "method_details":"WTS 60M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-12",
        "metrics":{
            "Top-1 Accuracy":"83.4",
            "Top-5 Accuracy":"96.2"
        },
        "raw_metrics":{
            "Top-1 Accuracy":83.4,
            "Top-5 Accuracy":96.2
        },
        "uses_additional_data":true,
        "paper":{
            "id":944453,
            "title":"Multiview Transformers for Video Recognition",
            "url":"\/paper\/multiview-transformers-for-video-recognition",
            "published":"2022-01-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiview-transformers-for-video-recognition\/review\/?hl=45378"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":172,
                "name":"MTV",
                "color":"#27b6d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":78933,
        "rank":5,
        "Model":"EVA",
        "mlmodel":{

        },
        "method_short":"EVA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-14",
        "metrics":{
            "Top-1 Accuracy":"82.9%",
            "Top-5 Accuracy":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":82.9,
            "Top-5 Accuracy":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1110592,
            "title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
            "url":"\/paper\/eva-exploring-the-limits-of-masked-visual",
            "published":"2022-11-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/eva-exploring-the-limits-of-masked-visual\/review\/?hl=78933"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":77320,
        "rank":6,
        "Model":"UniFormerV2-L",
        "mlmodel":{

        },
        "method_short":"UniFormerV2-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-22",
        "metrics":{
            "Top-1 Accuracy":"82.7",
            "Top-5 Accuracy":"96.2"
        },
        "raw_metrics":{
            "Top-1 Accuracy":82.7,
            "Top-5 Accuracy":96.2
        },
        "uses_additional_data":true,
        "paper":{
            "id":1087805,
            "title":"UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer",
            "url":"\/paper\/uniformerv2-spatiotemporal-learning-by-arming",
            "published":"2022-09-22T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":54003,
        "rank":7,
        "Model":"CoCa (finetuned)",
        "mlmodel":{

        },
        "method_short":"CoCa ",
        "method_details":"finetuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Top-1 Accuracy":"82.7",
            "Top-5 Accuracy":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":82.7,
            "Top-5 Accuracy":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=54003"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":53999,
        "rank":8,
        "Model":"CoCa (frozen)",
        "mlmodel":{

        },
        "method_short":"CoCa ",
        "method_details":"frozen",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Top-1 Accuracy":"81.1",
            "Top-5 Accuracy":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":81.1,
            "Top-5 Accuracy":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=53999"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":104385,
        "rank":9,
        "Model":"Hiera-H (no extra data)",
        "mlmodel":{

        },
        "method_short":"Hiera-H ",
        "method_details":"no extra data",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-01",
        "metrics":{
            "Top-1 Accuracy":"81.1",
            "Top-5 Accuracy":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":81.1,
            "Top-5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1221231,
            "title":"Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
            "url":"\/paper\/hiera-a-hierarchical-vision-transformer",
            "published":"2023-06-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hiera-a-hierarchical-vision-transformer\/review\/?hl=104385"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":44630,
        "rank":10,
        "Model":"MaskFeat (no extra data, MViT-L)",
        "mlmodel":{

        },
        "method_short":"MaskFeat ",
        "method_details":"no extra data, MViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-16",
        "metrics":{
            "Top-1 Accuracy":"80.4",
            "Top-5 Accuracy":"95.7"
        },
        "raw_metrics":{
            "Top-1 Accuracy":80.4,
            "Top-5 Accuracy":95.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":932132,
            "title":"Masked Feature Prediction for Self-Supervised Visual Pre-Training",
            "url":"\/paper\/masked-feature-prediction-for-self-supervised",
            "published":"2021-12-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-feature-prediction-for-self-supervised\/review\/?hl=44630"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            },
            {
                "id":154,
                "name":"MViT",
                "color":"#d327c5"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":96435,
        "rank":11,
        "Model":"mPLUG-2",
        "mlmodel":{

        },
        "method_short":"mPLUG-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-01",
        "metrics":{
            "Top-1 Accuracy":"80.4",
            "Top-5 Accuracy":"94.9"
        },
        "raw_metrics":{
            "Top-1 Accuracy":80.4,
            "Top-5 Accuracy":94.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":1151002,
            "title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",
            "url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation",
            "published":"2023-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation\/review\/?hl=96435"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":101813,
        "rank":12,
        "Model":"AIM (CLIP ViT-L\/14, 32x224)",
        "mlmodel":{

        },
        "method_short":"AIM ",
        "method_details":"CLIP ViT-L\/14, 32x224",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-06",
        "metrics":{
            "Top-1 Accuracy":"80.4",
            "Top-5 Accuracy":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":80.4,
            "Top-5 Accuracy":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1153006,
            "title":"AIM: Adapting Image Models for Efficient Video Action Recognition",
            "url":"\/paper\/aim-adapting-image-models-for-efficient-video",
            "published":"2023-02-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/aim-adapting-image-models-for-efficient-video\/review\/?hl=101813"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":44411,
        "rank":13,
        "Model":"CoVeR (JFT-3B)",
        "mlmodel":{

        },
        "method_short":"CoVeR ",
        "method_details":"JFT-3B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-14",
        "metrics":{
            "Top-1 Accuracy":"79.8",
            "Top-5 Accuracy":"94.9"
        },
        "raw_metrics":{
            "Top-1 Accuracy":79.8,
            "Top-5 Accuracy":94.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":931113,
            "title":"Co-training Transformer with Videos and Images Improves Action Recognition",
            "url":"\/paper\/co-training-transformer-with-videos-and",
            "published":"2021-12-14T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":53603,
        "rank":14,
        "Model":"MViTv2-L (ImageNet-21k pretrain)",
        "mlmodel":{

        },
        "method_short":"MViTv2-L ",
        "method_details":"ImageNet-21k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top-1 Accuracy":"79.4",
            "Top-5 Accuracy":"94.9"
        },
        "raw_metrics":{
            "Top-1 Accuracy":79.4,
            "Top-5 Accuracy":94.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=53603"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":59088,
        "rank":15,
        "Model":"MoViNet-A6",
        "mlmodel":{

        },
        "method_short":"MoViNet-A6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top-1 Accuracy":"79.4",
            "Top-5 Accuracy":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":79.4,
            "Top-5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=59088"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":44414,
        "rank":16,
        "Model":"CoVeR (JFT-300M)",
        "mlmodel":{

        },
        "method_short":"CoVeR ",
        "method_details":"JFT-300M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-14",
        "metrics":{
            "Top-1 Accuracy":"78.5",
            "Top-5 Accuracy":"94.2"
        },
        "raw_metrics":{
            "Top-1 Accuracy":78.5,
            "Top-5 Accuracy":94.2
        },
        "uses_additional_data":true,
        "paper":{
            "id":931113,
            "title":"Co-training Transformer with Videos and Images Improves Action Recognition",
            "url":"\/paper\/co-training-transformer-with-videos-and",
            "published":"2021-12-14T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":53602,
        "rank":17,
        "Model":"MViTv2-B",
        "mlmodel":{

        },
        "method_short":"MViTv2-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top-1 Accuracy":"76.6",
            "Top-5 Accuracy":"93.2"
        },
        "raw_metrics":{
            "Top-1 Accuracy":76.6,
            "Top-5 Accuracy":93.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=53602"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":30105,
        "rank":18,
        "Model":"MoViNet-A6",
        "mlmodel":{

        },
        "method_short":"MoViNet-A6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"72.3",
            "Top-5 Accuracy":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":72.3,
            "Top-5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=30105"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":30106,
        "rank":19,
        "Model":"MoViNet-A5",
        "mlmodel":{

        },
        "method_short":"MoViNet-A5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"71.7",
            "Top-5 Accuracy":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":71.7,
            "Top-5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=30106"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":31462,
        "rank":20,
        "Model":"En-VidTr-L",
        "mlmodel":{

        },
        "method_short":"En-VidTr-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-23",
        "metrics":{
            "Top-1 Accuracy":"70.8",
            "Top-5 Accuracy":"89.4"
        },
        "raw_metrics":{
            "Top-1 Accuracy":70.8,
            "Top-5 Accuracy":89.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":787687,
            "title":"VidTr: Video Transformer Without Convolutions",
            "url":"\/paper\/vidtr-video-transformer-without-convolutions",
            "published":"2021-04-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/vidtr-video-transformer-without-convolutions\/review\/?hl=31462"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":30107,
        "rank":21,
        "Model":"MoViNet-A4",
        "mlmodel":{

        },
        "method_short":"MoViNet-A4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"70.7",
            "Top-5 Accuracy":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":70.7,
            "Top-5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=30107"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":31461,
        "rank":22,
        "Model":"VidTr-L",
        "mlmodel":{

        },
        "method_short":"VidTr-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-23",
        "metrics":{
            "Top-1 Accuracy":"70.2",
            "Top-5 Accuracy":"89"
        },
        "raw_metrics":{
            "Top-1 Accuracy":70.2,
            "Top-5 Accuracy":89.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":787687,
            "title":"VidTr: Video Transformer Without Convolutions",
            "url":"\/paper\/vidtr-video-transformer-without-convolutions",
            "published":"2021-04-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/vidtr-video-transformer-without-convolutions\/review\/?hl=31461"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":31460,
        "rank":23,
        "Model":"VidTr-M",
        "mlmodel":{

        },
        "method_short":"VidTr-M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-23",
        "metrics":{
            "Top-1 Accuracy":"69.5",
            "Top-5 Accuracy":"88.3"
        },
        "raw_metrics":{
            "Top-1 Accuracy":69.5,
            "Top-5 Accuracy":88.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":787687,
            "title":"VidTr: Video Transformer Without Convolutions",
            "url":"\/paper\/vidtr-video-transformer-without-convolutions",
            "published":"2021-04-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/vidtr-video-transformer-without-convolutions\/review\/?hl=31460"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":30108,
        "rank":24,
        "Model":"MoViNet-A3",
        "mlmodel":{

        },
        "method_short":"MoViNet-A3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"68.0",
            "Top-5 Accuracy":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":68.0,
            "Top-5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=30108"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":31459,
        "rank":25,
        "Model":"VidTr-S",
        "mlmodel":{

        },
        "method_short":"VidTr-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-23",
        "metrics":{
            "Top-1 Accuracy":"67.3",
            "Top-5 Accuracy":"87.7"
        },
        "raw_metrics":{
            "Top-1 Accuracy":67.3,
            "Top-5 Accuracy":87.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":787687,
            "title":"VidTr: Video Transformer Without Convolutions",
            "url":"\/paper\/vidtr-video-transformer-without-convolutions",
            "published":"2021-04-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/vidtr-video-transformer-without-convolutions\/review\/?hl=31459"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":30109,
        "rank":26,
        "Model":"MoViNet-A2",
        "mlmodel":{

        },
        "method_short":"MoViNet-A2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"66.7",
            "Top-5 Accuracy":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":66.7,
            "Top-5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=30109"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":30110,
        "rank":27,
        "Model":"MoViNet-A1",
        "mlmodel":{

        },
        "method_short":"MoViNet-A1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"63.5",
            "Top-5 Accuracy":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":63.5,
            "Top-5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=30110"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":30111,
        "rank":28,
        "Model":"MoViNet-A0",
        "mlmodel":{

        },
        "method_short":"MoViNet-A0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top-1 Accuracy":"58.5",
            "Top-5 Accuracy":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":58.5,
            "Top-5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=30111"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":17734,
        "rank":29,
        "Model":"SRTG r3d-101",
        "mlmodel":{

        },
        "method_short":"SRTG r3d-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-15",
        "metrics":{
            "Top-1 Accuracy":"56.46",
            "Top-5 Accuracy":"76.82"
        },
        "raw_metrics":{
            "Top-1 Accuracy":56.46,
            "Top-5 Accuracy":76.82
        },
        "uses_additional_data":false,
        "paper":{
            "id":202256,
            "title":"Learn to cycle: Time-consistent feature discovery for action recognition",
            "url":"\/paper\/learn-to-cycle-time-consistent-feature",
            "published":"2020-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learn-to-cycle-time-consistent-feature\/review\/?hl=17734"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":17743,
        "rank":30,
        "Model":"SRTG r(2+1)d-50",
        "mlmodel":{

        },
        "method_short":"SRTG r",
        "method_details":"2+1",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-15",
        "metrics":{
            "Top-1 Accuracy":"54.17",
            "Top-5 Accuracy":"74.62"
        },
        "raw_metrics":{
            "Top-1 Accuracy":54.17,
            "Top-5 Accuracy":74.62
        },
        "uses_additional_data":false,
        "paper":{
            "id":202256,
            "title":"Learn to cycle: Time-consistent feature discovery for action recognition",
            "url":"\/paper\/learn-to-cycle-time-consistent-feature",
            "published":"2020-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learn-to-cycle-time-consistent-feature\/review\/?hl=17743"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":17729,
        "rank":31,
        "Model":"SRTG r3d-50",
        "mlmodel":{

        },
        "method_short":"SRTG r3d-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-15",
        "metrics":{
            "Top-1 Accuracy":"53.52",
            "Top-5 Accuracy":"74.17"
        },
        "raw_metrics":{
            "Top-1 Accuracy":53.52,
            "Top-5 Accuracy":74.17
        },
        "uses_additional_data":false,
        "paper":{
            "id":202256,
            "title":"Learn to cycle: Time-consistent feature discovery for action recognition",
            "url":"\/paper\/learn-to-cycle-time-consistent-feature",
            "published":"2020-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learn-to-cycle-time-consistent-feature\/review\/?hl=17729"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":47976,
        "rank":32,
        "Model":"SEER (RegNet10B)",
        "mlmodel":{

        },
        "method_short":"SEER ",
        "method_details":"RegNet10B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-16",
        "metrics":{
            "Top-1 Accuracy":"51.9",
            "Top-5 Accuracy":null
        },
        "raw_metrics":{
            "Top-1 Accuracy":51.9,
            "Top-5 Accuracy":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":963673,
            "title":"Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision",
            "url":"\/paper\/vision-models-are-more-robust-and-fair-when",
            "published":"2022-02-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-models-are-more-robust-and-fair-when\/review\/?hl=47976"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":17738,
        "rank":33,
        "Model":"SRTG r(2+1)d-34",
        "mlmodel":{

        },
        "method_short":"SRTG r",
        "method_details":"2+1",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-15",
        "metrics":{
            "Top-1 Accuracy":"49.43",
            "Top-5 Accuracy":"73.23"
        },
        "raw_metrics":{
            "Top-1 Accuracy":49.43,
            "Top-5 Accuracy":73.23
        },
        "uses_additional_data":false,
        "paper":{
            "id":202256,
            "title":"Learn to cycle: Time-consistent feature discovery for action recognition",
            "url":"\/paper\/learn-to-cycle-time-consistent-feature",
            "published":"2020-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learn-to-cycle-time-consistent-feature\/review\/?hl=17738"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3378,
        "row_id":17723,
        "rank":34,
        "Model":"SRTG r3d-34",
        "mlmodel":{

        },
        "method_short":"SRTG r3d-34",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-15",
        "metrics":{
            "Top-1 Accuracy":"49.15",
            "Top-5 Accuracy":"72.68"
        },
        "raw_metrics":{
            "Top-1 Accuracy":49.15,
            "Top-5 Accuracy":72.68
        },
        "uses_additional_data":false,
        "paper":{
            "id":202256,
            "title":"Learn to cycle: Time-consistent feature discovery for action recognition",
            "url":"\/paper\/learn-to-cycle-time-consistent-feature",
            "published":"2020-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learn-to-cycle-time-consistent-feature\/review\/?hl=17723"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]