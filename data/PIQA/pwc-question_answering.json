[
    {
        "table_id":8296,
        "row_id":102550,
        "rank":1,
        "method":"PaLM 2-L (one-shot)",
        "mlmodel":{

        },
        "method_short":"PaLM 2-L ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Accuracy":"85.0"
        },
        "raw_metrics":{
            "Accuracy":85.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102550"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":102549,
        "rank":2,
        "method":"PaLM 2-M (one-shot)",
        "mlmodel":{

        },
        "method_short":"PaLM 2-M ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Accuracy":"83.2"
        },
        "raw_metrics":{
            "Accuracy":83.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102549"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":112819,
        "rank":3,
        "method":"Mistral 7B",
        "mlmodel":{

        },
        "method_short":"Mistral 7B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-10",
        "metrics":{
            "Accuracy":"83.0"
        },
        "raw_metrics":{
            "Accuracy":83.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1297015,
            "title":"Mistral 7B",
            "url":"\/paper\/mistral-7b",
            "published":"2023-10-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mistral-7b\/review\/?hl=112819"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":97606,
        "rank":4,
        "method":"LLaMA 65B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 65B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"82.8"
        },
        "raw_metrics":{
            "Accuracy":82.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97606"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":106304,
        "rank":5,
        "method":"LLaMA 2 70B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 2 70B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Accuracy":"82.8"
        },
        "raw_metrics":{
            "Accuracy":82.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106304"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":97605,
        "rank":6,
        "method":"LLaMA 33B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 33B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"82.3"
        },
        "raw_metrics":{
            "Accuracy":82.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97605"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":102548,
        "rank":7,
        "method":"PaLM 2-S (one-shot)",
        "mlmodel":{

        },
        "method_short":"PaLM 2-S ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "Accuracy":"82.2"
        },
        "raw_metrics":{
            "Accuracy":82.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102548"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":66312,
        "rank":8,
        "method":"MT-NLG 530B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"MT-NLG 530B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-17",
        "metrics":{
            "Accuracy":"82.0"
        },
        "raw_metrics":{
            "Accuracy":82.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":154243,
            "title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
            "url":"\/paper\/megatron-lm-training-multi-billion-parameter",
            "published":"2019-09-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/megatron-lm-training-multi-billion-parameter\/review\/?hl=66312"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":106307,
        "rank":9,
        "method":"LLaMA 2 34B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 2 34B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Accuracy":"81.9"
        },
        "raw_metrics":{
            "Accuracy":81.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106307"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":66311,
        "rank":10,
        "method":"Gopher 280B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"Gopher 280B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-08",
        "metrics":{
            "Accuracy":"81.8"
        },
        "raw_metrics":{
            "Accuracy":81.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":942590,
            "title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
            "url":"\/paper\/scaling-language-models-methods-analysis-1",
            "published":"2021-12-08T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":66313,
        "rank":11,
        "method":"Chinchilla 70B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"Chinchilla 70B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-29",
        "metrics":{
            "Accuracy":"81.8"
        },
        "raw_metrics":{
            "Accuracy":81.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":985465,
            "title":"Training Compute-Optimal Large Language Models",
            "url":"\/paper\/training-compute-optimal-large-language",
            "published":"2022-03-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":88770,
        "rank":12,
        "method":"OPT-175B",
        "mlmodel":{

        },
        "method_short":"OPT-175B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Accuracy":"81.07"
        },
        "raw_metrics":{
            "Accuracy":81.07
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136959,
            "title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
            "url":"\/paper\/massive-language-models-can-be-accurately",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/massive-language-models-can-be-accurately\/review\/?hl=88770"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":30149,
        "rank":13,
        "method":"GPT-3 175B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"GPT-3 175B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "Accuracy":"81.0"
        },
        "raw_metrics":{
            "Accuracy":81.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=30149"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":88774,
        "rank":14,
        "method":"SparseGPT (175B, 50% Sparsity)",
        "mlmodel":{

        },
        "method_short":"SparseGPT ",
        "method_details":"175B, 50% Sparsity",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-22",
        "metrics":{
            "Accuracy":"80.63"
        },
        "raw_metrics":{
            "Accuracy":80.63
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136959,
            "title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
            "url":"\/paper\/massive-language-models-can-be-accurately",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/massive-language-models-can-be-accurately\/review\/?hl=88774"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":39038,
        "rank":15,
        "method":"FLAN 137B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"FLAN 137B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-03",
        "metrics":{
            "Accuracy":"80.5"
        },
        "raw_metrics":{
            "Accuracy":80.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":861409,
            "title":"Finetuned Language Models Are Zero-Shot Learners",
            "url":"\/paper\/finetuned-language-models-are-zero-shot",
            "published":"2021-09-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/finetuned-language-models-are-zero-shot\/review\/?hl=39038"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":106306,
        "rank":16,
        "method":"LLaMA 2 13B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 2 13B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Accuracy":"80.5"
        },
        "raw_metrics":{
            "Accuracy":80.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106306"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":106721,
        "rank":17,
        "method":"LLaMA 13B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 13B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"80.1"
        },
        "raw_metrics":{
            "Accuracy":80.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=106721"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":97603,
        "rank":18,
        "method":"LLaMA 7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":"79.8"
        },
        "raw_metrics":{
            "Accuracy":79.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97603"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":88776,
        "rank":19,
        "method":"SparseGPT (175B, 4:8 Sparsity)",
        "mlmodel":{

        },
        "method_short":"SparseGPT ",
        "method_details":"175B, 4:8 Sparsity",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-22",
        "metrics":{
            "Accuracy":"79.54"
        },
        "raw_metrics":{
            "Accuracy":79.54
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136959,
            "title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
            "url":"\/paper\/massive-language-models-can-be-accurately",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/massive-language-models-can-be-accurately\/review\/?hl=88776"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":88778,
        "rank":20,
        "method":"SparseGPT (175B, 2:4 Sparsity)",
        "mlmodel":{

        },
        "method_short":"SparseGPT ",
        "method_details":"175B, 2:4 Sparsity",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-22",
        "metrics":{
            "Accuracy":"79.54"
        },
        "raw_metrics":{
            "Accuracy":79.54
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136959,
            "title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
            "url":"\/paper\/massive-language-models-can-be-accurately",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/massive-language-models-can-be-accurately\/review\/?hl=88778"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":106305,
        "rank":21,
        "method":"LLaMA 2 7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 2 7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Accuracy":"78.8"
        },
        "raw_metrics":{
            "Accuracy":78.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248363,
            "title":"Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-2-open-foundation-and-fine-tuned-chat\/review\/?hl=106305"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":100776,
        "rank":22,
        "method":"Bloomberg GPT (one-shot)",
        "mlmodel":{

        },
        "method_short":"Bloomberg GPT ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"77.86"
        },
        "raw_metrics":{
            "Accuracy":77.86
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100776"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":100778,
        "rank":23,
        "method":"OPT 66B (one-shot)",
        "mlmodel":{

        },
        "method_short":"OPT 66B ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"77.58"
        },
        "raw_metrics":{
            "Accuracy":77.58
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100778"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":100779,
        "rank":24,
        "method":"BLOOM 176B (one-shot)",
        "mlmodel":{

        },
        "method_short":"BLOOM 176B ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"77.04"
        },
        "raw_metrics":{
            "Accuracy":77.04
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100779"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":112359,
        "rank":25,
        "method":"Open-LLaMA-3B-v2",
        "mlmodel":{

        },
        "method_short":"Open-LLaMA-3B-v2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-10",
        "metrics":{
            "Accuracy":"76.2"
        },
        "raw_metrics":{
            "Accuracy":76.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1297030,
            "title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
            "url":"\/paper\/sheared-llama-accelerating-language-model-pre",
            "published":"2023-10-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sheared-llama-accelerating-language-model-pre\/review\/?hl=112359"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":100777,
        "rank":26,
        "method":"GPT-NeoX (one-shot)",
        "mlmodel":{

        },
        "method_short":"GPT-NeoX ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":"75.84"
        },
        "raw_metrics":{
            "Accuracy":75.84
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100777"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":112360,
        "rank":27,
        "method":"Sheared-LLaMA-2.7B",
        "mlmodel":{

        },
        "method_short":"Sheared-LLaMA-2.7B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-10",
        "metrics":{
            "Accuracy":"75.8"
        },
        "raw_metrics":{
            "Accuracy":75.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1297030,
            "title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
            "url":"\/paper\/sheared-llama-accelerating-language-model-pre",
            "published":"2023-10-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sheared-llama-accelerating-language-model-pre\/review\/?hl=112360"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":112358,
        "rank":28,
        "method":"Sheared-LLaMA-1.3B",
        "mlmodel":{

        },
        "method_short":"Sheared-LLaMA-1.3B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-10",
        "metrics":{
            "Accuracy":"73.4"
        },
        "raw_metrics":{
            "Accuracy":73.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1297030,
            "title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
            "url":"\/paper\/sheared-llama-accelerating-language-model-pre",
            "published":"2023-10-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sheared-llama-accelerating-language-model-pre\/review\/?hl=112358"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":104241,
        "rank":29,
        "method":"FLAN-T5-large",
        "mlmodel":{

        },
        "method_short":"FLAN-T5-large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-27",
        "metrics":{
            "Accuracy":"72.2"
        },
        "raw_metrics":{
            "Accuracy":72.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1198818,
            "title":"LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions",
            "url":"\/paper\/lamini-lm-a-diverse-herd-of-distilled-models",
            "published":"2023-04-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lamini-lm-a-diverse-herd-of-distilled-models\/review\/?hl=104241"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":104243,
        "rank":30,
        "method":"LaMini-GPT2-XL",
        "mlmodel":{

        },
        "method_short":"LaMini-GPT2-XL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-27",
        "metrics":{
            "Accuracy":"71.3"
        },
        "raw_metrics":{
            "Accuracy":71.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1198818,
            "title":"LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions",
            "url":"\/paper\/lamini-lm-a-diverse-herd-of-distilled-models",
            "published":"2023-04-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lamini-lm-a-diverse-herd-of-distilled-models\/review\/?hl=104243"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":104246,
        "rank":31,
        "method":"LaMini-Flan-T5 large",
        "mlmodel":{

        },
        "method_short":"LaMini-Flan-T5 large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-27",
        "metrics":{
            "Accuracy":"70.6"
        },
        "raw_metrics":{
            "Accuracy":70.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1198818,
            "title":"LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions",
            "url":"\/paper\/lamini-lm-a-diverse-herd-of-distilled-models",
            "published":"2023-04-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lamini-lm-a-diverse-herd-of-distilled-models\/review\/?hl=104246"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":104242,
        "rank":32,
        "method":"GPT2-XL",
        "mlmodel":{

        },
        "method_short":"GPT2-XL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-27",
        "metrics":{
            "Accuracy":"70.5"
        },
        "raw_metrics":{
            "Accuracy":70.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1198818,
            "title":"LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions",
            "url":"\/paper\/lamini-lm-a-diverse-herd-of-distilled-models",
            "published":"2023-04-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lamini-lm-a-diverse-herd-of-distilled-models\/review\/?hl=104242"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":104240,
        "rank":33,
        "method":"T5-large",
        "mlmodel":{

        },
        "method_short":"T5-large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-27",
        "metrics":{
            "Accuracy":"55.9"
        },
        "raw_metrics":{
            "Accuracy":55.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":1198818,
            "title":"LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions",
            "url":"\/paper\/lamini-lm-a-diverse-herd-of-distilled-models",
            "published":"2023-04-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lamini-lm-a-diverse-herd-of-distilled-models\/review\/?hl=104240"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":8296,
        "row_id":88772,
        "rank":34,
        "method":"OPT-175B (50% Sparsity)",
        "mlmodel":{

        },
        "method_short":"OPT-175B ",
        "method_details":"50% Sparsity",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Accuracy":"54.73"
        },
        "raw_metrics":{
            "Accuracy":54.73
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136959,
            "title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
            "url":"\/paper\/massive-language-models-can-be-accurately",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/massive-language-models-can-be-accurately\/review\/?hl=88772"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]