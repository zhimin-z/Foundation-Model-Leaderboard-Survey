[
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "+60B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 40901,
        "Throughput (tokens\/s)": 12.5,
        "Energy (tokens\/kWh)": 121359,
        "Best Score (%)": "73.4**",
        "Model": "fangloveskari\/ORCA_LLaMA_70B_QLoRA"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "+60B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 43413,
        "Throughput (tokens\/s)": 11.4,
        "Energy (tokens\/kWh)": 98039,
        "Best Score (%)": "73.4**",
        "Model": "fangloveskari\/ORCA_LLaMA_70B_QLoRA"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "+60B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 43415,
        "Throughput (tokens\/s)": 10.5,
        "Energy (tokens\/kWh)": 97087,
        "Best Score (%)": "73.4**",
        "Model": "fangloveskari\/ORCA_LLaMA_70B_QLoRA"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~35B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 32556,
        "Throughput (tokens\/s)": 18.7,
        "Energy (tokens\/kWh)": 188679,
        "Best Score (%)": "68.4**",
        "Model": "ehartford\/samantha-1.1-llama-33b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~35B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 78631,
        "Throughput (tokens\/s)": 17.0,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 68.4,
        "Model": "ehartford\/samantha-1.1-llama-33b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~35B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 79692,
        "Throughput (tokens\/s)": 16.7,
        "Energy (tokens\/kWh)": 150602,
        "Best Score (%)": 68.4,
        "Model": "ehartford\/samantha-1.1-llama-33b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~35B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 32066,
        "Throughput (tokens\/s)": 16.3,
        "Energy (tokens\/kWh)": 156494,
        "Best Score (%)": "68.4**",
        "Model": "ehartford\/samantha-1.1-llama-33b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~35B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 32068,
        "Throughput (tokens\/s)": 14.8,
        "Energy (tokens\/kWh)": 153139,
        "Best Score (%)": "68.4**",
        "Model": "ehartford\/samantha-1.1-llama-33b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 33389,
        "Throughput (tokens\/s)": 30.1,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 66.2,
        "Model": "Voicelab\/trurl-2-13b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~13B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 14827,
        "Throughput (tokens\/s)": 27.4,
        "Energy (tokens\/kWh)": 332225,
        "Best Score (%)": "66.2**",
        "Model": "Voicelab\/trurl-2-13b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 33433,
        "Throughput (tokens\/s)": 27.0,
        "Energy (tokens\/kWh)": 255754,
        "Best Score (%)": 66.2,
        "Model": "Voicelab\/trurl-2-13b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~13B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 14695,
        "Throughput (tokens\/s)": 24.6,
        "Energy (tokens\/kWh)": 274725,
        "Best Score (%)": "66.2**",
        "Model": "Voicelab\/trurl-2-13b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 82527,
        "Throughput (tokens\/s)": 22.9,
        "Energy (tokens\/kWh)": 208333,
        "Best Score (%)": 66.2,
        "Model": "Voicelab\/trurl-2-13b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~13B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 14649,
        "Throughput (tokens\/s)": 21.6,
        "Energy (tokens\/kWh)": 247524,
        "Best Score (%)": "66.2**",
        "Model": "Voicelab\/trurl-2-13b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~35B",
        "Type": "Falcon \ud83e\udd85",
        "Memory (MB)": 33632,
        "Throughput (tokens\/s)": 1.96,
        "Energy (tokens\/kWh)": 18181,
        "Best Score (%)": "63.4**",
        "Model": "tiiuae\/falcon-40b-instruct"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~35B",
        "Type": "Falcon \ud83e\udd85",
        "Memory (MB)": 34620,
        "Throughput (tokens\/s)": 1.83,
        "Energy (tokens\/kWh)": 17094,
        "Best Score (%)": "63.4**",
        "Model": "tiiuae\/falcon-40b-instruct"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~35B",
        "Type": "MPT",
        "Memory (MB)": 21997,
        "Throughput (tokens\/s)": 27.7,
        "Energy (tokens\/kWh)": 239808,
        "Best Score (%)": "61.2**",
        "Model": "mosaicml\/mpt-30b-chat"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~35B",
        "Type": "MPT",
        "Memory (MB)": 66803,
        "Throughput (tokens\/s)": 20.7,
        "Energy (tokens\/kWh)": 191938,
        "Best Score (%)": 61.2,
        "Model": "mosaicml\/mpt-30b-chat"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 17631,
        "Throughput (tokens\/s)": 39.7,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 59.9,
        "Model": "HyperbeeAI\/Tulpar-7b-v0"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 39192,
        "Throughput (tokens\/s)": 36.6,
        "Energy (tokens\/kWh)": 330033,
        "Best Score (%)": 59.9,
        "Model": "HyperbeeAI\/Tulpar-7b-v0"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 17750,
        "Throughput (tokens\/s)": 35.1,
        "Energy (tokens\/kWh)": 380228,
        "Best Score (%)": 59.9,
        "Model": "HyperbeeAI\/Tulpar-7b-v0"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~7B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 8609,
        "Throughput (tokens\/s)": 34.5,
        "Energy (tokens\/kWh)": 446428,
        "Best Score (%)": "59.9**",
        "Model": "HyperbeeAI\/Tulpar-7b-v0"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 8412,
        "Throughput (tokens\/s)": 30.6,
        "Energy (tokens\/kWh)": 366300,
        "Best Score (%)": "59.9**",
        "Model": "HyperbeeAI\/Tulpar-7b-v0"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 8397,
        "Throughput (tokens\/s)": 27.5,
        "Energy (tokens\/kWh)": 369003,
        "Best Score (%)": "59.9**",
        "Model": "HyperbeeAI\/Tulpar-7b-v0"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~35B",
        "Type": "GPT-BigCode \ud83c\udf38",
        "Memory (MB)": 12247,
        "Throughput (tokens\/s)": 54.6,
        "Energy (tokens\/kWh)": 529100,
        "Best Score (%)": "55.8**",
        "Model": "HuggingFaceH4\/starchat-beta"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~35B",
        "Type": "GPT-BigCode \ud83c\udf38",
        "Memory (MB)": 12791,
        "Throughput (tokens\/s)": 46.5,
        "Energy (tokens\/kWh)": 418410,
        "Best Score (%)": "55.8**",
        "Model": "HuggingFaceH4\/starchat-beta"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~35B",
        "Type": "GPT-BigCode \ud83c\udf38",
        "Memory (MB)": 34354,
        "Throughput (tokens\/s)": 43.1,
        "Energy (tokens\/kWh)": 389105,
        "Best Score (%)": 55.8,
        "Model": "HuggingFaceH4\/starchat-beta"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~35B",
        "Type": "GPT-BigCode \ud83c\udf38",
        "Memory (MB)": 34345,
        "Throughput (tokens\/s)": 41.8,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 55.8,
        "Model": "HuggingFaceH4\/starchat-beta"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~35B",
        "Type": "GPT-BigCode \ud83c\udf38",
        "Memory (MB)": 13078,
        "Throughput (tokens\/s)": 38.5,
        "Energy (tokens\/kWh)": 401606,
        "Best Score (%)": "55.8**",
        "Model": "HuggingFaceH4\/starchat-beta"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~35B",
        "Type": "GPT-BigCode \ud83c\udf38",
        "Memory (MB)": 65484,
        "Throughput (tokens\/s)": 23.7,
        "Energy (tokens\/kWh)": 219298,
        "Best Score (%)": 55.8,
        "Model": "HuggingFaceH4\/starchat-beta"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "MPT",
        "Memory (MB)": 16480,
        "Throughput (tokens\/s)": 53.5,
        "Energy (tokens\/kWh)": 505050,
        "Best Score (%)": 52.8,
        "Model": "mosaicml\/mpt-7b-8k-chat"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "MPT",
        "Memory (MB)": 7177,
        "Throughput (tokens\/s)": 45.9,
        "Energy (tokens\/kWh)": 543478,
        "Best Score (%)": "52.8**",
        "Model": "mosaicml\/mpt-7b-8k-chat"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "MPT",
        "Memory (MB)": 32233,
        "Throughput (tokens\/s)": 44.1,
        "Energy (tokens\/kWh)": 399999,
        "Best Score (%)": 52.8,
        "Model": "mosaicml\/mpt-7b-8k-chat"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "StableLM-Alpha",
        "Memory (MB)": 17989,
        "Throughput (tokens\/s)": 41.5,
        "Energy (tokens\/kWh)": 408163,
        "Best Score (%)": 51.5,
        "Model": "stabilityai\/stablelm-base-alpha-7b-v2"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "StableLM-Alpha",
        "Memory (MB)": 36065,
        "Throughput (tokens\/s)": 37.2,
        "Energy (tokens\/kWh)": 334448,
        "Best Score (%)": 51.5,
        "Model": "stabilityai\/stablelm-base-alpha-7b-v2"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "StableLM-Alpha",
        "Memory (MB)": 8567,
        "Throughput (tokens\/s)": 35.3,
        "Energy (tokens\/kWh)": 434782,
        "Best Score (%)": "51.5**",
        "Model": "stabilityai\/stablelm-base-alpha-7b-v2"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "GPT-J",
        "Memory (MB)": 18885,
        "Throughput (tokens\/s)": 23.6,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 49.6,
        "Model": "togethercomputer\/GPT-JT-6B-v0"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "GPT-J",
        "Memory (MB)": 34708,
        "Throughput (tokens\/s)": 22.8,
        "Energy (tokens\/kWh)": 240384,
        "Best Score (%)": 49.6,
        "Model": "togethercomputer\/GPT-JT-6B-v0"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "GPT-J",
        "Memory (MB)": 19298,
        "Throughput (tokens\/s)": 22.6,
        "Energy (tokens\/kWh)": 294117,
        "Best Score (%)": 49.6,
        "Model": "togethercomputer\/GPT-JT-6B-v0"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~7B",
        "Type": "GPT-J",
        "Memory (MB)": 11004,
        "Throughput (tokens\/s)": 22.1,
        "Energy (tokens\/kWh)": 326797,
        "Best Score (%)": "49.6**",
        "Model": "togethercomputer\/GPT-JT-6B-v0"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "GPT-J",
        "Memory (MB)": 9119,
        "Throughput (tokens\/s)": 20.8,
        "Energy (tokens\/kWh)": 294985,
        "Best Score (%)": "49.6**",
        "Model": "togethercomputer\/GPT-JT-6B-v0"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "GPT-J",
        "Memory (MB)": 9049,
        "Throughput (tokens\/s)": 19.4,
        "Energy (tokens\/kWh)": 321543,
        "Best Score (%)": "49.6**",
        "Model": "togethercomputer\/GPT-JT-6B-v0"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~35B",
        "Type": "OPT",
        "Memory (MB)": 28725,
        "Throughput (tokens\/s)": 26.5,
        "Energy (tokens\/kWh)": 229885,
        "Best Score (%)": "48.9**",
        "Model": "facebook\/opt-iml-max-30b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~35B",
        "Type": "OPT",
        "Memory (MB)": 30841,
        "Throughput (tokens\/s)": 25.1,
        "Energy (tokens\/kWh)": 241545,
        "Best Score (%)": "48.9**",
        "Model": "facebook\/opt-iml-max-30b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~35B",
        "Type": "OPT",
        "Memory (MB)": 28725,
        "Throughput (tokens\/s)": 21.7,
        "Energy (tokens\/kWh)": 196850,
        "Best Score (%)": "48.9**",
        "Model": "facebook\/opt-iml-max-30b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~35B",
        "Type": "OPT",
        "Memory (MB)": 73962,
        "Throughput (tokens\/s)": 20.5,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 48.9,
        "Model": "facebook\/opt-iml-max-30b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~35B",
        "Type": "OPT",
        "Memory (MB)": 73891,
        "Throughput (tokens\/s)": 18.9,
        "Energy (tokens\/kWh)": 173310,
        "Best Score (%)": 48.9,
        "Model": "facebook\/opt-iml-max-30b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~7B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 9582,
        "Throughput (tokens\/s)": 53.8,
        "Energy (tokens\/kWh)": 606060,
        "Best Score (%)": "48.5**",
        "Model": "FreedomIntelligence\/phoenix-inst-chat-7b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 18321,
        "Throughput (tokens\/s)": 52.1,
        "Energy (tokens\/kWh)": 478468,
        "Best Score (%)": 48.5,
        "Model": "FreedomIntelligence\/phoenix-inst-chat-7b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 18323,
        "Throughput (tokens\/s)": 51.0,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 48.5,
        "Model": "FreedomIntelligence\/phoenix-inst-chat-7b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 33779,
        "Throughput (tokens\/s)": 41.3,
        "Energy (tokens\/kWh)": 378787,
        "Best Score (%)": 48.5,
        "Model": "FreedomIntelligence\/phoenix-inst-chat-7b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 9808,
        "Throughput (tokens\/s)": 40.8,
        "Energy (tokens\/kWh)": 478468,
        "Best Score (%)": "48.5**",
        "Model": "FreedomIntelligence\/phoenix-inst-chat-7b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 9808,
        "Throughput (tokens\/s)": 40.0,
        "Energy (tokens\/kWh)": 462962,
        "Best Score (%)": "48.5**",
        "Model": "FreedomIntelligence\/phoenix-inst-chat-7b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~35B",
        "Type": "GPT-2",
        "Memory (MB)": 19829,
        "Throughput (tokens\/s)": 33.1,
        "Energy (tokens\/kWh)": 334448,
        "Best Score (%)": "47.6**",
        "Model": "Writer\/InstructPalmyra-20b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~35B",
        "Type": "GPT-2",
        "Memory (MB)": 48992,
        "Throughput (tokens\/s)": 27.8,
        "Energy (tokens\/kWh)": 276243,
        "Best Score (%)": "47.6**",
        "Model": "Writer\/InstructPalmyra-20b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~35B",
        "Type": "GPT-2",
        "Memory (MB)": 48994,
        "Throughput (tokens\/s)": 27.5,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 47.6,
        "Model": "Writer\/InstructPalmyra-20b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~35B",
        "Type": "GPT-2",
        "Memory (MB)": 48994,
        "Throughput (tokens\/s)": 25.4,
        "Energy (tokens\/kWh)": 266666,
        "Best Score (%)": "47.6**",
        "Model": "Writer\/InstructPalmyra-20b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~35B",
        "Type": "GPT-2",
        "Memory (MB)": 48994,
        "Throughput (tokens\/s)": 25.1,
        "Energy (tokens\/kWh)": 227790,
        "Best Score (%)": 47.6,
        "Model": "Writer\/InstructPalmyra-20b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "Baichuan \ud83c\udf0a",
        "Memory (MB)": 36417,
        "Throughput (tokens\/s)": 37.5,
        "Energy (tokens\/kWh)": 327868,
        "Best Score (%)": 47.4,
        "Model": "baichuan-inc\/Baichuan-7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "Baichuan \ud83c\udf0a",
        "Memory (MB)": 18270,
        "Throughput (tokens\/s)": 34.0,
        "Energy (tokens\/kWh)": 361010,
        "Best Score (%)": 47.4,
        "Model": "baichuan-inc\/Baichuan-7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~7B",
        "Type": "Baichuan \ud83c\udf0a",
        "Memory (MB)": 8833,
        "Throughput (tokens\/s)": 33.9,
        "Energy (tokens\/kWh)": 429184,
        "Best Score (%)": "47.4**",
        "Model": "baichuan-inc\/Baichuan-7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "Baichuan \ud83c\udf0a",
        "Memory (MB)": 8861,
        "Throughput (tokens\/s)": 29.2,
        "Energy (tokens\/kWh)": 364963,
        "Best Score (%)": "47.4**",
        "Model": "baichuan-inc\/Baichuan-7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "Falcon \ud83e\udd85",
        "Memory (MB)": 12130,
        "Throughput (tokens\/s)": 6.71,
        "Energy (tokens\/kWh)": 62111,
        "Best Score (%)": "47.0**",
        "Model": "tiiuae\/falcon-7b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "Falcon \ud83e\udd85",
        "Memory (MB)": 23509,
        "Throughput (tokens\/s)": 6.71,
        "Energy (tokens\/kWh)": 62111,
        "Best Score (%)": 47.0,
        "Model": "tiiuae\/falcon-7b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "Falcon \ud83e\udd85",
        "Memory (MB)": 55279,
        "Throughput (tokens\/s)": 0.943,
        "Energy (tokens\/kWh)": 8849,
        "Best Score (%)": 47.0,
        "Model": "tiiuae\/falcon-7b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 22956,
        "Throughput (tokens\/s)": 27.5,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 46.9,
        "Model": "togethercomputer\/RedPajama-INCITE-Instruct-7B-v0.1"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~7B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 11398,
        "Throughput (tokens\/s)": 26.2,
        "Energy (tokens\/kWh)": 358422,
        "Best Score (%)": "46.9**",
        "Model": "togethercomputer\/RedPajama-INCITE-Instruct-7B-v0.1"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 20720,
        "Throughput (tokens\/s)": 25.7,
        "Energy (tokens\/kWh)": 294117,
        "Best Score (%)": 46.9,
        "Model": "togethercomputer\/RedPajama-INCITE-Instruct-7B-v0.1"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 36044,
        "Throughput (tokens\/s)": 23.8,
        "Energy (tokens\/kWh)": 236966,
        "Best Score (%)": 46.9,
        "Model": "togethercomputer\/RedPajama-INCITE-Instruct-7B-v0.1"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 9632,
        "Throughput (tokens\/s)": 23.7,
        "Energy (tokens\/kWh)": 303951,
        "Best Score (%)": "46.9**",
        "Model": "togethercomputer\/RedPajama-INCITE-Instruct-7B-v0.1"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 9634,
        "Throughput (tokens\/s)": 22.6,
        "Energy (tokens\/kWh)": 310559,
        "Best Score (%)": "46.9**",
        "Model": "togethercomputer\/RedPajama-INCITE-Instruct-7B-v0.1"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~35B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 26026,
        "Throughput (tokens\/s)": 20.0,
        "Energy (tokens\/kWh)": 230414,
        "Best Score (%)": "46.7**",
        "Model": "h2oai\/h2ogpt-gm-oasst1-en-1024-20b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~35B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 21389,
        "Throughput (tokens\/s)": 18.0,
        "Energy (tokens\/kWh)": 196078,
        "Best Score (%)": "46.7**",
        "Model": "h2oai\/h2ogpt-gm-oasst1-en-1024-20b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~35B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 63785,
        "Throughput (tokens\/s)": 17.7,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 46.7,
        "Model": "h2oai\/h2ogpt-gm-oasst1-en-1024-20b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~35B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 21186,
        "Throughput (tokens\/s)": 17.5,
        "Energy (tokens\/kWh)": 207039,
        "Best Score (%)": "46.7**",
        "Model": "h2oai\/h2ogpt-gm-oasst1-en-1024-20b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~35B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 55210,
        "Throughput (tokens\/s)": 17.0,
        "Energy (tokens\/kWh)": 160513,
        "Best Score (%)": 46.7,
        "Model": "h2oai\/h2ogpt-gm-oasst1-en-1024-20b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 20076,
        "Throughput (tokens\/s)": 45.5,
        "Energy (tokens\/kWh)": 452488,
        "Best Score (%)": 46.2,
        "Model": "acrastt\/Marx-3B-V2"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 10448,
        "Throughput (tokens\/s)": 44.1,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 46.2,
        "Model": "acrastt\/Marx-3B-V2"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 10339,
        "Throughput (tokens\/s)": 41.7,
        "Energy (tokens\/kWh)": 502512,
        "Best Score (%)": 46.2,
        "Model": "acrastt\/Marx-3B-V2"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~3B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 5662,
        "Throughput (tokens\/s)": 34.7,
        "Energy (tokens\/kWh)": 502512,
        "Best Score (%)": "46.2**",
        "Model": "acrastt\/Marx-3B-V2"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~3B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 5654,
        "Throughput (tokens\/s)": 33.7,
        "Energy (tokens\/kWh)": 497512,
        "Best Score (%)": "46.2**",
        "Model": "acrastt\/Marx-3B-V2"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~35B",
        "Type": "CodeGen",
        "Memory (MB)": 27104,
        "Throughput (tokens\/s)": 18.5,
        "Energy (tokens\/kWh)": 232558,
        "Best Score (%)": "46.2**",
        "Model": "Salesforce\/codegen-16B-nl"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~35B",
        "Type": "CodeGen",
        "Memory (MB)": 18421,
        "Throughput (tokens\/s)": 18.1,
        "Energy (tokens\/kWh)": 213219,
        "Best Score (%)": "46.2**",
        "Model": "Salesforce\/codegen-16B-nl"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~35B",
        "Type": "CodeGen",
        "Memory (MB)": 49659,
        "Throughput (tokens\/s)": 17.9,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 46.2,
        "Model": "Salesforce\/codegen-16B-nl"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~35B",
        "Type": "CodeGen",
        "Memory (MB)": 49728,
        "Throughput (tokens\/s)": 17.3,
        "Energy (tokens\/kWh)": 177935,
        "Best Score (%)": 46.2,
        "Model": "Salesforce\/codegen-16B-nl"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~35B",
        "Type": "CodeGen",
        "Memory (MB)": 18004,
        "Throughput (tokens\/s)": 17.0,
        "Energy (tokens\/kWh)": 215053,
        "Best Score (%)": "46.2**",
        "Model": "Salesforce\/codegen-16B-nl"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "+60B",
        "Type": "OPT",
        "Memory (MB)": 59907,
        "Throughput (tokens\/s)": 14.0,
        "Energy (tokens\/kWh)": 127388,
        "Best Score (%)": "46.2**",
        "Model": "facebook\/opt-66b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "+60B",
        "Type": "OPT",
        "Memory (MB)": 67375,
        "Throughput (tokens\/s)": 14.0,
        "Energy (tokens\/kWh)": 135685,
        "Best Score (%)": "46.2**",
        "Model": "facebook\/opt-66b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~35B",
        "Type": "CodeGen",
        "Memory (MB)": 74690,
        "Throughput (tokens\/s)": 13.6,
        "Energy (tokens\/kWh)": 129198,
        "Best Score (%)": 46.2,
        "Model": "Salesforce\/codegen-16B-nl"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "+60B",
        "Type": "OPT",
        "Memory (MB)": 59905,
        "Throughput (tokens\/s)": 12.6,
        "Energy (tokens\/kWh)": 112739,
        "Best Score (%)": "46.2**",
        "Model": "facebook\/opt-66b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 11444,
        "Throughput (tokens\/s)": 29.7,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 46.1,
        "Model": "Fredithefish\/ReasonixPajama-3B-HF"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 15597,
        "Throughput (tokens\/s)": 28.8,
        "Energy (tokens\/kWh)": 344827,
        "Best Score (%)": 46.1,
        "Model": "Fredithefish\/ReasonixPajama-3B-HF"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~3B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 6063,
        "Throughput (tokens\/s)": 28.1,
        "Energy (tokens\/kWh)": 434782,
        "Best Score (%)": "46.1**",
        "Model": "Fredithefish\/ReasonixPajama-3B-HF"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 9731,
        "Throughput (tokens\/s)": 27.6,
        "Energy (tokens\/kWh)": 403225,
        "Best Score (%)": 46.1,
        "Model": "Fredithefish\/ReasonixPajama-3B-HF"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~3B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 6120,
        "Throughput (tokens\/s)": 25.2,
        "Energy (tokens\/kWh)": 398406,
        "Best Score (%)": "46.1**",
        "Model": "Fredithefish\/ReasonixPajama-3B-HF"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~3B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 6073,
        "Throughput (tokens\/s)": 23.8,
        "Energy (tokens\/kWh)": 389105,
        "Best Score (%)": "46.1**",
        "Model": "Fredithefish\/ReasonixPajama-3B-HF"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "RWKV",
        "Memory (MB)": 30644,
        "Throughput (tokens\/s)": 20.4,
        "Energy (tokens\/kWh)": 210970,
        "Best Score (%)": 45.9,
        "Model": "RWKV\/rwkv-raven-14b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "RWKV",
        "Memory (MB)": 58433,
        "Throughput (tokens\/s)": 20.2,
        "Energy (tokens\/kWh)": 187617,
        "Best Score (%)": 45.9,
        "Model": "RWKV\/rwkv-raven-14b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~13B",
        "Type": "RWKV",
        "Memory (MB)": 11279,
        "Throughput (tokens\/s)": 17.5,
        "Energy (tokens\/kWh)": 209643,
        "Best Score (%)": "45.9**",
        "Model": "RWKV\/rwkv-raven-14b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~13B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 16605,
        "Throughput (tokens\/s)": 27.0,
        "Energy (tokens\/kWh)": 336700,
        "Best Score (%)": "45.4**",
        "Model": "OpenAssistant\/oasst-sft-1-pythia-12b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 37835,
        "Throughput (tokens\/s)": 25.3,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 45.4,
        "Model": "OpenAssistant\/oasst-sft-1-pythia-12b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~13B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 14359,
        "Throughput (tokens\/s)": 24.4,
        "Energy (tokens\/kWh)": 274725,
        "Best Score (%)": "45.4**",
        "Model": "OpenAssistant\/oasst-sft-1-pythia-12b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 33267,
        "Throughput (tokens\/s)": 23.7,
        "Energy (tokens\/kWh)": 246305,
        "Best Score (%)": 45.4,
        "Model": "OpenAssistant\/oasst-sft-1-pythia-12b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~13B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 13957,
        "Throughput (tokens\/s)": 22.7,
        "Energy (tokens\/kWh)": 265957,
        "Best Score (%)": "45.4**",
        "Model": "OpenAssistant\/oasst-sft-1-pythia-12b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 60817,
        "Throughput (tokens\/s)": 19.1,
        "Energy (tokens\/kWh)": 177619,
        "Best Score (%)": 45.4,
        "Model": "OpenAssistant\/oasst-sft-1-pythia-12b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~13B",
        "Type": "XGLM",
        "Memory (MB)": 14263,
        "Throughput (tokens\/s)": 40.7,
        "Energy (tokens\/kWh)": 438596,
        "Best Score (%)": "44.0**",
        "Model": "KoboldAI\/fairseq-dense-13B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "XGLM",
        "Memory (MB)": 32424,
        "Throughput (tokens\/s)": 34.5,
        "Energy (tokens\/kWh)": 312500,
        "Best Score (%)": 44.0,
        "Model": "KoboldAI\/fairseq-dense-13B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~13B",
        "Type": "XGLM",
        "Memory (MB)": 13829,
        "Throughput (tokens\/s)": 29.4,
        "Energy (tokens\/kWh)": 310559,
        "Best Score (%)": "44.0**",
        "Model": "KoboldAI\/fairseq-dense-13B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "XGLM",
        "Memory (MB)": 72471,
        "Throughput (tokens\/s)": 23.1,
        "Energy (tokens\/kWh)": 213219,
        "Best Score (%)": 44.0,
        "Model": "KoboldAI\/fairseq-dense-13B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "OPT",
        "Memory (MB)": 17488,
        "Throughput (tokens\/s)": 65.8,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 43.0,
        "Model": "OpenAssistant\/galactica-6.7b-finetuned"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~7B",
        "Type": "OPT",
        "Memory (MB)": 8382,
        "Throughput (tokens\/s)": 55.6,
        "Energy (tokens\/kWh)": 653594,
        "Best Score (%)": "43.0**",
        "Model": "OpenAssistant\/galactica-6.7b-finetuned"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "OPT",
        "Memory (MB)": 17492,
        "Throughput (tokens\/s)": 52.1,
        "Energy (tokens\/kWh)": 487804,
        "Best Score (%)": 43.0,
        "Model": "OpenAssistant\/galactica-6.7b-finetuned"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "OPT",
        "Memory (MB)": 8118,
        "Throughput (tokens\/s)": 44.2,
        "Energy (tokens\/kWh)": 487804,
        "Best Score (%)": "43.0**",
        "Model": "OpenAssistant\/galactica-6.7b-finetuned"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "OPT",
        "Memory (MB)": 37642,
        "Throughput (tokens\/s)": 38.9,
        "Energy (tokens\/kWh)": 353356,
        "Best Score (%)": 43.0,
        "Model": "OpenAssistant\/galactica-6.7b-finetuned"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "OPT",
        "Memory (MB)": 8120,
        "Throughput (tokens\/s)": 37.2,
        "Energy (tokens\/kWh)": 460829,
        "Best Score (%)": "43.0**",
        "Model": "OpenAssistant\/galactica-6.7b-finetuned"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "CodeGen",
        "Memory (MB)": 21659,
        "Throughput (tokens\/s)": 20.4,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 42.8,
        "Model": "Salesforce\/codegen-6B-nl"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~7B",
        "Type": "CodeGen",
        "Memory (MB)": 12384,
        "Throughput (tokens\/s)": 19.3,
        "Energy (tokens\/kWh)": 287356,
        "Best Score (%)": "42.8**",
        "Model": "Salesforce\/codegen-6B-nl"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "CodeGen",
        "Memory (MB)": 22152,
        "Throughput (tokens\/s)": 19.3,
        "Energy (tokens\/kWh)": 236406,
        "Best Score (%)": 42.8,
        "Model": "Salesforce\/codegen-6B-nl"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "CodeGen",
        "Memory (MB)": 40609,
        "Throughput (tokens\/s)": 19.3,
        "Energy (tokens\/kWh)": 204498,
        "Best Score (%)": 42.8,
        "Model": "Salesforce\/codegen-6B-nl"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "CodeGen",
        "Memory (MB)": 10350,
        "Throughput (tokens\/s)": 18.8,
        "Energy (tokens\/kWh)": 255102,
        "Best Score (%)": "42.8**",
        "Model": "Salesforce\/codegen-6B-nl"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "CodeGen",
        "Memory (MB)": 10280,
        "Throughput (tokens\/s)": 17.7,
        "Energy (tokens\/kWh)": 280112,
        "Best Score (%)": "42.8**",
        "Model": "Salesforce\/codegen-6B-nl"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~7B",
        "Type": "XGLM",
        "Memory (MB)": 8370,
        "Throughput (tokens\/s)": 55.9,
        "Energy (tokens\/kWh)": 653594,
        "Best Score (%)": "42.6**",
        "Model": "KoboldAI\/fairseq-dense-6.7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "XGLM",
        "Memory (MB)": 17497,
        "Throughput (tokens\/s)": 52.1,
        "Energy (tokens\/kWh)": 490196,
        "Best Score (%)": 42.6,
        "Model": "KoboldAI\/fairseq-dense-6.7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "OPT",
        "Memory (MB)": 32617,
        "Throughput (tokens\/s)": 40.7,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 42.6,
        "Model": "KoboldAI\/OPT-13B-Erebus"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~13B",
        "Type": "OPT",
        "Memory (MB)": 14477,
        "Throughput (tokens\/s)": 40.3,
        "Energy (tokens\/kWh)": 438596,
        "Best Score (%)": "42.6**",
        "Model": "KoboldAI\/OPT-13B-Erebus"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "XGLM",
        "Memory (MB)": 37558,
        "Throughput (tokens\/s)": 38.8,
        "Energy (tokens\/kWh)": 353356,
        "Best Score (%)": 42.6,
        "Model": "KoboldAI\/fairseq-dense-6.7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "XGLM",
        "Memory (MB)": 8187,
        "Throughput (tokens\/s)": 37.5,
        "Energy (tokens\/kWh)": 465116,
        "Best Score (%)": "42.6**",
        "Model": "KoboldAI\/fairseq-dense-6.7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~13B",
        "Type": "OPT",
        "Memory (MB)": 14319,
        "Throughput (tokens\/s)": 35.3,
        "Energy (tokens\/kWh)": 346020,
        "Best Score (%)": "42.6**",
        "Model": "KoboldAI\/OPT-13B-Erebus"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "OPT",
        "Memory (MB)": 32584,
        "Throughput (tokens\/s)": 34.4,
        "Energy (tokens\/kWh)": 310559,
        "Best Score (%)": 42.6,
        "Model": "KoboldAI\/OPT-13B-Erebus"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~13B",
        "Type": "OPT",
        "Memory (MB)": 14319,
        "Throughput (tokens\/s)": 29.3,
        "Energy (tokens\/kWh)": 306748,
        "Best Score (%)": "42.6**",
        "Model": "KoboldAI\/OPT-13B-Erebus"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "OPT",
        "Memory (MB)": 79237,
        "Throughput (tokens\/s)": 23.0,
        "Energy (tokens\/kWh)": 211864,
        "Best Score (%)": 42.6,
        "Model": "KoboldAI\/OPT-13B-Erebus"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~3B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 6103,
        "Throughput (tokens\/s)": 55.6,
        "Energy (tokens\/kWh)": 671140,
        "Best Score (%)": "41.6**",
        "Model": "ikala\/bloom-zh-3b-chat"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 16033,
        "Throughput (tokens\/s)": 54.6,
        "Energy (tokens\/kWh)": 529100,
        "Best Score (%)": 41.6,
        "Model": "ikala\/bloom-zh-3b-chat"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 9486,
        "Throughput (tokens\/s)": 54.3,
        "Energy (tokens\/kWh)": 625000,
        "Best Score (%)": 41.6,
        "Model": "ikala\/bloom-zh-3b-chat"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 9486,
        "Throughput (tokens\/s)": 52.1,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 41.6,
        "Model": "ikala\/bloom-zh-3b-chat"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~3B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 6128,
        "Throughput (tokens\/s)": 42.0,
        "Energy (tokens\/kWh)": 628930,
        "Best Score (%)": "41.6**",
        "Model": "ikala\/bloom-zh-3b-chat"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~3B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 6130,
        "Throughput (tokens\/s)": 41.5,
        "Energy (tokens\/kWh)": 578034,
        "Best Score (%)": "41.6**",
        "Model": "ikala\/bloom-zh-3b-chat"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~13B",
        "Type": "GPT-2",
        "Memory (MB)": 14313,
        "Throughput (tokens\/s)": 42.4,
        "Energy (tokens\/kWh)": 456621,
        "Best Score (%)": "40.8**",
        "Model": "cerebras\/Cerebras-GPT-13B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~13B",
        "Type": "GPT-2",
        "Memory (MB)": 32672,
        "Throughput (tokens\/s)": 38.3,
        "Energy (tokens\/kWh)": 396825,
        "Best Score (%)": "40.8**",
        "Model": "cerebras\/Cerebras-GPT-13B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "GPT-2",
        "Memory (MB)": 32672,
        "Throughput (tokens\/s)": 38.2,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 40.8,
        "Model": "cerebras\/Cerebras-GPT-13B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~13B",
        "Type": "GPT-2",
        "Memory (MB)": 32670,
        "Throughput (tokens\/s)": 34.5,
        "Energy (tokens\/kWh)": 364963,
        "Best Score (%)": "40.8**",
        "Model": "cerebras\/Cerebras-GPT-13B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "GPT-2",
        "Memory (MB)": 32672,
        "Throughput (tokens\/s)": 34.2,
        "Energy (tokens\/kWh)": 307692,
        "Best Score (%)": 40.8,
        "Model": "cerebras\/Cerebras-GPT-13B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~13B",
        "Type": "GPT-2",
        "Memory (MB)": 65838,
        "Throughput (tokens\/s)": 20.7,
        "Energy (tokens\/kWh)": 189393,
        "Best Score (%)": 40.8,
        "Model": "cerebras\/Cerebras-GPT-13B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "OPT",
        "Memory (MB)": 8963,
        "Throughput (tokens\/s)": 62.5,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 40.6,
        "Model": "aisquared\/chopt-2_7b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "OPT",
        "Memory (MB)": 15852,
        "Throughput (tokens\/s)": 56.2,
        "Energy (tokens\/kWh)": 537634,
        "Best Score (%)": 40.6,
        "Model": "aisquared\/chopt-2_7b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~3B",
        "Type": "OPT",
        "Memory (MB)": 5207,
        "Throughput (tokens\/s)": 54.9,
        "Energy (tokens\/kWh)": 704225,
        "Best Score (%)": "40.6**",
        "Model": "aisquared\/chopt-2_7b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "OPT",
        "Memory (MB)": 8959,
        "Throughput (tokens\/s)": 53.2,
        "Energy (tokens\/kWh)": 621118,
        "Best Score (%)": 40.6,
        "Model": "aisquared\/chopt-2_7b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~3B",
        "Type": "OPT",
        "Memory (MB)": 5142,
        "Throughput (tokens\/s)": 42.6,
        "Energy (tokens\/kWh)": 588235,
        "Best Score (%)": "40.6**",
        "Model": "aisquared\/chopt-2_7b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~3B",
        "Type": "OPT",
        "Memory (MB)": 5140,
        "Throughput (tokens\/s)": 37.6,
        "Energy (tokens\/kWh)": 574712,
        "Best Score (%)": "40.6**",
        "Model": "aisquared\/chopt-2_7b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "XGLM",
        "Memory (MB)": 15561,
        "Throughput (tokens\/s)": 56.2,
        "Energy (tokens\/kWh)": 543478,
        "Best Score (%)": 40.1,
        "Model": "KoboldAI\/fairseq-dense-2.7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "XGLM",
        "Memory (MB)": 8915,
        "Throughput (tokens\/s)": 54.3,
        "Energy (tokens\/kWh)": 628930,
        "Best Score (%)": 40.1,
        "Model": "KoboldAI\/fairseq-dense-2.7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~3B",
        "Type": "XGLM",
        "Memory (MB)": 5163,
        "Throughput (tokens\/s)": 52.4,
        "Energy (tokens\/kWh)": 714285,
        "Best Score (%)": "40.1**",
        "Model": "KoboldAI\/fairseq-dense-2.7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~3B",
        "Type": "XGLM",
        "Memory (MB)": 5088,
        "Throughput (tokens\/s)": 37.5,
        "Energy (tokens\/kWh)": 571428,
        "Best Score (%)": "40.1**",
        "Model": "KoboldAI\/fairseq-dense-2.7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "GPT-2",
        "Memory (MB)": 13839,
        "Throughput (tokens\/s)": 82.0,
        "Energy (tokens\/kWh)": 862068,
        "Best Score (%)": "39.9**",
        "Model": "Writer\/camel-5b-hf"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "GPT-2",
        "Memory (MB)": 13837,
        "Throughput (tokens\/s)": 81.3,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 39.9,
        "Model": "Writer\/camel-5b-hf"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~7B",
        "Type": "GPT-2",
        "Memory (MB)": 6843,
        "Throughput (tokens\/s)": 74.6,
        "Energy (tokens\/kWh)": 877192,
        "Best Score (%)": "39.9**",
        "Model": "Writer\/camel-5b-hf"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~7B",
        "Type": "GPT-2",
        "Memory (MB)": 13820,
        "Throughput (tokens\/s)": 71.4,
        "Energy (tokens\/kWh)": 793650,
        "Best Score (%)": "39.9**",
        "Model": "Writer\/camel-5b-hf"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "GPT-2",
        "Memory (MB)": 13822,
        "Throughput (tokens\/s)": 70.4,
        "Energy (tokens\/kWh)": 641025,
        "Best Score (%)": 39.9,
        "Model": "Writer\/camel-5b-hf"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~7B",
        "Type": "GPT-2",
        "Memory (MB)": 26386,
        "Throughput (tokens\/s)": 43.7,
        "Energy (tokens\/kWh)": 399999,
        "Best Score (%)": 39.9,
        "Model": "Writer\/camel-5b-hf"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "GPT-Neo",
        "Memory (MB)": 15949,
        "Throughput (tokens\/s)": 48.1,
        "Energy (tokens\/kWh)": 495049,
        "Best Score (%)": 39.0,
        "Model": "EleutherAI\/gpt-neo-2.7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "~3B",
        "Type": "GPT-Neo",
        "Memory (MB)": 5428,
        "Throughput (tokens\/s)": 46.1,
        "Energy (tokens\/kWh)": 591715,
        "Best Score (%)": "39.0**",
        "Model": "EleutherAI\/gpt-neo-2.7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "GPT-Neo",
        "Memory (MB)": 9249,
        "Throughput (tokens\/s)": 43.7,
        "Energy (tokens\/kWh)": 526315,
        "Best Score (%)": 39.0,
        "Model": "EleutherAI\/gpt-neo-2.7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "~3B",
        "Type": "GPT-Neo",
        "Memory (MB)": 9167,
        "Throughput (tokens\/s)": 41.0,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 39.0,
        "Model": "EleutherAI\/gpt-neo-2.7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "~3B",
        "Type": "GPT-Neo",
        "Memory (MB)": 5264,
        "Throughput (tokens\/s)": 35.2,
        "Energy (tokens\/kWh)": 537634,
        "Best Score (%)": "39.0**",
        "Model": "EleutherAI\/gpt-neo-2.7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "~3B",
        "Type": "GPT-Neo",
        "Memory (MB)": 5352,
        "Throughput (tokens\/s)": 34.0,
        "Energy (tokens\/kWh)": 518134,
        "Best Score (%)": "39.0**",
        "Model": "EleutherAI\/gpt-neo-2.7B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 6109,
        "Throughput (tokens\/s)": 46.1,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 38.6,
        "Model": "beaugogh\/pythia-1.4b-deduped-sharegpt"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 8724,
        "Throughput (tokens\/s)": 43.5,
        "Energy (tokens\/kWh)": 606060,
        "Best Score (%)": 38.6,
        "Model": "beaugogh\/pythia-1.4b-deduped-sharegpt"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "<1.5B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 4358,
        "Throughput (tokens\/s)": 41.8,
        "Energy (tokens\/kWh)": 689655,
        "Best Score (%)": "38.6**",
        "Model": "beaugogh\/pythia-1.4b-deduped-sharegpt"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 6109,
        "Throughput (tokens\/s)": 40.8,
        "Energy (tokens\/kWh)": 662251,
        "Best Score (%)": 38.6,
        "Model": "beaugogh\/pythia-1.4b-deduped-sharegpt"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 4356,
        "Throughput (tokens\/s)": 38.2,
        "Energy (tokens\/kWh)": 613496,
        "Best Score (%)": "38.6**",
        "Model": "beaugogh\/pythia-1.4b-deduped-sharegpt"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "GPT-NeoX",
        "Memory (MB)": 4347,
        "Throughput (tokens\/s)": 34.5,
        "Energy (tokens\/kWh)": 581395,
        "Best Score (%)": "38.6**",
        "Model": "beaugogh\/pythia-1.4b-deduped-sharegpt"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "OPT",
        "Memory (MB)": 5646,
        "Throughput (tokens\/s)": 84.0,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 38.4,
        "Model": "aisquared\/chopt-1_3b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "OPT",
        "Memory (MB)": 8166,
        "Throughput (tokens\/s)": 75.2,
        "Energy (tokens\/kWh)": 847457,
        "Best Score (%)": 38.4,
        "Model": "aisquared\/chopt-1_3b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "<1.5B",
        "Type": "OPT",
        "Memory (MB)": 3716,
        "Throughput (tokens\/s)": 71.9,
        "Energy (tokens\/kWh)": 980392,
        "Best Score (%)": "38.4**",
        "Model": "aisquared\/chopt-1_3b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "OPT",
        "Memory (MB)": 5646,
        "Throughput (tokens\/s)": 70.4,
        "Energy (tokens\/kWh)": 1012145,
        "Best Score (%)": 38.4,
        "Model": "aisquared\/chopt-1_3b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "OPT",
        "Memory (MB)": 3733,
        "Throughput (tokens\/s)": 55.9,
        "Energy (tokens\/kWh)": 862068,
        "Best Score (%)": "38.4**",
        "Model": "aisquared\/chopt-1_3b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "OPT",
        "Memory (MB)": 3733,
        "Throughput (tokens\/s)": 49.0,
        "Energy (tokens\/kWh)": 775193,
        "Best Score (%)": "38.4**",
        "Model": "aisquared\/chopt-1_3b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "<1.5B",
        "Type": "XGLM",
        "Memory (MB)": 3731,
        "Throughput (tokens\/s)": 72.5,
        "Energy (tokens\/kWh)": 980392,
        "Best Score (%)": "38.0**",
        "Model": "KoboldAI\/fairseq-dense-1.3B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "XGLM",
        "Memory (MB)": 8137,
        "Throughput (tokens\/s)": 72.5,
        "Energy (tokens\/kWh)": 833333,
        "Best Score (%)": 38.0,
        "Model": "KoboldAI\/fairseq-dense-1.3B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "XGLM",
        "Memory (MB)": 5646,
        "Throughput (tokens\/s)": 71.4,
        "Energy (tokens\/kWh)": 1006036,
        "Best Score (%)": 38.0,
        "Model": "KoboldAI\/fairseq-dense-1.3B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "GPT-Neo",
        "Memory (MB)": 8265,
        "Throughput (tokens\/s)": 62.9,
        "Energy (tokens\/kWh)": 746268,
        "Best Score (%)": 38.0,
        "Model": "MBZUAI\/lamini-neo-1.3b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "<1.5B",
        "Type": "GPT-Neo",
        "Memory (MB)": 3819,
        "Throughput (tokens\/s)": 60.6,
        "Energy (tokens\/kWh)": 854700,
        "Best Score (%)": "38.0**",
        "Model": "MBZUAI\/lamini-neo-1.3b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "GPT-Neo",
        "Memory (MB)": 5667,
        "Throughput (tokens\/s)": 58.5,
        "Energy (tokens\/kWh)": 854700,
        "Best Score (%)": 38.0,
        "Model": "MBZUAI\/lamini-neo-1.3b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "GPT-Neo",
        "Memory (MB)": 5688,
        "Throughput (tokens\/s)": 54.3,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 38.0,
        "Model": "MBZUAI\/lamini-neo-1.3b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "XGLM",
        "Memory (MB)": 3750,
        "Throughput (tokens\/s)": 49.5,
        "Energy (tokens\/kWh)": 781250,
        "Best Score (%)": "38.0**",
        "Model": "KoboldAI\/fairseq-dense-1.3B"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "GPT-Neo",
        "Memory (MB)": 3834,
        "Throughput (tokens\/s)": 44.8,
        "Energy (tokens\/kWh)": 724637,
        "Best Score (%)": "38.0**",
        "Model": "MBZUAI\/lamini-neo-1.3b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "GPT-Neo",
        "Memory (MB)": 3863,
        "Throughput (tokens\/s)": 43.9,
        "Energy (tokens\/kWh)": 709219,
        "Best Score (%)": "38.0**",
        "Model": "MBZUAI\/lamini-neo-1.3b"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "<1.5B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 4062,
        "Throughput (tokens\/s)": 69.9,
        "Energy (tokens\/kWh)": 990099,
        "Best Score (%)": "34.9**",
        "Model": "bigscience\/bloom-1b1"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 7244,
        "Throughput (tokens\/s)": 69.0,
        "Energy (tokens\/kWh)": 854700,
        "Best Score (%)": 34.9,
        "Model": "bigscience\/bloom-1b1"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 5098,
        "Throughput (tokens\/s)": 68.0,
        "Energy (tokens\/kWh)": 1060445,
        "Best Score (%)": 34.9,
        "Model": "bigscience\/bloom-1b1"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 5098,
        "Throughput (tokens\/s)": 65.4,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 34.9,
        "Model": "bigscience\/bloom-1b1"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 4092,
        "Throughput (tokens\/s)": 52.9,
        "Energy (tokens\/kWh)": 869565,
        "Best Score (%)": "34.9**",
        "Model": "bigscience\/bloom-1b1"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "Bloom \ud83c\udf38",
        "Memory (MB)": 4090,
        "Throughput (tokens\/s)": 51.5,
        "Energy (tokens\/kWh)": 806451,
        "Best Score (%)": "34.9**",
        "Model": "bigscience\/bloom-1b1"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "BART",
        "Memory (MB)": 2942,
        "Throughput (tokens\/s)": 201.0,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 31.5,
        "Model": "voidful\/changpt-bart"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "BART",
        "Memory (MB)": 2982,
        "Throughput (tokens\/s)": 194.0,
        "Energy (tokens\/kWh)": 3174603,
        "Best Score (%)": 31.5,
        "Model": "voidful\/changpt-bart"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "BART",
        "Memory (MB)": 2919,
        "Throughput (tokens\/s)": 192.0,
        "Energy (tokens\/kWh)": 3278688,
        "Best Score (%)": 31.5,
        "Model": "voidful\/changpt-bart"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "<1.5B",
        "Type": "BART",
        "Memory (MB)": 2831,
        "Throughput (tokens\/s)": 183.0,
        "Energy (tokens\/kWh)": 3048780,
        "Best Score (%)": "31.5**",
        "Model": "voidful\/changpt-bart"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "BART",
        "Memory (MB)": 2781,
        "Throughput (tokens\/s)": 131.0,
        "Energy (tokens\/kWh)": 2202643,
        "Best Score (%)": "31.5**",
        "Model": "voidful\/changpt-bart"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 2554,
        "Throughput (tokens\/s)": 101.0,
        "Energy (tokens\/kWh)": 1669449,
        "Best Score (%)": 31.3,
        "Model": "breadlicker45\/dough-instruct-base-001"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 2882,
        "Throughput (tokens\/s)": 100.0,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 31.3,
        "Model": "breadlicker45\/dough-instruct-base-001"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 2903,
        "Throughput (tokens\/s)": 91.7,
        "Energy (tokens\/kWh)": 1574803,
        "Best Score (%)": 31.3,
        "Model": "breadlicker45\/dough-instruct-base-001"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "<1.5B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 2731,
        "Throughput (tokens\/s)": 87.7,
        "Energy (tokens\/kWh)": 1451378,
        "Best Score (%)": "31.3**",
        "Model": "breadlicker45\/dough-instruct-base-001"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 2714,
        "Throughput (tokens\/s)": 76.9,
        "Energy (tokens\/kWh)": 1270648,
        "Best Score (%)": "31.3**",
        "Model": "breadlicker45\/dough-instruct-base-001"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "LLaMA \ud83e\udd99",
        "Memory (MB)": 2735,
        "Throughput (tokens\/s)": 71.9,
        "Energy (tokens\/kWh)": 1191895,
        "Best Score (%)": "31.3**",
        "Model": "breadlicker45\/dough-instruct-base-001"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "GPT-BigCode \ud83c\udf38",
        "Memory (MB)": 2708,
        "Throughput (tokens\/s)": 130.0,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 31.1,
        "Model": "bigcode\/tiny_starcoder_py"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "GPT-BigCode \ud83c\udf38",
        "Memory (MB)": 2527,
        "Throughput (tokens\/s)": 119.0,
        "Energy (tokens\/kWh)": 2004008,
        "Best Score (%)": 31.1,
        "Model": "bigcode\/tiny_starcoder_py"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "<1.5B",
        "Type": "GPT-BigCode \ud83c\udf38",
        "Memory (MB)": 2544,
        "Throughput (tokens\/s)": 110.0,
        "Energy (tokens\/kWh)": 1893939,
        "Best Score (%)": "31.1**",
        "Model": "bigcode\/tiny_starcoder_py"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "GPT-BigCode \ud83c\udf38",
        "Memory (MB)": 2716,
        "Throughput (tokens\/s)": 108.0,
        "Energy (tokens\/kWh)": 1923076,
        "Best Score (%)": 31.1,
        "Model": "bigcode\/tiny_starcoder_py"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "GPT-BigCode \ud83c\udf38",
        "Memory (MB)": 2542,
        "Throughput (tokens\/s)": 86.2,
        "Energy (tokens\/kWh)": 1426533,
        "Best Score (%)": "31.1**",
        "Model": "bigcode\/tiny_starcoder_py"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "GPT-BigCode \ud83c\udf38",
        "Memory (MB)": 2548,
        "Throughput (tokens\/s)": 77.5,
        "Energy (tokens\/kWh)": 1321003,
        "Best Score (%)": "31.1**",
        "Model": "bigcode\/tiny_starcoder_py"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float32",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "GPT-J",
        "Memory (MB)": 2036,
        "Throughput (tokens\/s)": 79.4,
        "Energy (tokens\/kWh)": 1379310,
        "Best Score (%)": 31.0,
        "Model": "anton-l\/gpt-j-tiny-random"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "GPT-J",
        "Memory (MB)": 2653,
        "Throughput (tokens\/s)": 79.4,
        "Energy (tokens\/kWh)": "N\/A",
        "Best Score (%)": 31.0,
        "Model": "anton-l\/gpt-j-tiny-random"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "GPTQ.4bit",
        "Class": "<1.5B",
        "Type": "GPT-J",
        "Memory (MB)": 2609,
        "Throughput (tokens\/s)": 74.6,
        "Energy (tokens\/kWh)": 1336898,
        "Best Score (%)": "31.0**",
        "Model": "anton-l\/gpt-j-tiny-random"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "None",
        "Class": "<1.5B",
        "Type": "GPT-J",
        "Memory (MB)": 2653,
        "Throughput (tokens\/s)": 74.1,
        "Energy (tokens\/kWh)": 1377410,
        "Best Score (%)": 31.0,
        "Model": "anton-l\/gpt-j-tiny-random"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "BetterTransformer",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "GPT-J",
        "Memory (MB)": 2609,
        "Throughput (tokens\/s)": 69.4,
        "Energy (tokens\/kWh)": 1226993,
        "Best Score (%)": "31.0**",
        "Model": "anton-l\/gpt-j-tiny-random"
    },
    {
        "Backend": "pytorch",
        "Dtype": "float16",
        "Optimizations": "None",
        "Quantization": "BnB.4bit",
        "Class": "<1.5B",
        "Type": "GPT-J",
        "Memory (MB)": 2607,
        "Throughput (tokens\/s)": 65.8,
        "Energy (tokens\/kWh)": 1183431,
        "Best Score (%)": "31.0**",
        "Model": "anton-l\/gpt-j-tiny-random"
    }
]