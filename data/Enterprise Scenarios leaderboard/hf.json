[
    {
        "T":"\ud83d\udd36",
        "Model":"VAGOsolutions\/SauerkrautLM-SOLAR-Instruct",
        "Average":70.24,
        "FinanceBench":56.67,
        "Legal Confidentiality":83,
        "Writing Prompts":77.78,
        "Customer Support Dialogue":100,
        "Toxic Prompts":51,
        "Enterprise PII":53,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub":39,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"macadeliccc\/SOLAR-10.7b-Instruct-truthy-dpo",
        "Average":69.23,
        "FinanceBench":52.67,
        "Legal Confidentiality":95,
        "Writing Prompts":68.69,
        "Customer Support Dialogue":97,
        "Toxic Prompts":52,
        "Enterprise PII":50,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jeonsworld\/CarbonVillain-en-10.7B-v4",
        "Average":68.16,
        "FinanceBench":53.33,
        "Legal Confidentiality":86,
        "Writing Prompts":64.65,
        "Customer Support Dialogue":100,
        "Toxic Prompts":52,
        "Enterprise PII":53,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":10.73,
        "Hub":5,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mistralai\/Mistral-7B-Instruct-v0.2",
        "Average":66.38,
        "FinanceBench":54.67,
        "Legal Confidentiality":84,
        "Writing Prompts":58.59,
        "Customer Support Dialogue":98,
        "Toxic Prompts":51,
        "Enterprise PII":52,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":789,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\u2b55",
        "Model":"vicgalle\/solarized-18B-dpo",
        "Average":65.66,
        "FinanceBench":47.33,
        "Legal Confidentiality":93,
        "Writing Prompts":62.63,
        "Customer Support Dialogue":99,
        "Toxic Prompts":44,
        "Enterprise PII":48,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":17.93,
        "Hub":2,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\u2b55",
        "Model":"vicgalle\/CarbonBeagle-11B",
        "Average":64.99,
        "FinanceBench":51.33,
        "Legal Confidentiality":75,
        "Writing Prompts":62.63,
        "Customer Support Dialogue":100,
        "Toxic Prompts":50,
        "Enterprise PII":51,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":5,
        "Available on the Hub":true,
        "Model Sha":"b34299185b541cc834e8bffe3c291774ebaa3007"
    },
    {
        "T":"\u2b55",
        "Model":"Mihaiii\/Bucharest-0.2",
        "Average":64.08,
        "FinanceBench":50.0,
        "Legal Confidentiality":94,
        "Writing Prompts":45.45,
        "Customer Support Dialogue":98,
        "Toxic Prompts":47,
        "Enterprise PII":50,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Bucharest-0.3",
        "Average":63.08,
        "FinanceBench":44.0,
        "Legal Confidentiality":95,
        "Writing Prompts":47.47,
        "Customer Support Dialogue":98,
        "Toxic Prompts":44,
        "Enterprise PII":50,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the Hub":false,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"VAGOsolutions\/SauerkrautLM-7b-LaserChat",
        "Average":62.99,
        "FinanceBench":43.33,
        "Legal Confidentiality":80,
        "Writing Prompts":60.61,
        "Customer Support Dialogue":100,
        "Toxic Prompts":45,
        "Enterprise PII":49,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"?",
        "Model":"AIJUUD\/juud-Mistral-7B",
        "Average":61.84,
        "FinanceBench":15.33,
        "Legal Confidentiality":82,
        "Writing Prompts":72.73,
        "Customer Support Dialogue":99,
        "Toxic Prompts":52,
        "Enterprise PII":50,
        "Type":"",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\u2b55",
        "Model":"andysalerno\/rainbowfish-v7",
        "Average":61.8,
        "FinanceBench":51.33,
        "Legal Confidentiality":77,
        "Writing Prompts":47.47,
        "Customer Support Dialogue":94,
        "Toxic Prompts":50,
        "Enterprise PII":51,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\u2b55",
        "Model":"Mihaiii\/Bucharest-0.1",
        "Average":61.52,
        "FinanceBench":44.67,
        "Legal Confidentiality":89,
        "Writing Prompts":42.42,
        "Customer Support Dialogue":100,
        "Toxic Prompts":48,
        "Enterprise PII":45,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"HuggingFaceH4\/zephyr-7b-beta",
        "Average":59.71,
        "FinanceBench":30.67,
        "Legal Confidentiality":75,
        "Writing Prompts":56.57,
        "Customer Support Dialogue":98,
        "Toxic Prompts":51,
        "Enterprise PII":47,
        "Type":"pretrained",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub":1208,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"?",
        "Model":"Mihaiii\/Metis-0.5",
        "Average":58.84,
        "FinanceBench":13.33,
        "Legal Confidentiality":69,
        "Writing Prompts":69.7,
        "Customer Support Dialogue":98,
        "Toxic Prompts":51,
        "Enterprise PII":52,
        "Type":"",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"mistralai\/Mistral-7B-Instruct-v0.1",
        "Average":58.37,
        "FinanceBench":44.6,
        "Legal Confidentiality":58,
        "Writing Prompts":64.65,
        "Customer Support Dialogue":99,
        "Toxic Prompts":32,
        "Enterprise PII":52,
        "Type":"pretrained",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1280,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/mistral-11b-slimorca",
        "Average":58.37,
        "FinanceBench":46.67,
        "Legal Confidentiality":60,
        "Writing Prompts":52.53,
        "Customer Support Dialogue":97,
        "Toxic Prompts":44,
        "Enterprise PII":50,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub":4,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\u2b55",
        "Model":"andysalerno\/rainbowfish-7B-v10",
        "Average":57.66,
        "FinanceBench":52.67,
        "Legal Confidentiality":60,
        "Writing Prompts":31.31,
        "Customer Support Dialogue":94,
        "Toxic Prompts":53,
        "Enterprise PII":55,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the Hub":false,
        "Model Sha":"main"
    },
    {
        "T":"\u2b55",
        "Model":"cognitivecomputations\/openchat-3.5-0106-laser",
        "Average":57.44,
        "FinanceBench":46.0,
        "Legal Confidentiality":40,
        "Writing Prompts":66.67,
        "Customer Support Dialogue":100,
        "Toxic Prompts":41,
        "Enterprise PII":51,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":13,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\u2b55",
        "Model":"andysalerno\/rainbowfish-7B-v9",
        "Average":57.06,
        "FinanceBench":54.0,
        "Legal Confidentiality":59,
        "Writing Prompts":34.34,
        "Customer Support Dialogue":96,
        "Toxic Prompts":46,
        "Enterprise PII":53,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"sethuiyer\/SynthIQ-7b",
        "Average":56.59,
        "FinanceBench":50.0,
        "Legal Confidentiality":34,
        "Writing Prompts":56.57,
        "Customer Support Dialogue":98,
        "Toxic Prompts":48,
        "Enterprise PII":53,
        "Type":"pretrained",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LHC88\/DPOpenHermes-7B-v2-PerfLaser",
        "Average":56.45,
        "FinanceBench":54.0,
        "Legal Confidentiality":22,
        "Writing Prompts":68.69,
        "Customer Support Dialogue":96,
        "Toxic Prompts":45,
        "Enterprise PII":53,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LHC88\/LaseredHermes-7B-v1",
        "Average":56.12,
        "FinanceBench":54.0,
        "Legal Confidentiality":14,
        "Writing Prompts":69.7,
        "Customer Support Dialogue":96,
        "Toxic Prompts":51,
        "Enterprise PII":52,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LHC88\/LaseredHermes-7B-v1",
        "Average":56.12,
        "FinanceBench":54.0,
        "Legal Confidentiality":14,
        "Writing Prompts":69.7,
        "Customer Support Dialogue":96,
        "Toxic Prompts":51,
        "Enterprise PII":52,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"teknium\/OpenHermes-2.5-Mistral-7B",
        "Average":54.76,
        "FinanceBench":46.0,
        "Legal Confidentiality":35,
        "Writing Prompts":54.55,
        "Customer Support Dialogue":96,
        "Toxic Prompts":47,
        "Enterprise PII":50,
        "Type":"pretrained",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":596,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/DPOpenHermes-7B-v2",
        "Average":54.27,
        "FinanceBench":58.0,
        "Legal Confidentiality":8,
        "Writing Prompts":63.64,
        "Customer Support Dialogue":97,
        "Toxic Prompts":49,
        "Enterprise PII":50,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":19,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aloobun\/bun_mistral_7b_v2",
        "Average":53.98,
        "FinanceBench":32.67,
        "Legal Confidentiality":71,
        "Writing Prompts":19.19,
        "Customer Support Dialogue":97,
        "Toxic Prompts":47,
        "Enterprise PII":57,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"macadeliccc\/WestLake-7B-v2-laser-truthy-dpo",
        "Average":53.82,
        "FinanceBench":47.33,
        "Legal Confidentiality":18,
        "Writing Prompts":58.59,
        "Customer Support Dialogue":99,
        "Toxic Prompts":45,
        "Enterprise PII":55,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mihaiii\/Metis-0.4",
        "Average":53.52,
        "FinanceBench":0.67,
        "Legal Confidentiality":79,
        "Writing Prompts":46.46,
        "Customer Support Dialogue":96,
        "Toxic Prompts":47,
        "Enterprise PII":52,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SanjiWatsuki\/Kunoichi-DPO-v2-7B",
        "Average":53.27,
        "FinanceBench":54.0,
        "Legal Confidentiality":3,
        "Writing Prompts":62.63,
        "Customer Support Dialogue":100,
        "Toxic Prompts":49,
        "Enterprise PII":51,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":21,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"berkeley-nest\/Starling-LM-7B-alpha",
        "Average":53.23,
        "FinanceBench":56.67,
        "Legal Confidentiality":0,
        "Writing Prompts":68.69,
        "Customer Support Dialogue":99,
        "Toxic Prompts":45,
        "Enterprise PII":50,
        "Type":"pretrained",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":444,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/loyal-piano-m7",
        "Average":53.16,
        "FinanceBench":49.33,
        "Legal Confidentiality":13,
        "Writing Prompts":63.64,
        "Customer Support Dialogue":99,
        "Toxic Prompts":40,
        "Enterprise PII":54,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":21,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"openchat\/openchat-3.5-0106",
        "Average":52.94,
        "FinanceBench":10.0,
        "Legal Confidentiality":52,
        "Writing Prompts":65.66,
        "Customer Support Dialogue":100,
        "Toxic Prompts":42,
        "Enterprise PII":48,
        "Type":"pretrained",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":140,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TinyLlama\/TinyLlama-1.1B-Chat-v1.0",
        "Average":52.77,
        "FinanceBench":10.0,
        "Legal Confidentiality":60,
        "Writing Prompts":59.6,
        "Customer Support Dialogue":84,
        "Toxic Prompts":48,
        "Enterprise PII":55,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub":716,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\u2b55",
        "Model":"SeaLLMs\/SeaLLM-7B-v2",
        "Average":52.57,
        "FinanceBench":50.67,
        "Legal Confidentiality":0,
        "Writing Prompts":74.75,
        "Customer Support Dialogue":98,
        "Toxic Prompts":43,
        "Enterprise PII":49,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub":19,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jan-hq\/stealth-v1.3",
        "Average":52.17,
        "FinanceBench":55.33,
        "Legal Confidentiality":0,
        "Writing Prompts":66.67,
        "Customer Support Dialogue":97,
        "Toxic Prompts":47,
        "Enterprise PII":47,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/OmniBeagle-7B",
        "Average":52.14,
        "FinanceBench":51.33,
        "Legal Confidentiality":0,
        "Writing Prompts":52.53,
        "Customer Support Dialogue":100,
        "Toxic Prompts":57,
        "Enterprise PII":52,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shadowml\/BeagSake-7B",
        "Average":51.98,
        "FinanceBench":49.33,
        "Legal Confidentiality":0,
        "Writing Prompts":55.56,
        "Customer Support Dialogue":98,
        "Toxic Prompts":54,
        "Enterprise PII":55,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"?",
        "Model":"mlabonne\/NeuralMonarch-7B",
        "Average":51.7,
        "FinanceBench":46.67,
        "Legal Confidentiality":0,
        "Writing Prompts":55.56,
        "Customer Support Dialogue":100,
        "Toxic Prompts":55,
        "Enterprise PII":53,
        "Type":"",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":9,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\u2b55",
        "Model":"abideen\/AlphaMonarch-laser",
        "Average":51.37,
        "FinanceBench":42.67,
        "Legal Confidentiality":0,
        "Writing Prompts":57.58,
        "Customer Support Dialogue":98,
        "Toxic Prompts":56,
        "Enterprise PII":54,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":3,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/NeuralBeagle14-7B",
        "Average":51.15,
        "FinanceBench":53.33,
        "Legal Confidentiality":0,
        "Writing Prompts":58.59,
        "Customer Support Dialogue":99,
        "Toxic Prompts":48,
        "Enterprise PII":48,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub":127,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BlouseJury\/Mistral-7B-Discord-0.1",
        "Average":51.12,
        "FinanceBench":38.67,
        "Legal Confidentiality":67,
        "Writing Prompts":4.04,
        "Customer Support Dialogue":93,
        "Toxic Prompts":36,
        "Enterprise PII":68,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":1,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"senseable\/WestLake-7B-v2",
        "Average":51.05,
        "FinanceBench":42.66,
        "Legal Confidentiality":6,
        "Writing Prompts":62.63,
        "Customer Support Dialogue":98,
        "Toxic Prompts":43,
        "Enterprise PII":54,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":37,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AiMavenAI\/AiMaven-Prometheus",
        "Average":49.53,
        "FinanceBench":50.67,
        "Legal Confidentiality":0,
        "Writing Prompts":52.53,
        "Customer Support Dialogue":97,
        "Toxic Prompts":47,
        "Enterprise PII":50,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jan-hq\/stealth-finance-v3",
        "Average":49.48,
        "FinanceBench":49.33,
        "Legal Confidentiality":0,
        "Writing Prompts":57.58,
        "Customer Support Dialogue":95,
        "Toxic Prompts":44,
        "Enterprise PII":51,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":0,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"aari1995\/germeo-7b-laser",
        "Average":40.92,
        "FinanceBench":22.0,
        "Legal Confidentiality":0,
        "Writing Prompts":52.53,
        "Customer Support Dialogue":91,
        "Toxic Prompts":37,
        "Enterprise PII":43,
        "Type":"RL-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub":2,
        "Available on the Hub":true,
        "Model Sha":"main"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"openlm-research\/open_llama_13b",
        "Average":37.31,
        "FinanceBench":22.6,
        "Legal Confidentiality":0,
        "Writing Prompts":24.24,
        "Customer Support Dialogue":88,
        "Toxic Prompts":42,
        "Enterprise PII":47,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub":447,
        "Available on the Hub":true,
        "Model Sha":"main"
    }
]