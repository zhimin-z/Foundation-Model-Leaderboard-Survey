[
    {
        "Bencmark (Higher is Better)":"MPT (7B)",
        "MMLU":26.8,
        "TriviaQA":59.6,
        "Natural Questions":17.8,
        "GSM8K":6.8,
        "HumanEval":18.3,
        "AGIEval":23.5,
        "BoolQ":75.0,
        "HellaSWAG":76.4,
        "OpenBookQA":51.4,
        "QuAC":37.7,
        "Winogrande":68.3
    },
    {
        "Bencmark (Higher is Better)":"Falcon (7B)",
        "MMLU":26.2,
        "TriviaQA":56.8,
        "Natural Questions":18.1,
        "GSM8K":6.8,
        "HumanEval":null,
        "AGIEval":21.2,
        "BoolQ":67.5,
        "HellaSWAG":74.1,
        "OpenBookQA":51.6,
        "QuAC":18.8,
        "Winogrande":66.3
    },
    {
        "Bencmark (Higher is Better)":"LLaMA-2 (7B)",
        "MMLU":45.3,
        "TriviaQA":68.9,
        "Natural Questions":22.7,
        "GSM8K":14.6,
        "HumanEval":12.8,
        "AGIEval":29.3,
        "BoolQ":77.4,
        "HellaSWAG":77.2,
        "OpenBookQA":58.6,
        "QuAC":39.7,
        "Winogrande":69.2
    },
    {
        "Bencmark (Higher is Better)":"Llama-2 (13B)",
        "MMLU":54.8,
        "TriviaQA":77.2,
        "Natural Questions":28.0,
        "GSM8K":28.7,
        "HumanEval":18.3,
        "AGIEval":39.1,
        "BoolQ":81.7,
        "HellaSWAG":80.7,
        "OpenBookQA":57.0,
        "QuAC":44.8,
        "Winogrande":72.8
    },
    {
        "Bencmark (Higher is Better)":"MPT (30B)",
        "MMLU":46.9,
        "TriviaQA":71.3,
        "Natural Questions":23.0,
        "GSM8K":15.2,
        "HumanEval":25.0,
        "AGIEval":33.8,
        "BoolQ":79.0,
        "HellaSWAG":79.9,
        "OpenBookQA":52.0,
        "QuAC":41.1,
        "Winogrande":71.0
    },
    {
        "Bencmark (Higher is Better)":"Falcon (40B)",
        "MMLU":55.4,
        "TriviaQA":78.6,
        "Natural Questions":29.5,
        "GSM8K":19.6,
        "HumanEval":null,
        "AGIEval":37.0,
        "BoolQ":83.1,
        "HellaSWAG":83.6,
        "OpenBookQA":56.6,
        "QuAC":43.3,
        "Winogrande":76.9
    },
    {
        "Bencmark (Higher is Better)":"LLaMA-1 (65B)",
        "MMLU":63.4,
        "TriviaQA":84.5,
        "Natural Questions":31.0,
        "GSM8K":50.9,
        "HumanEval":23.7,
        "AGIEval":47.6,
        "BoolQ":85.3,
        "HellaSWAG":84.2,
        "OpenBookQA":60.2,
        "QuAC":39.8,
        "Winogrande":77.0
    },
    {
        "Bencmark (Higher is Better)":"LLaMA-2 (70B)",
        "MMLU":68.9,
        "TriviaQA":85.0,
        "Natural Questions":33.0,
        "GSM8K":56.8,
        "HumanEval":29.9,
        "AGIEval":54.2,
        "BoolQ":85.0,
        "HellaSWAG":85.3,
        "OpenBookQA":60.2,
        "QuAC":49.3,
        "Winogrande":80.2
    }
]