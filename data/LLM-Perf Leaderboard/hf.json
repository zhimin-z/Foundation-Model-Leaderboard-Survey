{
    "version": "4.3.0",
    "mode": "blocks",
    "app_id": 699167329971487968,
    "dev_mode": false,
    "analytics_enabled": true,
    "components": [
        {
            "id": 1,
            "type": "html",
            "props": {
                "value": "<img src=\"https://huggingface.co/spaces/optimum/llm-perf-leaderboard/resolve/main/logo.png\">",
                "show_label": true,
                "visible": true,
                "elem_classes": [
                    "logo"
                ],
                "name": "html",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "0ef1a4eade65d710fcd9fc56f6586b0e",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "<p>Hello</p>"
        },
        {
            "id": 2,
            "type": "html",
            "props": {
                "value": "<h1 align=\"center\" id=\"space-title\">\ud83e\udd17 LLM-Perf Leaderboard \ud83c\udfcb\ufe0f</h1>",
                "show_label": true,
                "visible": true,
                "elem_classes": [
                    "title"
                ],
                "name": "html",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "0ef1a4eade65d710fcd9fc56f6586b0e",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "<p>Hello</p>"
        },
        {
            "id": 3,
            "type": "markdown",
            "props": {
                "value": "The \ud83e\udd17 LLM-Perf Leaderboard \ud83c\udfcb\ufe0f aims to benchmark the performance (latency, throughput, memory & energy) of Large Language Models (LLMs) with different hardwares, backends and optimizations using [Optimum-Benchmark](https://github.com/huggingface/optimum-benchmark) and [Optimum](https://github.com/huggingface/optimum) flavors.\n\nAnyone from the community can request a model or a hardware/backend/optimization configuration for automated benchmarking:\n- Model evaluation requests should be made in the [\ud83e\udd17 Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) and will be added to the [\ud83e\udd17 LLM Performance Leaderboard \ud83c\udfcb\ufe0f](https://huggingface.co/spaces/optimum/llm-perf-leaderboard) automatically.\n- Hardware/Backend/Optimization performance requests should be made in the [llm-perf-backend repository](https://github.com/IlyasMoutawwakil/llm-perf-backend) and will be added to the [\ud83e\udd17 LLM Performance Leaderboard \ud83c\udfcb\ufe0f](https://huggingface.co/spaces/optimum/llm-perf-leaderboard) automatically.",
                "show_label": true,
                "rtl": false,
                "latex_delimiters": [
                    {
                        "left": "$$",
                        "right": "$$",
                        "display": true
                    }
                ],
                "visible": true,
                "elem_classes": [
                    "descriptive-text"
                ],
                "sanitize_html": true,
                "line_breaks": false,
                "name": "markdown",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "810c5f8552642d85df14958a33f75123",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "# Hello!"
        },
        {
            "id": 4,
            "type": "tabs",
            "props": {
                "visible": true,
                "elem_classes": [
                    "tabs"
                ],
                "name": "tabs"
            },
            "skip_api": true,
            "component_class_id": "9e076167ca1265948472576573f64ef5"
        },
        {
            "id": 5,
            "type": "tabitem",
            "props": {
                "label": "A100-80GB-275W \ud83d\udda5\ufe0f",
                "id": 0,
                "name": "tabitem"
            },
            "skip_api": true,
            "component_class_id": "f72faf2b8abb615823b199272218625a"
        },
        {
            "id": 6,
            "type": "html",
            "props": {
                "value": "Use this control panel to filter the leaderboard.",
                "show_label": true,
                "visible": true,
                "elem_id": "text",
                "elem_classes": [],
                "name": "html",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "0ef1a4eade65d710fcd9fc56f6586b0e",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "<p>Hello</p>"
        },
        {
            "id": 7,
            "type": "textbox",
            "props": {
                "value": "hf-dgx-01",
                "lines": 1,
                "max_lines": 20,
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": false,
                "autofocus": false,
                "autoscroll": true,
                "elem_classes": [],
                "type": "text",
                "rtl": false,
                "show_copy_button": false,
                "name": "textbox",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "777333cda51df020195231508683831a",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "Hello!!"
        },
        {
            "id": 8,
            "type": "row",
            "props": {
                "variant": "default",
                "visible": true,
                "equal_height": true,
                "name": "row"
            },
            "skip_api": true,
            "component_class_id": "9a37ebd52f5e3ac2d2a1b25e6c1b3eba"
        },
        {
            "id": 9,
            "type": "column",
            "props": {
                "scale": 1,
                "min_width": 320,
                "variant": "default",
                "visible": true,
                "name": "column"
            },
            "skip_api": true,
            "component_class_id": "cc17d07a8a70ee1001b1e05bebd9e3a3"
        },
        {
            "id": 10,
            "type": "textbox",
            "props": {
                "value": "",
                "lines": 1,
                "max_lines": 20,
                "label": "Model \ud83e\udd17",
                "info": "\ud83d\udd0d Search for a model name",
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "search-bar",
                "autofocus": false,
                "autoscroll": true,
                "elem_classes": [],
                "type": "text",
                "rtl": false,
                "show_copy_button": false,
                "name": "textbox",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "777333cda51df020195231508683831a",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "Hello!!"
        },
        {
            "id": 11,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 12,
            "type": "row",
            "props": {
                "variant": "default",
                "visible": true,
                "equal_height": true,
                "name": "row"
            },
            "skip_api": true,
            "component_class_id": "9a37ebd52f5e3ac2d2a1b25e6c1b3eba"
        },
        {
            "id": 13,
            "type": "column",
            "props": {
                "scale": 1,
                "min_width": 320,
                "variant": "panel",
                "visible": true,
                "name": "column"
            },
            "skip_api": true,
            "component_class_id": "cc17d07a8a70ee1001b1e05bebd9e3a3"
        },
        {
            "id": 14,
            "type": "slider",
            "props": {
                "minimum": 0,
                "maximum": 100,
                "value": 0,
                "step": 1,
                "label": "Open LLM Score (%) \ud83d\udcc8",
                "info": "\ud83c\udf9a\ufe0f Slide to minimum Open LLM score",
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "threshold-slider",
                "elem_classes": [],
                "name": "slider",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "92d475377d8dbf22997635901e7c301a",
            "api_info": {
                "type": "number",
                "description": "numeric value between 0 and 100"
            },
            "example_inputs": 0
        },
        {
            "id": 15,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 16,
            "type": "column",
            "props": {
                "scale": 1,
                "min_width": 320,
                "variant": "panel",
                "visible": true,
                "name": "column"
            },
            "skip_api": true,
            "component_class_id": "cc17d07a8a70ee1001b1e05bebd9e3a3"
        },
        {
            "id": 17,
            "type": "slider",
            "props": {
                "minimum": 0,
                "maximum": 81920,
                "value": 81920,
                "step": 100,
                "label": "Peak Memory (MB) \ud83d\udcc8",
                "info": "\ud83c\udf9a\ufe0f Slide to maximum Peak Memory",
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "memory-slider",
                "elem_classes": [],
                "name": "slider",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "92d475377d8dbf22997635901e7c301a",
            "api_info": {
                "type": "number",
                "description": "numeric value between 0 and 81920"
            },
            "example_inputs": 0
        },
        {
            "id": 18,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 19,
            "type": "column",
            "props": {
                "scale": 1,
                "min_width": 320,
                "variant": "default",
                "visible": true,
                "name": "column"
            },
            "skip_api": true,
            "component_class_id": "cc17d07a8a70ee1001b1e05bebd9e3a3"
        },
        {
            "id": 20,
            "type": "checkboxgroup",
            "props": {
                "choices": [
                    [
                        "pytorch",
                        "pytorch"
                    ]
                ],
                "value": [
                    "pytorch"
                ],
                "type": "value",
                "label": "Backends \ud83c\udfed",
                "info": "\u2611\ufe0f Select the backends",
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "backend-checkboxes",
                "elem_classes": [],
                "name": "checkboxgroup",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "cf1be62e677c31d1fe76c5501de84c66",
            "api_info": {
                "items": {
                    "enum": [
                        "pytorch"
                    ],
                    "type": "string"
                },
                "title": "Checkbox Group",
                "type": "array"
            },
            "example_inputs": [
                "pytorch"
            ]
        },
        {
            "id": 21,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 22,
            "type": "row",
            "props": {
                "variant": "default",
                "visible": true,
                "equal_height": true,
                "name": "row"
            },
            "skip_api": true,
            "component_class_id": "9a37ebd52f5e3ac2d2a1b25e6c1b3eba"
        },
        {
            "id": 23,
            "type": "column",
            "props": {
                "scale": 1,
                "min_width": 320,
                "variant": "panel",
                "visible": true,
                "name": "column"
            },
            "skip_api": true,
            "component_class_id": "cc17d07a8a70ee1001b1e05bebd9e3a3"
        },
        {
            "id": 24,
            "type": "checkboxgroup",
            "props": {
                "choices": [
                    [
                        "float32",
                        "float32"
                    ],
                    [
                        "float16",
                        "float16"
                    ],
                    [
                        "bfloat16",
                        "bfloat16"
                    ]
                ],
                "value": [
                    "float32",
                    "float16",
                    "bfloat16"
                ],
                "type": "value",
                "label": "Load DTypes \ud83d\udce5",
                "info": "\u2611\ufe0f Select the load data types",
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "dtype-checkboxes",
                "elem_classes": [],
                "name": "checkboxgroup",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "cf1be62e677c31d1fe76c5501de84c66",
            "api_info": {
                "items": {
                    "enum": [
                        "float32",
                        "float16",
                        "bfloat16"
                    ],
                    "type": "string"
                },
                "title": "Checkbox Group",
                "type": "array"
            },
            "example_inputs": [
                "float32"
            ]
        },
        {
            "id": 25,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 26,
            "type": "column",
            "props": {
                "scale": 1,
                "min_width": 320,
                "variant": "panel",
                "visible": true,
                "name": "column"
            },
            "skip_api": true,
            "component_class_id": "cc17d07a8a70ee1001b1e05bebd9e3a3"
        },
        {
            "id": 27,
            "type": "checkboxgroup",
            "props": {
                "choices": [
                    [
                        "None",
                        "None"
                    ],
                    [
                        "BetterTransformer",
                        "BetterTransformer"
                    ],
                    [
                        "FlashAttentionV2",
                        "FlashAttentionV2"
                    ]
                ],
                "value": [
                    "None",
                    "BetterTransformer",
                    "FlashAttentionV2"
                ],
                "type": "value",
                "label": "Optimizations \ud83d\udee0\ufe0f",
                "info": "\u2611\ufe0f Select the optimization",
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "optimization-checkboxes",
                "elem_classes": [],
                "name": "checkboxgroup",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "cf1be62e677c31d1fe76c5501de84c66",
            "api_info": {
                "items": {
                    "enum": [
                        "None",
                        "BetterTransformer",
                        "FlashAttentionV2"
                    ],
                    "type": "string"
                },
                "title": "Checkbox Group",
                "type": "array"
            },
            "example_inputs": [
                "None"
            ]
        },
        {
            "id": 28,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 29,
            "type": "column",
            "props": {
                "scale": 2,
                "min_width": 320,
                "variant": "default",
                "visible": true,
                "name": "column"
            },
            "skip_api": true,
            "component_class_id": "cc17d07a8a70ee1001b1e05bebd9e3a3"
        },
        {
            "id": 30,
            "type": "checkboxgroup",
            "props": {
                "choices": [
                    [
                        "None",
                        "None"
                    ],
                    [
                        "BnB.4bit",
                        "BnB.4bit"
                    ],
                    [
                        "BnB.8bit",
                        "BnB.8bit"
                    ],
                    [
                        "GPTQ.4bit",
                        "GPTQ.4bit"
                    ],
                    [
                        "GPTQ.4bit+ExllamaV1",
                        "GPTQ.4bit+ExllamaV1"
                    ],
                    [
                        "GPTQ.4bit+ExllamaV2",
                        "GPTQ.4bit+ExllamaV2"
                    ],
                    [
                        "AWQ.4bit+GEMM",
                        "AWQ.4bit+GEMM"
                    ],
                    [
                        "AWQ.4bit+GEMV",
                        "AWQ.4bit+GEMV"
                    ]
                ],
                "value": [
                    "None",
                    "BnB.4bit",
                    "BnB.8bit",
                    "GPTQ.4bit",
                    "GPTQ.4bit+ExllamaV1",
                    "GPTQ.4bit+ExllamaV2",
                    "AWQ.4bit+GEMM",
                    "AWQ.4bit+GEMV"
                ],
                "type": "value",
                "label": "Quantizations \ud83d\udddc\ufe0f",
                "info": "\u2611\ufe0f Select the quantization schemes",
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "quantization-checkboxes",
                "elem_classes": [],
                "name": "checkboxgroup",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "cf1be62e677c31d1fe76c5501de84c66",
            "api_info": {
                "items": {
                    "enum": [
                        "None",
                        "BnB.4bit",
                        "BnB.8bit",
                        "GPTQ.4bit",
                        "GPTQ.4bit+ExllamaV1",
                        "GPTQ.4bit+ExllamaV2",
                        "AWQ.4bit+GEMM",
                        "AWQ.4bit+GEMV"
                    ],
                    "type": "string"
                },
                "title": "Checkbox Group",
                "type": "array"
            },
            "example_inputs": [
                "None"
            ]
        },
        {
            "id": 31,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 32,
            "type": "row",
            "props": {
                "variant": "default",
                "visible": true,
                "equal_height": true,
                "name": "row"
            },
            "skip_api": true,
            "component_class_id": "9a37ebd52f5e3ac2d2a1b25e6c1b3eba"
        },
        {
            "id": 33,
            "type": "button",
            "props": {
                "value": "Filter \ud83d\ude80",
                "variant": "secondary",
                "visible": true,
                "interactive": true,
                "elem_id": "filter-button",
                "elem_classes": [],
                "name": "button",
                "_selectable": false
            },
            "skip_api": true,
            "component_class_id": "e6d4e61f49a0e5522730f312b1fb6380"
        },
        {
            "id": 34,
            "type": "tabs",
            "props": {
                "visible": true,
                "elem_classes": [
                    "subtabs"
                ],
                "name": "tabs"
            },
            "skip_api": true,
            "component_class_id": "9e076167ca1265948472576573f64ef5"
        },
        {
            "id": 35,
            "type": "tabitem",
            "props": {
                "label": "Leaderboard \ud83c\udfc5",
                "id": 0,
                "name": "tabitem"
            },
            "skip_api": true,
            "component_class_id": "f72faf2b8abb615823b199272218625a"
        },
        {
            "id": 36,
            "type": "html",
            "props": {
                "value": "\ud83d\udc49 Scroll to the right \ud83d\udc49 for additional columns.",
                "show_label": true,
                "visible": true,
                "elem_id": "text",
                "elem_classes": [],
                "name": "html",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "0ef1a4eade65d710fcd9fc56f6586b0e",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "<p>Hello</p>"
        },
        {
            "id": 37,
            "type": "dataframe",
            "props": {
                "value": {
                    "headers": [
                        "Model \ud83e\udd17",
                        "Arch \ud83c\udfdb\ufe0f",
                        "Params (B)",
                        "Open LLM Score (%)",
                        "Backend \ud83c\udfed",
                        "DType \ud83d\udce5",
                        "Optimization \ud83d\udee0\ufe0f",
                        "Quantization \ud83d\udddc\ufe0f",
                        "Prefill Latency (s)",
                        "Decode Throughput (tokens/s)",
                        "Allocated Memory (MB)",
                        "Energy (tokens/kWh)",
                        "E2E Latency (s)",
                        "E2E Throughput (tokens/s)",
                        "Reserved Memory (MB)",
                        "Used Memory (MB)"
                    ],
                    "data": [
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_11Bx2_MoE_19B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_11Bx2_MoE_19B</a>",
                            "\u24c2\ufe0f Mixtral",
                            19.19,
                            "74.41 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.116,
                            11.6,
                            38693,
                            143678.0,
                            22.1,
                            11.6,
                            38958,
                            40418
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_11Bx2_MoE_19B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_11Bx2_MoE_19B</a>",
                            "\u24c2\ufe0f Mixtral",
                            19.19,
                            "74.41 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.866,
                            11.2,
                            77380,
                            105485.0,
                            23.6,
                            10.8,
                            77512,
                            78964
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx2_MoE\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx2_MoE</a>",
                            "\u24c2\ufe0f Mixtral",
                            12.88,
                            "73.43 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0777,
                            17.3,
                            26446,
                            215982.0,
                            14.8,
                            17.3,
                            26648,
                            28108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx2_MoE\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx2_MoE</a>",
                            "\u24c2\ufe0f Mixtral",
                            12.88,
                            "73.43 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0865,
                            17.5,
                            26446,
                            204918.0,
                            14.7,
                            17.4,
                            26648,
                            28108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx2_MoE\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx2_MoE</a>",
                            "\u24c2\ufe0f Mixtral",
                            12.88,
                            "73.43*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.175,
                            6.13,
                            13843,
                            90909.0,
                            41.8,
                            6.12,
                            14053,
                            15520
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx2_MoE\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx2_MoE</a>",
                            "\u24c2\ufe0f Mixtral",
                            12.88,
                            "73.43*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.542,
                            11.3,
                            8619,
                            158227.0,
                            23.1,
                            11.1,
                            8967,
                            10426
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx2_MoE\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx2_MoE</a>",
                            "\u24c2\ufe0f Mixtral",
                            12.88,
                            "73.43*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.554,
                            11.5,
                            8619,
                            153374.0,
                            22.8,
                            11.2,
                            8967,
                            10426
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx2_MoE\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx2_MoE</a>",
                            "\u24c2\ufe0f Mixtral",
                            12.88,
                            "73.43 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.567,
                            16.7,
                            52884,
                            162074.0,
                            15.9,
                            16.1,
                            52982,
                            54433
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rishiraj/CatPPT-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rishiraj/CatPPT-base</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "72.25 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0354,
                            34.7,
                            15171,
                            408163.0,
                            7.39,
                            34.6,
                            15374,
                            16833
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rishiraj/CatPPT-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rishiraj/CatPPT-base</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "72.25 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0413,
                            34.0,
                            15171,
                            387596.0,
                            7.53,
                            34.0,
                            15372,
                            16831
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rishiraj/CatPPT-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rishiraj/CatPPT-base</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "72.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0451,
                            32.2,
                            4963,
                            434782.0,
                            7.98,
                            32.1,
                            5161,
                            6622
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rishiraj/CatPPT-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rishiraj/CatPPT-base</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "72.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0461,
                            32.4,
                            5908,
                            465116.0,
                            7.92,
                            32.3,
                            6106,
                            7568
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rishiraj/CatPPT-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rishiraj/CatPPT-base</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "72.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0659,
                            33.5,
                            4838,
                            456621.0,
                            7.68,
                            33.3,
                            5052,
                            6511
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rishiraj/CatPPT-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rishiraj/CatPPT-base</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "72.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0766,
                            30.4,
                            4838,
                            427350.0,
                            8.47,
                            30.2,
                            5052,
                            6511
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rishiraj/CatPPT-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rishiraj/CatPPT-base</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "72.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.101,
                            9.88,
                            8197,
                            146412.0,
                            25.9,
                            9.88,
                            8399,
                            9867
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rishiraj/CatPPT-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rishiraj/CatPPT-base</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "72.25*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.103,
                            9.7,
                            8197,
                            143678.0,
                            26.4,
                            9.7,
                            8399,
                            9867
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rishiraj/CatPPT-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rishiraj/CatPPT-base</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "72.25 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.264,
                            37.2,
                            30334,
                            327868.0,
                            7.12,
                            36.0,
                            30431,
                            31883
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rishiraj/CatPPT-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rishiraj/CatPPT-base</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "72.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.289,
                            20.7,
                            5428,
                            274725.0,
                            12.6,
                            20.3,
                            5758,
                            7218
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rishiraj/CatPPT-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rishiraj/CatPPT-base</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "72.25*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.299,
                            20.7,
                            5428,
                            270270.0,
                            12.6,
                            20.3,
                            5758,
                            7218
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/Yi-34B-Llama\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/Yi-34B-Llama</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.95 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.134,
                            18.5,
                            69165,
                            166112.0,
                            13.9,
                            18.4,
                            69331,
                            70791
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/Yi-34B-Llama\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/Yi-34B-Llama</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.163,
                            19.6,
                            19971,
                            199203.0,
                            13.2,
                            19.4,
                            20336,
                            21797
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/Yi-34B-Llama\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/Yi-34B-Llama</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.164,
                            19.6,
                            21328,
                            241545.0,
                            13.2,
                            19.4,
                            21695,
                            23156
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/Yi-34B-Llama\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/Yi-34B-Llama</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.185,
                            5.41,
                            35851,
                            76923.0,
                            47.3,
                            5.41,
                            36383,
                            37851
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/Yi-34B-Llama\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/Yi-34B-Llama</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.95*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.187,
                            5.47,
                            35836,
                            73529.0,
                            46.8,
                            5.47,
                            36383,
                            37851
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/Yi-34B-Llama\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/Yi-34B-Llama</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.256,
                            20.2,
                            19654,
                            212314.0,
                            12.9,
                            19.8,
                            20076,
                            21535
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/Yi-34B-Llama\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/Yi-34B-Llama</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.32,
                            18.3,
                            19654,
                            204081.0,
                            14.2,
                            18.0,
                            20076,
                            21535
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/Yi-34B-Llama\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/Yi-34B-Llama</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            1.24,
                            11.5,
                            21798,
                            122850.0,
                            23.4,
                            10.9,
                            22802,
                            24261
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/Yi-34B-Llama\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/Yi-34B-Llama</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.95*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            1.25,
                            11.8,
                            21798,
                            124533.0,
                            22.9,
                            11.2,
                            22802,
                            24261
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.81 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.134,
                            18.2,
                            75184,
                            165562.0,
                            14.1,
                            18.2,
                            75803,
                            77263
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.163,
                            19.3,
                            25989,
                            198807.0,
                            13.4,
                            19.1,
                            26501,
                            27963
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.164,
                            19.6,
                            27347,
                            242718.0,
                            13.2,
                            19.4,
                            27862,
                            29324
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.81*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.186,
                            5.45,
                            41854,
                            74074.0,
                            47.0,
                            5.45,
                            42549,
                            44017
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.189,
                            5.43,
                            41869,
                            76335.0,
                            47.2,
                            5.42,
                            42549,
                            44017
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.254,
                            20.7,
                            25673,
                            212314.0,
                            12.6,
                            20.3,
                            26241,
                            27701
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.313,
                            18.2,
                            25673,
                            204498.0,
                            14.3,
                            17.9,
                            26241,
                            27701
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            1.24,
                            11.5,
                            27816,
                            123609.0,
                            23.4,
                            10.9,
                            28967,
                            30427
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "70.81*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            1.25,
                            11.8,
                            27816,
                            124533.0,
                            22.9,
                            11.2,
                            28967,
                            30427
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "69.42 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.134,
                            18.5,
                            69165,
                            166112.0,
                            13.9,
                            18.4,
                            69331,
                            70791
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "69.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.164,
                            19.2,
                            21328,
                            239234.0,
                            13.5,
                            19.0,
                            21695,
                            23156
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "69.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.166,
                            19.6,
                            19971,
                            198019.0,
                            13.2,
                            19.4,
                            20336,
                            21797
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "69.42*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.182,
                            5.39,
                            35836,
                            74626.0,
                            47.5,
                            5.39,
                            36383,
                            37851
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "69.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.184,
                            5.37,
                            35851,
                            76335.0,
                            47.7,
                            5.37,
                            36383,
                            37851
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "69.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.254,
                            20.2,
                            19654,
                            215053.0,
                            12.9,
                            19.8,
                            20076,
                            21535
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "69.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.314,
                            18.1,
                            19654,
                            200803.0,
                            14.4,
                            17.8,
                            20076,
                            21535
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "69.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            1.24,
                            11.5,
                            21798,
                            123456.0,
                            23.3,
                            11.0,
                            22802,
                            24261
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B</a>",
                            "\ud83e\udd99 LLaMA",
                            34.39,
                            "69.42*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            1.25,
                            11.7,
                            21798,
                            123915.0,
                            23.0,
                            11.1,
                            22802,
                            24261
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/deepseek-ai/deepseek-llm-67b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">deepseek-ai/deepseek-llm-67b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            67.0,
                            "69.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.295,
                            3.38,
                            69753,
                            47619.0,
                            75.7,
                            3.38,
                            70380,
                            71848
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/deepseek-ai/deepseek-llm-67b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">deepseek-ai/deepseek-llm-67b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            67.0,
                            "69.38*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.296,
                            3.4,
                            69737,
                            45045.0,
                            75.4,
                            3.4,
                            70380,
                            71848
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/deepseek-ai/deepseek-llm-67b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">deepseek-ai/deepseek-llm-67b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            67.0,
                            "69.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.308,
                            12.3,
                            40008,
                            146842.0,
                            21.1,
                            12.1,
                            40376,
                            41838
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/deepseek-ai/deepseek-llm-67b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">deepseek-ai/deepseek-llm-67b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            67.0,
                            "69.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.312,
                            12.1,
                            38539,
                            117647.0,
                            21.3,
                            12.0,
                            38904,
                            40365
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/deepseek-ai/deepseek-llm-67b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">deepseek-ai/deepseek-llm-67b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            67.0,
                            "69.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.591,
                            11.5,
                            38138,
                            123001.0,
                            22.8,
                            11.2,
                            38541,
                            40001
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/deepseek-ai/deepseek-llm-67b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">deepseek-ai/deepseek-llm-67b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            67.0,
                            "69.38*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            2.57,
                            6.39,
                            41781,
                            68493.0,
                            42.5,
                            6.02,
                            42941,
                            44400
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/deepseek-ai/deepseek-llm-67b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">deepseek-ai/deepseek-llm-67b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            67.0,
                            "69.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            2.58,
                            6.27,
                            41781,
                            66225.0,
                            43.3,
                            5.91,
                            42941,
                            44400
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/Mixtral-8x7B-v0.1-top3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/Mixtral-8x7B-v0.1-top3</a>",
                            "\u24c2\ufe0f Mixtral",
                            46.7,
                            "69.09*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.227,
                            4.59,
                            47691,
                            63694.0,
                            55.7,
                            4.6,
                            47867,
                            49335
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/Mixtral-8x7B-v0.1-top3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/Mixtral-8x7B-v0.1-top3</a>",
                            "\u24c2\ufe0f Mixtral",
                            46.7,
                            "69.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.229,
                            4.54,
                            47691,
                            68493.0,
                            56.4,
                            4.54,
                            47880,
                            49348
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/Mixtral-8x7B-v0.1-top3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/Mixtral-8x7B-v0.1-top3</a>",
                            "\u24c2\ufe0f Mixtral",
                            46.7,
                            "69.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.801,
                            7.97,
                            27662,
                            108459.0,
                            32.8,
                            7.8,
                            28196,
                            29655
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/Mixtral-8x7B-v0.1-top3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/Mixtral-8x7B-v0.1-top3</a>",
                            "\u24c2\ufe0f Mixtral",
                            46.7,
                            "69.09*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.809,
                            8.02,
                            27662,
                            108813.0,
                            32.6,
                            7.85,
                            28194,
                            29653
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx4_MOE_24B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx4_MOE_24B</a>",
                            "\u24c2\ufe0f Mixtral",
                            24.15,
                            "68.85 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.114,
                            15.8,
                            48995,
                            200400.0,
                            16.2,
                            15.8,
                            49197,
                            50656
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx4_MOE_24B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx4_MOE_24B</a>",
                            "\u24c2\ufe0f Mixtral",
                            24.15,
                            "68.85 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.121,
                            15.6,
                            48995,
                            193423.0,
                            16.4,
                            15.6,
                            49197,
                            50656
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx4_MOE_24B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx4_MOE_24B</a>",
                            "\u24c2\ufe0f Mixtral",
                            24.15,
                            "68.85*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.177,
                            5.92,
                            25125,
                            88495.0,
                            43.3,
                            5.91,
                            25337,
                            26805
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx4_MOE_24B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx4_MOE_24B</a>",
                            "\u24c2\ufe0f Mixtral",
                            24.15,
                            "68.85*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.181,
                            5.73,
                            25125,
                            78740.0,
                            44.7,
                            5.73,
                            25333,
                            26801
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx4_MOE_24B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx4_MOE_24B</a>",
                            "\u24c2\ufe0f Mixtral",
                            24.15,
                            "68.85*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.553,
                            10.4,
                            14970,
                            141843.0,
                            25.1,
                            10.2,
                            15325,
                            16785
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx4_MOE_24B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx4_MOE_24B</a>",
                            "\u24c2\ufe0f Mixtral",
                            24.15,
                            "68.85*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.562,
                            10.5,
                            14970,
                            142857.0,
                            24.9,
                            10.3,
                            15328,
                            16787
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mixtral-8x7B-v0.1</a>",
                            "\u24c2\ufe0f Mixtral",
                            46.7,
                            "68.42*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.185,
                            5.41,
                            47691,
                            78740.0,
                            47.3,
                            5.41,
                            47867,
                            49335
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mixtral-8x7B-v0.1</a>",
                            "\u24c2\ufe0f Mixtral",
                            46.7,
                            "68.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.188,
                            5.57,
                            47691,
                            84033.0,
                            46.0,
                            5.57,
                            47880,
                            49348
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mixtral-8x7B-v0.1</a>",
                            "\u24c2\ufe0f Mixtral",
                            46.7,
                            "68.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.561,
                            9.62,
                            27662,
                            133689.0,
                            27.1,
                            9.45,
                            28196,
                            29655
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mixtral-8x7B-v0.1</a>",
                            "\u24c2\ufe0f Mixtral",
                            46.7,
                            "68.42*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.569,
                            9.85,
                            27662,
                            136612.0,
                            26.5,
                            9.66,
                            28194,
                            29653
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Walmart-the-bag/WordWoven-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Walmart-the-bag/WordWoven-13B</a>",
                            "\u24c2\ufe0f Mixtral",
                            12.88,
                            "68.25 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0779,
                            17.2,
                            26446,
                            210084.0,
                            14.9,
                            17.2,
                            26648,
                            28108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Walmart-the-bag/WordWoven-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Walmart-the-bag/WordWoven-13B</a>",
                            "\u24c2\ufe0f Mixtral",
                            12.88,
                            "68.25 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.511,
                            16.8,
                            52884,
                            160256.0,
                            15.7,
                            16.3,
                            52982,
                            54433
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-70b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-70b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            68.98,
                            "67.87*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.244,
                            4.0,
                            70040,
                            52083.0,
                            64.0,
                            4.0,
                            70506,
                            71974
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-70b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-70b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            68.98,
                            "67.87*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.31,
                            14.6,
                            39553,
                            163934.0,
                            17.8,
                            14.4,
                            39795,
                            41257
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-70b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-70b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            68.98,
                            "67.87*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.315,
                            13.2,
                            37649,
                            123152.0,
                            19.6,
                            13.1,
                            37889,
                            39350
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-70b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-70b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            68.98,
                            "67.87*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.497,
                            15.4,
                            37141,
                            140056.0,
                            17.1,
                            15.0,
                            37453,
                            38912
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-70b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-70b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            68.98,
                            "67.87*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.602,
                            13.9,
                            37141,
                            137551.0,
                            19.0,
                            13.5,
                            37453,
                            38912
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/upstage/SOLAR-10.7B-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">upstage/SOLAR-10.7B-v1.0</a>",
                            "\ud83e\udd99 LLaMA",
                            10.73,
                            "66.04 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0497,
                            25.6,
                            21781,
                            301204.0,
                            10.0,
                            25.6,
                            22047,
                            23506
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/seungduk/KoSOLAR-10.7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">seungduk/KoSOLAR-10.7B-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            10.86,
                            "66.04 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0498,
                            25.2,
                            22045,
                            301204.0,
                            10.1,
                            25.3,
                            22353,
                            23813
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/upstage/SOLAR-10.7B-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">upstage/SOLAR-10.7B-v1.0</a>",
                            "\ud83e\udd99 LLaMA",
                            10.73,
                            "66.04 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0509,
                            26.6,
                            21773,
                            286532.0,
                            9.65,
                            26.5,
                            22047,
                            23506
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/seungduk/KoSOLAR-10.7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">seungduk/KoSOLAR-10.7B-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            10.86,
                            "66.04 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.051,
                            26.1,
                            22037,
                            287356.0,
                            9.83,
                            26.0,
                            22353,
                            23813
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/upstage/SOLAR-10.7B-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">upstage/SOLAR-10.7B-v1.0</a>",
                            "\ud83e\udd99 LLaMA",
                            10.73,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0628,
                            24.5,
                            6411,
                            317460.0,
                            10.5,
                            24.4,
                            6494,
                            7956
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/seungduk/KoSOLAR-10.7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">seungduk/KoSOLAR-10.7B-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            10.86,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0643,
                            24.3,
                            6675,
                            318471.0,
                            10.6,
                            24.2,
                            6801,
                            8262
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/upstage/SOLAR-10.7B-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">upstage/SOLAR-10.7B-v1.0</a>",
                            "\ud83e\udd99 LLaMA",
                            10.73,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0664,
                            24.3,
                            7358,
                            344827.0,
                            10.6,
                            24.2,
                            7442,
                            8904
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/seungduk/KoSOLAR-10.7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">seungduk/KoSOLAR-10.7B-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            10.86,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0667,
                            24.8,
                            7622,
                            341296.0,
                            10.4,
                            24.6,
                            7751,
                            9212
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/upstage/SOLAR-10.7B-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">upstage/SOLAR-10.7B-v1.0</a>",
                            "\ud83e\udd99 LLaMA",
                            10.73,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0957,
                            25.2,
                            6282,
                            337837.0,
                            10.2,
                            25.1,
                            6381,
                            7841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/seungduk/KoSOLAR-10.7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">seungduk/KoSOLAR-10.7B-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            10.86,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.096,
                            25.2,
                            6546,
                            337837.0,
                            10.2,
                            25.1,
                            6698,
                            8157
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/seungduk/KoSOLAR-10.7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">seungduk/KoSOLAR-10.7B-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            10.86,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.112,
                            22.6,
                            6546,
                            306748.0,
                            11.4,
                            22.5,
                            6698,
                            8157
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/upstage/SOLAR-10.7B-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">upstage/SOLAR-10.7B-v1.0</a>",
                            "\ud83e\udd99 LLaMA",
                            10.73,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.113,
                            22.4,
                            6282,
                            316455.0,
                            11.5,
                            22.3,
                            6381,
                            7841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/seungduk/KoSOLAR-10.7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">seungduk/KoSOLAR-10.7B-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            10.86,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.143,
                            6.87,
                            11577,
                            102669.0,
                            37.2,
                            6.88,
                            11729,
                            13197
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/upstage/SOLAR-10.7B-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">upstage/SOLAR-10.7B-v1.0</a>",
                            "\ud83e\udd99 LLaMA",
                            10.73,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.144,
                            6.84,
                            11312,
                            102459.0,
                            37.4,
                            6.84,
                            11421,
                            12889
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/seungduk/KoSOLAR-10.7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">seungduk/KoSOLAR-10.7B-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            10.86,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.148,
                            6.71,
                            11585,
                            99999.0,
                            38.1,
                            6.72,
                            11729,
                            13197
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/upstage/SOLAR-10.7B-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">upstage/SOLAR-10.7B-v1.0</a>",
                            "\ud83e\udd99 LLaMA",
                            10.73,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.149,
                            6.76,
                            11320,
                            100300.0,
                            37.8,
                            6.77,
                            11421,
                            12889
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/upstage/SOLAR-10.7B-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">upstage/SOLAR-10.7B-v1.0</a>",
                            "\ud83e\udd99 LLaMA",
                            10.73,
                            "66.04 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.39,
                            26.5,
                            43555,
                            231481.0,
                            10.0,
                            25.6,
                            43673,
                            45124
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/seungduk/KoSOLAR-10.7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">seungduk/KoSOLAR-10.7B-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            10.86,
                            "66.04 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.39,
                            26.5,
                            44079,
                            229357.0,
                            10.0,
                            25.6,
                            44214,
                            45665
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/seungduk/KoSOLAR-10.7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">seungduk/KoSOLAR-10.7B-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            10.86,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.429,
                            15.1,
                            7245,
                            199203.0,
                            17.3,
                            14.8,
                            7530,
                            8990
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/upstage/SOLAR-10.7B-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">upstage/SOLAR-10.7B-v1.0</a>",
                            "\ud83e\udd99 LLaMA",
                            10.73,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.43,
                            14.7,
                            6980,
                            195694.0,
                            17.8,
                            14.4,
                            7266,
                            8726
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/upstage/SOLAR-10.7B-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">upstage/SOLAR-10.7B-v1.0</a>",
                            "\ud83e\udd99 LLaMA",
                            10.73,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.434,
                            14.7,
                            6980,
                            191938.0,
                            17.7,
                            14.5,
                            7266,
                            8726
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/seungduk/KoSOLAR-10.7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">seungduk/KoSOLAR-10.7B-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            10.86,
                            "66.04*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.439,
                            14.7,
                            7245,
                            192678.0,
                            17.7,
                            14.5,
                            7530,
                            8990
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx2_MoE_13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx2_MoE_13B</a>",
                            null,
                            12.88,
                            "65.14 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0771,
                            17.5,
                            26446,
                            216919.0,
                            14.7,
                            17.4,
                            26648,
                            28108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx2_MoE_13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx2_MoE_13B</a>",
                            null,
                            12.88,
                            "65.14 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0869,
                            17.3,
                            26446,
                            206185.0,
                            14.8,
                            17.3,
                            26648,
                            28108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx2_MoE_13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx2_MoE_13B</a>",
                            null,
                            12.88,
                            "65.14*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.173,
                            6.0,
                            13843,
                            90090.0,
                            42.7,
                            6.0,
                            14053,
                            15520
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx2_MoE_13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx2_MoE_13B</a>",
                            null,
                            12.88,
                            "65.14*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.174,
                            5.94,
                            13843,
                            88495.0,
                            43.1,
                            5.94,
                            14050,
                            15518
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx2_MoE_13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx2_MoE_13B</a>",
                            null,
                            12.88,
                            "65.14*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.542,
                            11.4,
                            8619,
                            157480.0,
                            22.9,
                            11.2,
                            8967,
                            10426
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_7Bx2_MoE_13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_7Bx2_MoE_13B</a>",
                            null,
                            12.88,
                            "65.14*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.555,
                            11.5,
                            8619,
                            152439.0,
                            22.8,
                            11.2,
                            8967,
                            10426
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Delcos/Starling-LM-11B-alpha\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delcos/Starling-LM-11B-alpha</a>",
                            "\u24c2\ufe0f Mistral",
                            11.39,
                            "63.66 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0568,
                            22.2,
                            23216,
                            261096.0,
                            11.6,
                            22.1,
                            23305,
                            24765
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Delcos/Starling-LM-11B-alpha\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delcos/Starling-LM-11B-alpha</a>",
                            "\u24c2\ufe0f Mistral",
                            11.39,
                            "63.66 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0656,
                            21.6,
                            23216,
                            243309.0,
                            11.9,
                            21.5,
                            23303,
                            24763
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Delcos/Starling-LM-11B-alpha\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delcos/Starling-LM-11B-alpha</a>",
                            "\u24c2\ufe0f Mistral",
                            11.39,
                            "63.66*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0709,
                            21.1,
                            6878,
                            279329.0,
                            12.2,
                            21.0,
                            6962,
                            8424
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Delcos/Starling-LM-11B-alpha\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delcos/Starling-LM-11B-alpha</a>",
                            "\u24c2\ufe0f Mistral",
                            11.39,
                            "63.66*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0722,
                            20.9,
                            7825,
                            299401.0,
                            12.3,
                            20.8,
                            7912,
                            9374
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Delcos/Starling-LM-11B-alpha\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delcos/Starling-LM-11B-alpha</a>",
                            "\u24c2\ufe0f Mistral",
                            11.39,
                            "63.66*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.105,
                            21.4,
                            6748,
                            295857.0,
                            12.0,
                            21.3,
                            6853,
                            8313
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Delcos/Starling-LM-11B-alpha\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delcos/Starling-LM-11B-alpha</a>",
                            "\u24c2\ufe0f Mistral",
                            11.39,
                            "63.66*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.122,
                            19.8,
                            6748,
                            272479.0,
                            13.0,
                            19.7,
                            6853,
                            8313
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Delcos/Starling-LM-11B-alpha\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delcos/Starling-LM-11B-alpha</a>",
                            "\u24c2\ufe0f Mistral",
                            11.39,
                            "63.66*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.165,
                            6.13,
                            12101,
                            92592.0,
                            41.8,
                            6.12,
                            12197,
                            13664
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Delcos/Starling-LM-11B-alpha\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delcos/Starling-LM-11B-alpha</a>",
                            "\u24c2\ufe0f Mistral",
                            11.39,
                            "63.66*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.165,
                            6.13,
                            12101,
                            91743.0,
                            41.8,
                            6.12,
                            12224,
                            13692
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Delcos/Starling-LM-11B-alpha\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delcos/Starling-LM-11B-alpha</a>",
                            "\u24c2\ufe0f Mistral",
                            11.39,
                            "63.66 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.419,
                            24.3,
                            46424,
                            209205.0,
                            10.9,
                            23.5,
                            46548,
                            47999
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Delcos/Starling-LM-11B-alpha\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delcos/Starling-LM-11B-alpha</a>",
                            "\u24c2\ufe0f Mistral",
                            11.39,
                            "63.66*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.459,
                            13.1,
                            7468,
                            176366.0,
                            19.9,
                            12.9,
                            7767,
                            9227
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Delcos/Starling-LM-11B-alpha\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delcos/Starling-LM-11B-alpha</a>",
                            "\u24c2\ufe0f Mistral",
                            11.39,
                            "63.66*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.475,
                            13.0,
                            7468,
                            167504.0,
                            20.1,
                            12.7,
                            7769,
                            9229
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-65b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-65b</a>",
                            "\ud83e\udd99 LLaMA",
                            65.29,
                            "62.79*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.239,
                            4.14,
                            68608,
                            56497.0,
                            61.8,
                            4.14,
                            69396,
                            70864
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-65b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-65b</a>",
                            "\ud83e\udd99 LLaMA",
                            65.29,
                            "62.79*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.241,
                            4.04,
                            68608,
                            52356.0,
                            63.3,
                            4.04,
                            69396,
                            70864
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-65b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-65b</a>",
                            "\ud83e\udd99 LLaMA",
                            65.29,
                            "62.79*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.296,
                            16.3,
                            39328,
                            181159.0,
                            15.9,
                            16.1,
                            40185,
                            41647
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-65b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-65b</a>",
                            "\ud83e\udd99 LLaMA",
                            65.29,
                            "62.79*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.296,
                            13.7,
                            37862,
                            128865.0,
                            18.9,
                            13.5,
                            38717,
                            40179
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-65b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-65b</a>",
                            "\ud83e\udd99 LLaMA",
                            65.29,
                            "62.79*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.575,
                            15.2,
                            37465,
                            145348.0,
                            17.4,
                            14.7,
                            38128,
                            39587
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-65b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-65b</a>",
                            "\ud83e\udd99 LLaMA",
                            65.29,
                            "62.79*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            2.5,
                            6.57,
                            40249,
                            70422.0,
                            41.3,
                            6.2,
                            40793,
                            42253
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-65b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-65b</a>",
                            "\ud83e\udd99 LLaMA",
                            65.29,
                            "62.79*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            2.5,
                            6.57,
                            40249,
                            70422.0,
                            41.3,
                            6.2,
                            40793,
                            42253
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Deci/DeciLM-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Deci/DeciLM-7B</a>",
                            "\ud83d\udd35 deci",
                            7.04,
                            "61.55 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0341,
                            38.9,
                            14290,
                            452488.0,
                            6.59,
                            38.8,
                            14357,
                            15816
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Deci/DeciLM-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Deci/DeciLM-7B</a>",
                            "\ud83d\udd35 deci",
                            7.04,
                            "61.55 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0353,
                            39.1,
                            14290,
                            429184.0,
                            6.57,
                            39.0,
                            14357,
                            15816
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Deci/DeciLM-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Deci/DeciLM-7B</a>",
                            "\ud83d\udd35 deci",
                            7.04,
                            "61.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.065,
                            38.5,
                            4281,
                            526315.0,
                            6.7,
                            38.2,
                            4334,
                            5794
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Deci/DeciLM-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Deci/DeciLM-7B</a>",
                            "\ud83d\udd35 deci",
                            7.04,
                            "61.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0765,
                            34.7,
                            4281,
                            471698.0,
                            7.42,
                            34.5,
                            4334,
                            5794
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Deci/DeciLM-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Deci/DeciLM-7B</a>",
                            "\ud83d\udd35 deci",
                            7.04,
                            "61.55*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.102,
                            10.2,
                            7514,
                            152207.0,
                            25.1,
                            10.2,
                            7589,
                            9057
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Deci/DeciLM-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Deci/DeciLM-7B</a>",
                            "\ud83d\udd35 deci",
                            7.04,
                            "61.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.104,
                            10.2,
                            7514,
                            152905.0,
                            25.1,
                            10.2,
                            7589,
                            9057
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Deci/DeciLM-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Deci/DeciLM-7B</a>",
                            "\ud83d\udd35 deci",
                            7.04,
                            "61.55 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.255,
                            41.2,
                            28538,
                            359712.0,
                            6.44,
                            39.8,
                            28638,
                            30089
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Deci/DeciLM-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Deci/DeciLM-7B</a>",
                            "\ud83d\udd35 deci",
                            7.04,
                            "61.55*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.284,
                            22.2,
                            4884,
                            291545.0,
                            11.8,
                            21.7,
                            5066,
                            6526
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Deci/DeciLM-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Deci/DeciLM-7B</a>",
                            "\ud83d\udd35 deci",
                            7.04,
                            "61.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.284,
                            21.8,
                            4884,
                            284900.0,
                            12.0,
                            21.3,
                            5064,
                            6524
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-2</a>",
                            "phi",
                            2.78,
                            "61.33 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0354,
                            27.2,
                            6339,
                            381679.0,
                            9.42,
                            27.2,
                            6429,
                            7889
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-2</a>",
                            "phi",
                            2.78,
                            "61.33 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.04,
                            27.1,
                            6339,
                            387596.0,
                            9.45,
                            27.1,
                            6429,
                            7889
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-2</a>",
                            "phi",
                            2.78,
                            "61.33 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.107,
                            31.7,
                            12603,
                            421940.0,
                            8.15,
                            31.4,
                            12666,
                            14118
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-65b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-65b</a>",
                            "\ud83e\udd99 LLaMA",
                            65.29,
                            "61.19*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.239,
                            4.09,
                            68608,
                            52910.0,
                            62.5,
                            4.1,
                            69396,
                            70864
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-65b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-65b</a>",
                            "\ud83e\udd99 LLaMA",
                            65.29,
                            "61.19*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.295,
                            15.8,
                            39328,
                            178890.0,
                            16.4,
                            15.6,
                            40185,
                            41647
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-65b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-65b</a>",
                            "\ud83e\udd99 LLaMA",
                            65.29,
                            "61.19*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.295,
                            13.7,
                            37862,
                            129366.0,
                            18.9,
                            13.5,
                            38717,
                            40179
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-65b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-65b</a>",
                            "\ud83e\udd99 LLaMA",
                            65.29,
                            "61.19*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.577,
                            14.9,
                            37465,
                            146412.0,
                            17.7,
                            14.5,
                            38128,
                            39587
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-v0.1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "60.97 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0355,
                            34.9,
                            15171,
                            409836.0,
                            7.34,
                            34.9,
                            15374,
                            16833
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-v0.1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "60.97 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0414,
                            34.1,
                            15171,
                            390625.0,
                            7.52,
                            34.0,
                            15372,
                            16831
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-v0.1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "60.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0454,
                            31.8,
                            4963,
                            427350.0,
                            8.08,
                            31.7,
                            5161,
                            6622
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-v0.1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "60.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.046,
                            32.8,
                            5908,
                            471698.0,
                            7.82,
                            32.7,
                            6106,
                            7568
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-v0.1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "60.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0673,
                            32.9,
                            4838,
                            460829.0,
                            7.83,
                            32.7,
                            5052,
                            6511
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-v0.1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "60.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.077,
                            31.1,
                            4838,
                            431034.0,
                            8.29,
                            30.9,
                            5052,
                            6511
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-v0.1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "60.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.102,
                            9.85,
                            8197,
                            146627.0,
                            26.0,
                            9.85,
                            8399,
                            9867
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-v0.1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "60.97*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.104,
                            9.77,
                            8197,
                            144508.0,
                            26.2,
                            9.77,
                            8399,
                            9867
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-v0.1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "60.97 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.265,
                            38.5,
                            30334,
                            334448.0,
                            6.89,
                            37.2,
                            30431,
                            31883
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-v0.1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "60.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.29,
                            20.9,
                            5428,
                            274725.0,
                            12.5,
                            20.5,
                            5758,
                            7218
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-v0.1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "60.97*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.299,
                            20.2,
                            5428,
                            270270.0,
                            12.9,
                            19.8,
                            5758,
                            7218
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Walmart-the-bag/Influxient-4x13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Walmart-the-bag/Influxient-4x13B</a>",
                            "\u24c2\ufe0f Mixtral",
                            38.5,
                            "60.57 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.159,
                            12.5,
                            78429,
                            140252.0,
                            20.6,
                            12.4,
                            78477,
                            79937
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Walmart-the-bag/Influxient-4x13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Walmart-the-bag/Influxient-4x13B</a>",
                            "\u24c2\ufe0f Mixtral",
                            38.5,
                            "60.57*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.219,
                            4.8,
                            40198,
                            70921.0,
                            53.3,
                            4.8,
                            40246,
                            41714
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Walmart-the-bag/Influxient-4x13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Walmart-the-bag/Influxient-4x13B</a>",
                            "\u24c2\ufe0f Mixtral",
                            38.5,
                            "60.57*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.224,
                            4.74,
                            40205,
                            65789.0,
                            54.0,
                            4.74,
                            40288,
                            41756
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Walmart-the-bag/Influxient-4x13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Walmart-the-bag/Influxient-4x13B</a>",
                            "\u24c2\ufe0f Mixtral",
                            38.5,
                            "60.57*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.857,
                            8.17,
                            23181,
                            108108.0,
                            32.1,
                            7.98,
                            23660,
                            25119
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Walmart-the-bag/Influxient-4x13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Walmart-the-bag/Influxient-4x13B</a>",
                            "\u24c2\ufe0f Mixtral",
                            38.5,
                            "60.57*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.868,
                            8.17,
                            23182,
                            107181.0,
                            32.1,
                            7.98,
                            23660,
                            25119
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/internlm/internlm-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">internlm/internlm-20b</a>",
                            "\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f",
                            20.0,
                            "59.55 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0803,
                            18.8,
                            41901,
                            189035.0,
                            13.7,
                            18.7,
                            41934,
                            43394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/internlm/internlm-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">internlm/internlm-20b</a>",
                            "\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f",
                            20.0,
                            "59.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.104,
                            17.3,
                            13828,
                            209643.0,
                            14.8,
                            17.3,
                            13964,
                            15426
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/internlm/internlm-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">internlm/internlm-20b</a>",
                            "\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f",
                            20.0,
                            "59.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.163,
                            18.0,
                            13668,
                            219780.0,
                            14.4,
                            17.8,
                            13845,
                            15305
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/internlm/internlm-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">internlm/internlm-20b</a>",
                            "\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f",
                            20.0,
                            "59.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.19,
                            5.21,
                            23038,
                            74626.0,
                            49.1,
                            5.21,
                            23068,
                            24536
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/internlm/internlm-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">internlm/internlm-20b</a>",
                            "\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f",
                            20.0,
                            "59.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.203,
                            15.9,
                            13667,
                            204498.0,
                            16.2,
                            15.8,
                            13845,
                            15305
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/internlm/internlm-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">internlm/internlm-20b</a>",
                            "\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f",
                            20.0,
                            "59.55 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.699,
                            15.8,
                            83163,
                            138504.0,
                            16.8,
                            15.2,
                            83443,
                            84894
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/internlm/internlm-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">internlm/internlm-20b</a>",
                            "\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f",
                            20.0,
                            "59.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.749,
                            10.5,
                            14401,
                            131061.0,
                            24.9,
                            10.3,
                            14933,
                            16393
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-40b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-40b</a>",
                            "\ud83e\udd85 Falcon",
                            40.0,
                            "58.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.137,
                            8.04,
                            43877,
                            106723.0,
                            31.8,
                            8.05,
                            44300,
                            45768
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-40b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-40b</a>",
                            "\ud83e\udd85 Falcon",
                            40.0,
                            "58.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.193,
                            15.3,
                            24846,
                            162074.0,
                            16.9,
                            15.1,
                            25228,
                            26690
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-40b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-40b</a>",
                            "\ud83e\udd85 Falcon",
                            40.0,
                            "58.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.296,
                            17.7,
                            24289,
                            190839.0,
                            14.7,
                            17.4,
                            24687,
                            26147
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-40b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-40b</a>",
                            "\ud83e\udd85 Falcon",
                            40.0,
                            "58.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.367,
                            16.5,
                            24290,
                            185528.0,
                            15.9,
                            16.1,
                            24687,
                            26147
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-40b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-40b</a>",
                            "\ud83e\udd85 Falcon",
                            40.0,
                            "58.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            1.63,
                            8.23,
                            26201,
                            94339.0,
                            32.6,
                            7.85,
                            26344,
                            27803
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/scb10x/typhoon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">scb10x/typhoon-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.0,
                            "58.05 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0363,
                            34.5,
                            15225,
                            404858.0,
                            7.43,
                            34.5,
                            15430,
                            16890
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/scb10x/typhoon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">scb10x/typhoon-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.0,
                            "58.05 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0368,
                            34.4,
                            15225,
                            390625.0,
                            7.45,
                            34.4,
                            15430,
                            16890
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/scb10x/typhoon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">scb10x/typhoon-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.0,
                            "58.05 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0423,
                            33.7,
                            15225,
                            389105.0,
                            7.6,
                            33.7,
                            15430,
                            16890
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/scb10x/typhoon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">scb10x/typhoon-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.0,
                            "58.05*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0464,
                            32.1,
                            5018,
                            432900.0,
                            8.0,
                            32.0,
                            5219,
                            6681
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/scb10x/typhoon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">scb10x/typhoon-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.0,
                            "58.05*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0466,
                            32.3,
                            5962,
                            465116.0,
                            7.94,
                            32.2,
                            6165,
                            7627
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/scb10x/typhoon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">scb10x/typhoon-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.0,
                            "58.05*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0687,
                            34.0,
                            4893,
                            458715.0,
                            7.56,
                            33.9,
                            5106,
                            6566
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/scb10x/typhoon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">scb10x/typhoon-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.0,
                            "58.05*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.102,
                            10.0,
                            8252,
                            146842.0,
                            25.6,
                            10.0,
                            8457,
                            9925
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/scb10x/typhoon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">scb10x/typhoon-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.0,
                            "58.05*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.104,
                            9.81,
                            8252,
                            148148.0,
                            26.1,
                            9.81,
                            8457,
                            9925
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/scb10x/typhoon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">scb10x/typhoon-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.0,
                            "58.05 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.264,
                            38.1,
                            30440,
                            333333.0,
                            6.96,
                            36.8,
                            30524,
                            31975
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/scb10x/typhoon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">scb10x/typhoon-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.0,
                            "58.05*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.295,
                            20.6,
                            5483,
                            263157.0,
                            12.7,
                            20.2,
                            5815,
                            7274
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/scb10x/typhoon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">scb10x/typhoon-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.0,
                            "58.05*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.3,
                            20.6,
                            5483,
                            268817.0,
                            12.7,
                            20.2,
                            5815,
                            7274
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-30b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-30b</a>",
                            "\ud83e\udd99 LLaMA",
                            32.53,
                            "56.94*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.153,
                            21.6,
                            19768,
                            215982.0,
                            12.0,
                            21.3,
                            20006,
                            21468
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-30b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-30b</a>",
                            "\ud83e\udd99 LLaMA",
                            32.53,
                            "56.94*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.155,
                            21.6,
                            20957,
                            259740.0,
                            12.0,
                            21.3,
                            21214,
                            22676
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-30b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-30b</a>",
                            "\ud83e\udd99 LLaMA",
                            32.53,
                            "56.94*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.178,
                            5.57,
                            34766,
                            75757.0,
                            46.0,
                            5.57,
                            35257,
                            36725
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-30b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-30b</a>",
                            "\ud83e\udd99 LLaMA",
                            32.53,
                            "56.94*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.241,
                            22.0,
                            19528,
                            230414.0,
                            11.8,
                            21.7,
                            19868,
                            21328
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-30b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-30b</a>",
                            "\ud83e\udd99 LLaMA",
                            32.53,
                            "56.94*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.302,
                            19.8,
                            19494,
                            216450.0,
                            13.2,
                            19.4,
                            19820,
                            21279
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "56.69 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0275,
                            39.8,
                            15525,
                            467289.0,
                            6.44,
                            39.8,
                            16791,
                            18251
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "56.69 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.029,
                            39.8,
                            15525,
                            467289.0,
                            6.43,
                            39.8,
                            16791,
                            18251
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "56.69 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0312,
                            38.5,
                            15525,
                            431034.0,
                            6.66,
                            38.4,
                            16791,
                            18251
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "56.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0359,
                            36.6,
                            7476,
                            492610.0,
                            7.01,
                            36.5,
                            7776,
                            9237
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "56.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0379,
                            36.2,
                            8202,
                            537634.0,
                            7.09,
                            36.1,
                            8503,
                            9965
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "56.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0548,
                            37.8,
                            7378,
                            518134.0,
                            6.8,
                            37.6,
                            7696,
                            9156
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "56.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0627,
                            34.3,
                            7378,
                            483091.0,
                            7.49,
                            34.2,
                            7696,
                            9156
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "56.69*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0965,
                            10.4,
                            10093,
                            156006.0,
                            24.6,
                            10.4,
                            10242,
                            11710
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "56.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.101,
                            10.1,
                            10093,
                            147928.0,
                            25.3,
                            10.1,
                            10246,
                            11714
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "56.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.143,
                            25.2,
                            7524,
                            369003.0,
                            10.2,
                            25.1,
                            7826,
                            9286
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "56.69 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.212,
                            41.5,
                            30980,
                            380228.0,
                            6.35,
                            40.3,
                            31616,
                            33067
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "56.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.235,
                            22.0,
                            7811,
                            291545.0,
                            11.8,
                            21.7,
                            8118,
                            9577
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B-200K</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "56.69*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.238,
                            22.6,
                            7811,
                            296735.0,
                            11.5,
                            22.3,
                            8120,
                            9579
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "55.69 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0505,
                            33.6,
                            27089,
                            322580.0,
                            7.65,
                            33.5,
                            27118,
                            28577
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "55.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0667,
                            31.9,
                            8420,
                            375939.0,
                            8.06,
                            31.8,
                            8514,
                            9976
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "55.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0681,
                            32.1,
                            9334,
                            434782.0,
                            8.02,
                            31.9,
                            9426,
                            10888
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "55.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.105,
                            32.6,
                            8267,
                            403225.0,
                            7.93,
                            32.3,
                            8394,
                            9854
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "55.69*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.119,
                            8.36,
                            14514,
                            120772.0,
                            30.6,
                            8.37,
                            14550,
                            16017
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "55.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.132,
                            28.8,
                            8267,
                            369003.0,
                            8.98,
                            28.5,
                            8394,
                            9854
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "55.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.307,
                            23.8,
                            8279,
                            290697.0,
                            11.0,
                            23.3,
                            8522,
                            9982
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "55.69 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.454,
                            24.5,
                            53918,
                            215053.0,
                            10.9,
                            23.5,
                            54123,
                            55574
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-34b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-34b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            33.74,
                            "55.28*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.241,
                            24.8,
                            18941,
                            246305.0,
                            10.5,
                            24.4,
                            19232,
                            20692
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-34b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-34b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            33.74,
                            "55.28*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.305,
                            23.2,
                            18937,
                            238663.0,
                            11.3,
                            22.7,
                            19230,
                            20690
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "54.08 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0276,
                            39.1,
                            12315,
                            467289.0,
                            6.55,
                            39.1,
                            12406,
                            13866
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "54.08 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0284,
                            39.5,
                            12315,
                            462962.0,
                            6.49,
                            39.4,
                            12406,
                            13866
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "54.08 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0298,
                            38.0,
                            12315,
                            416666.0,
                            6.74,
                            38.0,
                            12406,
                            13866
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "54.08*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0368,
                            36.8,
                            4266,
                            495049.0,
                            6.97,
                            36.7,
                            4487,
                            5949
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "54.08*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0379,
                            37.1,
                            4992,
                            546448.0,
                            6.92,
                            37.0,
                            5215,
                            6677
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "54.08*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0553,
                            38.8,
                            4169,
                            526315.0,
                            6.63,
                            38.6,
                            4408,
                            5867
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "54.08*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0646,
                            33.6,
                            4168,
                            467289.0,
                            7.64,
                            33.5,
                            4408,
                            5867
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "54.08*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0956,
                            10.4,
                            6883,
                            156006.0,
                            24.6,
                            10.4,
                            6954,
                            8422
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "54.08*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0996,
                            10.0,
                            6883,
                            146412.0,
                            25.5,
                            10.0,
                            6958,
                            8426
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "54.08*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.144,
                            27.3,
                            4314,
                            377358.0,
                            9.49,
                            27.0,
                            4538,
                            5997
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "54.08 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.213,
                            41.3,
                            24537,
                            380228.0,
                            6.39,
                            40.1,
                            24672,
                            26124
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "54.08*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.238,
                            22.6,
                            4602,
                            299401.0,
                            11.5,
                            22.3,
                            4829,
                            6289
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "54.08*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.239,
                            22.0,
                            4602,
                            290697.0,
                            11.8,
                            21.7,
                            4829,
                            6289
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TheBloke/Llama-2-13B-fp16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheBloke/Llama-2-13B-fp16</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "53.67 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0499,
                            34.8,
                            27089,
                            335570.0,
                            7.37,
                            34.7,
                            27118,
                            28577
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TheBloke/Llama-2-13B-fp16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheBloke/Llama-2-13B-fp16</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "53.67 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0517,
                            31.8,
                            27089,
                            295857.0,
                            8.08,
                            31.7,
                            27118,
                            28577
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TheBloke/Llama-2-13B-fp16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheBloke/Llama-2-13B-fp16</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "53.67*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0675,
                            31.8,
                            8420,
                            380228.0,
                            8.1,
                            31.6,
                            8514,
                            9976
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TheBloke/Llama-2-13B-fp16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheBloke/Llama-2-13B-fp16</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "53.67*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0685,
                            31.7,
                            9334,
                            431034.0,
                            8.11,
                            31.6,
                            9426,
                            10888
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TheBloke/Llama-2-13B-fp16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheBloke/Llama-2-13B-fp16</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "53.67*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.105,
                            33.4,
                            8267,
                            401606.0,
                            7.74,
                            33.1,
                            8394,
                            9854
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TheBloke/Llama-2-13B-fp16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheBloke/Llama-2-13B-fp16</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "53.67*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.119,
                            8.28,
                            14514,
                            118764.0,
                            30.9,
                            8.28,
                            14550,
                            16017
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TheBloke/Llama-2-13B-fp16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheBloke/Llama-2-13B-fp16</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "53.67*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.122,
                            8.25,
                            14514,
                            117647.0,
                            31.0,
                            8.26,
                            14550,
                            16017
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TheBloke/Llama-2-13B-fp16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheBloke/Llama-2-13B-fp16</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "53.67*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.132,
                            29.2,
                            8267,
                            364963.0,
                            8.86,
                            28.9,
                            8394,
                            9854
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TheBloke/Llama-2-13B-fp16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheBloke/Llama-2-13B-fp16</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "53.67*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.307,
                            23.0,
                            8279,
                            282485.0,
                            11.4,
                            22.5,
                            8522,
                            9982
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TheBloke/Llama-2-13B-fp16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheBloke/Llama-2-13B-fp16</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "53.67 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.452,
                            24.5,
                            53918,
                            215517.0,
                            10.9,
                            23.5,
                            54123,
                            55574
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TheBloke/Llama-2-13B-fp16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheBloke/Llama-2-13B-fp16</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "53.67*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.503,
                            18.2,
                            8754,
                            218818.0,
                            14.5,
                            17.7,
                            9156,
                            10615
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TheBloke/Llama-2-13B-fp16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheBloke/Llama-2-13B-fp16</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "53.67*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.51,
                            18.9,
                            8754,
                            230946.0,
                            14.0,
                            18.3,
                            9156,
                            10615
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TigerResearch/tigerbot-13b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TigerResearch/tigerbot-13b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "53.42 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0654,
                            20.4,
                            27641,
                            228310.0,
                            12.6,
                            20.3,
                            27694,
                            29154
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TigerResearch/tigerbot-13b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TigerResearch/tigerbot-13b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "53.42 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0695,
                            19.6,
                            27641,
                            212765.0,
                            13.1,
                            19.5,
                            27694,
                            29154
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TigerResearch/tigerbot-13b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TigerResearch/tigerbot-13b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "53.42 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.507,
                            21.1,
                            55017,
                            180180.0,
                            12.6,
                            20.3,
                            55192,
                            56644
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-30b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mosaicml/mpt-30b</a>",
                            "\ud83e\uddf1 MPT",
                            30.0,
                            "52.77*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0895,
                            11.5,
                            31849,
                            151745.0,
                            22.3,
                            11.5,
                            32193,
                            33661
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-30b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mosaicml/mpt-30b</a>",
                            "\ud83e\uddf1 MPT",
                            30.0,
                            "52.77*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.213,
                            39.6,
                            17553,
                            346020.0,
                            6.65,
                            38.5,
                            17960,
                            19419
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-30b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mosaicml/mpt-30b</a>",
                            "\ud83e\uddf1 MPT",
                            30.0,
                            "52.77*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.264,
                            35.0,
                            17553,
                            328947.0,
                            7.55,
                            33.9,
                            17960,
                            19419
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-30b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mosaicml/mpt-30b</a>",
                            "\ud83e\uddf1 MPT",
                            30.0,
                            "52.77*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            1.14,
                            14.6,
                            19236,
                            156739.0,
                            18.6,
                            13.8,
                            19782,
                            21241
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/japanese-stablelm-base-gamma-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "52.59 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0355,
                            35.1,
                            14701,
                            414937.0,
                            7.31,
                            35.0,
                            14900,
                            16359
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/japanese-stablelm-base-gamma-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "52.59 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.041,
                            34.2,
                            14701,
                            389105.0,
                            7.5,
                            34.1,
                            14898,
                            16357
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/japanese-stablelm-base-gamma-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "52.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0446,
                            32.1,
                            4494,
                            440528.0,
                            7.98,
                            32.1,
                            4578,
                            6039
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/japanese-stablelm-base-gamma-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "52.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0458,
                            33.0,
                            5438,
                            476190.0,
                            7.78,
                            32.9,
                            5523,
                            6985
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/japanese-stablelm-base-gamma-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "52.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0669,
                            33.5,
                            4368,
                            465116.0,
                            7.68,
                            33.3,
                            4469,
                            5928
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/japanese-stablelm-base-gamma-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "52.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0767,
                            30.2,
                            4368,
                            421940.0,
                            8.51,
                            30.1,
                            4469,
                            5928
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/japanese-stablelm-base-gamma-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "52.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.102,
                            9.73,
                            7727,
                            146842.0,
                            26.3,
                            9.73,
                            7841,
                            9309
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/japanese-stablelm-base-gamma-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "52.59*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.103,
                            9.81,
                            7727,
                            147492.0,
                            26.1,
                            9.81,
                            7830,
                            9298
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/japanese-stablelm-base-gamma-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "52.59 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.275,
                            36.8,
                            29395,
                            326797.0,
                            7.19,
                            35.6,
                            29481,
                            30932
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/japanese-stablelm-base-gamma-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "52.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.29,
                            20.9,
                            4959,
                            277008.0,
                            12.5,
                            20.5,
                            5188,
                            6647
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/japanese-stablelm-base-gamma-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "52.59*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.299,
                            20.7,
                            4958,
                            272479.0,
                            12.6,
                            20.3,
                            5188,
                            6647
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TencentARC/LLaMA-Pro-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TencentARC/LLaMA-Pro-8B</a>",
                            "\ud83e\udd99 LLaMA",
                            8.36,
                            "51.67 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0373,
                            33.9,
                            17479,
                            387596.0,
                            7.56,
                            33.9,
                            17504,
                            18964
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TencentARC/LLaMA-Pro-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TencentARC/LLaMA-Pro-8B</a>",
                            "\ud83e\udd99 LLaMA",
                            8.36,
                            "51.67*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.077,
                            33.7,
                            5558,
                            442477.0,
                            7.64,
                            33.5,
                            5588,
                            7048
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TencentARC/LLaMA-Pro-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TencentARC/LLaMA-Pro-8B</a>",
                            "\ud83e\udd99 LLaMA",
                            8.36,
                            "51.67*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0886,
                            29.4,
                            5557,
                            403225.0,
                            8.77,
                            29.2,
                            5588,
                            7048
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TencentARC/LLaMA-Pro-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TencentARC/LLaMA-Pro-8B</a>",
                            "\ud83e\udd99 LLaMA",
                            8.36,
                            "51.67 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.298,
                            33.4,
                            34944,
                            289855.0,
                            7.94,
                            32.2,
                            35360,
                            36811
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/augmxnt/shisa-base-7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">augmxnt/shisa-base-7b-v1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.96,
                            "51.64 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0377,
                            34.2,
                            16692,
                            401606.0,
                            7.49,
                            34.2,
                            16972,
                            18431
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/augmxnt/shisa-base-7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">augmxnt/shisa-base-7b-v1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.96,
                            "51.64 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0428,
                            33.5,
                            16692,
                            371747.0,
                            7.66,
                            33.4,
                            16974,
                            18433
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/augmxnt/shisa-base-7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">augmxnt/shisa-base-7b-v1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.96,
                            "51.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0464,
                            32.4,
                            6485,
                            432900.0,
                            7.91,
                            32.4,
                            6761,
                            8222
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/augmxnt/shisa-base-7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">augmxnt/shisa-base-7b-v1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.96,
                            "51.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0468,
                            32.5,
                            7429,
                            462962.0,
                            7.89,
                            32.4,
                            7709,
                            9170
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/augmxnt/shisa-base-7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">augmxnt/shisa-base-7b-v1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.96,
                            "51.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0671,
                            34.3,
                            6360,
                            458715.0,
                            7.5,
                            34.1,
                            6685,
                            8145
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/augmxnt/shisa-base-7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">augmxnt/shisa-base-7b-v1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.96,
                            "51.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.078,
                            29.6,
                            6360,
                            420168.0,
                            8.69,
                            29.5,
                            6685,
                            8145
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/augmxnt/shisa-base-7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">augmxnt/shisa-base-7b-v1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.96,
                            "51.64*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.104,
                            9.85,
                            9718,
                            144927.0,
                            26.0,
                            9.85,
                            10009,
                            11477
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/augmxnt/shisa-base-7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">augmxnt/shisa-base-7b-v1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.96,
                            "51.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.105,
                            9.85,
                            9718,
                            144927.0,
                            26.0,
                            9.85,
                            10020,
                            11488
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/augmxnt/shisa-base-7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">augmxnt/shisa-base-7b-v1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.96,
                            "51.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.183,
                            24.5,
                            6491,
                            318471.0,
                            10.6,
                            24.2,
                            6750,
                            8210
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/augmxnt/shisa-base-7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">augmxnt/shisa-base-7b-v1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.96,
                            "51.64 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.276,
                            37.2,
                            33224,
                            321543.0,
                            7.14,
                            35.9,
                            33327,
                            34779
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/augmxnt/shisa-base-7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">augmxnt/shisa-base-7b-v1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.96,
                            "51.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.29,
                            20.7,
                            6871,
                            270270.0,
                            12.6,
                            20.3,
                            7207,
                            8667
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/augmxnt/shisa-base-7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">augmxnt/shisa-base-7b-v1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.96,
                            "51.64*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.302,
                            20.4,
                            6871,
                            264550.0,
                            12.8,
                            20.0,
                            7207,
                            8667
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.36 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.051,
                            33.8,
                            27047,
                            323624.0,
                            7.6,
                            33.7,
                            27078,
                            28538
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.36*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0673,
                            31.4,
                            8379,
                            378787.0,
                            8.19,
                            31.3,
                            8472,
                            9934
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.36*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0683,
                            32.3,
                            9292,
                            423728.0,
                            7.96,
                            32.2,
                            9384,
                            10846
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.36*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.105,
                            33.0,
                            8225,
                            403225.0,
                            7.83,
                            32.7,
                            8355,
                            9814
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.36*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.122,
                            7.99,
                            14472,
                            118343.0,
                            32.0,
                            8.0,
                            14508,
                            15976
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.36*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.132,
                            28.8,
                            8225,
                            363636.0,
                            8.97,
                            28.5,
                            8352,
                            9812
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.36*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.307,
                            23.4,
                            8237,
                            287356.0,
                            11.2,
                            22.9,
                            8480,
                            9940
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.36 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.453,
                            24.5,
                            53834,
                            216450.0,
                            10.9,
                            23.5,
                            54045,
                            55496
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.33 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0498,
                            34.6,
                            27047,
                            337837.0,
                            7.41,
                            34.5,
                            27078,
                            28537
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.33 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.052,
                            32.6,
                            27047,
                            304878.0,
                            7.87,
                            32.5,
                            27078,
                            28537
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0665,
                            31.4,
                            8379,
                            367647.0,
                            8.18,
                            31.3,
                            8472,
                            9934
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0685,
                            32.2,
                            9292,
                            434782.0,
                            7.98,
                            32.1,
                            9384,
                            10846
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.105,
                            32.1,
                            8225,
                            401606.0,
                            8.05,
                            31.8,
                            8355,
                            9814
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.119,
                            8.31,
                            14472,
                            119617.0,
                            30.8,
                            8.31,
                            14508,
                            15976
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.33*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.122,
                            7.16,
                            14472,
                            117233.0,
                            35.7,
                            7.17,
                            14508,
                            15976
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.132,
                            29.3,
                            8225,
                            370370.0,
                            8.82,
                            29.0,
                            8352,
                            9812
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.33 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.454,
                            24.5,
                            53834,
                            212765.0,
                            10.9,
                            23.5,
                            54045,
                            55496
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.5,
                            18.9,
                            8713,
                            231481.0,
                            14.0,
                            18.3,
                            9114,
                            10573
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "51.33*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.515,
                            18.5,
                            8713,
                            218818.0,
                            14.3,
                            17.9,
                            9114,
                            10573
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/llama-2-26b-trenchcoat-stack\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/llama-2-26b-trenchcoat-stack</a>",
                            "\ud83e\udd99 LLaMA",
                            25.7,
                            "51.13 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.1,
                            17.3,
                            53514,
                            165837.0,
                            14.8,
                            17.3,
                            53544,
                            55004
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/llama-2-26b-trenchcoat-stack\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/llama-2-26b-trenchcoat-stack</a>",
                            "\ud83e\udd99 LLaMA",
                            25.7,
                            "51.13 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.105,
                            15.6,
                            53514,
                            151515.0,
                            16.4,
                            15.6,
                            53544,
                            55004
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/llama-2-26b-trenchcoat-stack\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/llama-2-26b-trenchcoat-stack</a>",
                            "\ud83e\udd99 LLaMA",
                            25.7,
                            "51.13*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.134,
                            16.0,
                            16955,
                            219298.0,
                            16.0,
                            16.0,
                            17110,
                            18572
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/llama-2-26b-trenchcoat-stack\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/llama-2-26b-trenchcoat-stack</a>",
                            "\ud83e\udd99 LLaMA",
                            25.7,
                            "51.13*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.134,
                            15.7,
                            16035,
                            189393.0,
                            16.3,
                            15.7,
                            16190,
                            17651
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/llama-2-26b-trenchcoat-stack\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/llama-2-26b-trenchcoat-stack</a>",
                            "\ud83e\udd99 LLaMA",
                            25.7,
                            "51.13*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.211,
                            16.3,
                            15869,
                            199203.0,
                            15.8,
                            16.2,
                            16059,
                            17519
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/llama-2-26b-trenchcoat-stack\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/llama-2-26b-trenchcoat-stack</a>",
                            "\ud83e\udd99 LLaMA",
                            25.7,
                            "51.13*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.242,
                            4.13,
                            28365,
                            59880.0,
                            62.0,
                            4.13,
                            28380,
                            29848
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/llama-2-26b-trenchcoat-stack\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/llama-2-26b-trenchcoat-stack</a>",
                            "\ud83e\udd99 LLaMA",
                            25.7,
                            "51.13*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.242,
                            4.03,
                            28365,
                            57471.0,
                            63.4,
                            4.04,
                            28380,
                            29848
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/llama-2-26b-trenchcoat-stack\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/llama-2-26b-trenchcoat-stack</a>",
                            "\ud83e\udd99 LLaMA",
                            25.7,
                            "51.13*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.265,
                            14.9,
                            15869,
                            183486.0,
                            17.4,
                            14.7,
                            16057,
                            17517
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/llama-2-26b-trenchcoat-stack\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/llama-2-26b-trenchcoat-stack</a>",
                            "\ud83e\udd99 LLaMA",
                            25.7,
                            "51.13*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.995,
                            9.44,
                            16846,
                            113765.0,
                            28.0,
                            9.14,
                            17504,
                            18964
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/llama-2-26b-trenchcoat-stack\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/llama-2-26b-trenchcoat-stack</a>",
                            "\ud83e\udd99 LLaMA",
                            25.7,
                            "51.13*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            1.0,
                            9.24,
                            16846,
                            111111.0,
                            28.6,
                            8.95,
                            17504,
                            18964
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "50.97 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0311,
                            41.5,
                            14089,
                            473933.0,
                            6.18,
                            41.4,
                            14124,
                            15583
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "50.97 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0322,
                            42.0,
                            14089,
                            458715.0,
                            6.1,
                            42.0,
                            14124,
                            15583
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "50.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0398,
                            40.2,
                            4649,
                            526315.0,
                            6.38,
                            40.1,
                            4680,
                            6142
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "50.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.041,
                            39.8,
                            5375,
                            578034.0,
                            6.45,
                            39.7,
                            5408,
                            6870
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "50.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0632,
                            40.2,
                            4553,
                            537634.0,
                            6.4,
                            40.0,
                            4586,
                            6046
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "50.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0721,
                            36.5,
                            4552,
                            502512.0,
                            7.05,
                            36.3,
                            4586,
                            6046
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "50.97*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0975,
                            10.2,
                            7719,
                            154559.0,
                            25.0,
                            10.2,
                            7757,
                            9225
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "50.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.167,
                            28.9,
                            4561,
                            378787.0,
                            8.99,
                            28.5,
                            4607,
                            6066
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "50.97 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.238,
                            41.6,
                            28167,
                            362318.0,
                            6.37,
                            40.2,
                            28527,
                            29978
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/Yi-Ko-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/Yi-Ko-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.18,
                            "50.27 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0276,
                            38.1,
                            12542,
                            467289.0,
                            6.72,
                            38.1,
                            12635,
                            14094
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/Yi-Ko-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/Yi-Ko-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.18,
                            "50.27 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0286,
                            39.5,
                            12542,
                            462962.0,
                            6.48,
                            39.5,
                            12635,
                            14094
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/Yi-Ko-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/Yi-Ko-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.18,
                            "50.27 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0293,
                            38.1,
                            12542,
                            427350.0,
                            6.73,
                            38.0,
                            12635,
                            14094
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/Yi-Ko-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/Yi-Ko-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.18,
                            "50.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.036,
                            36.4,
                            4493,
                            483091.0,
                            7.05,
                            36.3,
                            4716,
                            6178
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/Yi-Ko-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/Yi-Ko-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.18,
                            "50.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0383,
                            35.8,
                            5219,
                            523560.0,
                            7.16,
                            35.8,
                            5444,
                            6905
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/Yi-Ko-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/Yi-Ko-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.18,
                            "50.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0548,
                            37.4,
                            4397,
                            515463.0,
                            6.87,
                            37.3,
                            4615,
                            6075
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/Yi-Ko-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/Yi-Ko-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.18,
                            "50.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.063,
                            34.0,
                            4396,
                            465116.0,
                            7.55,
                            33.9,
                            4615,
                            6075
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/Yi-Ko-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/Yi-Ko-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.18,
                            "50.27*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.097,
                            10.3,
                            7111,
                            156250.0,
                            24.9,
                            10.3,
                            7222,
                            8690
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/Yi-Ko-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/Yi-Ko-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.18,
                            "50.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.105,
                            9.73,
                            7111,
                            150375.0,
                            26.3,
                            9.73,
                            7226,
                            8694
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/Yi-Ko-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/Yi-Ko-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.18,
                            "50.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.148,
                            27.4,
                            4520,
                            378787.0,
                            9.46,
                            27.1,
                            4741,
                            6201
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/Yi-Ko-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/Yi-Ko-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.18,
                            "50.27 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.214,
                            41.1,
                            24949,
                            374531.0,
                            6.42,
                            39.9,
                            25186,
                            26638
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/Yi-Ko-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/Yi-Ko-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.18,
                            "50.27*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.235,
                            22.8,
                            4807,
                            298507.0,
                            11.4,
                            22.5,
                            5035,
                            6494
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/Yi-Ko-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/Yi-Ko-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.18,
                            "50.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.238,
                            21.8,
                            4807,
                            289855.0,
                            11.9,
                            21.5,
                            5035,
                            6494
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-13b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-13b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "49.50 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0518,
                            33.8,
                            27415,
                            326797.0,
                            7.6,
                            33.7,
                            27466,
                            28925
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-13b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-13b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "49.50 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0539,
                            32.0,
                            27415,
                            300300.0,
                            8.01,
                            32.0,
                            27466,
                            28925
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-13b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-13b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "49.50*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0681,
                            31.6,
                            8747,
                            377358.0,
                            8.15,
                            31.4,
                            8862,
                            10324
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-13b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-13b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "49.50*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0701,
                            32.1,
                            9660,
                            436681.0,
                            8.01,
                            32.0,
                            9774,
                            11236
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-13b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-13b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "49.50*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.107,
                            32.9,
                            8593,
                            401606.0,
                            7.85,
                            32.6,
                            8743,
                            10202
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-13b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-13b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "49.50*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.122,
                            8.25,
                            14841,
                            118203.0,
                            31.0,
                            8.26,
                            14885,
                            16353
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-13b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-13b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "49.50*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.124,
                            8.04,
                            14841,
                            119047.0,
                            31.8,
                            8.05,
                            14885,
                            16353
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-13b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-13b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "49.50*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.133,
                            29.7,
                            8593,
                            367647.0,
                            8.72,
                            29.4,
                            8743,
                            10202
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-13b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-13b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "49.50*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.308,
                            23.6,
                            8605,
                            285714.0,
                            11.1,
                            23.1,
                            8849,
                            10309
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-13b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-13b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "49.50 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.455,
                            24.3,
                            54569,
                            215053.0,
                            11.0,
                            23.3,
                            54794,
                            56245
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-13b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-13b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "49.50*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.498,
                            18.8,
                            9081,
                            228310.0,
                            14.1,
                            18.2,
                            9535,
                            10995
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-13b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-13b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "49.50*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.504,
                            18.1,
                            9081,
                            216450.0,
                            14.6,
                            17.5,
                            9535,
                            10995
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/una-llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/una-llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "48.64 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0301,
                            42.4,
                            14056,
                            487804.0,
                            6.05,
                            42.3,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/una-llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/una-llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "48.64 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0319,
                            41.1,
                            14056,
                            324675.0,
                            6.24,
                            41.0,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/una-llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/una-llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "48.64 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0321,
                            42.9,
                            14056,
                            467289.0,
                            5.97,
                            42.9,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/una-llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/una-llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "48.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0397,
                            40.5,
                            4616,
                            531914.0,
                            6.34,
                            40.4,
                            4647,
                            6108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/una-llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/una-llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "48.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0408,
                            39.7,
                            5342,
                            571428.0,
                            6.46,
                            39.6,
                            5375,
                            6836
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/una-llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/una-llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "48.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0629,
                            41.3,
                            4519,
                            546448.0,
                            6.24,
                            41.0,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/una-llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/una-llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "48.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.071,
                            36.0,
                            4519,
                            502512.0,
                            7.15,
                            35.8,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/una-llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/una-llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "48.64*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0953,
                            10.3,
                            7686,
                            154320.0,
                            24.8,
                            10.3,
                            7728,
                            9195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/una-llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/una-llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "48.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0964,
                            10.3,
                            7686,
                            151745.0,
                            24.8,
                            10.3,
                            7728,
                            9195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/una-llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/una-llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "48.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.165,
                            28.7,
                            4527,
                            386100.0,
                            9.03,
                            28.3,
                            4573,
                            6033
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/una-llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/una-llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "48.64 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.238,
                            41.8,
                            28100,
                            362318.0,
                            6.34,
                            40.4,
                            28458,
                            29909
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/una-llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/una-llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "48.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.271,
                            23.6,
                            4795,
                            302114.0,
                            11.1,
                            23.1,
                            4995,
                            6454
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/una-llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/una-llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "48.64*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.273,
                            22.8,
                            4795,
                            288184.0,
                            11.5,
                            22.3,
                            4995,
                            6454
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-icl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-icl</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.93 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0303,
                            42.5,
                            14056,
                            476190.0,
                            6.03,
                            42.5,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-icl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-icl</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.93 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0319,
                            39.0,
                            14056,
                            446428.0,
                            6.57,
                            39.0,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-icl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-icl</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.93*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0398,
                            39.1,
                            4616,
                            512820.0,
                            6.57,
                            39.0,
                            4647,
                            6108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-icl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-icl</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.93*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.041,
                            39.7,
                            5342,
                            561797.0,
                            6.46,
                            39.6,
                            5375,
                            6836
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-icl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-icl</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.93*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0613,
                            39.6,
                            4519,
                            549450.0,
                            6.5,
                            39.4,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-icl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-icl</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.93*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0716,
                            37.0,
                            4519,
                            497512.0,
                            6.97,
                            36.7,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-icl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-icl</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.93*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0933,
                            10.5,
                            7686,
                            157728.0,
                            24.4,
                            10.5,
                            7728,
                            9195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-icl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-icl</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.93*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0955,
                            10.3,
                            7686,
                            155763.0,
                            24.8,
                            10.3,
                            7728,
                            9195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-icl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-icl</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.93 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.238,
                            41.7,
                            28100,
                            367647.0,
                            6.35,
                            40.3,
                            28458,
                            29909
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-icl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-icl</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.93*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.268,
                            23.4,
                            4795,
                            302114.0,
                            11.2,
                            22.9,
                            4995,
                            6454
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-icl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-icl</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.93*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.268,
                            23.0,
                            4796,
                            292397.0,
                            11.4,
                            22.5,
                            4949,
                            6408
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-1_5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-1_5</a>",
                            "phi",
                            0.0,
                            "47.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0203,
                            52.5,
                            1280,
                            799999.0,
                            4.88,
                            52.5,
                            1331,
                            2791
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-1_5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-1_5</a>",
                            "phi",
                            0.0,
                            "47.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0208,
                            49.1,
                            1280,
                            769230.0,
                            5.21,
                            49.1,
                            1331,
                            2791
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-1_5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-1_5</a>",
                            "phi",
                            0.0,
                            "47.69 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0268,
                            36.4,
                            3329,
                            546448.0,
                            7.04,
                            36.4,
                            3521,
                            4980
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-1_5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-1_5</a>",
                            "phi",
                            0.0,
                            "47.69 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0273,
                            36.4,
                            3329,
                            568181.0,
                            7.04,
                            36.4,
                            3521,
                            4980
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-1_5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-1_5</a>",
                            "phi",
                            0.0,
                            "47.69 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0549,
                            41.1,
                            6550,
                            588235.0,
                            6.26,
                            40.9,
                            6603,
                            8055
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/golaxy/gowizardlm\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">golaxy/gowizardlm</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "47.64 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.031,
                            43.3,
                            14653,
                            485436.0,
                            5.92,
                            43.2,
                            14682,
                            16141
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/golaxy/gowizardlm\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">golaxy/gowizardlm</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "47.64 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0326,
                            42.6,
                            14653,
                            458715.0,
                            6.01,
                            42.6,
                            14682,
                            16141
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/golaxy/gowizardlm\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">golaxy/gowizardlm</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "47.64 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0329,
                            40.5,
                            14653,
                            442477.0,
                            6.33,
                            40.4,
                            14682,
                            16141
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/golaxy/gowizardlm\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">golaxy/gowizardlm</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "47.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0405,
                            39.2,
                            5213,
                            512820.0,
                            6.55,
                            39.1,
                            5236,
                            6698
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/golaxy/gowizardlm\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">golaxy/gowizardlm</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "47.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0418,
                            38.9,
                            5939,
                            561797.0,
                            6.59,
                            38.8,
                            5964,
                            7426
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/golaxy/gowizardlm\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">golaxy/gowizardlm</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "47.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0619,
                            38.8,
                            5117,
                            543478.0,
                            6.63,
                            38.6,
                            5156,
                            6616
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/golaxy/gowizardlm\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">golaxy/gowizardlm</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "47.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0714,
                            36.6,
                            5116,
                            492610.0,
                            7.03,
                            36.4,
                            5156,
                            6616
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/golaxy/gowizardlm\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">golaxy/gowizardlm</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "47.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0956,
                            10.5,
                            8283,
                            155763.0,
                            24.4,
                            10.5,
                            8321,
                            9789
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/golaxy/gowizardlm\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">golaxy/gowizardlm</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "47.64*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0991,
                            10.1,
                            8283,
                            152207.0,
                            25.4,
                            10.1,
                            8321,
                            9789
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/golaxy/gowizardlm\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">golaxy/gowizardlm</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "47.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.169,
                            29.2,
                            5124,
                            387596.0,
                            8.89,
                            28.8,
                            5173,
                            6633
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/golaxy/gowizardlm\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">golaxy/gowizardlm</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "47.64 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.242,
                            41.0,
                            29295,
                            358422.0,
                            6.46,
                            39.6,
                            29628,
                            31079
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/golaxy/gowizardlm\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">golaxy/gowizardlm</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "47.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.268,
                            23.2,
                            5393,
                            294117.0,
                            11.3,
                            22.7,
                            5549,
                            7008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/golaxy/gowizardlm\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">golaxy/gowizardlm</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "47.64*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.27,
                            22.8,
                            5393,
                            280112.0,
                            11.5,
                            22.3,
                            5595,
                            7054
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "47.26 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0504,
                            33.5,
                            27047,
                            332225.0,
                            7.66,
                            33.4,
                            27078,
                            28537
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "47.26 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0522,
                            32.5,
                            27047,
                            300300.0,
                            7.9,
                            32.4,
                            27078,
                            28537
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "47.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0661,
                            31.3,
                            8379,
                            377358.0,
                            8.22,
                            31.1,
                            8472,
                            9934
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "47.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0679,
                            31.5,
                            9292,
                            431034.0,
                            8.16,
                            31.4,
                            9384,
                            10846
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "47.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.105,
                            32.9,
                            8225,
                            396825.0,
                            7.85,
                            32.6,
                            8355,
                            9814
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "47.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.12,
                            8.25,
                            14472,
                            117096.0,
                            31.0,
                            8.26,
                            14508,
                            15976
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "47.26*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.126,
                            8.02,
                            14472,
                            117508.0,
                            31.9,
                            8.03,
                            14508,
                            15976
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "47.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.132,
                            29.1,
                            8225,
                            363636.0,
                            8.89,
                            28.8,
                            8352,
                            9812
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "47.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.307,
                            23.2,
                            8237,
                            286532.0,
                            11.3,
                            22.7,
                            8480,
                            9940
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "47.26 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.451,
                            24.5,
                            53834,
                            213675.0,
                            10.9,
                            23.5,
                            54045,
                            55496
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "47.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.498,
                            18.9,
                            8713,
                            229885.0,
                            14.0,
                            18.3,
                            9114,
                            10573
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_13b</a>",
                            "\ud83e\udd99 LLaMA",
                            13.0,
                            "47.26*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.507,
                            18.5,
                            8713,
                            220264.0,
                            14.3,
                            17.9,
                            9114,
                            10573
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.09 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0301,
                            42.4,
                            14056,
                            490196.0,
                            6.05,
                            42.3,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.09 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0323,
                            41.4,
                            14056,
                            454545.0,
                            6.19,
                            41.4,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0407,
                            39.7,
                            4616,
                            526315.0,
                            6.47,
                            39.6,
                            4647,
                            6108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0408,
                            38.0,
                            5342,
                            574712.0,
                            6.75,
                            37.9,
                            5375,
                            6836
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0617,
                            41.2,
                            4519,
                            540540.0,
                            6.25,
                            41.0,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0735,
                            35.6,
                            4519,
                            476190.0,
                            7.24,
                            35.4,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0936,
                            10.5,
                            7686,
                            157232.0,
                            24.3,
                            10.5,
                            7728,
                            9195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.09*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0966,
                            10.5,
                            7686,
                            154320.0,
                            24.4,
                            10.5,
                            7728,
                            9195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.168,
                            28.7,
                            4527,
                            381679.0,
                            9.04,
                            28.3,
                            4573,
                            6033
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.09 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.238,
                            41.8,
                            28100,
                            364963.0,
                            6.34,
                            40.4,
                            28458,
                            29909
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.09*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.268,
                            23.0,
                            4796,
                            294117.0,
                            11.4,
                            22.5,
                            4949,
                            6408
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.273,
                            23.4,
                            4795,
                            291545.0,
                            11.2,
                            22.9,
                            4995,
                            6454
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sarvamai/OpenHathi-7B-Hi-v0.1-Base</a>",
                            "\ud83e\udd99 LLaMA",
                            6.87,
                            "46.64 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0309,
                            43.1,
                            14354,
                            490196.0,
                            5.95,
                            43.0,
                            14388,
                            15848
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sarvamai/OpenHathi-7B-Hi-v0.1-Base</a>",
                            "\ud83e\udd99 LLaMA",
                            6.87,
                            "46.64 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0325,
                            40.7,
                            14354,
                            448430.0,
                            6.29,
                            40.7,
                            14388,
                            15848
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sarvamai/OpenHathi-7B-Hi-v0.1-Base</a>",
                            "\ud83e\udd99 LLaMA",
                            6.87,
                            "46.64 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0327,
                            41.5,
                            14354,
                            450450.0,
                            6.18,
                            41.4,
                            14388,
                            15848
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sarvamai/OpenHathi-7B-Hi-v0.1-Base</a>",
                            "\ud83e\udd99 LLaMA",
                            6.87,
                            "46.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0399,
                            39.4,
                            4914,
                            526315.0,
                            6.52,
                            39.3,
                            4945,
                            6406
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sarvamai/OpenHathi-7B-Hi-v0.1-Base</a>",
                            "\ud83e\udd99 LLaMA",
                            6.87,
                            "46.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0417,
                            39.5,
                            5640,
                            571428.0,
                            6.49,
                            39.4,
                            5672,
                            7134
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sarvamai/OpenHathi-7B-Hi-v0.1-Base</a>",
                            "\ud83e\udd99 LLaMA",
                            6.87,
                            "46.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0619,
                            41.1,
                            4818,
                            523560.0,
                            6.26,
                            40.9,
                            4867,
                            6327
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sarvamai/OpenHathi-7B-Hi-v0.1-Base</a>",
                            "\ud83e\udd99 LLaMA",
                            6.87,
                            "46.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0725,
                            35.6,
                            4817,
                            476190.0,
                            7.23,
                            35.4,
                            4867,
                            6327
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sarvamai/OpenHathi-7B-Hi-v0.1-Base</a>",
                            "\ud83e\udd99 LLaMA",
                            6.87,
                            "46.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0951,
                            10.3,
                            7984,
                            155279.0,
                            24.8,
                            10.3,
                            8009,
                            9476
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sarvamai/OpenHathi-7B-Hi-v0.1-Base</a>",
                            "\ud83e\udd99 LLaMA",
                            6.87,
                            "46.64*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0985,
                            9.96,
                            7984,
                            151745.0,
                            25.7,
                            9.96,
                            8009,
                            9476
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sarvamai/OpenHathi-7B-Hi-v0.1-Base</a>",
                            "\ud83e\udd99 LLaMA",
                            6.87,
                            "46.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.168,
                            28.8,
                            4825,
                            378787.0,
                            9.02,
                            28.4,
                            4871,
                            6331
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sarvamai/OpenHathi-7B-Hi-v0.1-Base</a>",
                            "\ud83e\udd99 LLaMA",
                            6.87,
                            "46.64 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.239,
                            41.5,
                            28696,
                            362318.0,
                            6.39,
                            40.1,
                            29030,
                            30482
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sarvamai/OpenHathi-7B-Hi-v0.1-Base</a>",
                            "\ud83e\udd99 LLaMA",
                            6.87,
                            "46.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.268,
                            23.2,
                            5094,
                            294985.0,
                            11.3,
                            22.7,
                            5293,
                            6752
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/sarvamai/OpenHathi-7B-Hi-v0.1-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sarvamai/OpenHathi-7B-Hi-v0.1-Base</a>",
                            "\ud83e\udd99 LLaMA",
                            6.87,
                            "46.64*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.274,
                            22.8,
                            5094,
                            288184.0,
                            11.5,
                            22.3,
                            5293,
                            6752
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-3b-4e1t\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-3b-4e1t</a>",
                            "\ud83d\udd34 StableLM-Epoch",
                            2.8,
                            "46.58 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0228,
                            42.4,
                            5972,
                            564971.0,
                            6.03,
                            42.5,
                            6205,
                            7665
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-3b-4e1t\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-3b-4e1t</a>",
                            "\ud83d\udd34 StableLM-Epoch",
                            2.8,
                            "46.58 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0234,
                            42.6,
                            5972,
                            568181.0,
                            6.01,
                            42.6,
                            6205,
                            7665
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-3b-4e1t\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-3b-4e1t</a>",
                            "\ud83d\udd34 StableLM-Epoch",
                            2.8,
                            "46.58*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0276,
                            40.5,
                            2230,
                            581395.0,
                            6.32,
                            40.5,
                            2334,
                            3795
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-3b-4e1t\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-3b-4e1t</a>",
                            "\ud83d\udd34 StableLM-Epoch",
                            2.8,
                            "46.58*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0371,
                            38.2,
                            2192,
                            578034.0,
                            6.71,
                            38.2,
                            2283,
                            3743
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-3b-4e1t\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-3b-4e1t</a>",
                            "\ud83d\udd34 StableLM-Epoch",
                            2.8,
                            "46.58*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0762,
                            29.9,
                            2195,
                            442477.0,
                            8.62,
                            29.7,
                            2300,
                            3760
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-3b-4e1t\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-3b-4e1t</a>",
                            "\ud83d\udd34 StableLM-Epoch",
                            2.8,
                            "46.58 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0944,
                            45.8,
                            12068,
                            546448.0,
                            5.66,
                            45.2,
                            12117,
                            13568
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b-v2</a>",
                            "\ud83d\udd34 StableLM-Alpha",
                            6.89,
                            "46.18 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0286,
                            43.5,
                            14353,
                            485436.0,
                            5.89,
                            43.5,
                            14392,
                            15852
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b-v2</a>",
                            "\ud83d\udd34 StableLM-Alpha",
                            6.89,
                            "46.18 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0303,
                            43.1,
                            14353,
                            450450.0,
                            5.95,
                            43.0,
                            14392,
                            15852
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b-v2</a>",
                            "\ud83d\udd34 StableLM-Alpha",
                            6.89,
                            "46.18*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0611,
                            41.7,
                            4816,
                            552486.0,
                            6.17,
                            41.5,
                            4894,
                            6354
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b-v2</a>",
                            "\ud83d\udd34 StableLM-Alpha",
                            6.89,
                            "46.18*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0643,
                            15.4,
                            7911,
                            224719.0,
                            16.7,
                            15.3,
                            7944,
                            9411
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b-v2</a>",
                            "\ud83d\udd34 StableLM-Alpha",
                            6.89,
                            "46.18*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0707,
                            38.9,
                            4815,
                            518134.0,
                            6.63,
                            38.6,
                            4894,
                            6354
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b-v2</a>",
                            "\ud83d\udd34 StableLM-Alpha",
                            6.89,
                            "46.18 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.239,
                            43.4,
                            28695,
                            378787.0,
                            6.11,
                            41.9,
                            29085,
                            30536
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b-v2</a>",
                            "\ud83d\udd34 StableLM-Alpha",
                            6.89,
                            "46.18*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.266,
                            27.4,
                            5398,
                            344827.0,
                            9.56,
                            26.8,
                            5695,
                            7155
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "45.65 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0325,
                            42.2,
                            14056,
                            456621.0,
                            6.07,
                            42.2,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "45.65*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0402,
                            38.1,
                            4616,
                            523560.0,
                            6.73,
                            38.0,
                            4647,
                            6108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "45.65*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0409,
                            39.5,
                            5342,
                            564971.0,
                            6.5,
                            39.4,
                            5375,
                            6836
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "45.65*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0617,
                            40.2,
                            4519,
                            537634.0,
                            6.41,
                            39.9,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "45.65*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0728,
                            36.3,
                            4519,
                            485436.0,
                            7.09,
                            36.1,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "45.65*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.095,
                            10.5,
                            7686,
                            156739.0,
                            24.5,
                            10.4,
                            7728,
                            9195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "45.65*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.166,
                            28.6,
                            4527,
                            380228.0,
                            9.1,
                            28.1,
                            4573,
                            6033
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/huggingface/llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggingface/llama-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "45.65 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.238,
                            41.7,
                            28100,
                            364963.0,
                            6.36,
                            40.3,
                            28458,
                            29909
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/DevaMalla/llama-base-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DevaMalla/llama-base-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.61,
                            "45.62 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0302,
                            44.0,
                            14056,
                            497512.0,
                            5.82,
                            44.0,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/DevaMalla/llama-base-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DevaMalla/llama-base-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.61,
                            "45.62 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0318,
                            40.5,
                            14056,
                            454545.0,
                            6.33,
                            40.4,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/DevaMalla/llama-base-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DevaMalla/llama-base-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.61,
                            "45.62 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.032,
                            42.4,
                            14056,
                            442477.0,
                            6.04,
                            42.4,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/DevaMalla/llama-base-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DevaMalla/llama-base-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.61,
                            "45.62*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0399,
                            39.2,
                            4616,
                            507614.0,
                            6.54,
                            39.1,
                            4647,
                            6108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/DevaMalla/llama-base-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DevaMalla/llama-base-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.61,
                            "45.62*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0408,
                            39.7,
                            5342,
                            564971.0,
                            6.46,
                            39.6,
                            5375,
                            6836
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/DevaMalla/llama-base-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DevaMalla/llama-base-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.61,
                            "45.62*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0611,
                            41.5,
                            4519,
                            564971.0,
                            6.2,
                            41.3,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/DevaMalla/llama-base-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DevaMalla/llama-base-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.61,
                            "45.62*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0708,
                            37.3,
                            4519,
                            510204.0,
                            6.91,
                            37.0,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/DevaMalla/llama-base-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DevaMalla/llama-base-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.61,
                            "45.62*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0971,
                            10.4,
                            7686,
                            151285.0,
                            24.7,
                            10.4,
                            7728,
                            9195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/DevaMalla/llama-base-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DevaMalla/llama-base-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.61,
                            "45.62*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0989,
                            10.2,
                            7686,
                            153609.0,
                            25.1,
                            10.2,
                            7728,
                            9195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/DevaMalla/llama-base-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DevaMalla/llama-base-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.61,
                            "45.62*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.166,
                            28.8,
                            4527,
                            389105.0,
                            9.02,
                            28.4,
                            4573,
                            6033
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/DevaMalla/llama-base-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DevaMalla/llama-base-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.61,
                            "45.62 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.237,
                            41.8,
                            28100,
                            361010.0,
                            6.34,
                            40.4,
                            28458,
                            29909
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/DevaMalla/llama-base-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DevaMalla/llama-base-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.61,
                            "45.62*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.268,
                            22.8,
                            4796,
                            289017.0,
                            11.5,
                            22.3,
                            4949,
                            6408
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/DevaMalla/llama-base-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DevaMalla/llama-base-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.61,
                            "45.62*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.269,
                            23.4,
                            4795,
                            301204.0,
                            11.2,
                            22.9,
                            4995,
                            6454
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-2-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "44.75 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0181,
                            54.0,
                            6401,
                            714285.0,
                            4.74,
                            54.0,
                            6452,
                            7912
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-2-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "44.75 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0184,
                            55.0,
                            6401,
                            684931.0,
                            4.66,
                            54.9,
                            6452,
                            7912
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-2-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "44.75 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0187,
                            51.9,
                            6401,
                            666666.0,
                            4.93,
                            51.9,
                            6452,
                            7912
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/KnutJaegersberg/Qwen-1_8B-Llamafied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">KnutJaegersberg/Qwen-1_8B-Llamafied</a>",
                            "\ud83e\udd99 LLaMA",
                            1.84,
                            "44.75 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0196,
                            51.9,
                            4106,
                            729927.0,
                            4.93,
                            51.9,
                            4232,
                            5691
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/KnutJaegersberg/Qwen-1_8B-Llamafied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">KnutJaegersberg/Qwen-1_8B-Llamafied</a>",
                            "\ud83e\udd99 LLaMA",
                            1.84,
                            "44.75 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0196,
                            51.6,
                            4106,
                            719424.0,
                            4.96,
                            51.6,
                            4232,
                            5691
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/KnutJaegersberg/Qwen-1_8B-Llamafied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">KnutJaegersberg/Qwen-1_8B-Llamafied</a>",
                            "\ud83e\udd99 LLaMA",
                            1.84,
                            "44.75*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0205,
                            52.7,
                            2666,
                            787401.0,
                            4.86,
                            52.7,
                            2730,
                            4192
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-2-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "44.75*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0213,
                            52.6,
                            2971,
                            793650.0,
                            4.87,
                            52.6,
                            3093,
                            4555
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-2-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "44.75*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0215,
                            50.4,
                            2432,
                            729927.0,
                            5.08,
                            50.4,
                            2529,
                            3990
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/KnutJaegersberg/Qwen-1_8B-Llamafied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">KnutJaegersberg/Qwen-1_8B-Llamafied</a>",
                            "\ud83e\udd99 LLaMA",
                            1.84,
                            "44.75*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0217,
                            50.7,
                            2279,
                            763358.0,
                            5.05,
                            50.7,
                            2365,
                            3825
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/KnutJaegersberg/Qwen-1_8B-Llamafied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">KnutJaegersberg/Qwen-1_8B-Llamafied</a>",
                            "\ud83e\udd99 LLaMA",
                            1.84,
                            "44.75*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.023,
                            45.8,
                            2278,
                            684931.0,
                            5.59,
                            45.8,
                            2365,
                            3825
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-2-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "44.75*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0313,
                            52.8,
                            2377,
                            793650.0,
                            4.86,
                            52.7,
                            2487,
                            3946
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-2-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "44.75*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0351,
                            49.0,
                            2377,
                            704225.0,
                            5.24,
                            48.9,
                            2487,
                            3946
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/KnutJaegersberg/Qwen-1_8B-Llamafied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">KnutJaegersberg/Qwen-1_8B-Llamafied</a>",
                            "\ud83e\udd99 LLaMA",
                            1.84,
                            "44.75 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0602,
                            54.8,
                            8039,
                            704225.0,
                            4.71,
                            54.4,
                            8080,
                            9531
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-2-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "44.75*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0716,
                            13.6,
                            3687,
                            213219.0,
                            18.8,
                            13.6,
                            3856,
                            5324
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-2-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "44.75*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0733,
                            13.7,
                            3687,
                            203665.0,
                            18.7,
                            13.7,
                            3856,
                            5324
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-2-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "44.75*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0755,
                            38.6,
                            2384,
                            552486.0,
                            6.69,
                            38.3,
                            2489,
                            3948
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-2-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "44.75 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.111,
                            60.6,
                            12793,
                            632911.0,
                            4.32,
                            59.3,
                            12893,
                            14344
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-2-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "44.75*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.125,
                            30.2,
                            2513,
                            420168.0,
                            8.57,
                            29.9,
                            2571,
                            4030
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-2-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "44.75*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.128,
                            29.1,
                            2513,
                            413223.0,
                            8.88,
                            28.8,
                            2571,
                            4030
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-7b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-7b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.52 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0317,
                            42.9,
                            14351,
                            478468.0,
                            5.97,
                            42.9,
                            14388,
                            15848
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-7b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-7b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.52 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0332,
                            40.7,
                            14351,
                            444444.0,
                            6.29,
                            40.7,
                            14388,
                            15848
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-7b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-7b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.52*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0412,
                            39.7,
                            4911,
                            512820.0,
                            6.47,
                            39.6,
                            4945,
                            6406
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-7b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-7b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.52*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0427,
                            39.5,
                            5637,
                            558659.0,
                            6.5,
                            39.4,
                            5672,
                            7134
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-7b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-7b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.52*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0633,
                            41.1,
                            4815,
                            537634.0,
                            6.26,
                            40.9,
                            4867,
                            6327
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-7b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-7b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.52*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0728,
                            35.2,
                            4814,
                            495049.0,
                            7.31,
                            35.0,
                            4867,
                            6327
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-7b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-7b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.52*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0937,
                            10.7,
                            7981,
                            156739.0,
                            24.0,
                            10.7,
                            8009,
                            9476
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-7b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-7b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.52*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.1,
                            10.2,
                            7981,
                            151745.0,
                            25.0,
                            10.2,
                            8009,
                            9476
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-7b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-7b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.52 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.239,
                            41.5,
                            28691,
                            361010.0,
                            6.39,
                            40.1,
                            29026,
                            30477
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-7b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-7b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.52*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.269,
                            23.0,
                            5091,
                            296735.0,
                            11.4,
                            22.5,
                            5268,
                            6727
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/abhinand/tamil-llama-7b-base-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhinand/tamil-llama-7b-base-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.52*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.27,
                            23.6,
                            5091,
                            300300.0,
                            11.1,
                            23.1,
                            5268,
                            6727
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mosaicml/mpt-7b</a>",
                            "\ud83e\uddf1 MPT",
                            7.0,
                            "44.28*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0557,
                            62.2,
                            4308,
                            763358.0,
                            4.16,
                            61.5,
                            4362,
                            5821
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mosaicml/mpt-7b</a>",
                            "\ud83e\uddf1 MPT",
                            7.0,
                            "44.28*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0612,
                            17.1,
                            7406,
                            241545.0,
                            15.0,
                            17.1,
                            7446,
                            8914
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mosaicml/mpt-7b</a>",
                            "\ud83e\uddf1 MPT",
                            7.0,
                            "44.28*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0649,
                            56.0,
                            4308,
                            699300.0,
                            4.61,
                            55.5,
                            4362,
                            5821
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mosaicml/mpt-7b</a>",
                            "\ud83e\uddf1 MPT",
                            7.0,
                            "44.28*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.25,
                            34.0,
                            4721,
                            403225.0,
                            7.74,
                            33.1,
                            5012,
                            6471
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.26 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0303,
                            42.2,
                            14056,
                            478468.0,
                            6.07,
                            42.2,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.26 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0321,
                            40.0,
                            14056,
                            448430.0,
                            6.4,
                            40.0,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.26 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0328,
                            42.5,
                            14056,
                            450450.0,
                            6.03,
                            42.5,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.04,
                            37.4,
                            4616,
                            512820.0,
                            6.85,
                            37.4,
                            4647,
                            6108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0409,
                            39.4,
                            5342,
                            558659.0,
                            6.52,
                            39.3,
                            5375,
                            6836
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0619,
                            39.9,
                            4519,
                            558659.0,
                            6.45,
                            39.7,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0714,
                            37.1,
                            4519,
                            492610.0,
                            6.94,
                            36.9,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0965,
                            10.4,
                            7686,
                            152439.0,
                            24.6,
                            10.4,
                            7728,
                            9195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.26*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0994,
                            10.2,
                            7686,
                            155038.0,
                            25.2,
                            10.2,
                            7728,
                            9195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.167,
                            28.7,
                            4527,
                            387596.0,
                            9.04,
                            28.3,
                            4573,
                            6033
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.26 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.237,
                            41.7,
                            28100,
                            367647.0,
                            6.35,
                            40.3,
                            28458,
                            29909
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.272,
                            23.6,
                            4795,
                            301204.0,
                            11.1,
                            23.1,
                            4995,
                            6454
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.26*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.273,
                            22.8,
                            4796,
                            290697.0,
                            11.5,
                            22.3,
                            4949,
                            6408
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-7b</a>",
                            "\ud83e\udd85 Falcon",
                            7.0,
                            "44.17 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0345,
                            40.5,
                            13945,
                            471698.0,
                            6.32,
                            40.5,
                            14061,
                            15520
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-7b</a>",
                            "\ud83e\udd85 Falcon",
                            7.0,
                            "44.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0645,
                            15.5,
                            7296,
                            229885.0,
                            16.6,
                            15.4,
                            7476,
                            8944
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-7b</a>",
                            "\ud83e\udd85 Falcon",
                            7.0,
                            "44.17 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.247,
                            31.9,
                            27839,
                            314465.0,
                            8.24,
                            31.1,
                            28028,
                            29479
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-7b</a>",
                            "\ud83e\udd85 Falcon",
                            7.0,
                            "44.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.272,
                            26.5,
                            4927,
                            331125.0,
                            9.9,
                            25.9,
                            5106,
                            6566
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-40b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-40b</a>",
                            "GPT-2",
                            39.93,
                            "43.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0988,
                            16.5,
                            80221,
                            173913.0,
                            15.6,
                            16.4,
                            80450,
                            81910
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-40b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-40b</a>",
                            "GPT-2",
                            39.93,
                            "43.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.101,
                            16.6,
                            80221,
                            173010.0,
                            15.5,
                            16.5,
                            80450,
                            81910
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-40b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-40b</a>",
                            "GPT-2",
                            39.93,
                            "43.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.165,
                            23.4,
                            23570,
                            221238.0,
                            11.1,
                            23.1,
                            23808,
                            25270
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "43.35 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0499,
                            33.8,
                            27341,
                            333333.0,
                            7.59,
                            33.7,
                            27369,
                            28829
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "43.35 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0525,
                            31.9,
                            27341,
                            298507.0,
                            8.05,
                            31.8,
                            27369,
                            28829
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "43.35*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0667,
                            32.5,
                            8672,
                            381679.0,
                            7.92,
                            32.3,
                            8803,
                            10265
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "43.35*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0697,
                            31.9,
                            9586,
                            431034.0,
                            8.07,
                            31.7,
                            9718,
                            11179
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "43.35*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.105,
                            33.2,
                            8519,
                            403225.0,
                            7.79,
                            32.9,
                            8671,
                            10131
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "43.35*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.119,
                            8.39,
                            14766,
                            119047.0,
                            30.5,
                            8.39,
                            14801,
                            16269
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "43.35*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.119,
                            8.28,
                            14766,
                            122100.0,
                            30.9,
                            8.28,
                            14801,
                            16269
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "43.35*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.135,
                            29.3,
                            8519,
                            366300.0,
                            8.82,
                            29.0,
                            8671,
                            10131
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "43.35 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.454,
                            24.5,
                            54421,
                            212765.0,
                            10.9,
                            23.5,
                            54773,
                            56224
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "43.35*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.496,
                            19.0,
                            9005,
                            230414.0,
                            13.9,
                            18.4,
                            9386,
                            10846
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-13b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            13.02,
                            "43.35*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.503,
                            18.2,
                            9005,
                            219780.0,
                            14.5,
                            17.7,
                            9386,
                            10846
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/calm2-7b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/calm2-7b-chat</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "43.27 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0306,
                            41.8,
                            15101,
                            487804.0,
                            6.13,
                            41.8,
                            15126,
                            16586
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/calm2-7b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/calm2-7b-chat</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "43.27 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0325,
                            40.7,
                            15101,
                            432900.0,
                            6.3,
                            40.6,
                            15126,
                            16586
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/calm2-7b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/calm2-7b-chat</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "43.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0411,
                            39.4,
                            5660,
                            520833.0,
                            6.51,
                            39.3,
                            5683,
                            7144
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/calm2-7b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/calm2-7b-chat</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "43.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0413,
                            39.1,
                            6386,
                            555555.0,
                            6.57,
                            39.0,
                            6410,
                            7872
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/calm2-7b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/calm2-7b-chat</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "43.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.062,
                            41.1,
                            5564,
                            523560.0,
                            6.27,
                            40.8,
                            5584,
                            7044
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/calm2-7b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/calm2-7b-chat</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "43.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0723,
                            36.6,
                            5563,
                            490196.0,
                            7.04,
                            36.4,
                            5582,
                            7042
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/calm2-7b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/calm2-7b-chat</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "43.27*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0941,
                            10.5,
                            8731,
                            156250.0,
                            24.5,
                            10.4,
                            8768,
                            10236
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/calm2-7b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/calm2-7b-chat</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "43.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0991,
                            10.2,
                            8731,
                            154320.0,
                            25.0,
                            10.2,
                            8768,
                            10236
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/calm2-7b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/calm2-7b-chat</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "43.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.168,
                            29.0,
                            5571,
                            384615.0,
                            8.96,
                            28.6,
                            5710,
                            7170
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/calm2-7b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/calm2-7b-chat</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "43.27 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.241,
                            41.1,
                            30189,
                            359712.0,
                            6.45,
                            39.7,
                            30515,
                            31966
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/calm2-7b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/calm2-7b-chat</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "43.27*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.268,
                            22.8,
                            5841,
                            291545.0,
                            11.5,
                            22.3,
                            6085,
                            7545
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/calm2-7b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/calm2-7b-chat</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "43.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.269,
                            23.0,
                            5841,
                            292397.0,
                            11.4,
                            22.5,
                            6085,
                            7545
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-16B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-16B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            16.0,
                            "42.59 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.061,
                            26.3,
                            33100,
                            258397.0,
                            9.77,
                            26.2,
                            33204,
                            34663
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-16B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-16B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            16.0,
                            "42.59*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0787,
                            12.4,
                            17713,
                            177304.0,
                            20.6,
                            12.4,
                            17834,
                            19302
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-16B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-16B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            16.0,
                            "42.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0806,
                            26.3,
                            10609,
                            314465.0,
                            9.76,
                            26.2,
                            10718,
                            12180
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-16B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-16B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            16.0,
                            "42.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0812,
                            26.3,
                            12226,
                            363636.0,
                            9.79,
                            26.1,
                            12350,
                            13811
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-16B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-16B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            16.0,
                            "42.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0842,
                            11.9,
                            17707,
                            170068.0,
                            21.6,
                            11.9,
                            17811,
                            19279
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-16B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-16B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            16.0,
                            "42.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.128,
                            25.8,
                            10305,
                            320512.0,
                            10.0,
                            25.6,
                            10433,
                            11892
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-16B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-16B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            16.0,
                            "42.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.154,
                            24.8,
                            10304,
                            313479.0,
                            10.5,
                            24.4,
                            10433,
                            11892
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-16B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-16B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            16.0,
                            "42.59 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.572,
                            20.6,
                            66008,
                            178890.0,
                            13.0,
                            19.7,
                            66418,
                            67870
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-16B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-16B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            16.0,
                            "42.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.621,
                            19.2,
                            11264,
                            223214.0,
                            13.9,
                            18.4,
                            12106,
                            13566
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-16B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-16B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            16.0,
                            "42.59*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.63,
                            20.1,
                            11264,
                            219298.0,
                            13.3,
                            19.2,
                            12106,
                            13566
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "42.31 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0301,
                            42.9,
                            14056,
                            492610.0,
                            5.97,
                            42.9,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "42.31 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0318,
                            40.9,
                            14056,
                            454545.0,
                            6.27,
                            40.8,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "42.31 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0334,
                            42.6,
                            14056,
                            448430.0,
                            6.01,
                            42.6,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "42.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0397,
                            40.0,
                            4616,
                            523560.0,
                            6.42,
                            39.9,
                            4647,
                            6108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "42.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0411,
                            39.0,
                            5342,
                            558659.0,
                            6.58,
                            38.9,
                            5375,
                            6836
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "42.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0614,
                            41.5,
                            4519,
                            543478.0,
                            6.2,
                            41.3,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "42.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0724,
                            37.0,
                            4519,
                            500000.0,
                            6.97,
                            36.7,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "42.31*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0954,
                            10.4,
                            7686,
                            155038.0,
                            24.7,
                            10.4,
                            7728,
                            9195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "42.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0986,
                            10.2,
                            7686,
                            151515.0,
                            25.1,
                            10.2,
                            7728,
                            9195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "42.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.166,
                            28.7,
                            4527,
                            389105.0,
                            9.06,
                            28.3,
                            4573,
                            6033
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "42.31 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.238,
                            41.7,
                            28100,
                            361010.0,
                            6.36,
                            40.3,
                            28458,
                            29909
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "42.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.268,
                            23.6,
                            4796,
                            299401.0,
                            11.1,
                            23.1,
                            4949,
                            6408
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "42.31*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.271,
                            22.8,
                            4795,
                            289017.0,
                            11.5,
                            22.3,
                            4995,
                            6454
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-raven-14b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-raven-14b</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            14.0,
                            "42.09 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0548,
                            23.2,
                            28350,
                            264550.0,
                            11.1,
                            23.1,
                            28393,
                            29852
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-large</a>",
                            "GPT-2",
                            0.0,
                            "42.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0578,
                            27.2,
                            41812,
                            304878.0,
                            9.42,
                            27.2,
                            41903,
                            43362
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-large</a>",
                            "GPT-2",
                            0.0,
                            "42.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0582,
                            27.2,
                            41812,
                            308641.0,
                            9.42,
                            27.2,
                            41903,
                            43362
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-large</a>",
                            "GPT-2",
                            0.0,
                            "42.09 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0674,
                            27.2,
                            41812,
                            251889.0,
                            9.43,
                            27.1,
                            41903,
                            43362
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-large</a>",
                            "GPT-2",
                            0.0,
                            "42.09 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.072,
                            28.7,
                            41812,
                            263157.0,
                            8.96,
                            28.6,
                            41894,
                            43352
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-large</a>",
                            "GPT-2",
                            0.0,
                            "42.09 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0758,
                            27.1,
                            41812,
                            245700.0,
                            9.49,
                            27.0,
                            41903,
                            43362
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-large</a>",
                            "GPT-2",
                            0.0,
                            "42.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0951,
                            35.2,
                            12620,
                            366300.0,
                            7.34,
                            34.9,
                            12748,
                            14210
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-raven-14b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-raven-14b</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            14.0,
                            "42.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.139,
                            7.41,
                            14963,
                            105708.0,
                            34.5,
                            7.42,
                            15013,
                            16481
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-raven-14b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-raven-14b</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            14.0,
                            "42.09 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.49,
                            23.0,
                            56690,
                            198807.0,
                            11.6,
                            22.1,
                            56757,
                            58208
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-raven-14b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-raven-14b</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            14.0,
                            "42.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.538,
                            14.1,
                            9149,
                            183486.0,
                            18.6,
                            13.8,
                            9829,
                            11288
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-large</a>",
                            "GPT-2",
                            0.0,
                            "42.09 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.652,
                            14.2,
                            83432,
                            126742.0,
                            18.6,
                            13.8,
                            83821,
                            85272
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neox-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neox-20b</a>",
                            "GPT-NeoX",
                            20.74,
                            "41.69 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0776,
                            25.0,
                            42420,
                            228310.0,
                            10.3,
                            24.9,
                            42511,
                            43970
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neox-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neox-20b</a>",
                            "GPT-NeoX",
                            20.74,
                            "41.69 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0778,
                            28.9,
                            42420,
                            256410.0,
                            8.9,
                            28.8,
                            42502,
                            43962
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neox-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neox-20b</a>",
                            "GPT-NeoX",
                            20.74,
                            "41.69*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0928,
                            10.8,
                            22498,
                            151057.0,
                            23.8,
                            10.8,
                            22588,
                            24056
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neox-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neox-20b</a>",
                            "GPT-NeoX",
                            20.74,
                            "41.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0972,
                            10.5,
                            22497,
                            145348.0,
                            24.5,
                            10.4,
                            22590,
                            24058
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neox-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neox-20b</a>",
                            "GPT-NeoX",
                            20.74,
                            "41.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.102,
                            27.6,
                            13222,
                            305810.0,
                            9.34,
                            27.4,
                            13342,
                            14803
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neox-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neox-20b</a>",
                            "GPT-NeoX",
                            20.74,
                            "41.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.104,
                            27.7,
                            14841,
                            354609.0,
                            9.32,
                            27.5,
                            14963,
                            16424
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neox-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neox-20b</a>",
                            "GPT-NeoX",
                            20.74,
                            "41.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.171,
                            25.9,
                            12919,
                            307692.0,
                            10.0,
                            25.6,
                            13056,
                            14516
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neox-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neox-20b</a>",
                            "GPT-NeoX",
                            20.74,
                            "41.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.206,
                            23.8,
                            12920,
                            286532.0,
                            10.9,
                            23.5,
                            13056,
                            14516
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neox-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neox-20b</a>",
                            "GPT-NeoX",
                            20.74,
                            "41.69*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.826,
                            15.5,
                            14172,
                            170648.0,
                            17.2,
                            14.9,
                            14709,
                            16168
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neox-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neox-20b</a>",
                            "GPT-NeoX",
                            20.74,
                            "41.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.826,
                            15.4,
                            14172,
                            176366.0,
                            17.4,
                            14.7,
                            14709,
                            16168
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>",
                            "GPT-NeoX",
                            7.0,
                            "41.49 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0281,
                            48.8,
                            14429,
                            510204.0,
                            5.26,
                            48.7,
                            14470,
                            15929
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>",
                            "GPT-NeoX",
                            7.0,
                            "41.49 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0305,
                            46.4,
                            14429,
                            462962.0,
                            5.52,
                            46.4,
                            14470,
                            15929
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>",
                            "GPT-NeoX",
                            7.0,
                            "41.49 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0311,
                            40.8,
                            14429,
                            469483.0,
                            6.28,
                            40.8,
                            14470,
                            15929
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>",
                            "GPT-NeoX",
                            7.0,
                            "41.49 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0329,
                            40.0,
                            14429,
                            425531.0,
                            6.4,
                            40.0,
                            14470,
                            15929
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>",
                            "GPT-NeoX",
                            7.0,
                            "41.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0395,
                            42.5,
                            5027,
                            552486.0,
                            6.04,
                            42.4,
                            5052,
                            6513
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>",
                            "GPT-NeoX",
                            7.0,
                            "41.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0409,
                            40.7,
                            6105,
                            602409.0,
                            6.3,
                            40.6,
                            6132,
                            7593
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>",
                            "GPT-NeoX",
                            7.0,
                            "41.49*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0623,
                            15.6,
                            7990,
                            230414.0,
                            16.4,
                            15.6,
                            8038,
                            9506
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>",
                            "GPT-NeoX",
                            7.0,
                            "41.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0639,
                            39.2,
                            4891,
                            505050.0,
                            6.57,
                            39.0,
                            4926,
                            6385
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>",
                            "GPT-NeoX",
                            7.0,
                            "41.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0676,
                            15.1,
                            7990,
                            214132.0,
                            17.0,
                            15.1,
                            8029,
                            9497
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>",
                            "GPT-NeoX",
                            7.0,
                            "41.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0725,
                            36.8,
                            4891,
                            500000.0,
                            7.0,
                            36.6,
                            4926,
                            6385
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>",
                            "GPT-NeoX",
                            7.0,
                            "41.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.163,
                            33.6,
                            4980,
                            440528.0,
                            7.74,
                            33.1,
                            5205,
                            6664
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>",
                            "GPT-NeoX",
                            7.0,
                            "41.49 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.247,
                            38.3,
                            28712,
                            340136.0,
                            6.91,
                            37.0,
                            28873,
                            30324
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>",
                            "GPT-NeoX",
                            7.0,
                            "41.49*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.277,
                            27.9,
                            5370,
                            342465.0,
                            9.43,
                            27.1,
                            5609,
                            7069
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>",
                            "GPT-NeoX",
                            7.0,
                            "41.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.284,
                            26.0,
                            5370,
                            317460.0,
                            10.1,
                            25.3,
                            5609,
                            7069
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.02,
                            "41.44 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0181,
                            53.9,
                            6401,
                            709219.0,
                            4.75,
                            53.9,
                            6452,
                            7912
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.02,
                            "41.44 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0185,
                            53.2,
                            6401,
                            684931.0,
                            4.81,
                            53.2,
                            6452,
                            7912
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.02,
                            "41.44 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0187,
                            51.6,
                            6401,
                            675675.0,
                            4.96,
                            51.6,
                            6452,
                            7912
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.02,
                            "41.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0206,
                            52.3,
                            2432,
                            735294.0,
                            4.9,
                            52.2,
                            2529,
                            3990
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.02,
                            "41.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.021,
                            52.3,
                            2971,
                            781250.0,
                            4.9,
                            52.2,
                            3093,
                            4555
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.02,
                            "41.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0309,
                            55.9,
                            2377,
                            799999.0,
                            4.59,
                            55.8,
                            2487,
                            3946
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.02,
                            "41.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.035,
                            49.4,
                            2377,
                            694444.0,
                            5.19,
                            49.3,
                            2487,
                            3946
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.02,
                            "41.44*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0722,
                            13.5,
                            3687,
                            212314.0,
                            19.0,
                            13.5,
                            3856,
                            5324
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.02,
                            "41.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0726,
                            13.7,
                            3687,
                            203665.0,
                            18.7,
                            13.7,
                            3856,
                            5324
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.02,
                            "41.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0755,
                            38.9,
                            2384,
                            555555.0,
                            6.64,
                            38.6,
                            2489,
                            3948
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.02,
                            "41.44 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.111,
                            61.3,
                            12793,
                            641025.0,
                            4.27,
                            60.0,
                            12893,
                            14344
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.02,
                            "41.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.122,
                            30.5,
                            2513,
                            416666.0,
                            8.49,
                            30.2,
                            2571,
                            4030
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.02,
                            "41.44*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.124,
                            30.3,
                            2513,
                            420168.0,
                            8.53,
                            30.0,
                            2571,
                            4030
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NucleusAI/nucleus-22B-token-500B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NucleusAI/nucleus-22B-token-500B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "41.33 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0769,
                            28.1,
                            44887,
                            253807.0,
                            9.16,
                            27.9,
                            45185,
                            46644
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NucleusAI/nucleus-22B-token-500B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NucleusAI/nucleus-22B-token-500B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "41.33 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0865,
                            28.0,
                            44887,
                            250000.0,
                            9.21,
                            27.8,
                            45185,
                            46644
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NucleusAI/nucleus-22B-token-500B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NucleusAI/nucleus-22B-token-500B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "41.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.102,
                            31.6,
                            13546,
                            325732.0,
                            8.16,
                            31.4,
                            13734,
                            15195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NucleusAI/nucleus-22B-token-500B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NucleusAI/nucleus-22B-token-500B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "41.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.103,
                            32.0,
                            14729,
                            392156.0,
                            8.08,
                            31.7,
                            14919,
                            16380
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NucleusAI/nucleus-22B-token-500B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NucleusAI/nucleus-22B-token-500B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "41.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.119,
                            8.31,
                            23464,
                            117924.0,
                            30.8,
                            8.31,
                            23792,
                            25260
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NucleusAI/nucleus-22B-token-500B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NucleusAI/nucleus-22B-token-500B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "41.33*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.121,
                            8.31,
                            23464,
                            116009.0,
                            30.8,
                            8.31,
                            23792,
                            25260
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NucleusAI/nucleus-22B-token-500B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NucleusAI/nucleus-22B-token-500B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "41.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.161,
                            32.7,
                            13306,
                            340136.0,
                            7.96,
                            32.2,
                            13551,
                            15011
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NucleusAI/nucleus-22B-token-500B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NucleusAI/nucleus-22B-token-500B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "41.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.201,
                            29.5,
                            13283,
                            326797.0,
                            8.83,
                            29.0,
                            13514,
                            14973
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NucleusAI/nucleus-22B-token-500B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NucleusAI/nucleus-22B-token-500B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "41.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.818,
                            17.7,
                            14203,
                            190839.0,
                            15.2,
                            16.8,
                            14847,
                            16307
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NucleusAI/nucleus-22B-token-500B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NucleusAI/nucleus-22B-token-500B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "41.33*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.824,
                            17.8,
                            14203,
                            185528.0,
                            15.1,
                            17.0,
                            14847,
                            16307
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-7B-v0.1</a>",
                            "GPT-NeoX",
                            6.65,
                            "41.25 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0297,
                            48.3,
                            14429,
                            502512.0,
                            5.31,
                            48.2,
                            14470,
                            15929
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-7B-v0.1</a>",
                            "GPT-NeoX",
                            6.65,
                            "41.25 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0309,
                            41.1,
                            14429,
                            471698.0,
                            6.23,
                            41.1,
                            14470,
                            15929
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-7B-v0.1</a>",
                            "GPT-NeoX",
                            6.65,
                            "41.25 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0329,
                            40.5,
                            14429,
                            429184.0,
                            6.32,
                            40.5,
                            14470,
                            15929
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-7B-v0.1</a>",
                            "GPT-NeoX",
                            6.65,
                            "41.25 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0338,
                            45.0,
                            14429,
                            471698.0,
                            5.7,
                            44.9,
                            14470,
                            15929
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-7B-v0.1</a>",
                            "GPT-NeoX",
                            6.65,
                            "41.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0406,
                            42.1,
                            5027,
                            555555.0,
                            6.1,
                            42.0,
                            5052,
                            6513
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-7B-v0.1</a>",
                            "GPT-NeoX",
                            6.65,
                            "41.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0408,
                            41.9,
                            6105,
                            602409.0,
                            6.12,
                            41.8,
                            6132,
                            7593
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-7B-v0.1</a>",
                            "GPT-NeoX",
                            6.65,
                            "41.25*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0618,
                            15.8,
                            7990,
                            228832.0,
                            16.2,
                            15.8,
                            8038,
                            9506
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-7B-v0.1</a>",
                            "GPT-NeoX",
                            6.65,
                            "41.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0634,
                            40.3,
                            4891,
                            526315.0,
                            6.39,
                            40.1,
                            4926,
                            6385
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-7B-v0.1</a>",
                            "GPT-NeoX",
                            6.65,
                            "41.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0675,
                            14.8,
                            7990,
                            211416.0,
                            17.3,
                            14.8,
                            8029,
                            9497
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-7B-v0.1</a>",
                            "GPT-NeoX",
                            6.65,
                            "41.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0718,
                            37.4,
                            4891,
                            510204.0,
                            6.88,
                            37.2,
                            4926,
                            6385
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-7B-v0.1</a>",
                            "GPT-NeoX",
                            6.65,
                            "41.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.163,
                            33.2,
                            4980,
                            444444.0,
                            7.83,
                            32.7,
                            5205,
                            6664
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-7B-v0.1</a>",
                            "GPT-NeoX",
                            6.65,
                            "41.25 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.247,
                            38.4,
                            28712,
                            341296.0,
                            6.89,
                            37.2,
                            28873,
                            30324
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-7B-v0.1</a>",
                            "GPT-NeoX",
                            6.65,
                            "41.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.277,
                            25.7,
                            5370,
                            324675.0,
                            10.2,
                            25.1,
                            5609,
                            7069
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-7B-v0.1</a>",
                            "GPT-NeoX",
                            6.65,
                            "41.25*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.28,
                            27.8,
                            5370,
                            346020.0,
                            9.45,
                            27.1,
                            5609,
                            7069
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/LLM360/Amber\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM360/Amber</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "40.97 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0314,
                            44.1,
                            14056,
                            500000.0,
                            5.81,
                            44.1,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/LLM360/Amber\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM360/Amber</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "40.97 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0315,
                            41.7,
                            14056,
                            478468.0,
                            6.14,
                            41.7,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/LLM360/Amber\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM360/Amber</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "40.97 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0316,
                            41.0,
                            14056,
                            471698.0,
                            6.25,
                            41.0,
                            14092,
                            15552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/LLM360/Amber\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM360/Amber</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "40.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0407,
                            39.8,
                            4616,
                            537634.0,
                            6.44,
                            39.8,
                            4647,
                            6108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/LLM360/Amber\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM360/Amber</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "40.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0408,
                            39.9,
                            5342,
                            571428.0,
                            6.43,
                            39.8,
                            5375,
                            6836
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/LLM360/Amber\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM360/Amber</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "40.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0614,
                            40.7,
                            4519,
                            543478.0,
                            6.33,
                            40.4,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/LLM360/Amber\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM360/Amber</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "40.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.071,
                            36.6,
                            4519,
                            500000.0,
                            7.03,
                            36.4,
                            4548,
                            6008
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/LLM360/Amber\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM360/Amber</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "40.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0958,
                            10.4,
                            7686,
                            154798.0,
                            24.6,
                            10.4,
                            7728,
                            9195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/LLM360/Amber\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM360/Amber</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "40.97*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0973,
                            10.3,
                            7686,
                            156739.0,
                            24.9,
                            10.3,
                            7728,
                            9195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/LLM360/Amber\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM360/Amber</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "40.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.163,
                            29.0,
                            4527,
                            404858.0,
                            8.95,
                            28.6,
                            4573,
                            6033
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/LLM360/Amber\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM360/Amber</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "40.97 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.24,
                            41.7,
                            28100,
                            361010.0,
                            6.35,
                            40.3,
                            28458,
                            29909
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/LLM360/Amber\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM360/Amber</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "40.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.263,
                            23.0,
                            4796,
                            302114.0,
                            11.4,
                            22.5,
                            4949,
                            6408
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/LLM360/Amber\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM360/Amber</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "40.97*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.27,
                            23.2,
                            4796,
                            298507.0,
                            11.3,
                            22.7,
                            4949,
                            6408
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-20b</a>",
                            "GPT-2",
                            20.92,
                            "40.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0572,
                            27.1,
                            41981,
                            300300.0,
                            9.46,
                            27.1,
                            42077,
                            43536
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-20b</a>",
                            "GPT-2",
                            20.92,
                            "40.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0573,
                            27.0,
                            41981,
                            302114.0,
                            9.49,
                            27.0,
                            42077,
                            43536
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-20b</a>",
                            "GPT-2",
                            20.92,
                            "40.71 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0663,
                            27.3,
                            41981,
                            251889.0,
                            9.42,
                            27.2,
                            42077,
                            43536
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-20b</a>",
                            "GPT-2",
                            20.92,
                            "40.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.092,
                            35.0,
                            12789,
                            366300.0,
                            7.37,
                            34.7,
                            12901,
                            14363
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-20b</a>",
                            "GPT-2",
                            20.92,
                            "40.71 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.673,
                            14.1,
                            83770,
                            125786.0,
                            18.8,
                            13.6,
                            84148,
                            85599
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "40.28 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0221,
                            43.8,
                            7283,
                            558659.0,
                            5.84,
                            43.8,
                            7440,
                            8900
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "40.28 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0225,
                            47.7,
                            7280,
                            625000.0,
                            5.37,
                            47.7,
                            7440,
                            8900
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "40.28 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.023,
                            46.4,
                            7280,
                            578034.0,
                            5.51,
                            46.5,
                            7440,
                            8900
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "40.28*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0813,
                            12.6,
                            4035,
                            185528.0,
                            20.3,
                            12.6,
                            4232,
                            5699
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "40.28*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0821,
                            12.3,
                            4039,
                            190476.0,
                            20.8,
                            12.3,
                            4232,
                            5699
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "40.28 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.13,
                            57.4,
                            14582,
                            571428.0,
                            4.57,
                            56.0,
                            14677,
                            16129
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "40.28*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.147,
                            26.3,
                            2669,
                            364963.0,
                            9.85,
                            26.0,
                            2728,
                            4187
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "40.28*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.149,
                            27.0,
                            2666,
                            370370.0,
                            9.61,
                            26.6,
                            2728,
                            4187
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-j-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-j-6b</a>",
                            "GPT-J",
                            6.0,
                            "40.10 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0318,
                            33.5,
                            12726,
                            395256.0,
                            7.65,
                            33.5,
                            12769,
                            14229
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-j-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-j-6b</a>",
                            "GPT-J",
                            6.0,
                            "40.10 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0318,
                            32.3,
                            12721,
                            409836.0,
                            7.92,
                            32.3,
                            12769,
                            14229
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-j-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-j-6b</a>",
                            "GPT-J",
                            6.0,
                            "40.10 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0332,
                            31.0,
                            12721,
                            377358.0,
                            8.25,
                            31.0,
                            12769,
                            14229
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-j-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-j-6b</a>",
                            "GPT-J",
                            6.0,
                            "40.10*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0406,
                            31.2,
                            5593,
                            469483.0,
                            8.21,
                            31.2,
                            5647,
                            7109
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-j-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-j-6b</a>",
                            "GPT-J",
                            6.0,
                            "40.10*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0412,
                            30.8,
                            4515,
                            434782.0,
                            8.31,
                            30.8,
                            4567,
                            6029
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-j-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-j-6b</a>",
                            "GPT-J",
                            6.0,
                            "40.10*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0571,
                            31.2,
                            4376,
                            448430.0,
                            8.22,
                            31.1,
                            4418,
                            5878
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-j-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-j-6b</a>",
                            "GPT-J",
                            6.0,
                            "40.10*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0647,
                            29.0,
                            4376,
                            418410.0,
                            8.84,
                            29.0,
                            4418,
                            5878
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-j-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-j-6b</a>",
                            "GPT-J",
                            6.0,
                            "40.10*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0833,
                            11.7,
                            7092,
                            176366.0,
                            21.9,
                            11.7,
                            7140,
                            8608
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-j-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-j-6b</a>",
                            "GPT-J",
                            6.0,
                            "40.10*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0881,
                            11.5,
                            7088,
                            168634.0,
                            22.3,
                            11.5,
                            7132,
                            8600
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-j-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-j-6b</a>",
                            "GPT-J",
                            6.0,
                            "40.10*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.148,
                            25.2,
                            4381,
                            357142.0,
                            10.2,
                            25.1,
                            4487,
                            5947
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-j-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-j-6b</a>",
                            "GPT-J",
                            6.0,
                            "40.10 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.213,
                            33.4,
                            25284,
                            333333.0,
                            7.84,
                            32.7,
                            25486,
                            26937
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-j-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-j-6b</a>",
                            "GPT-J",
                            6.0,
                            "40.10*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.242,
                            20.7,
                            4693,
                            283286.0,
                            12.5,
                            20.5,
                            4951,
                            6410
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-j-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-j-6b</a>",
                            "GPT-J",
                            6.0,
                            "40.10*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.243,
                            21.6,
                            4693,
                            285714.0,
                            12.0,
                            21.3,
                            4951,
                            6410
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "40.00 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0348,
                            29.2,
                            14859,
                            349650.0,
                            8.75,
                            29.3,
                            14923,
                            16382
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "40.00 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0377,
                            27.0,
                            14855,
                            342465.0,
                            9.48,
                            27.0,
                            14902,
                            16361
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "40.00 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0385,
                            26.9,
                            14855,
                            321543.0,
                            9.51,
                            26.9,
                            14902,
                            16361
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "40.00*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0457,
                            27.2,
                            5158,
                            377358.0,
                            9.44,
                            27.1,
                            5200,
                            6662
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "40.00*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0474,
                            27.1,
                            6235,
                            406504.0,
                            9.45,
                            27.1,
                            6280,
                            7742
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "40.00*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0685,
                            27.2,
                            5020,
                            389105.0,
                            9.44,
                            27.1,
                            5075,
                            6534
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "40.00*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0771,
                            26.0,
                            5020,
                            366300.0,
                            9.9,
                            25.9,
                            5075,
                            6534
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "40.00*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0786,
                            12.9,
                            8220,
                            192307.0,
                            19.9,
                            12.9,
                            8269,
                            9737
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "40.00*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0816,
                            12.3,
                            8216,
                            184162.0,
                            20.8,
                            12.3,
                            8248,
                            9716
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "40.00*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.176,
                            23.4,
                            5023,
                            321543.0,
                            11.1,
                            23.1,
                            5272,
                            6731
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "40.00 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.256,
                            28.6,
                            29527,
                            284900.0,
                            9.19,
                            27.9,
                            29750,
                            31201
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "40.00*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.284,
                            19.6,
                            5351,
                            256410.0,
                            13.3,
                            19.2,
                            5454,
                            6914
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "40.00*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.286,
                            20.9,
                            5351,
                            268817.0,
                            12.5,
                            20.5,
                            5723,
                            7182
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-14b-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-14b-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            14.0,
                            "39.92 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.055,
                            24.3,
                            28350,
                            264550.0,
                            10.6,
                            24.2,
                            28393,
                            29852
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-14b-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-14b-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            14.0,
                            "39.92*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.136,
                            7.57,
                            14963,
                            108813.0,
                            33.8,
                            7.57,
                            15013,
                            16481
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-14b-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-14b-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            14.0,
                            "39.92 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.517,
                            22.8,
                            56690,
                            196078.0,
                            11.7,
                            21.9,
                            56757,
                            58208
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-14b-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-14b-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            14.0,
                            "39.92*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.538,
                            13.9,
                            9149,
                            178890.0,
                            18.8,
                            13.6,
                            9829,
                            11288
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "39.81 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0301,
                            43.1,
                            14291,
                            492610.0,
                            5.94,
                            43.1,
                            14313,
                            15772
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "39.81 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0321,
                            40.7,
                            14291,
                            452488.0,
                            6.3,
                            40.6,
                            14313,
                            15772
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "39.81 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0329,
                            43.4,
                            14291,
                            458715.0,
                            5.91,
                            43.3,
                            14313,
                            15772
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "39.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0405,
                            39.1,
                            4851,
                            526315.0,
                            6.56,
                            39.0,
                            4890,
                            6352
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "39.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0409,
                            39.6,
                            5577,
                            574712.0,
                            6.48,
                            39.5,
                            5618,
                            7079
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "39.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0612,
                            40.1,
                            4755,
                            543478.0,
                            6.42,
                            39.9,
                            4804,
                            6264
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "39.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0712,
                            37.0,
                            4754,
                            492610.0,
                            6.97,
                            36.7,
                            4802,
                            6262
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "39.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0949,
                            10.5,
                            7921,
                            157480.0,
                            24.3,
                            10.5,
                            7971,
                            9439
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "39.81*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0988,
                            10.3,
                            7921,
                            152671.0,
                            24.9,
                            10.3,
                            7971,
                            9439
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "39.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.167,
                            29.4,
                            4762,
                            393700.0,
                            8.83,
                            29.0,
                            4817,
                            6276
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "39.81 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.238,
                            41.7,
                            28570,
                            361010.0,
                            6.35,
                            40.3,
                            29018,
                            30469
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "39.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.267,
                            23.2,
                            5031,
                            294117.0,
                            11.3,
                            22.7,
                            5184,
                            6643
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/codellama/CodeLlama-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">codellama/CodeLlama-7b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            6.74,
                            "39.81*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.271,
                            23.0,
                            5031,
                            285714.0,
                            11.4,
                            22.5,
                            5184,
                            6643
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b-deduped</a>",
                            "GPT-NeoX",
                            12.0,
                            "39.70 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0451,
                            43.3,
                            24619,
                            384615.0,
                            5.94,
                            43.1,
                            24658,
                            26117
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b-deduped</a>",
                            "GPT-NeoX",
                            12.0,
                            "39.70 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0453,
                            36.6,
                            24619,
                            363636.0,
                            7.01,
                            36.5,
                            24647,
                            26107
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b-deduped</a>",
                            "GPT-NeoX",
                            12.0,
                            "39.70*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0605,
                            36.7,
                            8106,
                            442477.0,
                            7.0,
                            36.6,
                            8122,
                            9583
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b-deduped</a>",
                            "GPT-NeoX",
                            12.0,
                            "39.70*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0617,
                            37.8,
                            9453,
                            510204.0,
                            6.8,
                            37.6,
                            9472,
                            10934
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b-deduped</a>",
                            "GPT-NeoX",
                            12.0,
                            "39.70*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0718,
                            13.8,
                            13360,
                            195312.0,
                            18.6,
                            13.8,
                            13390,
                            14858
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b-deduped</a>",
                            "GPT-NeoX",
                            12.0,
                            "39.70*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0771,
                            13.2,
                            13360,
                            182815.0,
                            19.4,
                            13.2,
                            13379,
                            14847
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b-deduped</a>",
                            "GPT-NeoX",
                            12.0,
                            "39.70*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0964,
                            36.0,
                            7892,
                            432900.0,
                            7.18,
                            35.7,
                            7956,
                            9416
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b-deduped</a>",
                            "GPT-NeoX",
                            12.0,
                            "39.70*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.119,
                            33.4,
                            7893,
                            418410.0,
                            7.76,
                            33.0,
                            7956,
                            9416
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b-deduped</a>",
                            "GPT-NeoX",
                            12.0,
                            "39.70 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.404,
                            25.5,
                            49074,
                            222222.0,
                            10.4,
                            24.6,
                            49111,
                            50562
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b-deduped</a>",
                            "GPT-NeoX",
                            12.0,
                            "39.70*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.449,
                            22.8,
                            8652,
                            274725.0,
                            11.6,
                            22.1,
                            9036,
                            10496
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b-deduped</a>",
                            "GPT-NeoX",
                            12.0,
                            "39.70*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.456,
                            23.8,
                            8652,
                            270270.0,
                            11.2,
                            22.9,
                            9036,
                            10496
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-6.7b-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-6.7b-v2</a>",
                            "GPT-2",
                            7.11,
                            "39.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0252,
                            54.5,
                            14110,
                            666666.0,
                            4.71,
                            54.4,
                            14136,
                            15596
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-6.7b-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-6.7b-v2</a>",
                            "GPT-2",
                            7.11,
                            "39.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0252,
                            53.9,
                            14110,
                            649350.0,
                            4.76,
                            53.8,
                            14136,
                            15596
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-6.7b-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-6.7b-v2</a>",
                            "GPT-2",
                            7.11,
                            "39.49 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0269,
                            55.6,
                            14110,
                            584795.0,
                            4.62,
                            55.4,
                            14136,
                            15596
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-6.7b-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-6.7b-v2</a>",
                            "GPT-2",
                            7.11,
                            "39.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.036,
                            58.1,
                            4709,
                            689655.0,
                            4.43,
                            57.8,
                            4739,
                            6201
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-6.7b-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-6.7b-v2</a>",
                            "GPT-2",
                            7.11,
                            "39.49 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.231,
                            35.5,
                            28075,
                            330033.0,
                            7.42,
                            34.5,
                            28233,
                            29685
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.9b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.9b-deduped</a>",
                            "GPT-NeoX",
                            6.9,
                            "39.30 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0276,
                            48.7,
                            14404,
                            510204.0,
                            5.27,
                            48.6,
                            14445,
                            15904
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.9b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.9b-deduped</a>",
                            "GPT-NeoX",
                            6.9,
                            "39.30 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0299,
                            47.1,
                            14404,
                            478468.0,
                            5.44,
                            47.1,
                            14445,
                            15904
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.9b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.9b-deduped</a>",
                            "GPT-NeoX",
                            6.9,
                            "39.30 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0299,
                            40.0,
                            14403,
                            473933.0,
                            6.41,
                            39.9,
                            14445,
                            15904
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.9b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.9b-deduped</a>",
                            "GPT-NeoX",
                            6.9,
                            "39.30 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0315,
                            40.4,
                            14403,
                            431034.0,
                            6.34,
                            40.4,
                            14445,
                            15904
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.9b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.9b-deduped</a>",
                            "GPT-NeoX",
                            6.9,
                            "39.30*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0394,
                            42.4,
                            5002,
                            558659.0,
                            6.05,
                            42.3,
                            5028,
                            6490
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.9b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.9b-deduped</a>",
                            "GPT-NeoX",
                            6.9,
                            "39.30*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0401,
                            42.8,
                            6080,
                            598802.0,
                            6.0,
                            42.7,
                            6109,
                            7570
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.9b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.9b-deduped</a>",
                            "GPT-NeoX",
                            6.9,
                            "39.30*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0637,
                            15.6,
                            7965,
                            230414.0,
                            16.4,
                            15.6,
                            8013,
                            9481
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.9b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.9b-deduped</a>",
                            "GPT-NeoX",
                            6.9,
                            "39.30*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0661,
                            15.1,
                            7965,
                            216450.0,
                            17.0,
                            15.1,
                            8004,
                            9472
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.9b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.9b-deduped</a>",
                            "GPT-NeoX",
                            6.9,
                            "39.30*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0728,
                            37.7,
                            4866,
                            505050.0,
                            6.84,
                            37.4,
                            4901,
                            6360
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.9b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.9b-deduped</a>",
                            "GPT-NeoX",
                            6.9,
                            "39.30*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.162,
                            33.4,
                            4952,
                            446428.0,
                            7.8,
                            32.8,
                            5182,
                            6641
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.9b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.9b-deduped</a>",
                            "GPT-NeoX",
                            6.9,
                            "39.30 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.247,
                            38.6,
                            28662,
                            340136.0,
                            6.86,
                            37.3,
                            28825,
                            30276
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.9b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.9b-deduped</a>",
                            "GPT-NeoX",
                            6.9,
                            "39.30*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.276,
                            27.9,
                            5343,
                            344827.0,
                            9.43,
                            27.1,
                            5584,
                            7044
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.9b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.9b-deduped</a>",
                            "GPT-NeoX",
                            6.9,
                            "39.30*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.277,
                            26.0,
                            5343,
                            322580.0,
                            10.1,
                            25.3,
                            5584,
                            7044
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-7b1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-7b1</a>",
                            "\ud83c\udf38 Bloom",
                            7.07,
                            "39.18 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0281,
                            53.1,
                            14651,
                            552486.0,
                            4.83,
                            53.0,
                            14690,
                            16150
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-7b1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-7b1</a>",
                            "\ud83c\udf38 Bloom",
                            7.07,
                            "39.18*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0364,
                            54.4,
                            5846,
                            666666.0,
                            4.73,
                            54.1,
                            5884,
                            7346
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-7b1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-7b1</a>",
                            "\ud83c\udf38 Bloom",
                            7.07,
                            "39.18*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0365,
                            54.3,
                            6923,
                            735294.0,
                            4.74,
                            54.0,
                            6964,
                            8426
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-7b1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-7b1</a>",
                            "\ud83c\udf38 Bloom",
                            7.07,
                            "39.18*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0557,
                            17.5,
                            8616,
                            248138.0,
                            14.7,
                            17.4,
                            8640,
                            10108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-7b1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-7b1</a>",
                            "\ud83c\udf38 Bloom",
                            7.07,
                            "39.18*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0573,
                            51.6,
                            5709,
                            649350.0,
                            5.0,
                            51.2,
                            5748,
                            7207
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-7b1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-7b1</a>",
                            "\ud83c\udf38 Bloom",
                            7.07,
                            "39.18*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.059,
                            17.1,
                            8615,
                            245098.0,
                            15.0,
                            17.1,
                            8665,
                            10133
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-7b1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-7b1</a>",
                            "\ud83c\udf38 Bloom",
                            7.07,
                            "39.18*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0675,
                            47.8,
                            5709,
                            602409.0,
                            5.4,
                            47.4,
                            5748,
                            7207
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-7b1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-7b1</a>",
                            "\ud83c\udf38 Bloom",
                            7.07,
                            "39.18 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.257,
                            46.0,
                            29292,
                            399999.0,
                            5.8,
                            44.1,
                            29410,
                            30861
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-7b1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-7b1</a>",
                            "\ud83c\udf38 Bloom",
                            7.07,
                            "39.18*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.257,
                            31.2,
                            6120,
                            381679.0,
                            8.44,
                            30.3,
                            6350,
                            7809
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-7b1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-7b1</a>",
                            "\ud83c\udf38 Bloom",
                            7.07,
                            "39.18*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.258,
                            31.8,
                            6120,
                            381679.0,
                            8.29,
                            30.9,
                            6350,
                            7809
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b</a>",
                            "GPT-NeoX",
                            12.0,
                            "38.82 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0446,
                            43.5,
                            24619,
                            381679.0,
                            5.9,
                            43.4,
                            24658,
                            26117
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b</a>",
                            "GPT-NeoX",
                            12.0,
                            "38.82 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0454,
                            35.6,
                            24619,
                            355871.0,
                            7.21,
                            35.5,
                            24647,
                            26107
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b</a>",
                            "GPT-NeoX",
                            12.0,
                            "38.82*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0604,
                            36.8,
                            8106,
                            438596.0,
                            6.99,
                            36.6,
                            8122,
                            9583
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b</a>",
                            "GPT-NeoX",
                            12.0,
                            "38.82*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0614,
                            37.9,
                            9453,
                            512820.0,
                            6.79,
                            37.7,
                            9472,
                            10934
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b</a>",
                            "GPT-NeoX",
                            12.0,
                            "38.82*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0737,
                            13.9,
                            13360,
                            195694.0,
                            18.5,
                            13.8,
                            13390,
                            14858
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b</a>",
                            "GPT-NeoX",
                            12.0,
                            "38.82*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0748,
                            13.3,
                            13360,
                            186915.0,
                            19.3,
                            13.3,
                            13379,
                            14847
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b</a>",
                            "GPT-NeoX",
                            12.0,
                            "38.82*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0955,
                            35.7,
                            7892,
                            442477.0,
                            7.25,
                            35.3,
                            7956,
                            9416
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b</a>",
                            "GPT-NeoX",
                            12.0,
                            "38.82*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.119,
                            33.2,
                            7893,
                            414937.0,
                            7.79,
                            32.9,
                            7956,
                            9416
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b</a>",
                            "GPT-NeoX",
                            12.0,
                            "38.82 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.406,
                            25.5,
                            49074,
                            224719.0,
                            10.4,
                            24.6,
                            49111,
                            50562
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b</a>",
                            "GPT-NeoX",
                            12.0,
                            "38.82*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.446,
                            22.8,
                            8652,
                            274725.0,
                            11.6,
                            22.1,
                            9036,
                            10496
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b</a>",
                            "GPT-NeoX",
                            12.0,
                            "38.82*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.457,
                            23.8,
                            8652,
                            264550.0,
                            11.2,
                            22.9,
                            9036,
                            10496
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/matsuo-lab/weblab-10b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matsuo-lab/weblab-10b</a>",
                            "GPT-NeoX",
                            10.0,
                            "38.59 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0416,
                            43.0,
                            22366,
                            387596.0,
                            5.97,
                            42.9,
                            22540,
                            23999
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/matsuo-lab/weblab-10b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matsuo-lab/weblab-10b</a>",
                            "GPT-NeoX",
                            10.0,
                            "38.59 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0428,
                            36.5,
                            22366,
                            378787.0,
                            7.02,
                            36.5,
                            22540,
                            23999
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/matsuo-lab/weblab-10b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matsuo-lab/weblab-10b</a>",
                            "GPT-NeoX",
                            10.0,
                            "38.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.057,
                            37.5,
                            8755,
                            497512.0,
                            6.86,
                            37.3,
                            8835,
                            10297
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/matsuo-lab/weblab-10b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matsuo-lab/weblab-10b</a>",
                            "GPT-NeoX",
                            10.0,
                            "38.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0571,
                            36.5,
                            7475,
                            460829.0,
                            7.04,
                            36.4,
                            7553,
                            9015
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/matsuo-lab/weblab-10b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matsuo-lab/weblab-10b</a>",
                            "GPT-NeoX",
                            10.0,
                            "38.59*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0712,
                            13.9,
                            12106,
                            198412.0,
                            18.5,
                            13.8,
                            12362,
                            13830
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/matsuo-lab/weblab-10b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matsuo-lab/weblab-10b</a>",
                            "GPT-NeoX",
                            10.0,
                            "38.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0747,
                            13.4,
                            12106,
                            190114.0,
                            19.1,
                            13.4,
                            12341,
                            13809
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/matsuo-lab/weblab-10b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matsuo-lab/weblab-10b</a>",
                            "GPT-NeoX",
                            10.0,
                            "38.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0872,
                            35.3,
                            7285,
                            440528.0,
                            7.31,
                            35.0,
                            7350,
                            8810
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/matsuo-lab/weblab-10b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matsuo-lab/weblab-10b</a>",
                            "GPT-NeoX",
                            10.0,
                            "38.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.108,
                            33.0,
                            7284,
                            425531.0,
                            7.84,
                            32.7,
                            7369,
                            8829
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/matsuo-lab/weblab-10b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matsuo-lab/weblab-10b</a>",
                            "GPT-NeoX",
                            10.0,
                            "38.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.26,
                            28.7,
                            7424,
                            364963.0,
                            9.14,
                            28.0,
                            7530,
                            8990
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/matsuo-lab/weblab-10b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matsuo-lab/weblab-10b</a>",
                            "GPT-NeoX",
                            10.0,
                            "38.59 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.39,
                            27.6,
                            44535,
                            240963.0,
                            9.62,
                            26.6,
                            44765,
                            46217
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/matsuo-lab/weblab-10b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matsuo-lab/weblab-10b</a>",
                            "GPT-NeoX",
                            10.0,
                            "38.59*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.425,
                            24.5,
                            7972,
                            275482.0,
                            10.8,
                            23.7,
                            8348,
                            9808
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/matsuo-lab/weblab-10b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matsuo-lab/weblab-10b</a>",
                            "GPT-NeoX",
                            10.0,
                            "38.59*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.425,
                            23.4,
                            7971,
                            281690.0,
                            11.3,
                            22.7,
                            8348,
                            9808
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0205,
                            48.9,
                            6068,
                            625000.0,
                            5.24,
                            48.9,
                            6144,
                            7604
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0232,
                            43.4,
                            6068,
                            555555.0,
                            5.89,
                            43.5,
                            6144,
                            7604
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0253,
                            41.6,
                            3061,
                            657894.0,
                            6.16,
                            41.6,
                            3189,
                            4651
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0253,
                            40.3,
                            6068,
                            540540.0,
                            6.35,
                            40.3,
                            6144,
                            7604
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0254,
                            42.7,
                            2388,
                            617283.0,
                            6.0,
                            42.7,
                            2512,
                            3974
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0277,
                            39.5,
                            6068,
                            523560.0,
                            6.49,
                            39.4,
                            6144,
                            7604
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0375,
                            39.7,
                            2340,
                            591715.0,
                            6.46,
                            39.6,
                            2480,
                            3940
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0401,
                            37.6,
                            2334,
                            558659.0,
                            6.82,
                            37.5,
                            2472,
                            3932
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0636,
                            15.5,
                            3605,
                            230946.0,
                            16.5,
                            15.5,
                            3711,
                            5179
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0669,
                            14.8,
                            3605,
                            219298.0,
                            17.3,
                            14.8,
                            3711,
                            5179
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0796,
                            33.2,
                            2338,
                            490196.0,
                            7.77,
                            32.9,
                            2489,
                            3948
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.106,
                            41.9,
                            12028,
                            502512.0,
                            6.2,
                            41.3,
                            12069,
                            13520
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.118,
                            27.0,
                            2457,
                            378787.0,
                            9.56,
                            26.8,
                            2589,
                            4049
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.123,
                            25.2,
                            2457,
                            362318.0,
                            10.2,
                            25.1,
                            2589,
                            4049
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "38.26 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0225,
                            42.9,
                            7283,
                            558659.0,
                            5.96,
                            43.0,
                            7440,
                            8900
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "38.26 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.023,
                            46.8,
                            7280,
                            584795.0,
                            5.47,
                            46.8,
                            7440,
                            8900
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "38.26 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0231,
                            47.0,
                            7280,
                            613496.0,
                            5.44,
                            47.1,
                            7440,
                            8900
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "38.26*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0825,
                            12.1,
                            4039,
                            188679.0,
                            21.1,
                            12.1,
                            4232,
                            5699
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "38.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0828,
                            12.4,
                            4035,
                            184162.0,
                            20.7,
                            12.4,
                            4232,
                            5699
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "38.26 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.13,
                            57.0,
                            14582,
                            571428.0,
                            4.6,
                            55.7,
                            14677,
                            16129
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "38.26*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.147,
                            26.7,
                            2666,
                            366300.0,
                            9.71,
                            26.4,
                            2728,
                            4187
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "38.26*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.147,
                            26.5,
                            2669,
                            355871.0,
                            9.79,
                            26.1,
                            2728,
                            4187
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.7b</a>",
                            "GPT-NeoX",
                            6.65,
                            "38.06 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0275,
                            48.8,
                            14404,
                            505050.0,
                            5.26,
                            48.7,
                            14445,
                            15904
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.7b</a>",
                            "GPT-NeoX",
                            6.65,
                            "38.06 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0296,
                            45.3,
                            14404,
                            452488.0,
                            5.66,
                            45.2,
                            14445,
                            15904
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.7b</a>",
                            "GPT-NeoX",
                            6.65,
                            "38.06 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.03,
                            41.5,
                            14403,
                            476190.0,
                            6.17,
                            41.5,
                            14445,
                            15904
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.7b</a>",
                            "GPT-NeoX",
                            6.65,
                            "38.06 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0315,
                            40.4,
                            14403,
                            431034.0,
                            6.34,
                            40.4,
                            14445,
                            15904
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.7b</a>",
                            "GPT-NeoX",
                            6.65,
                            "38.06*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0394,
                            40.0,
                            5002,
                            546448.0,
                            6.41,
                            39.9,
                            5028,
                            6490
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.7b</a>",
                            "GPT-NeoX",
                            6.65,
                            "38.06*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0401,
                            40.9,
                            6080,
                            595238.0,
                            6.28,
                            40.8,
                            6109,
                            7570
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.7b</a>",
                            "GPT-NeoX",
                            6.65,
                            "38.06*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0632,
                            39.7,
                            4866,
                            529100.0,
                            6.49,
                            39.4,
                            4901,
                            6360
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.7b</a>",
                            "GPT-NeoX",
                            6.65,
                            "38.06*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0638,
                            15.6,
                            7965,
                            227790.0,
                            16.4,
                            15.6,
                            8013,
                            9481
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.7b</a>",
                            "GPT-NeoX",
                            6.65,
                            "38.06*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0671,
                            15.2,
                            7965,
                            217864.0,
                            16.9,
                            15.1,
                            8004,
                            9472
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.7b</a>",
                            "GPT-NeoX",
                            6.65,
                            "38.06*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.071,
                            37.8,
                            4866,
                            502512.0,
                            6.82,
                            37.5,
                            4896,
                            6356
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.7b</a>",
                            "GPT-NeoX",
                            6.65,
                            "38.06*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.162,
                            33.9,
                            4952,
                            438596.0,
                            7.69,
                            33.3,
                            5182,
                            6641
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.7b</a>",
                            "GPT-NeoX",
                            6.65,
                            "38.06 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.246,
                            38.3,
                            28662,
                            341296.0,
                            6.91,
                            37.0,
                            28825,
                            30276
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.7b</a>",
                            "GPT-NeoX",
                            6.65,
                            "38.06*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.275,
                            27.9,
                            5343,
                            337837.0,
                            9.43,
                            27.1,
                            5584,
                            7044
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.7b</a>",
                            "GPT-NeoX",
                            6.65,
                            "38.06*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.276,
                            25.7,
                            5343,
                            324675.0,
                            10.2,
                            25.1,
                            5584,
                            7044
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-7b-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-7b-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            7.0,
                            "37.95 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0339,
                            29.9,
                            14829,
                            373134.0,
                            8.55,
                            29.9,
                            14881,
                            16340
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-7b-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-7b-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            7.0,
                            "37.95 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0354,
                            30.2,
                            14829,
                            362318.0,
                            8.49,
                            30.2,
                            14881,
                            16341
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-7b-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-7b-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            7.0,
                            "37.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.107,
                            9.41,
                            7875,
                            140845.0,
                            27.2,
                            9.41,
                            7891,
                            9359
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-7b-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-7b-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            7.0,
                            "37.95 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.241,
                            30.8,
                            29649,
                            300300.0,
                            8.53,
                            30.0,
                            29725,
                            31176
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-7b-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-7b-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            7.0,
                            "37.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.272,
                            17.5,
                            5081,
                            241545.0,
                            14.9,
                            17.2,
                            5360,
                            6819
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Devio/test-22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Devio/test-22B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "37.71 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.077,
                            28.1,
                            44887,
                            254452.0,
                            9.17,
                            27.9,
                            45185,
                            46644
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Devio/test-22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Devio/test-22B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "37.71 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0848,
                            28.0,
                            44887,
                            248756.0,
                            9.2,
                            27.8,
                            45185,
                            46644
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Devio/test-22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Devio/test-22B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "37.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.102,
                            30.8,
                            13546,
                            331125.0,
                            8.39,
                            30.5,
                            13734,
                            15195
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Devio/test-22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Devio/test-22B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "37.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.103,
                            32.1,
                            14729,
                            398406.0,
                            8.05,
                            31.8,
                            14919,
                            16380
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Devio/test-22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Devio/test-22B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "37.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.119,
                            8.33,
                            23464,
                            119331.0,
                            30.7,
                            8.34,
                            23792,
                            25260
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Devio/test-22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Devio/test-22B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "37.71*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.123,
                            7.97,
                            23464,
                            114547.0,
                            32.1,
                            7.98,
                            23792,
                            25260
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Devio/test-22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Devio/test-22B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "37.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.161,
                            32.7,
                            13306,
                            347222.0,
                            7.97,
                            32.1,
                            13551,
                            15011
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Devio/test-22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Devio/test-22B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "37.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.202,
                            29.0,
                            13283,
                            323624.0,
                            9.0,
                            28.4,
                            13514,
                            14973
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Devio/test-22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Devio/test-22B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "37.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.818,
                            17.6,
                            14203,
                            189035.0,
                            15.3,
                            16.7,
                            14847,
                            16307
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Devio/test-22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Devio/test-22B</a>",
                            "\ud83e\udd99 LLaMA",
                            21.83,
                            "37.71*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.825,
                            17.7,
                            14203,
                            186219.0,
                            15.2,
                            16.8,
                            14847,
                            16307
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-13B</a>",
                            "GPT-2",
                            13.0,
                            "37.40*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0426,
                            36.3,
                            26723,
                            418410.0,
                            7.06,
                            36.3,
                            26744,
                            28204
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-13B</a>",
                            "GPT-2",
                            13.0,
                            "37.40*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0434,
                            36.8,
                            26723,
                            420168.0,
                            6.96,
                            36.8,
                            26744,
                            28204
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-13B</a>",
                            "GPT-2",
                            13.0,
                            "37.40 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0471,
                            36.8,
                            26723,
                            358422.0,
                            6.97,
                            36.7,
                            26755,
                            28215
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-13B</a>",
                            "GPT-2",
                            13.0,
                            "37.40*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0642,
                            44.9,
                            8352,
                            492610.0,
                            5.74,
                            44.6,
                            8390,
                            9852
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-13B</a>",
                            "GPT-2",
                            13.0,
                            "37.40*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.309,
                            34.6,
                            8169,
                            389105.0,
                            7.67,
                            33.4,
                            8365,
                            9825
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-13B</a>",
                            "GPT-2",
                            13.0,
                            "37.40 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.412,
                            22.2,
                            53269,
                            195694.0,
                            11.9,
                            21.5,
                            53309,
                            54760
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0</a>",
                            "GPT-NeoX",
                            4.0,
                            "37.31 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0295,
                            33.9,
                            8666,
                            444444.0,
                            7.56,
                            33.9,
                            8745,
                            10204
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0</a>",
                            "GPT-NeoX",
                            4.0,
                            "37.31 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0319,
                            30.7,
                            8666,
                            390625.0,
                            8.34,
                            30.7,
                            8745,
                            10204
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0</a>",
                            "GPT-NeoX",
                            4.0,
                            "37.31 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.036,
                            28.2,
                            8666,
                            381679.0,
                            9.07,
                            28.2,
                            8745,
                            10204
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0</a>",
                            "GPT-NeoX",
                            4.0,
                            "37.31 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0367,
                            27.7,
                            8666,
                            370370.0,
                            9.25,
                            27.7,
                            8745,
                            10204
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0</a>",
                            "GPT-NeoX",
                            4.0,
                            "37.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0375,
                            28.9,
                            3911,
                            446428.0,
                            8.85,
                            28.9,
                            4083,
                            5544
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0</a>",
                            "GPT-NeoX",
                            4.0,
                            "37.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0382,
                            27.7,
                            3237,
                            409836.0,
                            9.25,
                            27.7,
                            3426,
                            4888
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0</a>",
                            "GPT-NeoX",
                            4.0,
                            "37.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0534,
                            26.7,
                            3191,
                            404858.0,
                            9.6,
                            26.7,
                            3374,
                            4833
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0</a>",
                            "GPT-NeoX",
                            4.0,
                            "37.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0577,
                            25.5,
                            3182,
                            378787.0,
                            10.1,
                            25.3,
                            3365,
                            4825
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0</a>",
                            "GPT-NeoX",
                            4.0,
                            "37.31*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0915,
                            10.5,
                            5049,
                            155763.0,
                            24.5,
                            10.4,
                            5184,
                            6652
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0</a>",
                            "GPT-NeoX",
                            4.0,
                            "37.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.101,
                            10.1,
                            5052,
                            150829.0,
                            25.3,
                            10.1,
                            5190,
                            6658
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0</a>",
                            "GPT-NeoX",
                            4.0,
                            "37.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.114,
                            22.6,
                            3184,
                            327868.0,
                            11.4,
                            22.5,
                            3380,
                            4840
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0</a>",
                            "GPT-NeoX",
                            4.0,
                            "37.31 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.157,
                            29.1,
                            17177,
                            346020.0,
                            8.93,
                            28.7,
                            17207,
                            18658
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0</a>",
                            "GPT-NeoX",
                            4.0,
                            "37.31*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.174,
                            18.5,
                            3333,
                            257069.0,
                            14.0,
                            18.3,
                            3527,
                            4986
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0</a>",
                            "GPT-NeoX",
                            4.0,
                            "37.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.175,
                            17.3,
                            3333,
                            242718.0,
                            14.9,
                            17.2,
                            3527,
                            4986
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-6.7b</a>",
                            "GPT-2",
                            7.11,
                            "37.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0251,
                            54.3,
                            14110,
                            653594.0,
                            4.73,
                            54.1,
                            14136,
                            15596
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-6.7b</a>",
                            "GPT-2",
                            7.11,
                            "37.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0253,
                            54.4,
                            14110,
                            662251.0,
                            4.72,
                            54.2,
                            14136,
                            15596
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-6.7b</a>",
                            "GPT-2",
                            7.11,
                            "37.23 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.027,
                            54.5,
                            14110,
                            571428.0,
                            4.71,
                            54.4,
                            14136,
                            15596
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-6.7b</a>",
                            "GPT-2",
                            7.11,
                            "37.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.036,
                            54.8,
                            4709,
                            684931.0,
                            4.69,
                            54.6,
                            4739,
                            6201
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-6.7b</a>",
                            "GPT-2",
                            7.11,
                            "37.23 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.238,
                            35.6,
                            28075,
                            324675.0,
                            7.4,
                            34.6,
                            28233,
                            29685
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0202,
                            49.6,
                            6052,
                            645161.0,
                            5.16,
                            49.6,
                            6123,
                            7583
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0236,
                            43.1,
                            6052,
                            578034.0,
                            5.94,
                            43.1,
                            6123,
                            7583
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0247,
                            40.3,
                            6052,
                            552486.0,
                            6.34,
                            40.4,
                            6123,
                            7583
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0254,
                            42.6,
                            3045,
                            649350.0,
                            6.01,
                            42.6,
                            3168,
                            4630
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0255,
                            42.5,
                            2372,
                            621118.0,
                            6.03,
                            42.5,
                            2493,
                            3955
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0256,
                            41.3,
                            6052,
                            531914.0,
                            6.21,
                            41.2,
                            6123,
                            7583
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0359,
                            40.3,
                            2324,
                            602409.0,
                            6.36,
                            40.3,
                            2438,
                            3898
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0404,
                            37.9,
                            2318,
                            558659.0,
                            6.77,
                            37.8,
                            2432,
                            3892
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0636,
                            15.4,
                            3588,
                            231481.0,
                            16.7,
                            15.3,
                            3688,
                            5156
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0667,
                            14.9,
                            3589,
                            222717.0,
                            17.2,
                            14.9,
                            3688,
                            5156
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0783,
                            34.0,
                            2321,
                            492610.0,
                            7.57,
                            33.8,
                            2468,
                            3927
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.106,
                            41.6,
                            11995,
                            507614.0,
                            6.24,
                            41.0,
                            12043,
                            13495
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.121,
                            25.0,
                            2439,
                            357142.0,
                            10.3,
                            24.9,
                            2548,
                            4007
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.122,
                            27.2,
                            2439,
                            370370.0,
                            9.5,
                            26.9,
                            2548,
                            4007
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-rw-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-rw-1b</a>",
                            "\ud83e\udd85 Falcon",
                            1.0,
                            "37.07 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.015,
                            66.8,
                            2834,
                            854700.0,
                            3.84,
                            66.7,
                            2858,
                            4317
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-rw-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-rw-1b</a>",
                            "\ud83e\udd85 Falcon",
                            1.0,
                            "37.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0155,
                            70.1,
                            1080,
                            1028806.0,
                            3.66,
                            69.9,
                            1121,
                            2583
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-rw-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-rw-1b</a>",
                            "\ud83e\udd85 Falcon",
                            1.0,
                            "37.07 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.017,
                            65.1,
                            2834,
                            892857.0,
                            3.94,
                            65.0,
                            2858,
                            4317
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-rw-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-rw-1b</a>",
                            "\ud83e\udd85 Falcon",
                            1.0,
                            "37.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0183,
                            65.7,
                            1045,
                            943396.0,
                            3.9,
                            65.6,
                            1075,
                            2535
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-rw-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-rw-1b</a>",
                            "\ud83e\udd85 Falcon",
                            1.0,
                            "37.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0193,
                            60.1,
                            1045,
                            877192.0,
                            4.26,
                            60.1,
                            1075,
                            2535
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-rw-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-rw-1b</a>",
                            "\ud83e\udd85 Falcon",
                            1.0,
                            "37.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0362,
                            53.9,
                            1047,
                            793650.0,
                            4.77,
                            53.7,
                            1082,
                            2541
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-rw-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-rw-1b</a>",
                            "\ud83e\udd85 Falcon",
                            1.0,
                            "37.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0473,
                            21.4,
                            1629,
                            320512.0,
                            11.9,
                            21.5,
                            1652,
                            3120
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-rw-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-rw-1b</a>",
                            "\ud83e\udd85 Falcon",
                            1.0,
                            "37.07 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0521,
                            72.2,
                            5659,
                            847457.0,
                            3.58,
                            71.5,
                            5700,
                            7151
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-rw-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-rw-1b</a>",
                            "\ud83e\udd85 Falcon",
                            1.0,
                            "37.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0578,
                            39.0,
                            1097,
                            558659.0,
                            6.6,
                            38.8,
                            1153,
                            2612
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0204,
                            48.9,
                            6052,
                            649350.0,
                            5.24,
                            48.9,
                            6123,
                            7583
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0236,
                            44.0,
                            6052,
                            549450.0,
                            5.82,
                            44.0,
                            6123,
                            7583
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0245,
                            40.7,
                            6052,
                            555555.0,
                            6.29,
                            40.7,
                            6123,
                            7583
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0251,
                            42.9,
                            3045,
                            657894.0,
                            5.97,
                            42.9,
                            3168,
                            4630
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0256,
                            42.2,
                            2372,
                            617283.0,
                            6.07,
                            42.2,
                            2493,
                            3955
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.026,
                            40.4,
                            6052,
                            537634.0,
                            6.34,
                            40.4,
                            6123,
                            7583
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0359,
                            39.8,
                            2324,
                            591715.0,
                            6.44,
                            39.8,
                            2438,
                            3898
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.04,
                            37.4,
                            2318,
                            561797.0,
                            6.86,
                            37.3,
                            2432,
                            3892
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0636,
                            15.4,
                            3588,
                            228310.0,
                            16.7,
                            15.3,
                            3688,
                            5156
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0674,
                            15.1,
                            3589,
                            219298.0,
                            17.0,
                            15.1,
                            3688,
                            5156
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0776,
                            34.2,
                            2321,
                            497512.0,
                            7.54,
                            34.0,
                            2468,
                            3927
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.105,
                            41.4,
                            11995,
                            497512.0,
                            6.27,
                            40.8,
                            12043,
                            13495
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.118,
                            26.9,
                            2439,
                            377358.0,
                            9.61,
                            26.6,
                            2548,
                            4007
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.123,
                            25.6,
                            2439,
                            355871.0,
                            10.1,
                            25.3,
                            2548,
                            4007
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "36.42 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0171,
                            57.2,
                            2279,
                            833333.0,
                            4.48,
                            57.1,
                            2403,
                            3862
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "36.42 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0179,
                            53.6,
                            2279,
                            740740.0,
                            4.78,
                            53.6,
                            2403,
                            3862
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "36.42 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0181,
                            53.9,
                            2279,
                            751879.0,
                            4.75,
                            53.9,
                            2403,
                            3862
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "36.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0195,
                            48.4,
                            851,
                            709219.0,
                            5.29,
                            48.4,
                            935,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "36.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0202,
                            51.1,
                            1248,
                            806451.0,
                            5.01,
                            51.1,
                            1304,
                            2766
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "36.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0202,
                            50.1,
                            852,
                            787401.0,
                            5.11,
                            50.1,
                            935,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "36.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0206,
                            50.8,
                            877,
                            775193.0,
                            5.04,
                            50.8,
                            933,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "36.42 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0432,
                            56.8,
                            4492,
                            751879.0,
                            4.53,
                            56.5,
                            4548,
                            5999
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "36.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0443,
                            37.0,
                            869,
                            591715.0,
                            6.94,
                            36.9,
                            922,
                            2382
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "36.42*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.0483,
                            32.2,
                            924,
                            471698.0,
                            7.97,
                            32.1,
                            1008,
                            2468
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "36.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0492,
                            31.4,
                            924,
                            465116.0,
                            8.18,
                            31.3,
                            1008,
                            2468
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "36.42*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0651,
                            15.2,
                            1380,
                            238095.0,
                            16.9,
                            15.1,
                            1421,
                            2889
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "36.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0701,
                            14.3,
                            1380,
                            216450.0,
                            17.9,
                            14.3,
                            1421,
                            2889
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-7.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-7.5B</a>",
                            "XGLM",
                            7.5,
                            "36.38 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0308,
                            52.5,
                            15548,
                            540540.0,
                            4.89,
                            52.4,
                            15583,
                            17043
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-7.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-7.5B</a>",
                            "XGLM",
                            7.5,
                            "36.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0402,
                            52.1,
                            6149,
                            632911.0,
                            4.93,
                            51.9,
                            6199,
                            7660
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-7.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-7.5B</a>",
                            "XGLM",
                            7.5,
                            "36.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0411,
                            53.0,
                            7228,
                            709219.0,
                            4.85,
                            52.8,
                            7279,
                            8740
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-7.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-7.5B</a>",
                            "XGLM",
                            7.5,
                            "36.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0612,
                            50.0,
                            6011,
                            617283.0,
                            5.16,
                            49.6,
                            6056,
                            7516
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-7.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-7.5B</a>",
                            "XGLM",
                            7.5,
                            "36.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0703,
                            43.5,
                            6011,
                            552486.0,
                            5.93,
                            43.2,
                            6056,
                            7516
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-7.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-7.5B</a>",
                            "XGLM",
                            7.5,
                            "36.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0823,
                            12.4,
                            9110,
                            179211.0,
                            20.7,
                            12.4,
                            9145,
                            10613
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-7.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-7.5B</a>",
                            "XGLM",
                            7.5,
                            "36.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.284,
                            25.7,
                            6318,
                            332225.0,
                            10.2,
                            25.1,
                            6394,
                            7853
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-7.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-7.5B</a>",
                            "XGLM",
                            7.5,
                            "36.38 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.286,
                            39.3,
                            31087,
                            349650.0,
                            6.78,
                            37.8,
                            31274,
                            32726
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-6.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-6.7B</a>",
                            "GPT-2",
                            6.7,
                            "36.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0263,
                            53.8,
                            13997,
                            645161.0,
                            4.77,
                            53.7,
                            14040,
                            15500
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-6.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-6.7B</a>",
                            "GPT-2",
                            6.7,
                            "36.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0264,
                            54.6,
                            13997,
                            671140.0,
                            4.7,
                            54.5,
                            14040,
                            15500
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-6.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-6.7B</a>",
                            "GPT-2",
                            6.7,
                            "36.27 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0272,
                            63.3,
                            13997,
                            602409.0,
                            4.06,
                            63.1,
                            14040,
                            15497
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-6.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-6.7B</a>",
                            "GPT-2",
                            6.7,
                            "36.27 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.028,
                            55.2,
                            13997,
                            581395.0,
                            4.65,
                            55.1,
                            14040,
                            15499
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-6.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-6.7B</a>",
                            "GPT-2",
                            6.7,
                            "36.27 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.029,
                            55.1,
                            13997,
                            552486.0,
                            4.66,
                            54.9,
                            14040,
                            15500
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-6.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-6.7B</a>",
                            "GPT-2",
                            6.7,
                            "36.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.037,
                            55.6,
                            4596,
                            699300.0,
                            4.63,
                            55.3,
                            4622,
                            6083
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-6.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-6.7B</a>",
                            "GPT-2",
                            6.7,
                            "36.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.161,
                            42.5,
                            4483,
                            543478.0,
                            6.16,
                            41.6,
                            4775,
                            6234
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-6.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-6.7B</a>",
                            "GPT-2",
                            6.7,
                            "36.27 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.229,
                            35.5,
                            27850,
                            331125.0,
                            7.41,
                            34.5,
                            28015,
                            29467
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>",
                            "GPT-Neo",
                            2.72,
                            "36.20 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.017,
                            56.7,
                            5782,
                            714285.0,
                            4.52,
                            56.6,
                            6006,
                            7463
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>",
                            "GPT-Neo",
                            2.72,
                            "36.20 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0219,
                            46.6,
                            5782,
                            591715.0,
                            5.49,
                            46.6,
                            6006,
                            7465
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>",
                            "GPT-Neo",
                            2.72,
                            "36.20 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0222,
                            44.4,
                            5787,
                            588235.0,
                            5.76,
                            44.4,
                            6027,
                            7486
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>",
                            "GPT-Neo",
                            2.72,
                            "36.20 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0241,
                            43.9,
                            5787,
                            571428.0,
                            5.83,
                            43.9,
                            6027,
                            7486
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>",
                            "GPT-Neo",
                            2.72,
                            "36.20*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0257,
                            45.5,
                            2129,
                            645161.0,
                            5.63,
                            45.5,
                            2254,
                            3716
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>",
                            "GPT-Neo",
                            2.72,
                            "36.20*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0264,
                            46.4,
                            2803,
                            704225.0,
                            5.52,
                            46.4,
                            2929,
                            4391
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>",
                            "GPT-Neo",
                            2.72,
                            "36.20*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0366,
                            44.9,
                            2080,
                            653594.0,
                            5.72,
                            44.8,
                            2197,
                            3657
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>",
                            "GPT-Neo",
                            2.72,
                            "36.20*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0402,
                            41.2,
                            2073,
                            606060.0,
                            6.23,
                            41.1,
                            2191,
                            3651
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>",
                            "GPT-Neo",
                            2.72,
                            "36.20*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0797,
                            32.7,
                            2077,
                            487804.0,
                            7.87,
                            32.5,
                            2193,
                            3653
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>",
                            "GPT-Neo",
                            2.72,
                            "36.20*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0822,
                            12.1,
                            3336,
                            181818.0,
                            21.1,
                            12.1,
                            3422,
                            4890
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>",
                            "GPT-Neo",
                            2.72,
                            "36.20*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0864,
                            11.8,
                            3341,
                            174520.0,
                            21.8,
                            11.7,
                            3422,
                            4890
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>",
                            "GPT-Neo",
                            2.72,
                            "36.20 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.104,
                            50.3,
                            11555,
                            568181.0,
                            5.17,
                            49.5,
                            11593,
                            13044
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>",
                            "GPT-Neo",
                            2.72,
                            "36.20*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.117,
                            25.2,
                            2164,
                            344827.0,
                            10.2,
                            25.1,
                            2308,
                            3768
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>",
                            "GPT-Neo",
                            2.72,
                            "36.20*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.121,
                            24.5,
                            2169,
                            338983.0,
                            10.5,
                            24.4,
                            2329,
                            3789
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0.2</a>",
                            "GPT-NeoX",
                            4.0,
                            "36.15 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.021,
                            47.6,
                            6068,
                            632911.0,
                            5.38,
                            47.6,
                            6144,
                            7604
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0.2</a>",
                            "GPT-NeoX",
                            4.0,
                            "36.15 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.022,
                            44.6,
                            6068,
                            564971.0,
                            5.74,
                            44.6,
                            6144,
                            7604
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0.2</a>",
                            "GPT-NeoX",
                            4.0,
                            "36.15 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0249,
                            40.2,
                            6068,
                            534759.0,
                            6.36,
                            40.3,
                            6144,
                            7604
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0.2</a>",
                            "GPT-NeoX",
                            4.0,
                            "36.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0255,
                            41.9,
                            3061,
                            621118.0,
                            6.11,
                            41.9,
                            3189,
                            4651
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0.2</a>",
                            "GPT-NeoX",
                            4.0,
                            "36.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0256,
                            42.4,
                            2388,
                            617283.0,
                            6.05,
                            42.3,
                            2512,
                            3974
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0.2</a>",
                            "GPT-NeoX",
                            4.0,
                            "36.15 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0266,
                            40.1,
                            6068,
                            546448.0,
                            6.39,
                            40.1,
                            6144,
                            7604
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0.2</a>",
                            "GPT-NeoX",
                            4.0,
                            "36.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.037,
                            40.5,
                            2340,
                            606060.0,
                            6.33,
                            40.4,
                            2480,
                            3940
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0.2</a>",
                            "GPT-NeoX",
                            4.0,
                            "36.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0393,
                            37.8,
                            2334,
                            561797.0,
                            6.79,
                            37.7,
                            2472,
                            3932
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0.2</a>",
                            "GPT-NeoX",
                            4.0,
                            "36.15*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0619,
                            15.5,
                            3605,
                            233100.0,
                            16.5,
                            15.5,
                            3711,
                            5179
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0.2</a>",
                            "GPT-NeoX",
                            4.0,
                            "36.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0674,
                            14.8,
                            3605,
                            220264.0,
                            17.3,
                            14.8,
                            3711,
                            5179
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0.2</a>",
                            "GPT-NeoX",
                            4.0,
                            "36.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0791,
                            34.4,
                            2338,
                            497512.0,
                            7.49,
                            34.2,
                            2489,
                            3948
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0.2</a>",
                            "GPT-NeoX",
                            4.0,
                            "36.15 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.106,
                            41.1,
                            12028,
                            497512.0,
                            6.31,
                            40.6,
                            12069,
                            13520
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0.2</a>",
                            "GPT-NeoX",
                            4.0,
                            "36.15*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.121,
                            27.0,
                            2457,
                            383141.0,
                            9.57,
                            26.8,
                            2589,
                            4049
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0.2</a>",
                            "GPT-NeoX",
                            4.0,
                            "36.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.122,
                            25.6,
                            2457,
                            362318.0,
                            10.1,
                            25.3,
                            2589,
                            4049
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>",
                            "\ud83c\udf38 Bloom",
                            3.0,
                            "36.07 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0177,
                            55.4,
                            6365,
                            657894.0,
                            4.62,
                            55.4,
                            6434,
                            7893
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>",
                            "\ud83c\udf38 Bloom",
                            3.0,
                            "36.07 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0193,
                            53.6,
                            6365,
                            671140.0,
                            4.78,
                            53.6,
                            6434,
                            7893
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>",
                            "\ud83c\udf38 Bloom",
                            3.0,
                            "36.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0201,
                            55.6,
                            3591,
                            819672.0,
                            4.61,
                            55.5,
                            3726,
                            5188
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>",
                            "\ud83c\udf38 Bloom",
                            3.0,
                            "36.07 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0202,
                            53.5,
                            6365,
                            671140.0,
                            4.79,
                            53.4,
                            6434,
                            7893
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>",
                            "\ud83c\udf38 Bloom",
                            3.0,
                            "36.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0203,
                            55.0,
                            2918,
                            775193.0,
                            4.66,
                            54.9,
                            3051,
                            4513
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>",
                            "\ud83c\udf38 Bloom",
                            3.0,
                            "36.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0312,
                            53.1,
                            2869,
                            746268.0,
                            4.83,
                            53.0,
                            3003,
                            4462
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>",
                            "\ud83c\udf38 Bloom",
                            3.0,
                            "36.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0357,
                            49.7,
                            2863,
                            694444.0,
                            5.17,
                            49.5,
                            2996,
                            4456
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>",
                            "\ud83c\udf38 Bloom",
                            3.0,
                            "36.07*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0561,
                            17.6,
                            4056,
                            255102.0,
                            14.6,
                            17.5,
                            4129,
                            5597
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>",
                            "\ud83c\udf38 Bloom",
                            3.0,
                            "36.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.058,
                            17.2,
                            4059,
                            250626.0,
                            14.9,
                            17.2,
                            4146,
                            5614
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>",
                            "\ud83c\udf38 Bloom",
                            3.0,
                            "36.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0669,
                            43.7,
                            2867,
                            613496.0,
                            5.91,
                            43.3,
                            3137,
                            4596
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>",
                            "\ud83c\udf38 Bloom",
                            3.0,
                            "36.07*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.107,
                            32.0,
                            2966,
                            434782.0,
                            8.09,
                            31.6,
                            3208,
                            4668
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>",
                            "\ud83c\udf38 Bloom",
                            3.0,
                            "36.07*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.11,
                            30.8,
                            2966,
                            425531.0,
                            8.38,
                            30.5,
                            3208,
                            4668
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>",
                            "\ud83c\udf38 Bloom",
                            3.0,
                            "36.07 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.113,
                            54.8,
                            12734,
                            588235.0,
                            4.76,
                            53.8,
                            12859,
                            14310
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</a>",
                            "\ud83e\udd99 LLaMA",
                            1.3,
                            "35.71 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.018,
                            56.2,
                            2991,
                            769230.0,
                            4.56,
                            56.1,
                            3114,
                            4573
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</a>",
                            "\ud83e\udd99 LLaMA",
                            1.3,
                            "35.71 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0182,
                            53.6,
                            2991,
                            746268.0,
                            4.78,
                            53.6,
                            3114,
                            4573
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</a>",
                            "\ud83e\udd99 LLaMA",
                            1.3,
                            "35.71 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0185,
                            54.8,
                            2991,
                            763358.0,
                            4.67,
                            54.8,
                            3114,
                            4573
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</a>",
                            "\ud83e\udd99 LLaMA",
                            1.3,
                            "35.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0201,
                            55.0,
                            1157,
                            813008.0,
                            4.66,
                            54.9,
                            1296,
                            2755
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</a>",
                            "\ud83e\udd99 LLaMA",
                            1.3,
                            "35.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0206,
                            52.8,
                            1544,
                            819672.0,
                            4.85,
                            52.8,
                            1675,
                            3137
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</a>",
                            "\ud83e\udd99 LLaMA",
                            1.3,
                            "35.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0206,
                            52.4,
                            1182,
                            793650.0,
                            4.89,
                            52.4,
                            1314,
                            2776
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</a>",
                            "\ud83e\udd99 LLaMA",
                            1.3,
                            "35.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0211,
                            48.1,
                            1156,
                            709219.0,
                            5.32,
                            48.1,
                            1293,
                            2753
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</a>",
                            "\ud83e\udd99 LLaMA",
                            1.3,
                            "35.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0463,
                            38.4,
                            1159,
                            588235.0,
                            6.69,
                            38.3,
                            1281,
                            2740
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</a>",
                            "\ud83e\udd99 LLaMA",
                            1.3,
                            "35.71 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0483,
                            60.0,
                            5969,
                            719424.0,
                            4.3,
                            59.5,
                            5991,
                            7442
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</a>",
                            "\ud83e\udd99 LLaMA",
                            1.3,
                            "35.71*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.0557,
                            30.2,
                            1226,
                            438596.0,
                            8.51,
                            30.1,
                            1340,
                            2799
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</a>",
                            "\ud83e\udd99 LLaMA",
                            1.3,
                            "35.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0558,
                            30.7,
                            1226,
                            456621.0,
                            8.36,
                            30.6,
                            1340,
                            2799
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</a>",
                            "\ud83e\udd99 LLaMA",
                            1.3,
                            "35.71*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0714,
                            13.9,
                            1741,
                            217864.0,
                            18.5,
                            13.8,
                            1912,
                            3380
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</a>",
                            "\ud83e\udd99 LLaMA",
                            1.3,
                            "35.71*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0731,
                            13.9,
                            1741,
                            206611.0,
                            18.5,
                            13.8,
                            1912,
                            3380
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-3b-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-3b-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            3.0,
                            "35.25 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0325,
                            29.1,
                            6007,
                            413223.0,
                            8.78,
                            29.2,
                            6272,
                            7732
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-3b-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-3b-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            3.0,
                            "35.25 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0343,
                            29.3,
                            6007,
                            393700.0,
                            8.73,
                            29.3,
                            6272,
                            7732
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-3b-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-3b-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            3.0,
                            "35.25 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.102,
                            31.8,
                            12171,
                            404858.0,
                            8.12,
                            31.5,
                            12226,
                            13677
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-3b-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-3b-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            3.0,
                            "35.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.109,
                            9.27,
                            3358,
                            138888.0,
                            27.6,
                            9.28,
                            3462,
                            4930
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-3b-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-3b-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            3.0,
                            "35.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.119,
                            17.6,
                            2195,
                            255754.0,
                            14.6,
                            17.5,
                            2422,
                            3881
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-base</a>",
                            "GPT-2",
                            0.0,
                            "35.18*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0201,
                            72.4,
                            10607,
                            862068.0,
                            3.54,
                            72.3,
                            10649,
                            12108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-base</a>",
                            "GPT-2",
                            0.0,
                            "35.18*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0201,
                            70.8,
                            10607,
                            869565.0,
                            3.62,
                            70.7,
                            10649,
                            12108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-base</a>",
                            "GPT-2",
                            0.0,
                            "35.18 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.021,
                            84.4,
                            10607,
                            757575.0,
                            3.04,
                            84.2,
                            10628,
                            12085
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-base</a>",
                            "GPT-2",
                            0.0,
                            "35.18 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0214,
                            72.2,
                            10607,
                            763358.0,
                            3.55,
                            72.1,
                            10649,
                            12108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-base</a>",
                            "GPT-2",
                            0.0,
                            "35.18 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0229,
                            71.4,
                            10607,
                            704225.0,
                            3.59,
                            71.3,
                            10649,
                            12108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-base</a>",
                            "GPT-2",
                            0.0,
                            "35.18*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0288,
                            76.8,
                            3590,
                            943396.0,
                            3.35,
                            76.4,
                            3619,
                            5081
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-base</a>",
                            "GPT-2",
                            0.0,
                            "35.18*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.122,
                            55.4,
                            3561,
                            719424.0,
                            4.72,
                            54.2,
                            3900,
                            5360
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-base</a>",
                            "GPT-2",
                            0.0,
                            "35.18 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.173,
                            47.1,
                            21105,
                            442477.0,
                            5.58,
                            45.9,
                            21214,
                            22666
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0154,
                            62.0,
                            3147,
                            877192.0,
                            4.13,
                            62.0,
                            3202,
                            4661
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0158,
                            61.7,
                            3147,
                            806451.0,
                            4.15,
                            61.7,
                            3202,
                            4661
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0184,
                            53.2,
                            3147,
                            740740.0,
                            4.81,
                            53.2,
                            3202,
                            4661
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0189,
                            53.7,
                            3147,
                            757575.0,
                            4.77,
                            53.7,
                            3202,
                            4661
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.019,
                            55.4,
                            1931,
                            862068.0,
                            4.62,
                            55.4,
                            1983,
                            3445
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0198,
                            54.8,
                            1393,
                            826446.0,
                            4.67,
                            54.8,
                            1442,
                            2904
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0218,
                            52.6,
                            1358,
                            806451.0,
                            4.87,
                            52.6,
                            1423,
                            2883
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0228,
                            48.8,
                            1358,
                            729927.0,
                            5.25,
                            48.8,
                            1423,
                            2883
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0441,
                            43.9,
                            1360,
                            671140.0,
                            5.85,
                            43.8,
                            1430,
                            2889
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0476,
                            20.4,
                            1942,
                            323624.0,
                            12.5,
                            20.5,
                            2004,
                            3472
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0559,
                            19.6,
                            1941,
                            297619.0,
                            13.1,
                            19.5,
                            2000,
                            3468
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.059,
                            36.8,
                            1428,
                            531914.0,
                            6.99,
                            36.6,
                            1478,
                            2938
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0604,
                            54.0,
                            6186,
                            653594.0,
                            4.78,
                            53.6,
                            6232,
                            7683
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0607,
                            32.9,
                            1428,
                            495049.0,
                            7.8,
                            32.8,
                            1478,
                            2938
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bn22/tinyllama_frankenmerge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bn22/tinyllama_frankenmerge</a>",
                            "\ud83e\udd99 LLaMA",
                            1.54,
                            "34.64 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.026,
                            37.0,
                            3168,
                            540540.0,
                            6.92,
                            37.0,
                            3342,
                            4802
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bn22/tinyllama_frankenmerge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bn22/tinyllama_frankenmerge</a>",
                            "\ud83e\udd99 LLaMA",
                            1.54,
                            "34.64 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0266,
                            37.3,
                            3168,
                            546448.0,
                            6.86,
                            37.3,
                            3342,
                            4802
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bn22/tinyllama_frankenmerge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bn22/tinyllama_frankenmerge</a>",
                            "\ud83e\udd99 LLaMA",
                            1.54,
                            "34.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0284,
                            33.2,
                            1091,
                            510204.0,
                            7.71,
                            33.2,
                            1195,
                            2654
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bn22/tinyllama_frankenmerge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bn22/tinyllama_frankenmerge</a>",
                            "\ud83e\udd99 LLaMA",
                            1.54,
                            "34.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0286,
                            36.2,
                            1092,
                            574712.0,
                            7.07,
                            36.2,
                            1195,
                            2654
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bn22/tinyllama_frankenmerge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bn22/tinyllama_frankenmerge</a>",
                            "\ud83e\udd99 LLaMA",
                            1.54,
                            "34.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0296,
                            34.4,
                            1490,
                            552486.0,
                            7.44,
                            34.4,
                            1568,
                            3030
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bn22/tinyllama_frankenmerge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bn22/tinyllama_frankenmerge</a>",
                            "\ud83e\udd99 LLaMA",
                            1.54,
                            "34.64 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0595,
                            42.4,
                            6279,
                            581395.0,
                            6.07,
                            42.2,
                            6341,
                            7793
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0154,
                            63.8,
                            3147,
                            884955.0,
                            4.02,
                            63.7,
                            3202,
                            4661
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.016,
                            61.6,
                            3147,
                            806451.0,
                            4.16,
                            61.5,
                            3202,
                            4661
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0191,
                            54.8,
                            1931,
                            862068.0,
                            4.67,
                            54.8,
                            1983,
                            3445
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0194,
                            53.5,
                            3147,
                            751879.0,
                            4.79,
                            53.4,
                            3202,
                            4661
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0197,
                            55.6,
                            1393,
                            840336.0,
                            4.61,
                            55.5,
                            1442,
                            2904
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0198,
                            54.1,
                            3147,
                            751879.0,
                            4.73,
                            54.1,
                            3202,
                            4661
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0222,
                            53.9,
                            1358,
                            806451.0,
                            4.75,
                            53.9,
                            1423,
                            2883
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0235,
                            49.5,
                            1358,
                            751879.0,
                            5.17,
                            49.5,
                            1423,
                            2883
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0448,
                            44.8,
                            1360,
                            689655.0,
                            5.73,
                            44.7,
                            1430,
                            2889
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0472,
                            20.7,
                            1942,
                            319488.0,
                            12.3,
                            20.8,
                            2004,
                            3472
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0514,
                            19.9,
                            1941,
                            298507.0,
                            12.9,
                            19.8,
                            2000,
                            3468
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0543,
                            52.5,
                            6186,
                            689655.0,
                            4.91,
                            52.1,
                            6232,
                            7683
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.0632,
                            36.4,
                            1428,
                            529100.0,
                            7.06,
                            36.3,
                            1478,
                            2938
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0635,
                            33.5,
                            1428,
                            497512.0,
                            7.67,
                            33.4,
                            1478,
                            2938
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-GPTrio\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-GPTrio</a>",
                            "GPT-NeoX",
                            0.0,
                            "34.42 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0295,
                            46.9,
                            16060,
                            473933.0,
                            5.47,
                            46.8,
                            16089,
                            17548
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-GPTrio\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-GPTrio</a>",
                            "GPT-NeoX",
                            0.0,
                            "34.42 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0309,
                            45.3,
                            16060,
                            478468.0,
                            5.66,
                            45.2,
                            16089,
                            17548
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-GPTrio\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-GPTrio</a>",
                            "GPT-NeoX",
                            0.0,
                            "34.42 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0317,
                            40.9,
                            16060,
                            462962.0,
                            6.27,
                            40.8,
                            16089,
                            17548
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-GPTrio\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-GPTrio</a>",
                            "GPT-NeoX",
                            0.0,
                            "34.42 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0333,
                            39.6,
                            16060,
                            438596.0,
                            6.47,
                            39.6,
                            16089,
                            17548
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-GPTrio\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-GPTrio</a>",
                            "GPT-NeoX",
                            0.0,
                            "34.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0406,
                            41.1,
                            6659,
                            534759.0,
                            6.25,
                            41.0,
                            6692,
                            8153
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-GPTrio\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-GPTrio</a>",
                            "GPT-NeoX",
                            0.0,
                            "34.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0418,
                            40.7,
                            7737,
                            568181.0,
                            6.31,
                            40.6,
                            7772,
                            9233
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-GPTrio\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-GPTrio</a>",
                            "GPT-NeoX",
                            0.0,
                            "34.42*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0619,
                            16.0,
                            9622,
                            232018.0,
                            16.0,
                            16.0,
                            9678,
                            11146
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-GPTrio\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-GPTrio</a>",
                            "GPT-NeoX",
                            0.0,
                            "34.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0636,
                            39.2,
                            6523,
                            515463.0,
                            6.56,
                            39.0,
                            6572,
                            8032
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-GPTrio\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-GPTrio</a>",
                            "GPT-NeoX",
                            0.0,
                            "34.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0674,
                            14.9,
                            9622,
                            220264.0,
                            17.2,
                            14.9,
                            9669,
                            11137
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-GPTrio\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-GPTrio</a>",
                            "GPT-NeoX",
                            0.0,
                            "34.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0758,
                            37.0,
                            6523,
                            492610.0,
                            6.98,
                            36.7,
                            6572,
                            8032
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-GPTrio\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-GPTrio</a>",
                            "GPT-NeoX",
                            0.0,
                            "34.42 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.26,
                            37.0,
                            31976,
                            327868.0,
                            7.16,
                            35.8,
                            32113,
                            33564
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-GPTrio\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-GPTrio</a>",
                            "GPT-NeoX",
                            0.0,
                            "34.42*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.282,
                            25.7,
                            7001,
                            322580.0,
                            10.2,
                            25.1,
                            7241,
                            8701
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-GPTrio\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-GPTrio</a>",
                            "GPT-NeoX",
                            0.0,
                            "34.42*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.288,
                            27.5,
                            7001,
                            334448.0,
                            9.55,
                            26.8,
                            7241,
                            8701
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2-xl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2-xl</a>",
                            "GPT-2",
                            1.61,
                            "34.38 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0271,
                            35.6,
                            3561,
                            497512.0,
                            7.2,
                            35.6,
                            3823,
                            5282
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2-xl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2-xl</a>",
                            "GPT-2",
                            1.61,
                            "34.38 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0316,
                            31.8,
                            3561,
                            448430.0,
                            8.06,
                            31.8,
                            3825,
                            5284
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2-xl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2-xl</a>",
                            "GPT-2",
                            1.61,
                            "34.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0319,
                            31.2,
                            3561,
                            476190.0,
                            8.19,
                            31.3,
                            3827,
                            5286
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2-xl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2-xl</a>",
                            "GPT-2",
                            1.61,
                            "34.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.032,
                            31.2,
                            3561,
                            485436.0,
                            8.19,
                            31.3,
                            3827,
                            5286
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2-xl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2-xl</a>",
                            "GPT-2",
                            1.61,
                            "34.38 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0342,
                            30.9,
                            3561,
                            460829.0,
                            8.27,
                            31.0,
                            3825,
                            5284
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2-xl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2-xl</a>",
                            "GPT-2",
                            1.61,
                            "34.38 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0647,
                            33.3,
                            7061,
                            425531.0,
                            7.72,
                            33.2,
                            7121,
                            8573
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-480k-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.03,
                            "34.37 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0172,
                            57.8,
                            2279,
                            840336.0,
                            4.43,
                            57.8,
                            2403,
                            3862
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-480k-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.03,
                            "34.37 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0186,
                            51.8,
                            2279,
                            735294.0,
                            4.94,
                            51.8,
                            2403,
                            3862
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-480k-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.03,
                            "34.37 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0187,
                            53.2,
                            2279,
                            787401.0,
                            4.81,
                            53.2,
                            2403,
                            3862
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-480k-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.03,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0199,
                            52.6,
                            852,
                            775193.0,
                            4.87,
                            52.6,
                            935,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-480k-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.03,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.02,
                            51.4,
                            1248,
                            813008.0,
                            4.98,
                            51.4,
                            1304,
                            2766
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-480k-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.03,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0206,
                            45.9,
                            851,
                            709219.0,
                            5.58,
                            45.9,
                            935,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-480k-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.03,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0209,
                            50.4,
                            877,
                            763358.0,
                            5.08,
                            50.4,
                            933,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b</a>",
                            "GPT-NeoX",
                            7.0,
                            "34.37 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0267,
                            68.0,
                            16428,
                            632911.0,
                            3.78,
                            67.7,
                            16481,
                            17941
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b</a>",
                            "GPT-NeoX",
                            7.0,
                            "34.37 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0275,
                            78.7,
                            16428,
                            709219.0,
                            3.27,
                            78.3,
                            16481,
                            17941
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b</a>",
                            "GPT-NeoX",
                            7.0,
                            "34.37 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0277,
                            71.2,
                            16428,
                            645161.0,
                            3.61,
                            70.9,
                            16485,
                            17945
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b</a>",
                            "GPT-NeoX",
                            7.0,
                            "34.37 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0288,
                            66.4,
                            16428,
                            602409.0,
                            3.87,
                            66.1,
                            16481,
                            17941
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b</a>",
                            "GPT-NeoX",
                            7.0,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0322,
                            31.2,
                            9183,
                            431034.0,
                            8.21,
                            31.2,
                            9231,
                            10699
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b</a>",
                            "GPT-NeoX",
                            7.0,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0339,
                            29.0,
                            9183,
                            404858.0,
                            8.83,
                            29.0,
                            9225,
                            10693
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b</a>",
                            "GPT-NeoX",
                            7.0,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0355,
                            79.9,
                            6002,
                            877192.0,
                            3.23,
                            79.3,
                            6064,
                            7526
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b</a>",
                            "GPT-NeoX",
                            7.0,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0357,
                            79.9,
                            7615,
                            1016260.0,
                            3.23,
                            79.3,
                            7679,
                            9141
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-480k-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.03,
                            "34.37 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0442,
                            58.1,
                            4492,
                            769230.0,
                            4.43,
                            57.8,
                            4548,
                            5999
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-480k-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.03,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0445,
                            38.8,
                            869,
                            602409.0,
                            6.61,
                            38.7,
                            922,
                            2382
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-480k-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.03,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.0483,
                            32.4,
                            924,
                            476190.0,
                            7.92,
                            32.3,
                            1008,
                            2468
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-480k-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.03,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0485,
                            31.1,
                            924,
                            471698.0,
                            8.24,
                            31.1,
                            1008,
                            2468
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b</a>",
                            "GPT-NeoX",
                            7.0,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0591,
                            75.2,
                            5699,
                            847457.0,
                            3.45,
                            74.2,
                            5790,
                            7249
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-480k-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.03,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0662,
                            14.9,
                            1380,
                            230946.0,
                            17.2,
                            14.9,
                            1421,
                            2889
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-480k-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.03,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0675,
                            14.5,
                            1380,
                            219780.0,
                            17.7,
                            14.5,
                            1421,
                            2889
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b</a>",
                            "GPT-NeoX",
                            7.0,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0741,
                            73.3,
                            5699,
                            840336.0,
                            3.55,
                            72.1,
                            5790,
                            7249
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b</a>",
                            "GPT-NeoX",
                            7.0,
                            "34.37 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.269,
                            43.1,
                            32575,
                            380228.0,
                            6.19,
                            41.4,
                            32711,
                            34162
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b</a>",
                            "GPT-NeoX",
                            7.0,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.291,
                            41.8,
                            6764,
                            485436.0,
                            6.39,
                            40.1,
                            7163,
                            8623
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-7b</a>",
                            "GPT-NeoX",
                            7.0,
                            "34.37*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.299,
                            43.1,
                            6764,
                            480769.0,
                            6.21,
                            41.2,
                            7163,
                            8623
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-1.3b</a>",
                            "GPT-2",
                            1.44,
                            "34.31 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0113,
                            78.2,
                            2999,
                            1060445.0,
                            3.27,
                            78.3,
                            3045,
                            4502
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-1.3b</a>",
                            "GPT-2",
                            1.44,
                            "34.31 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0139,
                            70.4,
                            2999,
                            934579.0,
                            3.63,
                            70.5,
                            3045,
                            4504
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-1.3b</a>",
                            "GPT-2",
                            1.44,
                            "34.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.014,
                            72.6,
                            2999,
                            1043841.0,
                            3.52,
                            72.7,
                            3045,
                            4504
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-1.3b</a>",
                            "GPT-2",
                            1.44,
                            "34.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0143,
                            71.2,
                            2999,
                            1033057.0,
                            3.59,
                            71.3,
                            3045,
                            4504
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-1.3b</a>",
                            "GPT-2",
                            1.44,
                            "34.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0144,
                            75.4,
                            1245,
                            1063829.0,
                            3.39,
                            75.5,
                            1287,
                            2749
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-1.3b</a>",
                            "GPT-2",
                            1.44,
                            "34.31 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0147,
                            70.4,
                            2999,
                            961538.0,
                            3.63,
                            70.5,
                            3045,
                            4504
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-4.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-4.5B</a>",
                            "XGLM",
                            5.08,
                            "34.31 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0292,
                            36.6,
                            9526,
                            460829.0,
                            6.99,
                            36.6,
                            9791,
                            11251
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-4.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-4.5B</a>",
                            "XGLM",
                            5.08,
                            "34.31 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.033,
                            35.3,
                            9526,
                            427350.0,
                            7.26,
                            35.3,
                            9791,
                            11251
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-4.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-4.5B</a>",
                            "XGLM",
                            5.08,
                            "34.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0361,
                            36.6,
                            3636,
                            510204.0,
                            7.0,
                            36.6,
                            3768,
                            5230
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-4.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-4.5B</a>",
                            "XGLM",
                            5.08,
                            "34.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0382,
                            36.7,
                            4715,
                            537634.0,
                            6.98,
                            36.7,
                            4850,
                            6312
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-1.3b</a>",
                            "GPT-2",
                            1.44,
                            "34.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0413,
                            57.8,
                            1212,
                            854700.0,
                            4.45,
                            57.5,
                            1262,
                            2722
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-1.3b</a>",
                            "GPT-2",
                            1.44,
                            "34.31 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0535,
                            72.6,
                            5888,
                            854700.0,
                            3.56,
                            71.9,
                            5928,
                            7379
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-4.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-4.5B</a>",
                            "XGLM",
                            5.08,
                            "34.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0567,
                            34.4,
                            3564,
                            492610.0,
                            7.48,
                            34.2,
                            3722,
                            5182
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-4.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-4.5B</a>",
                            "XGLM",
                            5.08,
                            "34.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0594,
                            30.0,
                            3564,
                            432900.0,
                            8.55,
                            29.9,
                            3722,
                            5182
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-4.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-4.5B</a>",
                            "XGLM",
                            5.08,
                            "34.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.121,
                            25.6,
                            3570,
                            357142.0,
                            10.1,
                            25.3,
                            3714,
                            5173
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-4.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-4.5B</a>",
                            "XGLM",
                            5.08,
                            "34.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.134,
                            8.15,
                            5503,
                            121359.0,
                            31.4,
                            8.15,
                            5651,
                            7119
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-4.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-4.5B</a>",
                            "XGLM",
                            5.08,
                            "34.31 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.167,
                            37.4,
                            19042,
                            396825.0,
                            6.99,
                            36.6,
                            19081,
                            20533
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-4.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-4.5B</a>",
                            "XGLM",
                            5.08,
                            "34.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.19,
                            17.2,
                            3737,
                            240384.0,
                            15.0,
                            17.1,
                            3896,
                            5356
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-240k-503b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "33.72 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0169,
                            57.7,
                            2279,
                            847457.0,
                            4.44,
                            57.7,
                            2403,
                            3862
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-240k-503b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "33.72 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0178,
                            53.8,
                            2279,
                            781250.0,
                            4.76,
                            53.8,
                            2403,
                            3862
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-240k-503b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "33.72 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0182,
                            52.0,
                            2279,
                            757575.0,
                            4.92,
                            52.0,
                            2403,
                            3862
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-240k-503b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "33.72*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0204,
                            50.6,
                            1248,
                            799999.0,
                            5.06,
                            50.6,
                            1304,
                            2766
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-240k-503b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "33.72*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0205,
                            47.5,
                            851,
                            694444.0,
                            5.39,
                            47.5,
                            935,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-240k-503b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "33.72*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0208,
                            51.4,
                            852,
                            769230.0,
                            4.98,
                            51.4,
                            935,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-240k-503b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "33.72*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.021,
                            48.1,
                            877,
                            781250.0,
                            5.32,
                            48.1,
                            933,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-240k-503b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "33.72 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0432,
                            57.6,
                            4492,
                            769230.0,
                            4.47,
                            57.3,
                            4548,
                            5999
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-240k-503b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "33.72*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0448,
                            38.3,
                            869,
                            584795.0,
                            6.69,
                            38.3,
                            922,
                            2382
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-240k-503b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "33.72*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0486,
                            31.6,
                            924,
                            476190.0,
                            8.11,
                            31.6,
                            1008,
                            2468
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-240k-503b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "33.72*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.0487,
                            32.6,
                            924,
                            476190.0,
                            7.88,
                            32.5,
                            1008,
                            2468
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-240k-503b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "33.72*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.066,
                            14.8,
                            1380,
                            236406.0,
                            17.3,
                            14.8,
                            1421,
                            2889
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-240k-503b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "33.72*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0688,
                            14.5,
                            1380,
                            218340.0,
                            17.7,
                            14.5,
                            1421,
                            2889
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>",
                            "GPT-Neo",
                            1.37,
                            "33.58 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0129,
                            75.4,
                            2943,
                            1013171.0,
                            3.39,
                            75.5,
                            3017,
                            4475
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>",
                            "GPT-Neo",
                            1.37,
                            "33.58 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0158,
                            63.9,
                            2943,
                            862068.0,
                            4.01,
                            63.8,
                            3017,
                            4477
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>",
                            "GPT-Neo",
                            1.37,
                            "33.58 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0172,
                            59.7,
                            2946,
                            819672.0,
                            4.29,
                            59.7,
                            3038,
                            4498
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>",
                            "GPT-Neo",
                            1.37,
                            "33.58*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0182,
                            60.9,
                            1732,
                            925925.0,
                            4.21,
                            60.8,
                            1818,
                            3279
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>",
                            "GPT-Neo",
                            1.37,
                            "33.58*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0187,
                            60.9,
                            1193,
                            884955.0,
                            4.21,
                            60.8,
                            1277,
                            2738
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>",
                            "GPT-Neo",
                            1.37,
                            "33.58 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.019,
                            59.7,
                            2946,
                            793650.0,
                            4.29,
                            59.7,
                            3038,
                            4498
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>",
                            "GPT-Neo",
                            1.37,
                            "33.58*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0227,
                            60.6,
                            1157,
                            909090.0,
                            4.23,
                            60.5,
                            1247,
                            2707
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>",
                            "GPT-Neo",
                            1.37,
                            "33.58*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0239,
                            52.8,
                            1157,
                            775193.0,
                            4.85,
                            52.8,
                            1247,
                            2707
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>",
                            "GPT-Neo",
                            1.37,
                            "33.58*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0467,
                            44.0,
                            1160,
                            662251.0,
                            5.84,
                            43.8,
                            1249,
                            2709
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>",
                            "GPT-Neo",
                            1.37,
                            "33.58 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0567,
                            62.3,
                            5775,
                            781250.0,
                            4.15,
                            61.7,
                            5817,
                            7268
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>",
                            "GPT-Neo",
                            1.37,
                            "33.58*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.0606,
                            32.7,
                            1206,
                            485436.0,
                            7.85,
                            32.6,
                            1300,
                            2759
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>",
                            "GPT-Neo",
                            1.37,
                            "33.58*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0637,
                            16.1,
                            1736,
                            241545.0,
                            15.9,
                            16.1,
                            1814,
                            3281
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>",
                            "GPT-Neo",
                            1.37,
                            "33.58*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.065,
                            32.4,
                            1210,
                            465116.0,
                            7.95,
                            32.2,
                            1300,
                            2759
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>",
                            "GPT-Neo",
                            1.37,
                            "33.58*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0676,
                            15.6,
                            1740,
                            234192.0,
                            16.4,
                            15.6,
                            1839,
                            3307
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/polyglot-ko-12.8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/polyglot-ko-12.8b</a>",
                            "GPT-NeoX",
                            13.06,
                            "33.33 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0507,
                            32.6,
                            26825,
                            324675.0,
                            7.88,
                            32.5,
                            26866,
                            28326
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/polyglot-ko-12.8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/polyglot-ko-12.8b</a>",
                            "GPT-NeoX",
                            13.06,
                            "33.33 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0515,
                            38.9,
                            26825,
                            347222.0,
                            6.6,
                            38.8,
                            26856,
                            28315
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/polyglot-ko-12.8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/polyglot-ko-12.8b</a>",
                            "GPT-NeoX",
                            13.06,
                            "33.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0684,
                            33.6,
                            8454,
                            398406.0,
                            7.67,
                            33.4,
                            8480,
                            9942
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/polyglot-ko-12.8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/polyglot-ko-12.8b</a>",
                            "GPT-NeoX",
                            13.06,
                            "33.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0686,
                            34.0,
                            9802,
                            462962.0,
                            7.57,
                            33.8,
                            9835,
                            11297
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/polyglot-ko-12.8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/polyglot-ko-12.8b</a>",
                            "GPT-NeoX",
                            13.06,
                            "33.33*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0766,
                            12.6,
                            14315,
                            179856.0,
                            20.4,
                            12.5,
                            14352,
                            15820
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/polyglot-ko-12.8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/polyglot-ko-12.8b</a>",
                            "GPT-NeoX",
                            13.06,
                            "33.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0871,
                            11.9,
                            14313,
                            164473.0,
                            21.6,
                            11.9,
                            14344,
                            15812
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/polyglot-ko-12.8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/polyglot-ko-12.8b</a>",
                            "GPT-NeoX",
                            13.06,
                            "33.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.109,
                            31.7,
                            8240,
                            389105.0,
                            8.16,
                            31.4,
                            8290,
                            9749
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/polyglot-ko-12.8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/polyglot-ko-12.8b</a>",
                            "GPT-NeoX",
                            13.06,
                            "33.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.133,
                            29.7,
                            8240,
                            367647.0,
                            8.72,
                            29.4,
                            8311,
                            9770
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/polyglot-ko-12.8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/polyglot-ko-12.8b</a>",
                            "GPT-NeoX",
                            13.06,
                            "33.33 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.445,
                            23.0,
                            53472,
                            204498.0,
                            11.5,
                            22.3,
                            53517,
                            54968
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/polyglot-ko-12.8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/polyglot-ko-12.8b</a>",
                            "GPT-NeoX",
                            13.06,
                            "33.33*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.496,
                            20.7,
                            9009,
                            251256.0,
                            12.8,
                            20.0,
                            9502,
                            10961
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/polyglot-ko-12.8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/polyglot-ko-12.8b</a>",
                            "GPT-NeoX",
                            13.06,
                            "33.33*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.505,
                            21.4,
                            9009,
                            239808.0,
                            12.4,
                            20.6,
                            9502,
                            10961
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-2.7B</a>",
                            "GPT-2",
                            2.7,
                            "33.25 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0163,
                            62.3,
                            5799,
                            729927.0,
                            4.11,
                            62.3,
                            5869,
                            7329
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-2.7B</a>",
                            "GPT-2",
                            2.7,
                            "33.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0189,
                            55.7,
                            5799,
                            775193.0,
                            4.6,
                            55.7,
                            5867,
                            7327
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-2.7B</a>",
                            "GPT-2",
                            2.7,
                            "33.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.019,
                            55.0,
                            5799,
                            787401.0,
                            4.66,
                            54.9,
                            5867,
                            7327
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-2.7B</a>",
                            "GPT-2",
                            2.7,
                            "33.25 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.019,
                            54.4,
                            5799,
                            684931.0,
                            4.71,
                            54.4,
                            5869,
                            7329
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-2.7B</a>",
                            "GPT-2",
                            2.7,
                            "33.25 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0196,
                            53.9,
                            5799,
                            709219.0,
                            4.75,
                            53.9,
                            5869,
                            7329
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-2.7B</a>",
                            "GPT-2",
                            2.7,
                            "33.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0224,
                            57.4,
                            2119,
                            813008.0,
                            4.46,
                            57.4,
                            2239,
                            3701
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-1b5-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-1b5-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            1.0,
                            "33.25 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.025,
                            39.1,
                            3066,
                            549450.0,
                            6.55,
                            39.1,
                            3344,
                            4804
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-1b5-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-1b5-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            1.0,
                            "33.25 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0253,
                            39.0,
                            3066,
                            558659.0,
                            6.57,
                            39.0,
                            3347,
                            4806
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-1b5-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-1b5-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            1.0,
                            "33.25 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0562,
                            40.6,
                            6124,
                            529100.0,
                            6.34,
                            40.4,
                            6180,
                            7631
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-1b5-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-1b5-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            1.0,
                            "33.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.064,
                            23.2,
                            1255,
                            346020.0,
                            11.1,
                            23.1,
                            1304,
                            2763
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-2.7B</a>",
                            "GPT-2",
                            2.7,
                            "33.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.076,
                            43.1,
                            2067,
                            609756.0,
                            6.0,
                            42.7,
                            2216,
                            3676
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-1b5-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-1b5-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            1.0,
                            "33.25*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0832,
                            12.1,
                            1759,
                            183150.0,
                            21.1,
                            12.1,
                            1809,
                            3277
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-2.7B</a>",
                            "GPT-2",
                            2.7,
                            "33.25 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0938,
                            56.8,
                            11488,
                            617283.0,
                            4.58,
                            55.9,
                            11525,
                            12977
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-13B</a>",
                            "\ud83c\udf38 Bloom",
                            13.0,
                            "32.95 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0495,
                            40.7,
                            27362,
                            373134.0,
                            6.31,
                            40.6,
                            27401,
                            28860
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-13B</a>",
                            "\ud83c\udf38 Bloom",
                            13.0,
                            "32.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0663,
                            41.7,
                            10325,
                            531914.0,
                            6.18,
                            41.4,
                            10366,
                            11827
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-13B</a>",
                            "\ud83c\udf38 Bloom",
                            13.0,
                            "32.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.067,
                            41.5,
                            8977,
                            458715.0,
                            6.22,
                            41.2,
                            9017,
                            10479
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-13B</a>",
                            "\ud83c\udf38 Bloom",
                            13.0,
                            "32.95*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0739,
                            13.0,
                            14853,
                            182149.0,
                            19.7,
                            13.0,
                            14879,
                            16347
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-13B</a>",
                            "\ud83c\udf38 Bloom",
                            13.0,
                            "32.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0785,
                            12.8,
                            14857,
                            175131.0,
                            20.0,
                            12.8,
                            14893,
                            16361
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-13B</a>",
                            "\ud83c\udf38 Bloom",
                            13.0,
                            "32.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.106,
                            40.0,
                            8762,
                            456621.0,
                            6.49,
                            39.4,
                            8808,
                            10267
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-13B</a>",
                            "\ud83c\udf38 Bloom",
                            13.0,
                            "32.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.133,
                            37.3,
                            8762,
                            429184.0,
                            6.97,
                            36.7,
                            8808,
                            10267
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-13B</a>",
                            "\ud83c\udf38 Bloom",
                            13.0,
                            "32.95 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.461,
                            25.9,
                            54714,
                            229357.0,
                            10.3,
                            24.9,
                            54748,
                            56199
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-13B</a>",
                            "\ud83c\udf38 Bloom",
                            13.0,
                            "32.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.491,
                            24.1,
                            9440,
                            271739.0,
                            11.1,
                            23.1,
                            9619,
                            11079
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-13B</a>",
                            "\ud83c\udf38 Bloom",
                            13.0,
                            "32.95*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.51,
                            24.3,
                            9440,
                            259740.0,
                            11.0,
                            23.3,
                            9619,
                            11079
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0107,
                            93.1,
                            2243,
                            1336898.0,
                            2.75,
                            93.1,
                            2292,
                            3751
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0122,
                            88.5,
                            2243,
                            1169590.0,
                            2.89,
                            88.6,
                            2290,
                            3749
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0125,
                            78.2,
                            2243,
                            1096491.0,
                            3.27,
                            78.3,
                            2292,
                            3751
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0131,
                            79.7,
                            1623,
                            1261034.0,
                            3.21,
                            79.8,
                            1675,
                            3137
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0131,
                            75.4,
                            2243,
                            1066098.0,
                            3.39,
                            75.5,
                            2292,
                            3751
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0132,
                            81.2,
                            1085,
                            1218026.0,
                            3.15,
                            81.3,
                            1134,
                            2596
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0154,
                            76.1,
                            1050,
                            1091703.0,
                            3.37,
                            76.0,
                            1094,
                            2554
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0154,
                            72.4,
                            1050,
                            1068376.0,
                            3.54,
                            72.3,
                            1094,
                            2554
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0291,
                            64.2,
                            1076,
                            980392.0,
                            4.0,
                            64.0,
                            1117,
                            2577
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0323,
                            30.3,
                            1448,
                            467289.0,
                            8.45,
                            30.3,
                            1495,
                            2963
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0335,
                            29.1,
                            1450,
                            436681.0,
                            8.8,
                            29.1,
                            1499,
                            2967
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.038,
                            79.2,
                            4411,
                            990099.0,
                            3.26,
                            78.5,
                            4445,
                            5897
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.0397,
                            54.3,
                            1135,
                            793650.0,
                            4.74,
                            54.0,
                            1216,
                            2675
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0408,
                            50.1,
                            1135,
                            729927.0,
                            5.13,
                            49.9,
                            1216,
                            2675
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Kunhao/pile-7b-250b-tokens\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kunhao/pile-7b-250b-tokens</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "32.44 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0282,
                            38.6,
                            12365,
                            456621.0,
                            6.64,
                            38.6,
                            12603,
                            14063
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Kunhao/pile-7b-250b-tokens\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kunhao/pile-7b-250b-tokens</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "32.44 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0288,
                            40.2,
                            12357,
                            465116.0,
                            6.38,
                            40.1,
                            12603,
                            14063
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Kunhao/pile-7b-250b-tokens\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kunhao/pile-7b-250b-tokens</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "32.44 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0316,
                            38.7,
                            12365,
                            432900.0,
                            6.62,
                            38.7,
                            12603,
                            14063
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Kunhao/pile-7b-250b-tokens\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kunhao/pile-7b-250b-tokens</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "32.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0374,
                            37.2,
                            4117,
                            492610.0,
                            6.9,
                            37.1,
                            4221,
                            5683
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Kunhao/pile-7b-250b-tokens\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kunhao/pile-7b-250b-tokens</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "32.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0386,
                            36.7,
                            4843,
                            540540.0,
                            6.99,
                            36.6,
                            4949,
                            6410
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Kunhao/pile-7b-250b-tokens\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kunhao/pile-7b-250b-tokens</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "32.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0554,
                            37.2,
                            4021,
                            512820.0,
                            6.92,
                            37.0,
                            4158,
                            5618
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Kunhao/pile-7b-250b-tokens\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kunhao/pile-7b-250b-tokens</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "32.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0641,
                            34.0,
                            4020,
                            462962.0,
                            7.56,
                            33.9,
                            4158,
                            5618
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Kunhao/pile-7b-250b-tokens\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kunhao/pile-7b-250b-tokens</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "32.44*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0958,
                            10.5,
                            6791,
                            157480.0,
                            24.5,
                            10.4,
                            6937,
                            8405
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Kunhao/pile-7b-250b-tokens\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kunhao/pile-7b-250b-tokens</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "32.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.102,
                            9.85,
                            6800,
                            144927.0,
                            26.0,
                            9.85,
                            6928,
                            8396
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Kunhao/pile-7b-250b-tokens\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kunhao/pile-7b-250b-tokens</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "32.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.15,
                            26.8,
                            4158,
                            364963.0,
                            9.68,
                            26.4,
                            4303,
                            5762
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Kunhao/pile-7b-250b-tokens\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kunhao/pile-7b-250b-tokens</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "32.44 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.216,
                            41.7,
                            24722,
                            373134.0,
                            6.33,
                            40.4,
                            24807,
                            26258
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Kunhao/pile-7b-250b-tokens\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kunhao/pile-7b-250b-tokens</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "32.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.24,
                            21.4,
                            4451,
                            288184.0,
                            12.1,
                            21.2,
                            4722,
                            6182
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Kunhao/pile-7b-250b-tokens\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kunhao/pile-7b-250b-tokens</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "32.44*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.246,
                            22.4,
                            4451,
                            293255.0,
                            11.6,
                            22.1,
                            4722,
                            6182
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-multi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-multi</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "32.43 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.034,
                            28.9,
                            14859,
                            346020.0,
                            8.84,
                            29.0,
                            14923,
                            16382
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-multi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-multi</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "32.43 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0383,
                            26.4,
                            14855,
                            338983.0,
                            9.71,
                            26.4,
                            14902,
                            16361
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-multi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-multi</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "32.43 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.04,
                            26.7,
                            14855,
                            326797.0,
                            9.6,
                            26.7,
                            14902,
                            16361
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-multi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-multi</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "32.43*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0455,
                            26.4,
                            5158,
                            374531.0,
                            9.7,
                            26.4,
                            5200,
                            6662
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-multi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-multi</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "32.43*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0474,
                            27.1,
                            6235,
                            408163.0,
                            9.45,
                            27.1,
                            6280,
                            7742
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-multi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-multi</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "32.43*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0682,
                            26.8,
                            5020,
                            386100.0,
                            9.58,
                            26.7,
                            5075,
                            6534
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-multi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-multi</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "32.43*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0758,
                            12.9,
                            8220,
                            194931.0,
                            19.8,
                            12.9,
                            8269,
                            9737
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-multi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-multi</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "32.43*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0771,
                            25.5,
                            5020,
                            369003.0,
                            10.1,
                            25.3,
                            5075,
                            6534
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-multi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-multi</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "32.43*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0808,
                            12.5,
                            8216,
                            184842.0,
                            20.5,
                            12.5,
                            8248,
                            9716
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-multi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-multi</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "32.43*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.173,
                            23.4,
                            5023,
                            323624.0,
                            11.1,
                            23.1,
                            5272,
                            6731
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-multi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-multi</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "32.43 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.256,
                            29.0,
                            29527,
                            283286.0,
                            9.06,
                            28.3,
                            29750,
                            31201
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-multi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-multi</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "32.43*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.285,
                            21.1,
                            5351,
                            271002.0,
                            12.4,
                            20.6,
                            5723,
                            7182
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-multi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-multi</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "32.43*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.29,
                            19.6,
                            5351,
                            255102.0,
                            13.3,
                            19.2,
                            5454,
                            6914
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b-8k</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.23 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0233,
                            43.3,
                            8277,
                            546448.0,
                            5.91,
                            43.3,
                            8411,
                            9871
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b-8k</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.23 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0261,
                            41.7,
                            8277,
                            507614.0,
                            6.14,
                            41.7,
                            8411,
                            9871
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b-8k</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.23 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0274,
                            37.3,
                            8277,
                            497512.0,
                            6.86,
                            37.3,
                            8411,
                            9871
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b-8k</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.23 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.03,
                            36.2,
                            8277,
                            462962.0,
                            7.07,
                            36.2,
                            8411,
                            9871
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b-8k</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0301,
                            37.8,
                            4056,
                            571428.0,
                            6.78,
                            37.8,
                            4093,
                            5555
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b-8k</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0306,
                            37.0,
                            3315,
                            529100.0,
                            6.92,
                            37.0,
                            3351,
                            4812
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b-8k</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0447,
                            35.7,
                            3242,
                            515463.0,
                            7.19,
                            35.6,
                            3275,
                            4735
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b-8k</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0482,
                            33.6,
                            3238,
                            497512.0,
                            7.64,
                            33.5,
                            3271,
                            4731
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b-8k</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.23*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0704,
                            13.9,
                            4819,
                            205761.0,
                            18.5,
                            13.8,
                            5098,
                            6566
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b-8k</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0743,
                            13.4,
                            4816,
                            199600.0,
                            19.1,
                            13.4,
                            5077,
                            6545
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b-8k</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.101,
                            30.5,
                            3235,
                            440528.0,
                            8.46,
                            30.3,
                            3340,
                            4800
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b-8k</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.23 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.15,
                            37.4,
                            16325,
                            418410.0,
                            6.96,
                            36.8,
                            16601,
                            18052
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b-8k</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.23*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.165,
                            24.5,
                            3362,
                            340136.0,
                            10.6,
                            24.2,
                            3535,
                            4993
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b-8k</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.171,
                            22.6,
                            3359,
                            314465.0,
                            11.5,
                            22.3,
                            3535,
                            4995
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.14 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.023,
                            43.0,
                            8277,
                            549450.0,
                            5.95,
                            43.0,
                            8411,
                            9871
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.14 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0249,
                            40.6,
                            8277,
                            497512.0,
                            6.3,
                            40.6,
                            8411,
                            9871
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.14 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0278,
                            36.2,
                            8277,
                            487804.0,
                            7.08,
                            36.2,
                            8411,
                            9871
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.14 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0284,
                            35.8,
                            8277,
                            450450.0,
                            7.16,
                            35.8,
                            8411,
                            9871
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.14*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0297,
                            37.2,
                            3315,
                            526315.0,
                            6.88,
                            37.2,
                            3351,
                            4812
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.14*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0303,
                            37.1,
                            4056,
                            564971.0,
                            6.9,
                            37.1,
                            4093,
                            5555
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.14*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.045,
                            35.6,
                            3242,
                            526315.0,
                            7.21,
                            35.5,
                            3275,
                            4735
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.14*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0481,
                            32.7,
                            3238,
                            469483.0,
                            7.85,
                            32.6,
                            3271,
                            4731
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.14*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0731,
                            13.9,
                            4819,
                            205338.0,
                            18.4,
                            13.9,
                            5098,
                            6566
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.14*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0761,
                            13.1,
                            4816,
                            194174.0,
                            19.5,
                            13.1,
                            5077,
                            6545
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.14*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.103,
                            30.1,
                            3235,
                            432900.0,
                            8.56,
                            29.9,
                            3340,
                            4800
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.14 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.15,
                            36.5,
                            16325,
                            413223.0,
                            7.14,
                            35.9,
                            16601,
                            18052
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.14*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.17,
                            22.6,
                            3359,
                            308641.0,
                            11.5,
                            22.3,
                            3535,
                            4995
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.14*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.171,
                            25.0,
                            3362,
                            335570.0,
                            10.4,
                            24.6,
                            3535,
                            4993
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-step-50K-105b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "31.86 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0168,
                            58.0,
                            2279,
                            826446.0,
                            4.42,
                            57.9,
                            2403,
                            3862
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-step-50K-105b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "31.86 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0178,
                            54.8,
                            2279,
                            757575.0,
                            4.67,
                            54.8,
                            2403,
                            3862
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-step-50K-105b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "31.86 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.018,
                            52.1,
                            2279,
                            769230.0,
                            4.91,
                            52.1,
                            2403,
                            3862
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-step-50K-105b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "31.86*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0196,
                            47.8,
                            851,
                            714285.0,
                            5.36,
                            47.8,
                            935,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-step-50K-105b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "31.86*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.02,
                            50.1,
                            1248,
                            813008.0,
                            5.11,
                            50.1,
                            1304,
                            2766
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-step-50K-105b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "31.86*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0201,
                            52.4,
                            852,
                            787401.0,
                            4.89,
                            52.4,
                            935,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-step-50K-105b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "31.86*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0218,
                            48.9,
                            877,
                            775193.0,
                            5.24,
                            48.9,
                            933,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-step-50K-105b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "31.86 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0434,
                            57.3,
                            4492,
                            763358.0,
                            4.49,
                            57.0,
                            4548,
                            5999
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-step-50K-105b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "31.86*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0451,
                            37.9,
                            869,
                            588235.0,
                            6.77,
                            37.8,
                            922,
                            2382
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-step-50K-105b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "31.86*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.0487,
                            32.4,
                            924,
                            485436.0,
                            7.91,
                            32.4,
                            1008,
                            2468
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-step-50K-105b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "31.86*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0496,
                            31.2,
                            924,
                            460829.0,
                            8.23,
                            31.1,
                            1008,
                            2468
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-step-50K-105b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "31.86*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0651,
                            15.1,
                            1380,
                            238095.0,
                            17.0,
                            15.1,
                            1421,
                            2889
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-step-50K-105b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "31.86*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0693,
                            14.5,
                            1380,
                            219780.0,
                            17.7,
                            14.5,
                            1421,
                            2889
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0157,
                            63.9,
                            1028,
                            990099.0,
                            4.01,
                            63.8,
                            1121,
                            2581
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0164,
                            59.2,
                            1028,
                            877192.0,
                            4.33,
                            59.1,
                            1121,
                            2581
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0181,
                            55.3,
                            1028,
                            877192.0,
                            4.63,
                            55.3,
                            1121,
                            2581
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0186,
                            56.0,
                            869,
                            900900.0,
                            4.57,
                            56.0,
                            958,
                            2420
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0193,
                            53.6,
                            1028,
                            793650.0,
                            4.78,
                            53.6,
                            1121,
                            2581
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0195,
                            54.5,
                            601,
                            854700.0,
                            4.7,
                            54.5,
                            679,
                            2141
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0202,
                            52.8,
                            590,
                            799999.0,
                            4.85,
                            52.8,
                            685,
                            2145
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0203,
                            49.0,
                            590,
                            775193.0,
                            5.22,
                            49.0,
                            685,
                            2145
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0231,
                            53.1,
                            1948,
                            813008.0,
                            4.82,
                            53.1,
                            2006,
                            3458
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.0324,
                            35.9,
                            603,
                            555555.0,
                            7.13,
                            35.9,
                            713,
                            2172
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0351,
                            44.7,
                            592,
                            709219.0,
                            5.75,
                            44.5,
                            700,
                            2160
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0356,
                            34.3,
                            603,
                            534759.0,
                            7.47,
                            34.3,
                            713,
                            2172
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0471,
                            20.2,
                            742,
                            318471.0,
                            12.6,
                            20.3,
                            832,
                            2300
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0506,
                            20.1,
                            743,
                            306748.0,
                            12.8,
                            20.0,
                            834,
                            2302
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0143,
                            93.8,
                            7829,
                            961538.0,
                            2.73,
                            93.8,
                            7853,
                            9313
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0154,
                            90.1,
                            7829,
                            917431.0,
                            2.85,
                            89.8,
                            7853,
                            9313
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0154,
                            79.2,
                            7829,
                            900900.0,
                            3.24,
                            79.0,
                            7853,
                            9313
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0172,
                            79.2,
                            7829,
                            854700.0,
                            3.24,
                            79.0,
                            7853,
                            9313
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0201,
                            81.5,
                            3195,
                            1072961.0,
                            3.15,
                            81.3,
                            3212,
                            4674
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0205,
                            79.7,
                            4271,
                            1176470.0,
                            3.22,
                            79.5,
                            4290,
                            5752
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0314,
                            78.0,
                            3060,
                            1048218.0,
                            3.3,
                            77.6,
                            3091,
                            4550
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0319,
                            31.1,
                            4609,
                            442477.0,
                            8.24,
                            31.1,
                            4640,
                            6108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0351,
                            29.2,
                            4609,
                            403225.0,
                            8.76,
                            29.2,
                            4636,
                            6104
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0365,
                            73.5,
                            3060,
                            1012145.0,
                            3.51,
                            72.9,
                            3091,
                            4550
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0825,
                            65.6,
                            3280,
                            869565.0,
                            3.97,
                            64.5,
                            3391,
                            4850
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.127,
                            74.1,
                            15380,
                            657894.0,
                            3.57,
                            71.7,
                            15466,
                            16917
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.138,
                            54.6,
                            3516,
                            662251.0,
                            4.81,
                            53.2,
                            3879,
                            5339
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.138,
                            50.3,
                            3516,
                            621118.0,
                            5.21,
                            49.1,
                            3879,
                            5339
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-1.3B</a>",
                            "GPT-2",
                            1.3,
                            "31.30 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0115,
                            86.4,
                            2943,
                            1129943.0,
                            2.96,
                            86.5,
                            2984,
                            4441
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-1.3B</a>",
                            "GPT-2",
                            1.3,
                            "31.30*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0142,
                            72.0,
                            2943,
                            1039501.0,
                            3.55,
                            72.1,
                            2984,
                            4443
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-1.3B</a>",
                            "GPT-2",
                            1.3,
                            "31.30*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0146,
                            77.0,
                            1188,
                            1098901.0,
                            3.32,
                            77.1,
                            1226,
                            2688
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-1.3B</a>",
                            "GPT-2",
                            1.3,
                            "31.30 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0147,
                            68.2,
                            2943,
                            900900.0,
                            3.75,
                            68.3,
                            2984,
                            4443
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-1.3B</a>",
                            "GPT-2",
                            1.3,
                            "31.30 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0151,
                            70.4,
                            2943,
                            1000000.0,
                            3.64,
                            70.3,
                            2984,
                            4443
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-1.3B</a>",
                            "GPT-2",
                            1.3,
                            "31.30*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0153,
                            67.6,
                            2943,
                            990099.0,
                            3.79,
                            67.5,
                            2984,
                            4443
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-1.3B</a>",
                            "GPT-2",
                            1.3,
                            "31.30*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0425,
                            57.4,
                            1156,
                            869565.0,
                            4.48,
                            57.1,
                            1186,
                            2646
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-1.3B</a>",
                            "GPT-2",
                            1.3,
                            "31.30 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0524,
                            73.1,
                            5776,
                            833333.0,
                            3.54,
                            72.3,
                            5802,
                            7253
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m-deduped</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.29 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0153,
                            65.6,
                            1028,
                            1023541.0,
                            3.91,
                            65.5,
                            1121,
                            2581
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m-deduped</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.29 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0158,
                            59.7,
                            1028,
                            877192.0,
                            4.29,
                            59.7,
                            1121,
                            2581
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m-deduped</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.29*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0186,
                            55.9,
                            869,
                            892857.0,
                            4.58,
                            55.9,
                            958,
                            2420
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m-deduped</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.29 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0191,
                            55.1,
                            1028,
                            826446.0,
                            4.65,
                            55.1,
                            1121,
                            2581
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m-deduped</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.29*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0195,
                            55.6,
                            601,
                            847457.0,
                            4.61,
                            55.5,
                            681,
                            2143
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m-deduped</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.29*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0201,
                            53.1,
                            590,
                            793650.0,
                            4.82,
                            53.1,
                            685,
                            2145
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m-deduped</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.29*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0203,
                            49.1,
                            590,
                            714285.0,
                            5.21,
                            49.1,
                            685,
                            2145
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m-deduped</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.29 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0237,
                            54.3,
                            1948,
                            806451.0,
                            4.72,
                            54.2,
                            2006,
                            3458
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m-deduped</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.29*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.0332,
                            36.4,
                            603,
                            552486.0,
                            7.04,
                            36.4,
                            710,
                            2170
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m-deduped</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.29*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0346,
                            44.4,
                            592,
                            694444.0,
                            5.77,
                            44.4,
                            700,
                            2160
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m-deduped</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.29*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0362,
                            33.7,
                            603,
                            523560.0,
                            7.61,
                            33.6,
                            713,
                            2172
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m-deduped</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.29*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0484,
                            20.2,
                            742,
                            315457.0,
                            12.6,
                            20.3,
                            832,
                            2300
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m-deduped</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.29*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0545,
                            19.2,
                            743,
                            298507.0,
                            13.4,
                            19.1,
                            832,
                            2300
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-430m-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-430m-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            0.43,
                            "30.45 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0237,
                            41.1,
                            896,
                            657894.0,
                            6.23,
                            41.1,
                            918,
                            2378
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-430m-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-430m-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            0.43,
                            "30.45 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.024,
                            39.3,
                            895,
                            578034.0,
                            6.51,
                            39.3,
                            914,
                            2373
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-430m-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-430m-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            0.43,
                            "30.45 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0248,
                            41.3,
                            1784,
                            645161.0,
                            6.19,
                            41.4,
                            1809,
                            3261
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-430m-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-430m-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            0.43,
                            "30.45*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0529,
                            23.4,
                            428,
                            366300.0,
                            11.0,
                            23.3,
                            488,
                            1948
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-430m-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-430m-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            0.43,
                            "30.45*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0813,
                            12.4,
                            569,
                            193050.0,
                            20.6,
                            12.4,
                            606,
                            2074
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-356m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-356m</a>",
                            "GPT-2",
                            0.47,
                            "30.41 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0113,
                            78.5,
                            951,
                            1190476.0,
                            3.26,
                            78.5,
                            1048,
                            2506
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-356m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-356m</a>",
                            "GPT-2",
                            0.47,
                            "30.41 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0133,
                            71.0,
                            951,
                            1119820.0,
                            3.6,
                            71.1,
                            1048,
                            2506
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-356m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-356m</a>",
                            "GPT-2",
                            0.47,
                            "30.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0136,
                            70.8,
                            951,
                            1084598.0,
                            3.61,
                            70.9,
                            1048,
                            2506
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-356m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-356m</a>",
                            "GPT-2",
                            0.47,
                            "30.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.014,
                            69.7,
                            951,
                            1067235.0,
                            3.67,
                            69.8,
                            1048,
                            2506
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-356m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-356m</a>",
                            "GPT-2",
                            0.47,
                            "30.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0142,
                            75.4,
                            525,
                            1166861.0,
                            3.39,
                            75.5,
                            627,
                            2088
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-356m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-356m</a>",
                            "GPT-2",
                            0.47,
                            "30.41 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0149,
                            69.5,
                            951,
                            1036269.0,
                            3.68,
                            69.6,
                            1048,
                            2506
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-356m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-356m</a>",
                            "GPT-2",
                            0.47,
                            "30.41 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0209,
                            71.8,
                            1792,
                            1052631.0,
                            3.57,
                            71.7,
                            1828,
                            3279
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-356m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-356m</a>",
                            "GPT-2",
                            0.47,
                            "30.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0295,
                            56.7,
                            517,
                            892857.0,
                            4.53,
                            56.5,
                            635,
                            2094
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/robowaifudev/megatron-gpt2-345m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">robowaifudev/megatron-gpt2-345m</a>",
                            "GPT-2",
                            0.38,
                            "30.40 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0138,
                            68.5,
                            844,
                            1026694.0,
                            3.73,
                            68.6,
                            935,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/robowaifudev/megatron-gpt2-345m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">robowaifudev/megatron-gpt2-345m</a>",
                            "GPT-2",
                            0.38,
                            "30.40 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0155,
                            62.8,
                            844,
                            1000000.0,
                            4.08,
                            62.7,
                            935,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/robowaifudev/megatron-gpt2-345m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">robowaifudev/megatron-gpt2-345m</a>",
                            "GPT-2",
                            0.38,
                            "30.40 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0158,
                            61.2,
                            844,
                            917431.0,
                            4.19,
                            61.1,
                            935,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/robowaifudev/megatron-gpt2-345m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">robowaifudev/megatron-gpt2-345m</a>",
                            "GPT-2",
                            0.38,
                            "30.40*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0161,
                            61.9,
                            844,
                            952380.0,
                            4.14,
                            61.8,
                            935,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/robowaifudev/megatron-gpt2-345m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">robowaifudev/megatron-gpt2-345m</a>",
                            "GPT-2",
                            0.38,
                            "30.40*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0162,
                            61.4,
                            844,
                            909090.0,
                            4.17,
                            61.4,
                            935,
                            2394
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/robowaifudev/megatron-gpt2-345m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">robowaifudev/megatron-gpt2-345m</a>",
                            "GPT-2",
                            0.38,
                            "30.40*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0168,
                            64.9,
                            407,
                            1040582.0,
                            3.95,
                            64.8,
                            492,
                            1954
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/robowaifudev/megatron-gpt2-345m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">robowaifudev/megatron-gpt2-345m</a>",
                            "GPT-2",
                            0.38,
                            "30.40 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0206,
                            62.8,
                            1656,
                            925925.0,
                            4.08,
                            62.7,
                            1696,
                            3147
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/robowaifudev/megatron-gpt2-345m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">robowaifudev/megatron-gpt2-345m</a>",
                            "GPT-2",
                            0.38,
                            "30.40*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0309,
                            51.6,
                            399,
                            813008.0,
                            4.97,
                            51.5,
                            499,
                            1958
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ahxt/llama2_xs_460M_experimental\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ahxt/llama2_xs_460M_experimental</a>",
                            "\ud83e\udd99 LLaMA",
                            0.46,
                            "30.17 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0182,
                            52.8,
                            1020,
                            854700.0,
                            4.85,
                            52.8,
                            1101,
                            2560
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ahxt/llama2_xs_460M_experimental\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ahxt/llama2_xs_460M_experimental</a>",
                            "\ud83e\udd99 LLaMA",
                            0.46,
                            "30.17 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0188,
                            51.0,
                            1020,
                            819672.0,
                            5.02,
                            51.0,
                            1103,
                            2562
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ahxt/llama2_xs_460M_experimental\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ahxt/llama2_xs_460M_experimental</a>",
                            "\ud83e\udd99 LLaMA",
                            0.46,
                            "30.17 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0196,
                            49.2,
                            1020,
                            740740.0,
                            5.2,
                            49.2,
                            1103,
                            2562
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ahxt/llama2_xs_460M_experimental\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ahxt/llama2_xs_460M_experimental</a>",
                            "\ud83e\udd99 LLaMA",
                            0.46,
                            "30.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0213,
                            44.0,
                            489,
                            671140.0,
                            5.81,
                            44.1,
                            532,
                            1992
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ahxt/llama2_xs_460M_experimental\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ahxt/llama2_xs_460M_experimental</a>",
                            "\ud83e\udd99 LLaMA",
                            0.46,
                            "30.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0216,
                            47.4,
                            768,
                            751879.0,
                            5.4,
                            47.4,
                            796,
                            2258
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ahxt/llama2_xs_460M_experimental\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ahxt/llama2_xs_460M_experimental</a>",
                            "\ud83e\udd99 LLaMA",
                            0.46,
                            "30.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0217,
                            48.9,
                            489,
                            751879.0,
                            5.24,
                            48.9,
                            532,
                            1992
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ahxt/llama2_xs_460M_experimental\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ahxt/llama2_xs_460M_experimental</a>",
                            "\ud83e\udd99 LLaMA",
                            0.46,
                            "30.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0225,
                            46.2,
                            499,
                            704225.0,
                            5.54,
                            46.2,
                            538,
                            2000
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ahxt/llama2_xs_460M_experimental\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ahxt/llama2_xs_460M_experimental</a>",
                            "\ud83e\udd99 LLaMA",
                            0.46,
                            "30.17 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.023,
                            55.0,
                            1936,
                            787401.0,
                            4.66,
                            54.9,
                            1986,
                            3437
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ahxt/llama2_xs_460M_experimental\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ahxt/llama2_xs_460M_experimental</a>",
                            "\ud83e\udd99 LLaMA",
                            0.46,
                            "30.17*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.0447,
                            29.4,
                            504,
                            454545.0,
                            8.71,
                            29.4,
                            568,
                            2027
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ahxt/llama2_xs_460M_experimental\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ahxt/llama2_xs_460M_experimental</a>",
                            "\ud83e\udd99 LLaMA",
                            0.46,
                            "30.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0464,
                            28.3,
                            504,
                            444444.0,
                            9.05,
                            28.3,
                            570,
                            2029
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ahxt/llama2_xs_460M_experimental\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ahxt/llama2_xs_460M_experimental</a>",
                            "\ud83e\udd99 LLaMA",
                            0.46,
                            "30.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0483,
                            35.4,
                            490,
                            564971.0,
                            7.25,
                            35.3,
                            551,
                            2011
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ahxt/llama2_xs_460M_experimental\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ahxt/llama2_xs_460M_experimental</a>",
                            "\ud83e\udd99 LLaMA",
                            0.46,
                            "30.17*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0724,
                            13.8,
                            662,
                            220264.0,
                            18.6,
                            13.8,
                            704,
                            2172
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ahxt/llama2_xs_460M_experimental\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ahxt/llama2_xs_460M_experimental</a>",
                            "\ud83e\udd99 LLaMA",
                            0.46,
                            "30.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0743,
                            13.1,
                            662,
                            207468.0,
                            19.5,
                            13.1,
                            706,
                            2174
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/Orca-2-7b-f16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/Orca-2-7b-f16</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "30.15 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0307,
                            43.1,
                            14089,
                            487804.0,
                            5.94,
                            43.1,
                            14128,
                            15588
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/Orca-2-7b-f16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/Orca-2-7b-f16</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "30.15 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0328,
                            40.8,
                            14089,
                            450450.0,
                            6.28,
                            40.8,
                            14128,
                            15588
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/Orca-2-7b-f16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/Orca-2-7b-f16</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "30.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0413,
                            40.2,
                            4649,
                            523560.0,
                            6.39,
                            40.1,
                            4685,
                            6146
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/Orca-2-7b-f16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/Orca-2-7b-f16</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "30.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0417,
                            40.2,
                            5375,
                            568181.0,
                            6.38,
                            40.1,
                            5412,
                            6874
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/Orca-2-7b-f16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/Orca-2-7b-f16</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "30.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0635,
                            40.1,
                            4553,
                            543478.0,
                            6.42,
                            39.9,
                            4590,
                            6050
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/Orca-2-7b-f16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/Orca-2-7b-f16</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "30.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0725,
                            36.7,
                            4552,
                            495049.0,
                            7.01,
                            36.5,
                            4590,
                            6050
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/Orca-2-7b-f16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/Orca-2-7b-f16</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "30.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0935,
                            10.6,
                            7719,
                            157232.0,
                            24.1,
                            10.6,
                            7761,
                            9229
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/Orca-2-7b-f16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/Orca-2-7b-f16</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "30.15*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.1,
                            10.2,
                            7719,
                            153139.0,
                            25.1,
                            10.2,
                            7761,
                            9229
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/Orca-2-7b-f16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/Orca-2-7b-f16</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "30.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.166,
                            28.3,
                            4561,
                            387596.0,
                            9.19,
                            27.9,
                            4611,
                            6071
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/Orca-2-7b-f16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/Orca-2-7b-f16</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "30.15 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.238,
                            41.7,
                            28167,
                            366300.0,
                            6.35,
                            40.3,
                            28531,
                            29982
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/Orca-2-7b-f16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/Orca-2-7b-f16</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "30.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.269,
                            23.2,
                            4829,
                            291545.0,
                            11.3,
                            22.7,
                            4987,
                            6446
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/Orca-2-7b-f16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/Orca-2-7b-f16</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "30.15*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.273,
                            22.6,
                            4829,
                            287356.0,
                            11.6,
                            22.1,
                            5033,
                            6492
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-564M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-564M</a>",
                            "XGLM",
                            0.56,
                            "29.55 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0155,
                            75.7,
                            1325,
                            1094091.0,
                            3.39,
                            75.5,
                            1396,
                            2856
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-564M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-564M</a>",
                            "XGLM",
                            0.56,
                            "29.55 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0158,
                            74.6,
                            1325,
                            1091703.0,
                            3.44,
                            74.4,
                            1396,
                            2856
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-564M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-564M</a>",
                            "XGLM",
                            0.56,
                            "29.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0162,
                            74.1,
                            1156,
                            1146788.0,
                            3.46,
                            74.0,
                            1228,
                            2690
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-564M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-564M</a>",
                            "XGLM",
                            0.56,
                            "29.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0164,
                            71.4,
                            886,
                            1043841.0,
                            3.59,
                            71.3,
                            968,
                            2430
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-564M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-564M</a>",
                            "XGLM",
                            0.56,
                            "29.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0177,
                            68.7,
                            877,
                            970873.0,
                            3.73,
                            68.6,
                            964,
                            2424
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-564M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-564M</a>",
                            "XGLM",
                            0.56,
                            "29.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0179,
                            58.4,
                            877,
                            862068.0,
                            4.39,
                            58.3,
                            962,
                            2422
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-564M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-564M</a>",
                            "XGLM",
                            0.56,
                            "29.55 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0282,
                            73.7,
                            2644,
                            990099.0,
                            3.49,
                            73.4,
                            2667,
                            4118
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-564M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-564M</a>",
                            "XGLM",
                            0.56,
                            "29.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0392,
                            49.7,
                            878,
                            763358.0,
                            5.17,
                            49.5,
                            966,
                            2426
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-564M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-564M</a>",
                            "XGLM",
                            0.56,
                            "29.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0401,
                            33.6,
                            890,
                            520833.0,
                            7.64,
                            33.5,
                            998,
                            2457
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-564M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-564M</a>",
                            "XGLM",
                            0.56,
                            "29.55*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0626,
                            16.6,
                            1023,
                            248138.0,
                            15.5,
                            16.5,
                            1105,
                            2573
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/winglian/Llama-2-3b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">winglian/Llama-2-3b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "29.53 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0154,
                            78.7,
                            7311,
                            952380.0,
                            3.26,
                            78.5,
                            7342,
                            8801
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/winglian/Llama-2-3b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">winglian/Llama-2-3b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "29.53 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0163,
                            82.5,
                            7311,
                            884955.0,
                            3.11,
                            82.3,
                            7342,
                            8801
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/winglian/Llama-2-3b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">winglian/Llama-2-3b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "29.53 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0165,
                            79.4,
                            7311,
                            877192.0,
                            3.23,
                            79.3,
                            7342,
                            8801
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/winglian/Llama-2-3b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">winglian/Llama-2-3b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "29.53*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0203,
                            76.3,
                            2637,
                            1000000.0,
                            3.36,
                            76.2,
                            2675,
                            4137
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/winglian/Llama-2-3b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">winglian/Llama-2-3b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "29.53*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0208,
                            76.6,
                            3360,
                            1100110.0,
                            3.35,
                            76.4,
                            3401,
                            4863
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/winglian/Llama-2-3b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">winglian/Llama-2-3b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "29.53*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0308,
                            82.0,
                            2543,
                            1083423.0,
                            3.14,
                            81.5,
                            2581,
                            4041
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/winglian/Llama-2-3b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">winglian/Llama-2-3b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "29.53*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0362,
                            70.2,
                            2543,
                            961538.0,
                            3.67,
                            69.8,
                            2560,
                            4020
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/winglian/Llama-2-3b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">winglian/Llama-2-3b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "29.53*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0486,
                            20.4,
                            4126,
                            303951.0,
                            12.5,
                            20.5,
                            4158,
                            5626
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/winglian/Llama-2-3b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">winglian/Llama-2-3b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "29.53*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0491,
                            20.7,
                            4126,
                            300300.0,
                            12.3,
                            20.8,
                            4158,
                            5626
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/winglian/Llama-2-3b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">winglian/Llama-2-3b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "29.53*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0845,
                            58.1,
                            2584,
                            781250.0,
                            4.47,
                            57.3,
                            2686,
                            4146
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/winglian/Llama-2-3b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">winglian/Llama-2-3b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "29.53 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.121,
                            80.4,
                            14612,
                            704225.0,
                            3.29,
                            77.8,
                            14807,
                            16259
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/winglian/Llama-2-3b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">winglian/Llama-2-3b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "29.53*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.134,
                            45.6,
                            2776,
                            571428.0,
                            5.72,
                            44.8,
                            3053,
                            4513
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/winglian/Llama-2-3b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">winglian/Llama-2-3b-hf</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "29.53*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.134,
                            45.2,
                            2776,
                            568181.0,
                            5.77,
                            44.4,
                            3053,
                            4513
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>",
                            "GPT-Neo",
                            0.15,
                            "29.47 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00641,
                            147.0,
                            363,
                            2325581.0,
                            1.74,
                            147.0,
                            411,
                            1868
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>",
                            "GPT-Neo",
                            0.15,
                            "29.47 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00791,
                            122.0,
                            363,
                            1869158.0,
                            2.1,
                            122.0,
                            411,
                            1870
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>",
                            "GPT-Neo",
                            0.15,
                            "29.47 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00838,
                            121.0,
                            363,
                            1953124.0,
                            2.12,
                            121.0,
                            411,
                            1870
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>",
                            "GPT-Neo",
                            0.15,
                            "29.47 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00875,
                            115.0,
                            363,
                            1748251.0,
                            2.22,
                            115.0,
                            411,
                            1870
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>",
                            "GPT-Neo",
                            0.15,
                            "29.47*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00928,
                            115.0,
                            436,
                            1858736.0,
                            2.22,
                            115.0,
                            486,
                            1948
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>",
                            "GPT-Neo",
                            0.15,
                            "29.47*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00953,
                            114.0,
                            235,
                            1834862.0,
                            2.25,
                            114.0,
                            278,
                            1740
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>",
                            "GPT-Neo",
                            0.15,
                            "29.47*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00962,
                            103.0,
                            230,
                            1599999.0,
                            2.49,
                            103.0,
                            291,
                            1751
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>",
                            "GPT-Neo",
                            0.15,
                            "29.47*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00975,
                            116.0,
                            230,
                            1757469.0,
                            2.2,
                            116.0,
                            291,
                            1751
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>",
                            "GPT-Neo",
                            0.15,
                            "29.47 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00981,
                            123.0,
                            658,
                            1838235.0,
                            2.08,
                            123.0,
                            746,
                            2197
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>",
                            "GPT-Neo",
                            0.15,
                            "29.47*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.0196,
                            64.2,
                            233,
                            980392.0,
                            3.99,
                            64.2,
                            304,
                            1763
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>",
                            "GPT-Neo",
                            0.15,
                            "29.47*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0208,
                            85.9,
                            230,
                            1369863.0,
                            2.99,
                            85.6,
                            299,
                            1759
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>",
                            "GPT-Neo",
                            0.15,
                            "29.47*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.021,
                            59.9,
                            233,
                            961538.0,
                            4.28,
                            59.8,
                            301,
                            1761
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>",
                            "GPT-Neo",
                            0.15,
                            "29.47*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0324,
                            31.2,
                            271,
                            487804.0,
                            8.2,
                            31.2,
                            339,
                            1807
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>",
                            "GPT-Neo",
                            0.15,
                            "29.47*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0325,
                            31.3,
                            270,
                            487804.0,
                            8.18,
                            31.3,
                            339,
                            1807
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.44 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00784,
                            118.0,
                            500,
                            1872659.0,
                            2.18,
                            117.0,
                            549,
                            2009
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.44 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00789,
                            120.0,
                            500,
                            1897533.0,
                            2.14,
                            120.0,
                            547,
                            2006
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.44 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00855,
                            109.0,
                            500,
                            1736111.0,
                            2.35,
                            109.0,
                            549,
                            2009
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00919,
                            109.0,
                            275,
                            1697792.0,
                            2.34,
                            109.0,
                            301,
                            1761
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00921,
                            109.0,
                            553,
                            1715265.0,
                            2.35,
                            109.0,
                            583,
                            2044
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00938,
                            105.0,
                            283,
                            1658374.0,
                            2.43,
                            105.0,
                            308,
                            1769
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0096,
                            98.5,
                            275,
                            1550387.0,
                            2.6,
                            98.5,
                            301,
                            1761
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.44 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0108,
                            126.0,
                            932,
                            1779359.0,
                            2.04,
                            125.0,
                            968,
                            2420
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.44*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.0185,
                            70.1,
                            281,
                            1066098.0,
                            3.66,
                            69.9,
                            322,
                            1782
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0188,
                            67.8,
                            281,
                            1049317.0,
                            3.78,
                            67.7,
                            325,
                            1784
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.02,
                            82.0,
                            275,
                            1321003.0,
                            3.13,
                            81.8,
                            320,
                            1780
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.44*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0306,
                            32.2,
                            348,
                            510204.0,
                            7.94,
                            32.2,
                            375,
                            1843
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0312,
                            31.4,
                            348,
                            485436.0,
                            8.14,
                            31.4,
                            375,
                            1843
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/tiny_starcoder_py\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/tiny_starcoder_py</a>",
                            "\u2b50 StarCoder",
                            0.16,
                            "29.41 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0078,
                            116.0,
                            446,
                            1862197.0,
                            2.21,
                            116.0,
                            469,
                            1927
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/tiny_starcoder_py\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/tiny_starcoder_py</a>",
                            "\u2b50 StarCoder",
                            0.16,
                            "29.41 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0089,
                            112.0,
                            446,
                            1824817.0,
                            2.29,
                            112.0,
                            469,
                            1929
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/tiny_starcoder_py\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/tiny_starcoder_py</a>",
                            "\u2b50 StarCoder",
                            0.16,
                            "29.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00962,
                            111.0,
                            470,
                            1798561.0,
                            2.31,
                            111.0,
                            482,
                            1944
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/tiny_starcoder_py\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/tiny_starcoder_py</a>",
                            "\u2b50 StarCoder",
                            0.16,
                            "29.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00997,
                            109.0,
                            269,
                            1745200.0,
                            2.34,
                            109.0,
                            295,
                            1757
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/tiny_starcoder_py\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/tiny_starcoder_py</a>",
                            "\u2b50 StarCoder",
                            0.16,
                            "29.41 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0104,
                            111.0,
                            446,
                            1718213.0,
                            2.31,
                            111.0,
                            469,
                            1929
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/tiny_starcoder_py\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/tiny_starcoder_py</a>",
                            "\u2b50 StarCoder",
                            0.16,
                            "29.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0105,
                            103.0,
                            264,
                            1623376.0,
                            2.49,
                            103.0,
                            287,
                            1746
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/tiny_starcoder_py\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/tiny_starcoder_py</a>",
                            "\u2b50 StarCoder",
                            0.16,
                            "29.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0108,
                            92.1,
                            263,
                            1445086.0,
                            2.78,
                            92.1,
                            287,
                            1746
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/tiny_starcoder_py\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/tiny_starcoder_py</a>",
                            "\u2b50 StarCoder",
                            0.16,
                            "29.41 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0115,
                            109.0,
                            804,
                            1727115.0,
                            2.36,
                            108.0,
                            870,
                            2321
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/tiny_starcoder_py\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/tiny_starcoder_py</a>",
                            "\u2b50 StarCoder",
                            0.16,
                            "29.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0226,
                            79.2,
                            264,
                            1293661.0,
                            3.24,
                            79.0,
                            295,
                            1755
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/tiny_starcoder_py\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/tiny_starcoder_py</a>",
                            "\u2b50 StarCoder",
                            0.16,
                            "29.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0229,
                            55.2,
                            269,
                            877192.0,
                            4.64,
                            55.2,
                            304,
                            1763
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/tiny_starcoder_py\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/tiny_starcoder_py</a>",
                            "\u2b50 StarCoder",
                            0.16,
                            "29.41*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.0232,
                            55.4,
                            269,
                            892857.0,
                            4.62,
                            55.4,
                            301,
                            1759
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/tiny_starcoder_py\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/tiny_starcoder_py</a>",
                            "\u2b50 StarCoder",
                            0.16,
                            "29.41*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0335,
                            29.3,
                            320,
                            460829.0,
                            8.74,
                            29.3,
                            354,
                            1820
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/tiny_starcoder_py\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/tiny_starcoder_py</a>",
                            "\u2b50 StarCoder",
                            0.16,
                            "29.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0373,
                            27.7,
                            320,
                            434782.0,
                            9.25,
                            27.7,
                            352,
                            1820
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-256M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-256M</a>",
                            "GPT-2",
                            0.26,
                            "29.38 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00683,
                            130.0,
                            665,
                            1855287.0,
                            1.97,
                            130.0,
                            748,
                            2208
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00793,
                            128.0,
                            458,
                            2028397.0,
                            2.01,
                            127.0,
                            509,
                            1969
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00807,
                            118.0,
                            458,
                            1763668.0,
                            2.17,
                            118.0,
                            509,
                            1969
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-256M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-256M</a>",
                            "GPT-2",
                            0.26,
                            "29.38 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00825,
                            120.0,
                            665,
                            1872659.0,
                            2.14,
                            120.0,
                            748,
                            2208
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-256M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-256M</a>",
                            "GPT-2",
                            0.26,
                            "29.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00836,
                            115.0,
                            664,
                            1724137.0,
                            2.22,
                            115.0,
                            748,
                            2208
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-256M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-256M</a>",
                            "GPT-2",
                            0.26,
                            "29.38 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0085,
                            114.0,
                            665,
                            1721170.0,
                            2.25,
                            114.0,
                            748,
                            2208
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-256M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-256M</a>",
                            "GPT-2",
                            0.26,
                            "29.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0085,
                            114.0,
                            664,
                            1727115.0,
                            2.24,
                            114.0,
                            748,
                            2208
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00925,
                            109.0,
                            458,
                            1773049.0,
                            2.35,
                            109.0,
                            509,
                            1969
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00928,
                            106.0,
                            458,
                            1661129.0,
                            2.41,
                            106.0,
                            509,
                            1969
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00966,
                            107.0,
                            521,
                            1721170.0,
                            2.39,
                            107.0,
                            566,
                            2027
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00978,
                            107.0,
                            320,
                            1721170.0,
                            2.4,
                            107.0,
                            377,
                            1839
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0103,
                            95.5,
                            315,
                            1477104.0,
                            2.68,
                            95.5,
                            392,
                            1851
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0106,
                            108.0,
                            828,
                            1642036.0,
                            2.37,
                            108.0,
                            903,
                            2355
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0106,
                            102.0,
                            315,
                            1538461.0,
                            2.5,
                            102.0,
                            392,
                            1851
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-256M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-256M</a>",
                            "GPT-2",
                            0.26,
                            "29.38 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0127,
                            123.0,
                            1257,
                            1757469.0,
                            2.08,
                            123.0,
                            1321,
                            2772
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.0165,
                            68.9,
                            320,
                            1077586.0,
                            3.72,
                            68.8,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0172,
                            87.9,
                            317,
                            1386962.0,
                            2.92,
                            87.7,
                            379,
                            1839
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0186,
                            65.1,
                            320,
                            1027749.0,
                            3.94,
                            65.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0244,
                            39.4,
                            363,
                            621118.0,
                            6.49,
                            39.4,
                            415,
                            1883
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0253,
                            39.4,
                            363,
                            602409.0,
                            6.51,
                            39.3,
                            415,
                            1883
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/SmolLlamix-8x101M-take2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/SmolLlamix-8x101M-take2</a>",
                            "\u24c2\ufe0f Mixtral",
                            0.4,
                            "29.35 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.025,
                            78.5,
                            894,
                            1215066.0,
                            3.28,
                            78.0,
                            945,
                            2405
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/SmolLlamix-8x101M-take2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/SmolLlamix-8x101M-take2</a>",
                            "\u24c2\ufe0f Mixtral",
                            0.4,
                            "29.35 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0251,
                            83.1,
                            1697,
                            1344086.0,
                            3.1,
                            82.6,
                            1895,
                            3347
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/SmolLlamix-8x101M-take2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/SmolLlamix-8x101M-take2</a>",
                            "\u24c2\ufe0f Mixtral",
                            0.4,
                            "29.35 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0251,
                            77.5,
                            894,
                            1230012.0,
                            3.32,
                            77.1,
                            945,
                            2405
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0135,
                            75.4,
                            3045,
                            1024590.0,
                            3.39,
                            75.5,
                            3131,
                            4588
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0157,
                            65.7,
                            3045,
                            854700.0,
                            3.9,
                            65.6,
                            3131,
                            4590
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0172,
                            61.4,
                            3048,
                            877192.0,
                            4.17,
                            61.4,
                            3131,
                            4590
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0172,
                            58.9,
                            3048,
                            819672.0,
                            4.35,
                            58.9,
                            3131,
                            4590
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0189,
                            59.9,
                            1834,
                            917431.0,
                            4.28,
                            59.8,
                            1931,
                            3393
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0195,
                            59.3,
                            1296,
                            884955.0,
                            4.32,
                            59.3,
                            1390,
                            2852
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0224,
                            57.6,
                            1260,
                            877192.0,
                            4.45,
                            57.5,
                            1346,
                            2805
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0233,
                            51.9,
                            1260,
                            757575.0,
                            4.93,
                            51.9,
                            1346,
                            2805
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0471,
                            43.4,
                            1262,
                            657894.0,
                            5.93,
                            43.2,
                            1350,
                            2810
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0553,
                            63.8,
                            5980,
                            813008.0,
                            4.06,
                            63.1,
                            6025,
                            7476
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.062,
                            33.1,
                            1308,
                            490196.0,
                            7.77,
                            32.9,
                            1400,
                            2860
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0633,
                            15.8,
                            1838,
                            238663.0,
                            16.2,
                            15.8,
                            1927,
                            3395
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0642,
                            15.6,
                            1842,
                            241545.0,
                            16.4,
                            15.6,
                            1931,
                            3399
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.066,
                            32.2,
                            1312,
                            485436.0,
                            7.98,
                            32.1,
                            1400,
                            2860
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.23 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00778,
                            121.0,
                            500,
                            1930501.0,
                            2.11,
                            121.0,
                            547,
                            2006
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.23 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00789,
                            116.0,
                            500,
                            1869158.0,
                            2.21,
                            116.0,
                            549,
                            2009
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.23 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00816,
                            114.0,
                            500,
                            1669449.0,
                            2.24,
                            114.0,
                            549,
                            2009
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00908,
                            102.0,
                            275,
                            1567398.0,
                            2.5,
                            102.0,
                            301,
                            1761
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00926,
                            112.0,
                            275,
                            1692047.0,
                            2.28,
                            112.0,
                            301,
                            1761
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00938,
                            107.0,
                            553,
                            1724137.0,
                            2.39,
                            107.0,
                            583,
                            2044
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00991,
                            108.0,
                            283,
                            1715265.0,
                            2.38,
                            108.0,
                            308,
                            1769
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.23 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0103,
                            122.0,
                            932,
                            1805054.0,
                            2.1,
                            122.0,
                            968,
                            2420
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.23*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.0182,
                            69.5,
                            281,
                            1041666.0,
                            3.69,
                            69.4,
                            322,
                            1782
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0188,
                            67.8,
                            281,
                            1064962.0,
                            3.78,
                            67.7,
                            325,
                            1784
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0203,
                            82.8,
                            275,
                            1267427.0,
                            3.1,
                            82.6,
                            320,
                            1780
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.23*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0305,
                            32.1,
                            348,
                            510204.0,
                            7.98,
                            32.1,
                            373,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.23*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0319,
                            31.2,
                            348,
                            485436.0,
                            8.2,
                            31.2,
                            375,
                            1843
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00391,
                            243.0,
                            126,
                            3984063.0,
                            1.05,
                            244.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00431,
                            220.0,
                            126,
                            3521126.0,
                            1.16,
                            221.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00482,
                            207.0,
                            126,
                            3496503.0,
                            1.23,
                            208.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00484,
                            204.0,
                            126,
                            3236245.0,
                            1.25,
                            205.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00499,
                            198.0,
                            186,
                            3267973.0,
                            1.29,
                            198.0,
                            205,
                            1667
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00512,
                            204.0,
                            119,
                            3278688.0,
                            1.26,
                            203.0,
                            136,
                            1597
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00521,
                            182.0,
                            118,
                            2906976.0,
                            1.41,
                            182.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00534,
                            206.0,
                            221,
                            3257328.0,
                            1.25,
                            205.0,
                            234,
                            1686
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00535,
                            192.0,
                            118,
                            3086419.0,
                            1.34,
                            191.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.00844,
                            134.0,
                            118,
                            2079002.0,
                            1.92,
                            133.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.00905,
                            126.0,
                            118,
                            2049180.0,
                            2.04,
                            125.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.00911,
                            167.0,
                            118,
                            2710027.0,
                            1.54,
                            166.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0124,
                            78.7,
                            121,
                            1231527.0,
                            3.25,
                            78.8,
                            138,
                            1606
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0129,
                            75.9,
                            121,
                            1203369.0,
                            3.37,
                            76.0,
                            138,
                            1606
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-1M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-1M</a>",
                            "GPT-Neo",
                            0.0,
                            "29.14 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0047,
                            193.0,
                            77,
                            3257328.0,
                            1.32,
                            194.0,
                            96,
                            1556
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-1M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-1M</a>",
                            "GPT-Neo",
                            0.0,
                            "29.14 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00526,
                            182.0,
                            112,
                            2890173.0,
                            1.41,
                            182.0,
                            140,
                            1591
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-1M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-1M</a>",
                            "GPT-Neo",
                            0.0,
                            "29.14 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00542,
                            182.0,
                            77,
                            3030303.0,
                            1.41,
                            182.0,
                            96,
                            1556
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-1M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-1M</a>",
                            "GPT-Neo",
                            0.0,
                            "29.14 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00571,
                            175.0,
                            77,
                            2777777.0,
                            1.47,
                            174.0,
                            96,
                            1556
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-1M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-1M</a>",
                            "GPT-Neo",
                            0.0,
                            "29.14 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00643,
                            173.0,
                            77,
                            2793296.0,
                            1.48,
                            173.0,
                            96,
                            1556
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-1M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-1M</a>",
                            "GPT-Neo",
                            0.0,
                            "29.14*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0132,
                            93.8,
                            75,
                            1515151.0,
                            2.73,
                            93.8,
                            94,
                            1553
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-1M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-1M</a>",
                            "GPT-Neo",
                            0.0,
                            "29.14*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.0138,
                            90.7,
                            75,
                            1464128.0,
                            2.82,
                            90.8,
                            94,
                            1553
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-1M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-1M</a>",
                            "GPT-Neo",
                            0.0,
                            "29.14*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0214,
                            47.0,
                            76,
                            735294.0,
                            5.45,
                            47.0,
                            94,
                            1562
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-1M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-1M</a>",
                            "GPT-Neo",
                            0.0,
                            "29.14*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0217,
                            45.4,
                            76,
                            735294.0,
                            5.64,
                            45.4,
                            94,
                            1562
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0135,
                            69.5,
                            1939,
                            970873.0,
                            3.68,
                            69.6,
                            2084,
                            3544
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0151,
                            67.3,
                            1939,
                            1000000.0,
                            3.81,
                            67.2,
                            2084,
                            3544
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0152,
                            68.0,
                            1939,
                            925925.0,
                            3.77,
                            67.9,
                            2084,
                            3544
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0155,
                            70.4,
                            962,
                            1054852.0,
                            3.64,
                            70.3,
                            1090,
                            2552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0156,
                            71.4,
                            1365,
                            1104972.0,
                            3.59,
                            71.3,
                            1497,
                            2959
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0163,
                            60.7,
                            942,
                            917431.0,
                            4.22,
                            60.7,
                            1073,
                            2533
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0164,
                            66.9,
                            945,
                            1007049.0,
                            3.83,
                            66.8,
                            1075,
                            2535
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0301,
                            55.2,
                            943,
                            833333.0,
                            4.65,
                            55.1,
                            1084,
                            2543
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.0354,
                            39.2,
                            972,
                            595238.0,
                            6.54,
                            39.1,
                            1147,
                            2606
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.036,
                            38.9,
                            972,
                            598802.0,
                            6.59,
                            38.8,
                            1147,
                            2606
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0373,
                            70.8,
                            3893,
                            909090.0,
                            3.64,
                            70.3,
                            3942,
                            5393
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0454,
                            21.6,
                            1262,
                            330033.0,
                            11.8,
                            21.7,
                            1436,
                            2904
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0469,
                            20.7,
                            1259,
                            319488.0,
                            12.3,
                            20.8,
                            1436,
                            2904
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0079,
                            129.0,
                            458,
                            2044989.0,
                            1.98,
                            129.0,
                            509,
                            1969
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00844,
                            115.0,
                            458,
                            1779359.0,
                            2.22,
                            115.0,
                            509,
                            1969
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00918,
                            108.0,
                            458,
                            1776198.0,
                            2.38,
                            108.0,
                            509,
                            1969
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00968,
                            106.0,
                            521,
                            1683501.0,
                            2.41,
                            106.0,
                            566,
                            2027
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00969,
                            107.0,
                            458,
                            1536098.0,
                            2.4,
                            107.0,
                            509,
                            1969
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00975,
                            106.0,
                            320,
                            1697792.0,
                            2.42,
                            106.0,
                            377,
                            1839
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0103,
                            108.0,
                            828,
                            1628664.0,
                            2.37,
                            108.0,
                            903,
                            2355
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0103,
                            103.0,
                            315,
                            1562500.0,
                            2.49,
                            103.0,
                            392,
                            1851
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0105,
                            92.4,
                            315,
                            1470588.0,
                            2.77,
                            92.4,
                            392,
                            1851
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.0163,
                            68.9,
                            320,
                            1064962.0,
                            3.72,
                            68.8,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0179,
                            65.9,
                            320,
                            1036269.0,
                            3.89,
                            65.8,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.018,
                            82.8,
                            317,
                            1383125.0,
                            3.1,
                            82.6,
                            379,
                            1839
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.024,
                            40.1,
                            363,
                            625000.0,
                            6.38,
                            40.1,
                            415,
                            1883
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0256,
                            39.1,
                            363,
                            609756.0,
                            6.56,
                            39.0,
                            415,
                            1883
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/SmolLlamix-8x101M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/SmolLlamix-8x101M</a>",
                            "\u24c2\ufe0f Mixtral",
                            0.4,
                            "28.98 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0248,
                            77.0,
                            895,
                            1222493.0,
                            3.33,
                            76.9,
                            945,
                            2405
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/SmolLlamix-8x101M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/SmolLlamix-8x101M</a>",
                            "\u24c2\ufe0f Mixtral",
                            0.4,
                            "28.98 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0252,
                            82.5,
                            1697,
                            1349527.0,
                            3.12,
                            82.1,
                            1895,
                            3347
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/SmolLlamix-8x101M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/SmolLlamix-8x101M</a>",
                            "\u24c2\ufe0f Mixtral",
                            0.4,
                            "28.98 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0255,
                            76.8,
                            895,
                            1184834.0,
                            3.35,
                            76.4,
                            945,
                            2405
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.1,
                            "28.97 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00491,
                            192.0,
                            269,
                            3048780.0,
                            1.33,
                            192.0,
                            291,
                            1751
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.1,
                            "28.97 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00495,
                            185.0,
                            269,
                            3012048.0,
                            1.38,
                            186.0,
                            293,
                            1753
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.1,
                            "28.97 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00517,
                            181.0,
                            269,
                            2777777.0,
                            1.42,
                            180.0,
                            293,
                            1753
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.1,
                            "28.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00559,
                            177.0,
                            190,
                            2816901.0,
                            1.45,
                            177.0,
                            213,
                            1673
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.1,
                            "28.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00567,
                            173.0,
                            396,
                            2793296.0,
                            1.48,
                            173.0,
                            417,
                            1879
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.1,
                            "28.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00567,
                            158.0,
                            189,
                            2386634.0,
                            1.62,
                            158.0,
                            213,
                            1673
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.1,
                            "28.97 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00572,
                            199.0,
                            456,
                            3021148.0,
                            1.29,
                            198.0,
                            513,
                            1964
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.1,
                            "28.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00598,
                            170.0,
                            195,
                            2702702.0,
                            1.51,
                            170.0,
                            209,
                            1671
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.1,
                            "28.97*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.0113,
                            112.0,
                            192,
                            1715265.0,
                            2.28,
                            112.0,
                            213,
                            1673
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.1,
                            "28.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0114,
                            110.0,
                            192,
                            1703577.0,
                            2.33,
                            110.0,
                            216,
                            1675
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.1,
                            "28.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0126,
                            130.0,
                            190,
                            2079002.0,
                            1.97,
                            130.0,
                            213,
                            1673
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.1,
                            "28.97*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0185,
                            53.8,
                            214,
                            826446.0,
                            4.76,
                            53.8,
                            234,
                            1702
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.1,
                            "28.97*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.019,
                            51.1,
                            214,
                            813008.0,
                            5.01,
                            51.1,
                            236,
                            1704
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0155,
                            63.4,
                            1976,
                            952380.0,
                            4.04,
                            63.4,
                            2113,
                            3573
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0173,
                            58.1,
                            1976,
                            854700.0,
                            4.41,
                            58.0,
                            2113,
                            3573
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0191,
                            55.0,
                            1387,
                            869565.0,
                            4.66,
                            54.9,
                            1522,
                            2984
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0196,
                            50.4,
                            1976,
                            806451.0,
                            5.08,
                            50.4,
                            2113,
                            3573
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.02,
                            53.8,
                            968,
                            819672.0,
                            4.76,
                            53.8,
                            1098,
                            2558
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.02,
                            52.4,
                            1976,
                            769230.0,
                            4.89,
                            52.4,
                            2113,
                            3573
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0201,
                            49.8,
                            965,
                            724637.0,
                            5.14,
                            49.8,
                            1096,
                            2556
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0204,
                            54.5,
                            983,
                            833333.0,
                            4.7,
                            54.5,
                            1115,
                            2577
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0353,
                            53.9,
                            3836,
                            746268.0,
                            4.77,
                            53.7,
                            3915,
                            5366
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0361,
                            44.5,
                            962,
                            694444.0,
                            5.77,
                            44.4,
                            1090,
                            2550
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.0425,
                            36.0,
                            998,
                            537634.0,
                            7.13,
                            35.9,
                            1124,
                            2583
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0444,
                            33.3,
                            998,
                            510204.0,
                            7.7,
                            33.2,
                            1124,
                            2583
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0483,
                            20.1,
                            1305,
                            311526.0,
                            12.7,
                            20.2,
                            1440,
                            2908
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0507,
                            19.8,
                            1303,
                            310559.0,
                            13.0,
                            19.7,
                            1442,
                            2910
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00389,
                            245.0,
                            126,
                            4032258.0,
                            1.04,
                            246.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00413,
                            224.0,
                            126,
                            3571428.0,
                            1.14,
                            225.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00474,
                            204.0,
                            126,
                            3225806.0,
                            1.25,
                            205.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00493,
                            201.0,
                            186,
                            3300330.0,
                            1.27,
                            202.0,
                            205,
                            1667
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00494,
                            206.0,
                            126,
                            3436426.0,
                            1.24,
                            206.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.005,
                            199.0,
                            119,
                            3184713.0,
                            1.28,
                            200.0,
                            136,
                            1597
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0051,
                            204.0,
                            221,
                            3225806.0,
                            1.26,
                            203.0,
                            234,
                            1686
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00513,
                            185.0,
                            118,
                            2801120.0,
                            1.39,
                            184.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00515,
                            196.0,
                            118,
                            2906976.0,
                            1.31,
                            195.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.00875,
                            163.0,
                            118,
                            2645502.0,
                            1.57,
                            163.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.00893,
                            131.0,
                            118,
                            2066115.0,
                            1.96,
                            131.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0092,
                            126.0,
                            118,
                            2016129.0,
                            2.03,
                            126.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0123,
                            79.7,
                            121,
                            1251564.0,
                            3.21,
                            79.8,
                            138,
                            1606
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0131,
                            73.5,
                            121,
                            1189060.0,
                            3.48,
                            73.6,
                            138,
                            1606
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0041,
                            238.0,
                            126,
                            3937007.0,
                            1.07,
                            239.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00417,
                            224.0,
                            126,
                            3508771.0,
                            1.14,
                            225.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00468,
                            209.0,
                            126,
                            3484320.0,
                            1.22,
                            210.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0048,
                            204.0,
                            126,
                            3267973.0,
                            1.25,
                            205.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00483,
                            206.0,
                            186,
                            3289473.0,
                            1.24,
                            206.0,
                            205,
                            1667
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00516,
                            202.0,
                            119,
                            3184713.0,
                            1.27,
                            202.0,
                            136,
                            1597
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00526,
                            202.0,
                            221,
                            3205128.0,
                            1.27,
                            202.0,
                            234,
                            1686
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0053,
                            195.0,
                            118,
                            2949852.0,
                            1.32,
                            194.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00552,
                            176.0,
                            118,
                            2710027.0,
                            1.46,
                            175.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.00875,
                            166.0,
                            118,
                            2702702.0,
                            1.55,
                            165.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.00877,
                            131.0,
                            118,
                            2092050.0,
                            1.95,
                            131.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.00906,
                            126.0,
                            118,
                            2032520.0,
                            2.03,
                            126.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0123,
                            78.0,
                            121,
                            1197604.0,
                            3.28,
                            78.0,
                            138,
                            1606
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0127,
                            75.9,
                            121,
                            1213592.0,
                            3.37,
                            76.0,
                            138,
                            1606
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248M-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248M-v2</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "28.78 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0113,
                            87.3,
                            608,
                            1356852.0,
                            2.93,
                            87.4,
                            660,
                            2120
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248M-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248M-v2</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "28.78 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0114,
                            88.2,
                            608,
                            1356852.0,
                            2.9,
                            88.3,
                            656,
                            2116
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248M-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248M-v2</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "28.78 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0128,
                            95.5,
                            1153,
                            1412429.0,
                            2.68,
                            95.5,
                            1199,
                            2650
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248M-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248M-v2</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "28.78*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0128,
                            85.9,
                            338,
                            1290322.0,
                            2.98,
                            85.9,
                            371,
                            1830
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248M-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248M-v2</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "28.78*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.013,
                            76.6,
                            338,
                            1169590.0,
                            3.34,
                            76.6,
                            371,
                            1830
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/verysmol_llama-v11-KIx2</a>",
                            "\ud83e\udd99 LLaMA",
                            0.06,
                            "28.70 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00452,
                            196.0,
                            180,
                            3215434.0,
                            1.3,
                            197.0,
                            209,
                            1669
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/verysmol_llama-v11-KIx2</a>",
                            "\ud83e\udd99 LLaMA",
                            0.06,
                            "28.70 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00474,
                            190.0,
                            180,
                            2881844.0,
                            1.34,
                            191.0,
                            209,
                            1669
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/verysmol_llama-v11-KIx2</a>",
                            "\ud83e\udd99 LLaMA",
                            0.06,
                            "28.70 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00483,
                            190.0,
                            180,
                            3134796.0,
                            1.34,
                            191.0,
                            209,
                            1669
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/verysmol_llama-v11-KIx2</a>",
                            "\ud83e\udd99 LLaMA",
                            0.06,
                            "28.70 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00484,
                            204.0,
                            288,
                            3215434.0,
                            1.25,
                            205.0,
                            327,
                            1778
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/verysmol_llama-v11-KIx2</a>",
                            "\ud83e\udd99 LLaMA",
                            0.06,
                            "28.70*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00523,
                            186.0,
                            279,
                            3021148.0,
                            1.38,
                            186.0,
                            308,
                            1769
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/verysmol_llama-v11-KIx2</a>",
                            "\ud83e\udd99 LLaMA",
                            0.06,
                            "28.70*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00526,
                            189.0,
                            143,
                            2941176.0,
                            1.36,
                            188.0,
                            169,
                            1629
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/verysmol_llama-v11-KIx2</a>",
                            "\ud83e\udd99 LLaMA",
                            0.06,
                            "28.70*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00528,
                            175.0,
                            143,
                            2710027.0,
                            1.47,
                            174.0,
                            169,
                            1629
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/verysmol_llama-v11-KIx2</a>",
                            "\ud83e\udd99 LLaMA",
                            0.06,
                            "28.70*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00533,
                            185.0,
                            145,
                            2865329.0,
                            1.39,
                            184.0,
                            167,
                            1629
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/verysmol_llama-v11-KIx2</a>",
                            "\ud83e\udd99 LLaMA",
                            0.06,
                            "28.70*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.011,
                            113.0,
                            144,
                            1727115.0,
                            2.26,
                            113.0,
                            169,
                            1629
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/verysmol_llama-v11-KIx2</a>",
                            "\ud83e\udd99 LLaMA",
                            0.06,
                            "28.70*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0114,
                            107.0,
                            144,
                            1718213.0,
                            2.39,
                            107.0,
                            169,
                            1629
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/verysmol_llama-v11-KIx2</a>",
                            "\ud83e\udd99 LLaMA",
                            0.06,
                            "28.70*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0115,
                            139.0,
                            143,
                            2262443.0,
                            1.85,
                            138.0,
                            169,
                            1629
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/verysmol_llama-v11-KIx2</a>",
                            "\ud83e\udd99 LLaMA",
                            0.06,
                            "28.70*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0186,
                            53.2,
                            155,
                            840336.0,
                            4.81,
                            53.2,
                            182,
                            1650
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/verysmol_llama-v11-KIx2</a>",
                            "\ud83e\udd99 LLaMA",
                            0.06,
                            "28.70*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0191,
                            52.6,
                            155,
                            813008.0,
                            4.87,
                            52.6,
                            182,
                            1650
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-169m-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-169m-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            0.17,
                            "28.64 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0126,
                            78.5,
                            381,
                            1210653.0,
                            3.26,
                            78.5,
                            398,
                            1858
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-169m-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-169m-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            0.17,
                            "28.64 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.013,
                            78.5,
                            381,
                            1280409.0,
                            3.26,
                            78.5,
                            400,
                            1860
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-169m-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-169m-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            0.17,
                            "28.64 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0137,
                            80.2,
                            740,
                            1226993.0,
                            3.19,
                            80.3,
                            815,
                            2266
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-169m-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-169m-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            0.17,
                            "28.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0274,
                            45.9,
                            241,
                            719424.0,
                            5.59,
                            45.8,
                            276,
                            1736
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-169m-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-169m-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            0.17,
                            "28.64*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0411,
                            24.5,
                            283,
                            377358.0,
                            10.4,
                            24.6,
                            316,
                            1784
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00391,
                            243.0,
                            126,
                            3937007.0,
                            1.05,
                            244.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00413,
                            218.0,
                            126,
                            3401360.0,
                            1.17,
                            219.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00472,
                            207.0,
                            126,
                            3436426.0,
                            1.23,
                            208.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00492,
                            199.0,
                            186,
                            3267973.0,
                            1.28,
                            200.0,
                            205,
                            1667
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00496,
                            201.0,
                            119,
                            3236245.0,
                            1.27,
                            202.0,
                            136,
                            1597
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00513,
                            195.0,
                            118,
                            3095975.0,
                            1.32,
                            194.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0053,
                            204.0,
                            126,
                            3194888.0,
                            1.26,
                            203.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00532,
                            183.0,
                            118,
                            2865329.0,
                            1.4,
                            183.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00545,
                            207.0,
                            221,
                            3311258.0,
                            1.24,
                            206.0,
                            234,
                            1686
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.00845,
                            133.0,
                            118,
                            2061855.0,
                            1.93,
                            133.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.00881,
                            166.0,
                            118,
                            2695417.0,
                            1.55,
                            165.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.00917,
                            128.0,
                            118,
                            2049180.0,
                            2.01,
                            127.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0125,
                            78.0,
                            121,
                            1226993.0,
                            3.28,
                            78.0,
                            138,
                            1606
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0127,
                            75.9,
                            121,
                            1183431.0,
                            3.37,
                            76.0,
                            138,
                            1606
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00387,
                            241.0,
                            126,
                            4065040.0,
                            1.06,
                            242.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00419,
                            220.0,
                            126,
                            3472222.0,
                            1.16,
                            221.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0047,
                            207.0,
                            126,
                            3472222.0,
                            1.23,
                            208.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00481,
                            204.0,
                            126,
                            3278688.0,
                            1.25,
                            205.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00482,
                            204.0,
                            186,
                            3311258.0,
                            1.25,
                            205.0,
                            205,
                            1667
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00499,
                            198.0,
                            119,
                            3205128.0,
                            1.29,
                            198.0,
                            136,
                            1597
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00516,
                            193.0,
                            118,
                            3076923.0,
                            1.33,
                            192.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00545,
                            209.0,
                            221,
                            3278688.0,
                            1.23,
                            208.0,
                            234,
                            1686
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00545,
                            182.0,
                            118,
                            2881844.0,
                            1.41,
                            182.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.00854,
                            134.0,
                            118,
                            2150537.0,
                            1.91,
                            134.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.00893,
                            167.0,
                            118,
                            2702702.0,
                            1.54,
                            166.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.00917,
                            125.0,
                            118,
                            2008032.0,
                            2.05,
                            125.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0121,
                            79.9,
                            121,
                            1261034.0,
                            3.2,
                            80.0,
                            138,
                            1606
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0128,
                            74.6,
                            121,
                            1201923.0,
                            3.43,
                            74.6,
                            138,
                            1606
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2</a>",
                            "GPT-2",
                            0.14,
                            "28.53 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00695,
                            131.0,
                            328,
                            1934235.0,
                            1.95,
                            131.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2</a>",
                            "GPT-2",
                            0.14,
                            "28.53*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00804,
                            121.0,
                            328,
                            1934235.0,
                            2.11,
                            121.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2</a>",
                            "GPT-2",
                            0.14,
                            "28.53 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00807,
                            120.0,
                            328,
                            1953124.0,
                            2.13,
                            120.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2</a>",
                            "GPT-2",
                            0.14,
                            "28.53 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00827,
                            118.0,
                            328,
                            1776198.0,
                            2.17,
                            118.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2</a>",
                            "GPT-2",
                            0.14,
                            "28.53*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00859,
                            125.0,
                            200,
                            1992031.0,
                            2.05,
                            125.0,
                            270,
                            1732
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2</a>",
                            "GPT-2",
                            0.14,
                            "28.53*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00867,
                            115.0,
                            328,
                            1876172.0,
                            2.23,
                            115.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2</a>",
                            "GPT-2",
                            0.14,
                            "28.53 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00945,
                            120.0,
                            621,
                            1788908.0,
                            2.13,
                            120.0,
                            717,
                            2168
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2</a>",
                            "GPT-2",
                            0.14,
                            "28.53*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0161,
                            98.5,
                            197,
                            1584786.0,
                            2.61,
                            98.1,
                            270,
                            1730
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/gpt_bigcode-santacoder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/gpt_bigcode-santacoder</a>",
                            "\u2b50 StarCoder",
                            1.12,
                            "28.49 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00938,
                            97.3,
                            2329,
                            1344086.0,
                            2.63,
                            97.3,
                            2403,
                            3860
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/gpt_bigcode-santacoder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/gpt_bigcode-santacoder</a>",
                            "\u2b50 StarCoder",
                            1.12,
                            "28.49 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.011,
                            86.4,
                            2329,
                            1256281.0,
                            2.96,
                            86.5,
                            2403,
                            3862
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/gpt_bigcode-santacoder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/gpt_bigcode-santacoder</a>",
                            "\u2b50 StarCoder",
                            1.12,
                            "28.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0115,
                            93.4,
                            1395,
                            1468428.0,
                            2.74,
                            93.4,
                            1430,
                            2891
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/gpt_bigcode-santacoder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/gpt_bigcode-santacoder</a>",
                            "\u2b50 StarCoder",
                            1.12,
                            "28.49 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0117,
                            83.9,
                            2329,
                            1194743.0,
                            3.05,
                            83.9,
                            2403,
                            3862
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/gpt_bigcode-santacoder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/gpt_bigcode-santacoder</a>",
                            "\u2b50 StarCoder",
                            1.12,
                            "28.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0118,
                            91.1,
                            856,
                            1404494.0,
                            2.81,
                            91.1,
                            889,
                            2350
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/gpt_bigcode-santacoder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/gpt_bigcode-santacoder</a>",
                            "\u2b50 StarCoder",
                            1.12,
                            "28.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0144,
                            87.9,
                            838,
                            1293661.0,
                            2.91,
                            88.0,
                            859,
                            2319
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/gpt_bigcode-santacoder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/gpt_bigcode-santacoder</a>",
                            "\u2b50 StarCoder",
                            1.12,
                            "28.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0151,
                            79.2,
                            838,
                            1154734.0,
                            3.24,
                            79.0,
                            859,
                            2319
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/gpt_bigcode-santacoder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/gpt_bigcode-santacoder</a>",
                            "\u2b50 StarCoder",
                            1.12,
                            "28.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0306,
                            67.8,
                            890,
                            1007049.0,
                            3.79,
                            67.5,
                            912,
                            2371
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/gpt_bigcode-santacoder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/gpt_bigcode-santacoder</a>",
                            "\u2b50 StarCoder",
                            1.12,
                            "28.49*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0421,
                            23.8,
                            1312,
                            366300.0,
                            10.7,
                            23.9,
                            1356,
                            2822
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/gpt_bigcode-santacoder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/gpt_bigcode-santacoder</a>",
                            "\u2b50 StarCoder",
                            1.12,
                            "28.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0428,
                            23.4,
                            1311,
                            341296.0,
                            10.9,
                            23.5,
                            1369,
                            2837
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/gpt_bigcode-santacoder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/gpt_bigcode-santacoder</a>",
                            "\u2b50 StarCoder",
                            1.12,
                            "28.49*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.0464,
                            48.7,
                            970,
                            689655.0,
                            5.29,
                            48.4,
                            991,
                            2449
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/gpt_bigcode-santacoder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/gpt_bigcode-santacoder</a>",
                            "\u2b50 StarCoder",
                            1.12,
                            "28.49*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0472,
                            47.0,
                            970,
                            689655.0,
                            5.48,
                            46.7,
                            991,
                            2451
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/gpt_bigcode-santacoder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/gpt_bigcode-santacoder</a>",
                            "\u2b50 StarCoder",
                            1.12,
                            "28.49 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0476,
                            88.5,
                            4628,
                            1104972.0,
                            2.93,
                            87.4,
                            4708,
                            6159
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-126m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-126m</a>",
                            "GPT-2",
                            0.19,
                            "28.45 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00554,
                            155.0,
                            404,
                            2358490.0,
                            1.66,
                            154.0,
                            459,
                            1916
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-126m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-126m</a>",
                            "GPT-2",
                            0.19,
                            "28.45 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00673,
                            137.0,
                            404,
                            2109704.0,
                            1.87,
                            137.0,
                            459,
                            1916
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-126m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-126m</a>",
                            "GPT-2",
                            0.19,
                            "28.45 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00693,
                            139.0,
                            404,
                            2207505.0,
                            1.84,
                            139.0,
                            459,
                            1916
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-126m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-126m</a>",
                            "GPT-2",
                            0.19,
                            "28.45*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0071,
                            136.0,
                            392,
                            2136752.0,
                            1.88,
                            136.0,
                            461,
                            1918
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-126m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-126m</a>",
                            "GPT-2",
                            0.19,
                            "28.45*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00715,
                            135.0,
                            392,
                            2024291.0,
                            1.9,
                            135.0,
                            461,
                            1918
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-126m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-126m</a>",
                            "GPT-2",
                            0.19,
                            "28.45*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00718,
                            143.0,
                            268,
                            2267573.0,
                            1.79,
                            143.0,
                            327,
                            1788
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-126m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-126m</a>",
                            "GPT-2",
                            0.19,
                            "28.45 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00813,
                            133.0,
                            721,
                            2057613.0,
                            1.93,
                            133.0,
                            803,
                            2254
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-126m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-126m</a>",
                            "GPT-2",
                            0.19,
                            "28.45*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0146,
                            109.0,
                            266,
                            1766784.0,
                            2.36,
                            108.0,
                            327,
                            1786
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/Mixtral-GQA-400m-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/Mixtral-GQA-400m-v2</a>",
                            "\u24c2\ufe0f Mixtral",
                            2.01,
                            "28.45 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0246,
                            79.9,
                            4083,
                            1161440.0,
                            3.21,
                            79.8,
                            4112,
                            5572
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/Mixtral-GQA-400m-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/Mixtral-GQA-400m-v2</a>",
                            "\u24c2\ufe0f Mixtral",
                            2.01,
                            "28.45 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0248,
                            78.2,
                            4083,
                            1160092.0,
                            3.28,
                            78.0,
                            4108,
                            5567
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/Mixtral-GQA-400m-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/Mixtral-GQA-400m-v2</a>",
                            "\u24c2\ufe0f Mixtral",
                            2.01,
                            "28.45 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0248,
                            75.9,
                            4083,
                            1118568.0,
                            3.38,
                            75.7,
                            4112,
                            5572
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/Mixtral-GQA-400m-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/Mixtral-GQA-400m-v2</a>",
                            "\u24c2\ufe0f Mixtral",
                            2.01,
                            "28.45*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0343,
                            29.6,
                            2211,
                            480769.0,
                            8.64,
                            29.6,
                            2241,
                            3709
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/Mixtral-GQA-400m-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/Mixtral-GQA-400m-v2</a>",
                            "\u24c2\ufe0f Mixtral",
                            2.01,
                            "28.45*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0361,
                            29.4,
                            2211,
                            454545.0,
                            8.7,
                            29.4,
                            2248,
                            3716
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/Mixtral-GQA-400m-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/Mixtral-GQA-400m-v2</a>",
                            "\u24c2\ufe0f Mixtral",
                            2.01,
                            "28.45*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.0367,
                            54.5,
                            1429,
                            819672.0,
                            4.72,
                            54.2,
                            1568,
                            3028
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/Mixtral-GQA-400m-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/Mixtral-GQA-400m-v2</a>",
                            "\u24c2\ufe0f Mixtral",
                            2.01,
                            "28.45*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0372,
                            54.1,
                            1429,
                            806451.0,
                            4.75,
                            53.9,
                            1568,
                            3028
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/Mixtral-GQA-400m-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/Mixtral-GQA-400m-v2</a>",
                            "\u24c2\ufe0f Mixtral",
                            2.01,
                            "28.45 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0415,
                            84.2,
                            8095,
                            1218026.0,
                            3.07,
                            83.4,
                            8143,
                            9594
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00394,
                            238.0,
                            216,
                            3846153.0,
                            1.07,
                            239.0,
                            236,
                            1696
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00421,
                            220.0,
                            216,
                            3484320.0,
                            1.16,
                            221.0,
                            236,
                            1696
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/budecosystem/boomer-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">budecosystem/boomer-1b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.0,
                            "28.44 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00443,
                            283.0,
                            2229,
                            3144654.0,
                            0.905,
                            283.0,
                            2250,
                            3709
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-28M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-28M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.44 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00444,
                            212.0,
                            182,
                            3412969.0,
                            1.2,
                            213.0,
                            222,
                            1681
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/budecosystem/boomer-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">budecosystem/boomer-1b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.0,
                            "28.44 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00449,
                            268.0,
                            2229,
                            3086419.0,
                            0.956,
                            268.0,
                            2250,
                            3709
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/budecosystem/boomer-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">budecosystem/boomer-1b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.0,
                            "28.44 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00458,
                            278.0,
                            2229,
                            3184713.0,
                            0.921,
                            278.0,
                            2250,
                            3709
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00476,
                            204.0,
                            216,
                            3355704.0,
                            1.25,
                            205.0,
                            236,
                            1696
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00486,
                            201.0,
                            216,
                            3154574.0,
                            1.27,
                            202.0,
                            236,
                            1696
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00492,
                            201.0,
                            321,
                            3205128.0,
                            1.27,
                            202.0,
                            339,
                            1801
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.005,
                            198.0,
                            186,
                            3184713.0,
                            1.29,
                            198.0,
                            201,
                            1662
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00529,
                            181.0,
                            184,
                            2801120.0,
                            1.42,
                            180.0,
                            222,
                            1681
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-28M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-28M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.44 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00536,
                            177.0,
                            182,
                            2808988.0,
                            1.45,
                            177.0,
                            222,
                            1681
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00545,
                            185.0,
                            184,
                            3058103.0,
                            1.39,
                            184.0,
                            222,
                            1681
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/budecosystem/boomer-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">budecosystem/boomer-1b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.0,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00548,
                            265.0,
                            1128,
                            3558718.0,
                            0.968,
                            264.0,
                            1161,
                            2623
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00549,
                            207.0,
                            397,
                            3125000.0,
                            1.24,
                            206.0,
                            425,
                            1876
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/budecosystem/boomer-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">budecosystem/boomer-1b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.0,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00559,
                            274.0,
                            1850,
                            3937007.0,
                            0.938,
                            273.0,
                            1885,
                            3347
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-28M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-28M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.44 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00584,
                            176.0,
                            182,
                            2898550.0,
                            1.46,
                            175.0,
                            222,
                            1681
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-28M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-28M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.44 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00608,
                            171.0,
                            182,
                            2688172.0,
                            1.5,
                            171.0,
                            222,
                            1681
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-28M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-28M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.44 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00613,
                            183.0,
                            320,
                            2688172.0,
                            1.4,
                            183.0,
                            356,
                            1807
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-28M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-28M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0064,
                            170.0,
                            146,
                            2659574.0,
                            1.51,
                            170.0,
                            178,
                            1639
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-28M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-28M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0065,
                            170.0,
                            281,
                            2710027.0,
                            1.51,
                            170.0,
                            316,
                            1778
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-28M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-28M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00674,
                            151.0,
                            144,
                            2272727.0,
                            1.7,
                            151.0,
                            178,
                            1637
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-28M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-28M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00684,
                            163.0,
                            144,
                            2638522.0,
                            1.57,
                            163.0,
                            178,
                            1637
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/budecosystem/boomer-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">budecosystem/boomer-1b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.0,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00827,
                            275.0,
                            1050,
                            3472222.0,
                            0.935,
                            274.0,
                            1088,
                            2548
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.00834,
                            133.0,
                            185,
                            2008032.0,
                            1.93,
                            133.0,
                            222,
                            1681
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.00888,
                            165.0,
                            184,
                            2666666.0,
                            1.56,
                            164.0,
                            201,
                            1660
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.00903,
                            128.0,
                            185,
                            1992031.0,
                            2.01,
                            127.0,
                            222,
                            1681
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/budecosystem/boomer-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">budecosystem/boomer-1b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.0,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00972,
                            252.0,
                            1050,
                            3311258.0,
                            1.02,
                            251.0,
                            1088,
                            2548
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0121,
                            78.5,
                            193,
                            1234567.0,
                            3.26,
                            78.5,
                            213,
                            1681
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/budecosystem/boomer-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">budecosystem/boomer-1b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.0,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0124,
                            78.5,
                            1432,
                            1213592.0,
                            3.26,
                            78.5,
                            1476,
                            2944
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/budecosystem/boomer-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">budecosystem/boomer-1b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.0,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.013,
                            78.0,
                            1432,
                            1144164.0,
                            3.28,
                            78.0,
                            1476,
                            2944
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.013,
                            75.4,
                            193,
                            1135073.0,
                            3.39,
                            75.5,
                            213,
                            1681
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-28M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-28M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.0136,
                            93.8,
                            145,
                            1455604.0,
                            2.73,
                            93.8,
                            178,
                            1637
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-28M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-28M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0137,
                            123.0,
                            144,
                            2040816.0,
                            2.08,
                            123.0,
                            178,
                            1637
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-28M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-28M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0138,
                            91.1,
                            145,
                            1451378.0,
                            2.81,
                            91.1,
                            178,
                            1637
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-28M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-28M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0208,
                            46.8,
                            156,
                            735294.0,
                            5.47,
                            46.8,
                            190,
                            1658
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/budecosystem/boomer-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">budecosystem/boomer-1b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.0,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0215,
                            202.0,
                            1225,
                            2785515.0,
                            1.28,
                            200.0,
                            1277,
                            2736
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-28M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-28M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0216,
                            45.5,
                            156,
                            719424.0,
                            5.62,
                            45.6,
                            190,
                            1658
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/budecosystem/boomer-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">budecosystem/boomer-1b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.0,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0338,
                            165.0,
                            1315,
                            2192982.0,
                            1.58,
                            162.0,
                            1476,
                            2935
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/budecosystem/boomer-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">budecosystem/boomer-1b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.0,
                            "28.44*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.034,
                            158.0,
                            1315,
                            2057613.0,
                            1.64,
                            156.0,
                            1476,
                            2935
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/budecosystem/boomer-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">budecosystem/boomer-1b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.0,
                            "28.44 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0347,
                            270.0,
                            4446,
                            2341920.0,
                            0.978,
                            262.0,
                            4510,
                            5962
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-33M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-33M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.41 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00256,
                            354.0,
                            197,
                            5714285.0,
                            0.723,
                            354.0,
                            230,
                            1690
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-33M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-33M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.41 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00311,
                            313.0,
                            197,
                            4608294.0,
                            0.818,
                            313.0,
                            230,
                            1690
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-33M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-33M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.41 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00332,
                            309.0,
                            197,
                            4926108.0,
                            0.829,
                            309.0,
                            230,
                            1690
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-33M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-33M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.41 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00337,
                            300.0,
                            197,
                            4608294.0,
                            0.853,
                            300.0,
                            230,
                            1690
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-33M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-33M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00359,
                            302.0,
                            360,
                            4807692.0,
                            0.847,
                            302.0,
                            400,
                            1862
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-33M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-33M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0036,
                            301.0,
                            159,
                            4716981.0,
                            0.851,
                            301.0,
                            192,
                            1654
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-33M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-33M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00371,
                            272.0,
                            154,
                            4149377.0,
                            0.942,
                            272.0,
                            205,
                            1665
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-33M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-33M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00375,
                            293.0,
                            154,
                            4587155.0,
                            0.873,
                            293.0,
                            205,
                            1665
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-33M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-33M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.41 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.004,
                            324.0,
                            366,
                            4716981.0,
                            0.79,
                            324.0,
                            419,
                            1870
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-33M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-33M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.41*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.00698,
                            176.0,
                            154,
                            2747252.0,
                            1.46,
                            175.0,
                            192,
                            1652
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-33M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-33M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.00745,
                            228.0,
                            154,
                            3623188.0,
                            1.13,
                            227.0,
                            192,
                            1652
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-33M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-33M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.00749,
                            172.0,
                            155,
                            2645502.0,
                            1.49,
                            172.0,
                            192,
                            1652
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-33M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-33M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.41*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0109,
                            89.5,
                            166,
                            1414427.0,
                            2.86,
                            89.5,
                            199,
                            1667
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-33M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-33M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.41*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0112,
                            88.9,
                            168,
                            1369863.0,
                            2.88,
                            88.9,
                            220,
                            1688
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/SaylorTwift/gpt2_test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SaylorTwift/gpt2_test</a>",
                            "GPT-2",
                            0.14,
                            "28.40 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00696,
                            132.0,
                            328,
                            1988071.0,
                            1.94,
                            132.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/SaylorTwift/gpt2_test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SaylorTwift/gpt2_test</a>",
                            "GPT-2",
                            0.14,
                            "28.40 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00816,
                            120.0,
                            328,
                            1930501.0,
                            2.14,
                            120.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/SaylorTwift/gpt2_test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SaylorTwift/gpt2_test</a>",
                            "GPT-2",
                            0.14,
                            "28.40*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00817,
                            120.0,
                            328,
                            1858736.0,
                            2.13,
                            120.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/SaylorTwift/gpt2_test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SaylorTwift/gpt2_test</a>",
                            "GPT-2",
                            0.14,
                            "28.40 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0083,
                            119.0,
                            328,
                            1792114.0,
                            2.15,
                            119.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/SaylorTwift/gpt2_test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SaylorTwift/gpt2_test</a>",
                            "GPT-2",
                            0.14,
                            "28.40*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00847,
                            120.0,
                            328,
                            1858736.0,
                            2.13,
                            120.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/SaylorTwift/gpt2_test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SaylorTwift/gpt2_test</a>",
                            "GPT-2",
                            0.14,
                            "28.40*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00858,
                            122.0,
                            200,
                            1956947.0,
                            2.1,
                            122.0,
                            270,
                            1732
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/SaylorTwift/gpt2_test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SaylorTwift/gpt2_test</a>",
                            "GPT-2",
                            0.14,
                            "28.40 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00967,
                            118.0,
                            621,
                            1824817.0,
                            2.18,
                            117.0,
                            717,
                            2168
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/SaylorTwift/gpt2_test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SaylorTwift/gpt2_test</a>",
                            "GPT-2",
                            0.14,
                            "28.40*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0157,
                            100.0,
                            197,
                            1589825.0,
                            2.57,
                            99.6,
                            270,
                            1730
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-8M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-8M</a>",
                            "GPT-Neo",
                            0.01,
                            "28.31 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00446,
                            220.0,
                            112,
                            3636363.0,
                            1.16,
                            221.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-8M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-8M</a>",
                            "GPT-Neo",
                            0.01,
                            "28.31 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00531,
                            183.0,
                            112,
                            2881844.0,
                            1.4,
                            183.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-8M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-8M</a>",
                            "GPT-Neo",
                            0.01,
                            "28.31 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00564,
                            186.0,
                            183,
                            2958579.0,
                            1.38,
                            186.0,
                            213,
                            1665
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-8M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-8M</a>",
                            "GPT-Neo",
                            0.01,
                            "28.31 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00583,
                            175.0,
                            112,
                            2739726.0,
                            1.47,
                            174.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-8M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-8M</a>",
                            "GPT-Neo",
                            0.01,
                            "28.31 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00591,
                            178.0,
                            112,
                            2958579.0,
                            1.44,
                            178.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-8M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-8M</a>",
                            "GPT-Neo",
                            0.01,
                            "28.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00631,
                            169.0,
                            170,
                            2724795.0,
                            1.52,
                            168.0,
                            203,
                            1665
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-8M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-8M</a>",
                            "GPT-Neo",
                            0.01,
                            "28.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00635,
                            153.0,
                            102,
                            2427184.0,
                            1.68,
                            152.0,
                            134,
                            1593
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-8M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-8M</a>",
                            "GPT-Neo",
                            0.01,
                            "28.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00647,
                            168.0,
                            103,
                            2590673.0,
                            1.53,
                            167.0,
                            134,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-8M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-8M</a>",
                            "GPT-Neo",
                            0.01,
                            "28.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00651,
                            169.0,
                            102,
                            2659574.0,
                            1.52,
                            168.0,
                            134,
                            1593
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-8M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-8M</a>",
                            "GPT-Neo",
                            0.01,
                            "28.31*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.0129,
                            93.8,
                            102,
                            1483679.0,
                            2.73,
                            93.8,
                            134,
                            1593
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-8M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-8M</a>",
                            "GPT-Neo",
                            0.01,
                            "28.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0137,
                            92.1,
                            102,
                            1477104.0,
                            2.78,
                            92.1,
                            134,
                            1593
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-8M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-8M</a>",
                            "GPT-Neo",
                            0.01,
                            "28.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0139,
                            126.0,
                            102,
                            2044989.0,
                            2.04,
                            125.0,
                            134,
                            1593
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-8M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-8M</a>",
                            "GPT-Neo",
                            0.01,
                            "28.31*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.0209,
                            46.4,
                            105,
                            729927.0,
                            5.52,
                            46.4,
                            138,
                            1606
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-8M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-8M</a>",
                            "GPT-Neo",
                            0.01,
                            "28.31*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.022,
                            45.5,
                            105,
                            714285.0,
                            5.62,
                            45.6,
                            136,
                            1604
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/instructkr/ko-wand-136M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">instructkr/ko-wand-136M</a>",
                            "\u24c2\ufe0f Mistral",
                            0.14,
                            "28.29 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00761,
                            127.0,
                            347,
                            2070393.0,
                            2.02,
                            127.0,
                            387,
                            1847
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/instructkr/ko-wand-136M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">instructkr/ko-wand-136M</a>",
                            "\u24c2\ufe0f Mistral",
                            0.14,
                            "28.29 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0077,
                            128.0,
                            347,
                            1996007.0,
                            2.0,
                            128.0,
                            383,
                            1843
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/instructkr/ko-wand-136M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">instructkr/ko-wand-136M</a>",
                            "\u24c2\ufe0f Mistral",
                            0.14,
                            "28.29 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00784,
                            140.0,
                            613,
                            2237136.0,
                            1.83,
                            140.0,
                            668,
                            2120
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/instructkr/ko-wand-136M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">instructkr/ko-wand-136M</a>",
                            "\u24c2\ufe0f Mistral",
                            0.14,
                            "28.29*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00837,
                            122.0,
                            488,
                            1956947.0,
                            2.1,
                            122.0,
                            517,
                            1979
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/instructkr/ko-wand-136M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">instructkr/ko-wand-136M</a>",
                            "\u24c2\ufe0f Mistral",
                            0.14,
                            "28.29*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00841,
                            125.0,
                            213,
                            1992031.0,
                            2.05,
                            125.0,
                            241,
                            1700
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00389,
                            248.0,
                            126,
                            4000000.0,
                            1.03,
                            249.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00429,
                            220.0,
                            126,
                            3508771.0,
                            1.16,
                            221.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00472,
                            207.0,
                            126,
                            3436426.0,
                            1.23,
                            208.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00476,
                            204.0,
                            126,
                            3236245.0,
                            1.25,
                            205.0,
                            142,
                            1602
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0049,
                            202.0,
                            186,
                            3344481.0,
                            1.26,
                            203.0,
                            205,
                            1667
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00499,
                            206.0,
                            221,
                            3134796.0,
                            1.24,
                            206.0,
                            234,
                            1686
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00499,
                            199.0,
                            119,
                            3205128.0,
                            1.28,
                            200.0,
                            136,
                            1597
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00513,
                            183.0,
                            118,
                            2724795.0,
                            1.4,
                            183.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00521,
                            193.0,
                            118,
                            3086419.0,
                            1.33,
                            192.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.00821,
                            134.0,
                            118,
                            2105263.0,
                            1.92,
                            133.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.00895,
                            158.0,
                            118,
                            2702702.0,
                            1.62,
                            158.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.00931,
                            128.0,
                            118,
                            2036659.0,
                            2.01,
                            127.0,
                            136,
                            1595
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.012,
                            78.9,
                            121,
                            1236093.0,
                            3.24,
                            79.0,
                            138,
                            1606
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0129,
                            75.7,
                            121,
                            1169590.0,
                            3.38,
                            75.7,
                            138,
                            1606
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-3M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-3M</a>",
                            "GPT-Neo",
                            0.0,
                            "28.19 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00452,
                            218.0,
                            87,
                            3546099.0,
                            1.17,
                            219.0,
                            115,
                            1574
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-3M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-3M</a>",
                            "GPT-Neo",
                            0.0,
                            "28.19 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00536,
                            180.0,
                            87,
                            2906976.0,
                            1.43,
                            179.0,
                            115,
                            1574
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-3M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-3M</a>",
                            "GPT-Neo",
                            0.0,
                            "28.19 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00546,
                            185.0,
                            132,
                            2923976.0,
                            1.39,
                            184.0,
                            161,
                            1612
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-3M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-3M</a>",
                            "GPT-Neo",
                            0.0,
                            "28.19 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00582,
                            178.0,
                            87,
                            2976190.0,
                            1.44,
                            178.0,
                            115,
                            1574
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-3M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-3M</a>",
                            "GPT-Neo",
                            0.0,
                            "28.19 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00591,
                            171.0,
                            87,
                            2832861.0,
                            1.5,
                            171.0,
                            115,
                            1574
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-3M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-3M</a>",
                            "GPT-Neo",
                            0.0,
                            "28.19*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00634,
                            169.0,
                            84,
                            2717391.0,
                            1.52,
                            168.0,
                            113,
                            1572
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-3M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-3M</a>",
                            "GPT-Neo",
                            0.0,
                            "28.19*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00643,
                            149.0,
                            84,
                            2450980.0,
                            1.72,
                            149.0,
                            113,
                            1572
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-3M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-3M</a>",
                            "GPT-Neo",
                            0.0,
                            "28.19*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00651,
                            167.0,
                            84,
                            2739726.0,
                            1.54,
                            166.0,
                            113,
                            1574
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-3M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-3M</a>",
                            "GPT-Neo",
                            0.0,
                            "28.19*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.0128,
                            95.1,
                            84,
                            1497005.0,
                            2.69,
                            95.2,
                            113,
                            1572
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-3M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-3M</a>",
                            "GPT-Neo",
                            0.0,
                            "28.19*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0137,
                            92.7,
                            84,
                            1492537.0,
                            2.76,
                            92.8,
                            113,
                            1572
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-3M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-3M</a>",
                            "GPT-Neo",
                            0.0,
                            "28.19*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0142,
                            128.0,
                            84,
                            2087682.0,
                            2.0,
                            128.0,
                            113,
                            1572
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-3M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-3M</a>",
                            "GPT-Neo",
                            0.0,
                            "28.19*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.021,
                            46.4,
                            85,
                            746268.0,
                            5.52,
                            46.4,
                            113,
                            1581
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-3M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-3M</a>",
                            "GPT-Neo",
                            0.0,
                            "28.19*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0212,
                            46.4,
                            85,
                            746268.0,
                            5.52,
                            46.4,
                            113,
                            1581
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/KoRWKV-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/KoRWKV-6B</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            6.53,
                            "28.19 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0296,
                            34.2,
                            13112,
                            427350.0,
                            7.49,
                            34.2,
                            13165,
                            14625
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/KoRWKV-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/KoRWKV-6B</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            6.53,
                            "28.19 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0316,
                            33.9,
                            13112,
                            395256.0,
                            7.55,
                            33.9,
                            13165,
                            14625
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/KoRWKV-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/KoRWKV-6B</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            6.53,
                            "28.19*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0943,
                            10.4,
                            7030,
                            155038.0,
                            24.6,
                            10.4,
                            7048,
                            8514
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/KoRWKV-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/KoRWKV-6B</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            6.53,
                            "28.19 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.211,
                            35.8,
                            26214,
                            347222.0,
                            7.34,
                            34.9,
                            26294,
                            27745
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/beomi/KoRWKV-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/KoRWKV-6B</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            6.53,
                            "28.19*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.25,
                            20.6,
                            4633,
                            265251.0,
                            12.7,
                            20.2,
                            4888,
                            6348
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-81M-tied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-81M-tied</a>",
                            "\ud83e\udd99 LLaMA",
                            0.08,
                            "28.17 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00454,
                            193.0,
                            232,
                            2941176.0,
                            1.32,
                            194.0,
                            274,
                            1734
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-81M-tied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-81M-tied</a>",
                            "\ud83e\udd99 LLaMA",
                            0.08,
                            "28.17 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00459,
                            195.0,
                            232,
                            3144654.0,
                            1.31,
                            195.0,
                            274,
                            1734
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-81M-tied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-81M-tied</a>",
                            "\ud83e\udd99 LLaMA",
                            0.08,
                            "28.17 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.00468,
                            195.0,
                            232,
                            3058103.0,
                            1.31,
                            195.0,
                            274,
                            1734
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-81M-tied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-81M-tied</a>",
                            "\ud83e\udd99 LLaMA",
                            0.08,
                            "28.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00528,
                            172.0,
                            145,
                            2652519.0,
                            1.49,
                            172.0,
                            184,
                            1644
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-81M-tied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-81M-tied</a>",
                            "\ud83e\udd99 LLaMA",
                            0.08,
                            "28.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00531,
                            185.0,
                            351,
                            2949852.0,
                            1.39,
                            184.0,
                            387,
                            1849
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-81M-tied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-81M-tied</a>",
                            "\ud83e\udd99 LLaMA",
                            0.08,
                            "28.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00534,
                            193.0,
                            145,
                            2941176.0,
                            1.33,
                            192.0,
                            186,
                            1646
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-81M-tied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-81M-tied</a>",
                            "\ud83e\udd99 LLaMA",
                            0.08,
                            "28.17 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00537,
                            214.0,
                            387,
                            3215434.0,
                            1.2,
                            213.0,
                            438,
                            1889
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-81M-tied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-81M-tied</a>",
                            "\ud83e\udd99 LLaMA",
                            0.08,
                            "28.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00537,
                            185.0,
                            150,
                            2840909.0,
                            1.39,
                            184.0,
                            180,
                            1642
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-81M-tied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-81M-tied</a>",
                            "\ud83e\udd99 LLaMA",
                            0.08,
                            "28.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0112,
                            112.0,
                            147,
                            1742160.0,
                            2.29,
                            112.0,
                            184,
                            1644
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-81M-tied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-81M-tied</a>",
                            "\ud83e\udd99 LLaMA",
                            0.08,
                            "28.17*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.0116,
                            111.0,
                            147,
                            1715265.0,
                            2.3,
                            111.0,
                            184,
                            1644
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-81M-tied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-81M-tied</a>",
                            "\ud83e\udd99 LLaMA",
                            0.08,
                            "28.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0117,
                            139.0,
                            145,
                            2242152.0,
                            1.84,
                            139.0,
                            184,
                            1644
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-81M-tied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-81M-tied</a>",
                            "\ud83e\udd99 LLaMA",
                            0.08,
                            "28.17*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0181,
                            53.6,
                            171,
                            826446.0,
                            4.78,
                            53.6,
                            207,
                            1675
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-81M-tied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-81M-tied</a>",
                            "\ud83e\udd99 LLaMA",
                            0.08,
                            "28.17*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0183,
                            52.8,
                            171,
                            833333.0,
                            4.85,
                            52.8,
                            207,
                            1675
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-small</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "27.95 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0072,
                            134.0,
                            480,
                            2012072.0,
                            1.92,
                            133.0,
                            532,
                            1992
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-small</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "27.95 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00752,
                            134.0,
                            480,
                            2159827.0,
                            1.91,
                            134.0,
                            553,
                            2013
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-small</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "27.95 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00784,
                            128.0,
                            480,
                            1919385.0,
                            2.0,
                            128.0,
                            553,
                            2013
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-small</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "27.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.00786,
                            132.0,
                            553,
                            2096436.0,
                            1.94,
                            132.0,
                            629,
                            2090
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-small</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "27.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00803,
                            129.0,
                            352,
                            2061855.0,
                            1.99,
                            129.0,
                            421,
                            1883
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-small</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "27.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00843,
                            126.0,
                            347,
                            1945525.0,
                            2.03,
                            126.0,
                            413,
                            1872
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-small</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "27.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00856,
                            115.0,
                            347,
                            1766784.0,
                            2.23,
                            115.0,
                            415,
                            1874
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-small</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "27.95 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.011,
                            135.0,
                            934,
                            2004008.0,
                            1.9,
                            135.0,
                            1035,
                            2487
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-small</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "27.95*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.4bit",
                            0.0149,
                            76.8,
                            350,
                            1176470.0,
                            3.33,
                            76.9,
                            425,
                            1885
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-small</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "27.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0157,
                            106.0,
                            350,
                            1618122.0,
                            2.42,
                            106.0,
                            421,
                            1881
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-small</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "27.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0163,
                            75.7,
                            350,
                            1187648.0,
                            3.39,
                            75.5,
                            425,
                            1885
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-small</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "27.95*",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "BnB.8bit",
                            0.023,
                            41.7,
                            392,
                            653594.0,
                            6.13,
                            41.8,
                            457,
                            1925
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-small</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "27.95*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0238,
                            41.8,
                            393,
                            649350.0,
                            6.12,
                            41.8,
                            457,
                            1925
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-111M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-111M</a>",
                            "GPT-2",
                            0.11,
                            "27.75 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.00514,
                            180.0,
                            332,
                            2710027.0,
                            1.43,
                            179.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-111M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-111M</a>",
                            "GPT-2",
                            0.11,
                            "27.75 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00601,
                            162.0,
                            332,
                            2590673.0,
                            1.58,
                            162.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-111M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-111M</a>",
                            "GPT-2",
                            0.11,
                            "27.75 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.00613,
                            153.0,
                            332,
                            2314814.0,
                            1.68,
                            152.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-111M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-111M</a>",
                            "GPT-2",
                            0.11,
                            "27.75*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00615,
                            158.0,
                            323,
                            2500000.0,
                            1.62,
                            158.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-111M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-111M</a>",
                            "GPT-2",
                            0.11,
                            "27.75*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.00625,
                            158.0,
                            323,
                            2577319.0,
                            1.62,
                            158.0,
                            381,
                            1841
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-111M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-111M</a>",
                            "GPT-2",
                            0.11,
                            "27.75*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0064,
                            167.0,
                            219,
                            2673796.0,
                            1.54,
                            166.0,
                            268,
                            1730
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-111M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-111M</a>",
                            "GPT-2",
                            0.11,
                            "27.75 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00707,
                            157.0,
                            592,
                            2364066.0,
                            1.63,
                            157.0,
                            675,
                            2126
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-111M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-111M</a>",
                            "GPT-2",
                            0.11,
                            "27.75*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0125,
                            131.0,
                            219,
                            2049180.0,
                            1.95,
                            131.0,
                            289,
                            1748
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248m</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "27.73 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.011,
                            90.4,
                            608,
                            1477104.0,
                            2.83,
                            90.5,
                            656,
                            2115
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248m</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "27.73 ",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "None",
                            0.0111,
                            83.9,
                            608,
                            1285347.0,
                            3.05,
                            83.9,
                            656,
                            2115
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248m</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "27.73 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0118,
                            87.9,
                            608,
                            1305483.0,
                            2.91,
                            88.0,
                            660,
                            2120
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248m</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "27.73*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0129,
                            85.9,
                            347,
                            1314060.0,
                            2.98,
                            85.9,
                            373,
                            1834
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248m</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "27.73*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0129,
                            84.4,
                            338,
                            1317523.0,
                            3.03,
                            84.5,
                            371,
                            1830
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248m</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "27.73*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0129,
                            75.7,
                            338,
                            1200480.0,
                            3.38,
                            75.7,
                            371,
                            1830
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248m</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "27.73 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.013,
                            97.0,
                            1153,
                            1416430.0,
                            2.64,
                            97.0,
                            1199,
                            2650
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248m</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "27.73*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV2",
                            0.0134,
                            81.5,
                            616,
                            1362397.0,
                            3.14,
                            81.5,
                            652,
                            2113
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248m</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "27.73*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.4bit",
                            0.0239,
                            51.4,
                            346,
                            781250.0,
                            4.98,
                            51.4,
                            396,
                            1855
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248m</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "27.73*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0241,
                            53.3,
                            346,
                            819672.0,
                            4.8,
                            53.3,
                            396,
                            1855
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248m</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "27.73*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0252,
                            66.4,
                            339,
                            1050420.0,
                            3.87,
                            66.1,
                            373,
                            1832
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248m</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "27.73*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0377,
                            25.6,
                            427,
                            408163.0,
                            10.0,
                            25.6,
                            465,
                            1933
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248m</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "27.73*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0396,
                            25.6,
                            427,
                            384615.0,
                            10.0,
                            25.6,
                            465,
                            1933
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ai-forever/mGPT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ai-forever/mGPT</a>",
                            "GPT-2",
                            0.0,
                            "27.61 ",
                            "pytorch",
                            "float16",
                            "BetterTransformer",
                            "None",
                            0.0135,
                            71.8,
                            3147,
                            980392.0,
                            3.56,
                            71.9,
                            3191,
                            4649
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ai-forever/mGPT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ai-forever/mGPT</a>",
                            "GPT-2",
                            0.0,
                            "27.61 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0156,
                            64.4,
                            3147,
                            909090.0,
                            3.98,
                            64.3,
                            3191,
                            4651
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ai-forever/mGPT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ai-forever/mGPT</a>",
                            "GPT-2",
                            0.0,
                            "27.61 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0159,
                            61.6,
                            3147,
                            884955.0,
                            4.16,
                            61.5,
                            3191,
                            4651
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ai-forever/mGPT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ai-forever/mGPT</a>",
                            "GPT-2",
                            0.0,
                            "27.61*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.0162,
                            66.9,
                            1392,
                            1004016.0,
                            3.83,
                            66.8,
                            1434,
                            2896
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ai-forever/mGPT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ai-forever/mGPT</a>",
                            "GPT-2",
                            0.0,
                            "27.61*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0162,
                            63.3,
                            3147,
                            934579.0,
                            4.05,
                            63.2,
                            3191,
                            4649
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ai-forever/mGPT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ai-forever/mGPT</a>",
                            "GPT-2",
                            0.0,
                            "27.61*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0165,
                            63.0,
                            3147,
                            892857.0,
                            4.07,
                            62.9,
                            3191,
                            4649
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ai-forever/mGPT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ai-forever/mGPT</a>",
                            "GPT-2",
                            0.0,
                            "27.61*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0433,
                            51.2,
                            1360,
                            787401.0,
                            5.02,
                            51.0,
                            1390,
                            2849
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/team-lucid/mptk-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">team-lucid/mptk-1b</a>",
                            "\ud83e\uddf1 MPT",
                            1.31,
                            "20.84 ",
                            "pytorch",
                            "bfloat16",
                            "None",
                            "None",
                            0.0116,
                            87.3,
                            2834,
                            1118568.0,
                            2.93,
                            87.4,
                            2856,
                            4315
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/team-lucid/mptk-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">team-lucid/mptk-1b</a>",
                            "\ud83e\uddf1 MPT",
                            1.31,
                            "20.84 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.012,
                            85.3,
                            2834,
                            1137656.0,
                            3.0,
                            85.3,
                            2856,
                            4315
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/team-lucid/mptk-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">team-lucid/mptk-1b</a>",
                            "\ud83e\uddf1 MPT",
                            1.31,
                            "20.84*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMV",
                            0.0161,
                            84.2,
                            1046,
                            1250000.0,
                            3.05,
                            83.9,
                            1080,
                            2539
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/team-lucid/mptk-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">team-lucid/mptk-1b</a>",
                            "\ud83e\uddf1 MPT",
                            1.31,
                            "20.84*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0174,
                            76.6,
                            1046,
                            1114827.0,
                            3.35,
                            76.4,
                            1080,
                            2539
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/team-lucid/mptk-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">team-lucid/mptk-1b</a>",
                            "\ud83e\uddf1 MPT",
                            1.31,
                            "20.84*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.8bit",
                            0.0434,
                            22.8,
                            1628,
                            338983.0,
                            11.2,
                            22.9,
                            1658,
                            3126
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/team-lucid/mptk-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">team-lucid/mptk-1b</a>",
                            "\ud83e\uddf1 MPT",
                            1.31,
                            "20.84 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0508,
                            93.4,
                            5657,
                            1013171.0,
                            2.78,
                            92.1,
                            5697,
                            7149
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/team-lucid/mptk-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">team-lucid/mptk-1b</a>",
                            "\ud83e\uddf1 MPT",
                            1.31,
                            "20.84*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0533,
                            47.0,
                            1099,
                            675675.0,
                            5.48,
                            46.7,
                            1151,
                            2610
                        ]
                    ],
                    "metadata": null
                },
                "headers": [
                    "Model \ud83e\udd17",
                    "Arch \ud83c\udfdb\ufe0f",
                    "Params (B)",
                    "Open LLM Score (%)",
                    "DType \ud83d\udce5",
                    "Backend \ud83c\udfed",
                    "Optimization \ud83d\udee0\ufe0f",
                    "Quantization \ud83d\udddc\ufe0f",
                    "Prefill Latency (s)",
                    "Decode Throughput (tokens/s)",
                    "Allocated Memory (MB)",
                    "Energy (tokens/kWh)",
                    "E2E Latency (s)",
                    "E2E Throughput (tokens/s)",
                    "Reserved Memory (MB)",
                    "Used Memory (MB)"
                ],
                "row_count": [
                    1,
                    "dynamic"
                ],
                "col_count": [
                    16,
                    "dynamic"
                ],
                "datatype": [
                    "markdown",
                    "markdown",
                    "number",
                    "number",
                    "str",
                    "str",
                    "str",
                    "str",
                    "number",
                    "number",
                    "number",
                    "number",
                    "number",
                    "number",
                    "number",
                    "number"
                ],
                "type": "pandas",
                "latex_delimiters": [
                    {
                        "left": "$$",
                        "right": "$$",
                        "display": true
                    }
                ],
                "show_label": true,
                "height": 500,
                "min_width": 160,
                "visible": true,
                "elem_id": "table",
                "elem_classes": [],
                "wrap": false,
                "line_breaks": true,
                "column_widths": [],
                "name": "dataframe",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "dd8782b182950d6602be53e99ace6aa6",
            "api_info": {
                "properties": {
                    "headers": {
                        "items": {
                            "type": "string"
                        },
                        "title": "Headers",
                        "type": "array"
                    },
                    "data": {
                        "items": {
                            "items": {},
                            "type": "array"
                        },
                        "title": "Data",
                        "type": "array"
                    },
                    "metadata": {
                        "anyOf": [
                            {
                                "additionalProperties": {
                                    "anyOf": [
                                        {
                                            "items": {},
                                            "type": "array"
                                        },
                                        {
                                            "type": "null"
                                        }
                                    ]
                                },
                                "type": "object"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "title": "Metadata"
                    }
                },
                "required": [
                    "headers",
                    "data"
                ],
                "title": "DataframeData",
                "type": "object"
            },
            "example_inputs": {
                "headers": [
                    "a",
                    "b"
                ],
                "data": [
                    [
                        "foo",
                        "bar"
                    ]
                ]
            }
        },
        {
            "id": 38,
            "type": "html",
            "props": {
                "value": "\ud83d\udc46 Hover over the points \ud83d\udc46 for additional information. ",
                "show_label": true,
                "visible": true,
                "elem_id": "text",
                "elem_classes": [],
                "name": "html",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "0ef1a4eade65d710fcd9fc56f6586b0e",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "<p>Hello</p>"
        },
        {
            "id": 39,
            "type": "plot",
            "props": {
                "value": {
                    "type": "plotly",
                    "plot": "{\"data\":[{\"customdata\":[[\"cloudyu\\u002fMixtral_11Bx2_MoE_19B\",\"\u24c2\ufe0f Mixtral\",19.19,\"bfloat16\",\"pytorch\",\"None\",\"None\",74.41,0.116,11.6,38693,22.1,11.6],[\"cloudyu\\u002fMixtral_11Bx2_MoE_19B\",\"\u24c2\ufe0f Mixtral\",19.19,\"float32\",\"pytorch\",\"None\",\"None\",74.41,0.866,11.2,77380,23.6,10.8],[\"cloudyu\\u002fMixtral_7Bx2_MoE\",\"\u24c2\ufe0f Mixtral\",12.88,\"bfloat16\",\"pytorch\",\"None\",\"None\",73.43,0.0777,17.3,26446,14.8,17.3],[\"cloudyu\\u002fMixtral_7Bx2_MoE\",\"\u24c2\ufe0f Mixtral\",12.88,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",73.43,0.0865,17.5,26446,14.7,17.4],[\"cloudyu\\u002fMixtral_7Bx2_MoE\",\"\u24c2\ufe0f Mixtral\",12.88,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",73.43,0.175,6.13,13843,41.8,6.12],[\"cloudyu\\u002fMixtral_7Bx2_MoE\",\"\u24c2\ufe0f Mixtral\",12.88,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",73.43,0.542,11.3,8619,23.1,11.1],[\"cloudyu\\u002fMixtral_7Bx2_MoE\",\"\u24c2\ufe0f Mixtral\",12.88,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",73.43,0.554,11.5,8619,22.8,11.2],[\"cloudyu\\u002fMixtral_7Bx2_MoE\",\"\u24c2\ufe0f Mixtral\",12.88,\"float32\",\"pytorch\",\"None\",\"None\",73.43,0.567,16.7,52884,15.9,16.1],[\"itsliupeng\\u002fMixtral-8x7B-v0.1-top3\",\"\u24c2\ufe0f Mixtral\",46.7,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",69.09,0.227,4.59,47691,55.7,4.6],[\"itsliupeng\\u002fMixtral-8x7B-v0.1-top3\",\"\u24c2\ufe0f Mixtral\",46.7,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",69.09,0.229,4.54,47691,56.4,4.54],[\"itsliupeng\\u002fMixtral-8x7B-v0.1-top3\",\"\u24c2\ufe0f Mixtral\",46.7,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",69.09,0.801,7.97,27662,32.8,7.8],[\"itsliupeng\\u002fMixtral-8x7B-v0.1-top3\",\"\u24c2\ufe0f Mixtral\",46.7,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",69.09,0.809,8.02,27662,32.6,7.85],[\"cloudyu\\u002fMixtral_7Bx4_MOE_24B\",\"\u24c2\ufe0f Mixtral\",24.15,\"bfloat16\",\"pytorch\",\"None\",\"None\",68.85,0.114,15.8,48995,16.2,15.8],[\"cloudyu\\u002fMixtral_7Bx4_MOE_24B\",\"\u24c2\ufe0f Mixtral\",24.15,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",68.85,0.121,15.6,48995,16.4,15.6],[\"cloudyu\\u002fMixtral_7Bx4_MOE_24B\",\"\u24c2\ufe0f Mixtral\",24.15,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",68.85,0.177,5.92,25125,43.3,5.91],[\"cloudyu\\u002fMixtral_7Bx4_MOE_24B\",\"\u24c2\ufe0f Mixtral\",24.15,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",68.85,0.181,5.73,25125,44.7,5.73],[\"cloudyu\\u002fMixtral_7Bx4_MOE_24B\",\"\u24c2\ufe0f Mixtral\",24.15,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",68.85,0.553,10.4,14970,25.1,10.2],[\"cloudyu\\u002fMixtral_7Bx4_MOE_24B\",\"\u24c2\ufe0f Mixtral\",24.15,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",68.85,0.562,10.5,14970,24.9,10.3],[\"mistralai\\u002fMixtral-8x7B-v0.1\",\"\u24c2\ufe0f Mixtral\",46.7,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",68.42,0.185,5.41,47691,47.3,5.41],[\"mistralai\\u002fMixtral-8x7B-v0.1\",\"\u24c2\ufe0f Mixtral\",46.7,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",68.42,0.188,5.57,47691,46.0,5.57],[\"mistralai\\u002fMixtral-8x7B-v0.1\",\"\u24c2\ufe0f Mixtral\",46.7,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",68.42,0.561,9.62,27662,27.1,9.45],[\"mistralai\\u002fMixtral-8x7B-v0.1\",\"\u24c2\ufe0f Mixtral\",46.7,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",68.42,0.569,9.85,27662,26.5,9.66],[\"Walmart-the-bag\\u002fWordWoven-13B\",\"\u24c2\ufe0f Mixtral\",12.88,\"bfloat16\",\"pytorch\",\"None\",\"None\",68.25,0.0779,17.2,26446,14.9,17.2],[\"Walmart-the-bag\\u002fWordWoven-13B\",\"\u24c2\ufe0f Mixtral\",12.88,\"float32\",\"pytorch\",\"None\",\"None\",68.25,0.511,16.8,52884,15.7,16.3],[\"Walmart-the-bag\\u002fInfluxient-4x13B\",\"\u24c2\ufe0f Mixtral\",38.5,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",60.57,0.159,12.5,78429,20.6,12.4],[\"Walmart-the-bag\\u002fInfluxient-4x13B\",\"\u24c2\ufe0f Mixtral\",38.5,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",60.57,0.219,4.8,40198,53.3,4.8],[\"Walmart-the-bag\\u002fInfluxient-4x13B\",\"\u24c2\ufe0f Mixtral\",38.5,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",60.57,0.224,4.74,40205,54.0,4.74],[\"Walmart-the-bag\\u002fInfluxient-4x13B\",\"\u24c2\ufe0f Mixtral\",38.5,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",60.57,0.857,8.17,23181,32.1,7.98],[\"Walmart-the-bag\\u002fInfluxient-4x13B\",\"\u24c2\ufe0f Mixtral\",38.5,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",60.57,0.868,8.17,23182,32.1,7.98],[\"chargoddard\\u002fSmolLlamix-8x101M-take2\",\"\u24c2\ufe0f Mixtral\",0.4,\"float16\",\"pytorch\",\"None\",\"None\",29.35,0.025,78.5,894,3.28,78.0],[\"chargoddard\\u002fSmolLlamix-8x101M-take2\",\"\u24c2\ufe0f Mixtral\",0.4,\"float32\",\"pytorch\",\"None\",\"None\",29.35,0.0251,83.1,1697,3.1,82.6],[\"chargoddard\\u002fSmolLlamix-8x101M-take2\",\"\u24c2\ufe0f Mixtral\",0.4,\"bfloat16\",\"pytorch\",\"None\",\"None\",29.35,0.0251,77.5,894,3.32,77.1],[\"chargoddard\\u002fSmolLlamix-8x101M\",\"\u24c2\ufe0f Mixtral\",0.4,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.98,0.0248,77.0,895,3.33,76.9],[\"chargoddard\\u002fSmolLlamix-8x101M\",\"\u24c2\ufe0f Mixtral\",0.4,\"float32\",\"pytorch\",\"None\",\"None\",28.98,0.0252,82.5,1697,3.12,82.1],[\"chargoddard\\u002fSmolLlamix-8x101M\",\"\u24c2\ufe0f Mixtral\",0.4,\"float16\",\"pytorch\",\"None\",\"None\",28.98,0.0255,76.8,895,3.35,76.4],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",2.01,\"float16\",\"pytorch\",\"None\",\"None\",28.45,0.0246,79.9,4083,3.21,79.8],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",2.01,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.45,0.0248,78.2,4083,3.28,78.0],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",2.01,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.45,0.0248,75.9,4083,3.38,75.7],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",2.01,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",28.45,0.0343,29.6,2211,8.64,29.6],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",2.01,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.45,0.0361,29.4,2211,8.7,29.4],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",2.01,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",28.45,0.0367,54.5,1429,4.72,54.2],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",2.01,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.45,0.0372,54.1,1429,4.75,53.9],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",2.01,\"float32\",\"pytorch\",\"None\",\"None\",28.45,0.0415,84.2,8095,3.07,83.4]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\u24c2\ufe0f Mixtral\",\"marker\":{\"color\":\"#FD3216\",\"size\":[38693,77380,26446,26446,13843,8619,8619,52884,47691,47691,27662,27662,48995,48995,25125,25125,14970,14970,47691,47691,27662,27662,26446,52884,78429,40198,40205,23181,23182,894,1697,894,895,1697,895,4083,4083,4083,2211,2211,1429,1429,8095],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\u24c2\ufe0f Mixtral\",\"showlegend\":true,\"x\":[22.1,23.6,14.8,14.7,41.8,23.1,22.8,15.9,55.7,56.4,32.8,32.6,16.2,16.4,43.3,44.7,25.1,24.9,47.3,46.0,27.1,26.5,14.9,15.7,20.6,53.3,54.0,32.1,32.1,3.28,3.1,3.32,3.33,3.12,3.35,3.21,3.28,3.38,8.64,8.7,4.72,4.75,3.07],\"xaxis\":\"x\",\"y\":[74.41,74.41,73.43,73.43,73.43,73.43,73.43,73.43,69.09,69.09,69.09,69.09,68.85,68.85,68.85,68.85,68.85,68.85,68.42,68.42,68.42,68.42,68.25,68.25,60.57,60.57,60.57,60.57,60.57,29.35,29.35,29.35,28.98,28.98,28.98,28.45,28.45,28.45,28.45,28.45,28.45,28.45,28.45],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",7.24,\"bfloat16\",\"pytorch\",\"None\",\"None\",72.25,0.0354,34.7,15171,7.39,34.6],[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",72.25,0.0413,34.0,15171,7.53,34.0],[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",72.25,0.0451,32.2,4963,7.98,32.1],[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",72.25,0.0461,32.4,5908,7.92,32.3],[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",72.25,0.0659,33.5,4838,7.68,33.3],[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",72.25,0.0766,30.4,4838,8.47,30.2],[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",72.25,0.101,9.88,8197,25.9,9.88],[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",72.25,0.103,9.7,8197,26.4,9.7],[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",7.24,\"float32\",\"pytorch\",\"None\",\"None\",72.25,0.264,37.2,30334,7.12,36.0],[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",72.25,0.289,20.7,5428,12.6,20.3],[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",72.25,0.299,20.7,5428,12.6,20.3],[\"Delcos\\u002fStarling-LM-11B-alpha\",\"\u24c2\ufe0f Mistral\",11.39,\"bfloat16\",\"pytorch\",\"None\",\"None\",63.66,0.0568,22.2,23216,11.6,22.1],[\"Delcos\\u002fStarling-LM-11B-alpha\",\"\u24c2\ufe0f Mistral\",11.39,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",63.66,0.0656,21.6,23216,11.9,21.5],[\"Delcos\\u002fStarling-LM-11B-alpha\",\"\u24c2\ufe0f Mistral\",11.39,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",63.66,0.0709,21.1,6878,12.2,21.0],[\"Delcos\\u002fStarling-LM-11B-alpha\",\"\u24c2\ufe0f Mistral\",11.39,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",63.66,0.0722,20.9,7825,12.3,20.8],[\"Delcos\\u002fStarling-LM-11B-alpha\",\"\u24c2\ufe0f Mistral\",11.39,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",63.66,0.105,21.4,6748,12.0,21.3],[\"Delcos\\u002fStarling-LM-11B-alpha\",\"\u24c2\ufe0f Mistral\",11.39,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",63.66,0.122,19.8,6748,13.0,19.7],[\"Delcos\\u002fStarling-LM-11B-alpha\",\"\u24c2\ufe0f Mistral\",11.39,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",63.66,0.165,6.13,12101,41.8,6.12],[\"Delcos\\u002fStarling-LM-11B-alpha\",\"\u24c2\ufe0f Mistral\",11.39,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",63.66,0.165,6.13,12101,41.8,6.12],[\"Delcos\\u002fStarling-LM-11B-alpha\",\"\u24c2\ufe0f Mistral\",11.39,\"float32\",\"pytorch\",\"None\",\"None\",63.66,0.419,24.3,46424,10.9,23.5],[\"Delcos\\u002fStarling-LM-11B-alpha\",\"\u24c2\ufe0f Mistral\",11.39,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",63.66,0.459,13.1,7468,19.9,12.9],[\"Delcos\\u002fStarling-LM-11B-alpha\",\"\u24c2\ufe0f Mistral\",11.39,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",63.66,0.475,13.0,7468,20.1,12.7],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",7.24,\"bfloat16\",\"pytorch\",\"None\",\"None\",60.97,0.0355,34.9,15171,7.34,34.9],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",60.97,0.0414,34.1,15171,7.52,34.0],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",60.97,0.0454,31.8,4963,8.08,31.7],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",60.97,0.046,32.8,5908,7.82,32.7],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",60.97,0.0673,32.9,4838,7.83,32.7],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",60.97,0.077,31.1,4838,8.29,30.9],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",60.97,0.102,9.85,8197,26.0,9.85],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",60.97,0.104,9.77,8197,26.2,9.77],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",7.24,\"float32\",\"pytorch\",\"None\",\"None\",60.97,0.265,38.5,30334,6.89,37.2],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",60.97,0.29,20.9,5428,12.5,20.5],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",60.97,0.299,20.2,5428,12.9,19.8],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",7.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",58.05,0.0363,34.5,15225,7.43,34.5],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",7.0,\"float16\",\"pytorch\",\"None\",\"None\",58.05,0.0368,34.4,15225,7.45,34.4],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",58.05,0.0423,33.7,15225,7.6,33.7],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",58.05,0.0464,32.1,5018,8.0,32.0],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",58.05,0.0466,32.3,5962,7.94,32.2],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",58.05,0.0687,34.0,4893,7.56,33.9],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",58.05,0.102,10.0,8252,25.6,10.0],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",58.05,0.104,9.81,8252,26.1,9.81],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",7.0,\"float32\",\"pytorch\",\"None\",\"None\",58.05,0.264,38.1,30440,6.96,36.8],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",58.05,0.295,20.6,5483,12.7,20.2],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",58.05,0.3,20.6,5483,12.7,20.2],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",7.24,\"bfloat16\",\"pytorch\",\"None\",\"None\",52.59,0.0355,35.1,14701,7.31,35.0],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",52.59,0.041,34.2,14701,7.5,34.1],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",52.59,0.0446,32.1,4494,7.98,32.1],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",52.59,0.0458,33.0,5438,7.78,32.9],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",52.59,0.0669,33.5,4368,7.68,33.3],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",52.59,0.0767,30.2,4368,8.51,30.1],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",52.59,0.102,9.73,7727,26.3,9.73],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",52.59,0.103,9.81,7727,26.1,9.81],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",7.24,\"float32\",\"pytorch\",\"None\",\"None\",52.59,0.275,36.8,29395,7.19,35.6],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",52.59,0.29,20.9,4959,12.5,20.5],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",52.59,0.299,20.7,4958,12.6,20.3],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",7.96,\"bfloat16\",\"pytorch\",\"None\",\"None\",51.64,0.0377,34.2,16692,7.49,34.2],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",7.96,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",51.64,0.0428,33.5,16692,7.66,33.4],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",7.96,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",51.64,0.0464,32.4,6485,7.91,32.4],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",7.96,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",51.64,0.0468,32.5,7429,7.89,32.4],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",7.96,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",51.64,0.0671,34.3,6360,7.5,34.1],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",7.96,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",51.64,0.078,29.6,6360,8.69,29.5],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",7.96,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",51.64,0.104,9.85,9718,26.0,9.85],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",7.96,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",51.64,0.105,9.85,9718,26.0,9.85],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",7.96,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",51.64,0.183,24.5,6491,10.6,24.2],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",7.96,\"float32\",\"pytorch\",\"None\",\"None\",51.64,0.276,37.2,33224,7.14,35.9],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",7.96,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",51.64,0.29,20.7,6871,12.6,20.3],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",7.96,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",51.64,0.302,20.4,6871,12.8,20.0],[\"Locutusque\\u002fTinyMistral-248M-v2\",\"\u24c2\ufe0f Mistral\",0.25,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.78,0.0113,87.3,608,2.93,87.4],[\"Locutusque\\u002fTinyMistral-248M-v2\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"None\",\"None\",28.78,0.0114,88.2,608,2.9,88.3],[\"Locutusque\\u002fTinyMistral-248M-v2\",\"\u24c2\ufe0f Mistral\",0.25,\"float32\",\"pytorch\",\"None\",\"None\",28.78,0.0128,95.5,1153,2.68,95.5],[\"Locutusque\\u002fTinyMistral-248M-v2\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.78,0.0128,85.9,338,2.98,85.9],[\"Locutusque\\u002fTinyMistral-248M-v2\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.78,0.013,76.6,338,3.34,76.6],[\"instructkr\\u002fko-wand-136M\",\"\u24c2\ufe0f Mistral\",0.14,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.29,0.00761,127.0,347,2.02,127.0],[\"instructkr\\u002fko-wand-136M\",\"\u24c2\ufe0f Mistral\",0.14,\"float16\",\"pytorch\",\"None\",\"None\",28.29,0.0077,128.0,347,2.0,128.0],[\"instructkr\\u002fko-wand-136M\",\"\u24c2\ufe0f Mistral\",0.14,\"float32\",\"pytorch\",\"None\",\"None\",28.29,0.00784,140.0,613,1.83,140.0],[\"instructkr\\u002fko-wand-136M\",\"\u24c2\ufe0f Mistral\",0.14,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.29,0.00837,122.0,488,2.1,122.0],[\"instructkr\\u002fko-wand-136M\",\"\u24c2\ufe0f Mistral\",0.14,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.29,0.00841,125.0,213,2.05,125.0],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"None\",\"None\",27.73,0.011,90.4,608,2.83,90.5],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",27.73,0.0111,83.9,608,3.05,83.9],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",0.25,\"bfloat16\",\"pytorch\",\"None\",\"None\",27.73,0.0118,87.9,608,2.91,88.0],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",27.73,0.0129,85.9,347,2.98,85.9],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",27.73,0.0129,84.4,338,3.03,84.5],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",27.73,0.0129,75.7,338,3.38,75.7],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",0.25,\"float32\",\"pytorch\",\"None\",\"None\",27.73,0.013,97.0,1153,2.64,97.0],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",27.73,0.0134,81.5,616,3.14,81.5],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",27.73,0.0239,51.4,346,4.98,51.4],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",27.73,0.0241,53.3,346,4.8,53.3],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",27.73,0.0252,66.4,339,3.87,66.1],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",27.73,0.0377,25.6,427,10.0,25.6],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",27.73,0.0396,25.6,427,10.0,25.6]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\u24c2\ufe0f Mistral\",\"marker\":{\"color\":\"#00FE35\",\"size\":[15171,15171,4963,5908,4838,4838,8197,8197,30334,5428,5428,23216,23216,6878,7825,6748,6748,12101,12101,46424,7468,7468,15171,15171,4963,5908,4838,4838,8197,8197,30334,5428,5428,15225,15225,15225,5018,5962,4893,8252,8252,30440,5483,5483,14701,14701,4494,5438,4368,4368,7727,7727,29395,4959,4958,16692,16692,6485,7429,6360,6360,9718,9718,6491,33224,6871,6871,608,608,1153,338,338,347,347,613,488,213,608,608,608,347,338,338,1153,616,346,346,339,427,427],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\u24c2\ufe0f Mistral\",\"showlegend\":true,\"x\":[7.39,7.53,7.98,7.92,7.68,8.47,25.9,26.4,7.12,12.6,12.6,11.6,11.9,12.2,12.3,12.0,13.0,41.8,41.8,10.9,19.9,20.1,7.34,7.52,8.08,7.82,7.83,8.29,26.0,26.2,6.89,12.5,12.9,7.43,7.45,7.6,8.0,7.94,7.56,25.6,26.1,6.96,12.7,12.7,7.31,7.5,7.98,7.78,7.68,8.51,26.3,26.1,7.19,12.5,12.6,7.49,7.66,7.91,7.89,7.5,8.69,26.0,26.0,10.6,7.14,12.6,12.8,2.93,2.9,2.68,2.98,3.34,2.02,2.0,1.83,2.1,2.05,2.83,3.05,2.91,2.98,3.03,3.38,2.64,3.14,4.98,4.8,3.87,10.0,10.0],\"xaxis\":\"x\",\"y\":[72.25,72.25,72.25,72.25,72.25,72.25,72.25,72.25,72.25,72.25,72.25,63.66,63.66,63.66,63.66,63.66,63.66,63.66,63.66,63.66,63.66,63.66,60.97,60.97,60.97,60.97,60.97,60.97,60.97,60.97,60.97,60.97,60.97,58.05,58.05,58.05,58.05,58.05,58.05,58.05,58.05,58.05,58.05,58.05,52.59,52.59,52.59,52.59,52.59,52.59,52.59,52.59,52.59,52.59,52.59,51.64,51.64,51.64,51.64,51.64,51.64,51.64,51.64,51.64,51.64,51.64,51.64,28.78,28.78,28.78,28.78,28.78,28.29,28.29,28.29,28.29,28.29,27.73,27.73,27.73,27.73,27.73,27.73,27.73,27.73,27.73,27.73,27.73,27.73,27.73],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"chargoddard\\u002fYi-34B-Llama\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",70.95,0.134,18.5,69165,13.9,18.4],[\"chargoddard\\u002fYi-34B-Llama\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",70.95,0.163,19.6,19971,13.2,19.4],[\"chargoddard\\u002fYi-34B-Llama\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",70.95,0.164,19.6,21328,13.2,19.4],[\"chargoddard\\u002fYi-34B-Llama\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",70.95,0.185,5.41,35851,47.3,5.41],[\"chargoddard\\u002fYi-34B-Llama\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",70.95,0.187,5.47,35836,46.8,5.47],[\"chargoddard\\u002fYi-34B-Llama\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",70.95,0.256,20.2,19654,12.9,19.8],[\"chargoddard\\u002fYi-34B-Llama\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",70.95,0.32,18.3,19654,14.2,18.0],[\"chargoddard\\u002fYi-34B-Llama\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",70.95,1.24,11.5,21798,23.4,10.9],[\"chargoddard\\u002fYi-34B-Llama\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",70.95,1.25,11.8,21798,22.9,11.2],[\"01-ai\\u002fYi-34B-200K\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",70.81,0.134,18.2,75184,14.1,18.2],[\"01-ai\\u002fYi-34B-200K\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",70.81,0.163,19.3,25989,13.4,19.1],[\"01-ai\\u002fYi-34B-200K\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",70.81,0.164,19.6,27347,13.2,19.4],[\"01-ai\\u002fYi-34B-200K\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",70.81,0.186,5.45,41854,47.0,5.45],[\"01-ai\\u002fYi-34B-200K\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",70.81,0.189,5.43,41869,47.2,5.42],[\"01-ai\\u002fYi-34B-200K\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",70.81,0.254,20.7,25673,12.6,20.3],[\"01-ai\\u002fYi-34B-200K\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",70.81,0.313,18.2,25673,14.3,17.9],[\"01-ai\\u002fYi-34B-200K\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",70.81,1.24,11.5,27816,23.4,10.9],[\"01-ai\\u002fYi-34B-200K\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",70.81,1.25,11.8,27816,22.9,11.2],[\"01-ai\\u002fYi-34B\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",69.42,0.134,18.5,69165,13.9,18.4],[\"01-ai\\u002fYi-34B\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",69.42,0.164,19.2,21328,13.5,19.0],[\"01-ai\\u002fYi-34B\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",69.42,0.166,19.6,19971,13.2,19.4],[\"01-ai\\u002fYi-34B\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",69.42,0.182,5.39,35836,47.5,5.39],[\"01-ai\\u002fYi-34B\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",69.42,0.184,5.37,35851,47.7,5.37],[\"01-ai\\u002fYi-34B\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",69.42,0.254,20.2,19654,12.9,19.8],[\"01-ai\\u002fYi-34B\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",69.42,0.314,18.1,19654,14.4,17.8],[\"01-ai\\u002fYi-34B\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",69.42,1.24,11.5,21798,23.3,11.0],[\"01-ai\\u002fYi-34B\",\"\ud83e\udd99 LLaMA\",34.39,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",69.42,1.25,11.7,21798,23.0,11.1],[\"deepseek-ai\\u002fdeepseek-llm-67b-base\",\"\ud83e\udd99 LLaMA\",67.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",69.38,0.295,3.38,69753,75.7,3.38],[\"deepseek-ai\\u002fdeepseek-llm-67b-base\",\"\ud83e\udd99 LLaMA\",67.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",69.38,0.296,3.4,69737,75.4,3.4],[\"deepseek-ai\\u002fdeepseek-llm-67b-base\",\"\ud83e\udd99 LLaMA\",67.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",69.38,0.308,12.3,40008,21.1,12.1],[\"deepseek-ai\\u002fdeepseek-llm-67b-base\",\"\ud83e\udd99 LLaMA\",67.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",69.38,0.312,12.1,38539,21.3,12.0],[\"deepseek-ai\\u002fdeepseek-llm-67b-base\",\"\ud83e\udd99 LLaMA\",67.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",69.38,0.591,11.5,38138,22.8,11.2],[\"deepseek-ai\\u002fdeepseek-llm-67b-base\",\"\ud83e\udd99 LLaMA\",67.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",69.38,2.57,6.39,41781,42.5,6.02],[\"deepseek-ai\\u002fdeepseek-llm-67b-base\",\"\ud83e\udd99 LLaMA\",67.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",69.38,2.58,6.27,41781,43.3,5.91],[\"meta-llama\\u002fLlama-2-70b-hf\",\"\ud83e\udd99 LLaMA\",68.98,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",67.87,0.244,4.0,70040,64.0,4.0],[\"meta-llama\\u002fLlama-2-70b-hf\",\"\ud83e\udd99 LLaMA\",68.98,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",67.87,0.31,14.6,39553,17.8,14.4],[\"meta-llama\\u002fLlama-2-70b-hf\",\"\ud83e\udd99 LLaMA\",68.98,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",67.87,0.315,13.2,37649,19.6,13.1],[\"meta-llama\\u002fLlama-2-70b-hf\",\"\ud83e\udd99 LLaMA\",68.98,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",67.87,0.497,15.4,37141,17.1,15.0],[\"meta-llama\\u002fLlama-2-70b-hf\",\"\ud83e\udd99 LLaMA\",68.98,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",67.87,0.602,13.9,37141,19.0,13.5],[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",10.73,\"bfloat16\",\"pytorch\",\"None\",\"None\",66.04,0.0497,25.6,21781,10.0,25.6],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",10.86,\"bfloat16\",\"pytorch\",\"None\",\"None\",66.04,0.0498,25.2,22045,10.1,25.3],[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",10.73,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",66.04,0.0509,26.6,21773,9.65,26.5],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",10.86,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",66.04,0.051,26.1,22037,9.83,26.0],[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",10.73,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",66.04,0.0628,24.5,6411,10.5,24.4],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",10.86,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",66.04,0.0643,24.3,6675,10.6,24.2],[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",10.73,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",66.04,0.0664,24.3,7358,10.6,24.2],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",10.86,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",66.04,0.0667,24.8,7622,10.4,24.6],[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",10.73,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",66.04,0.0957,25.2,6282,10.2,25.1],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",10.86,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",66.04,0.096,25.2,6546,10.2,25.1],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",10.86,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",66.04,0.112,22.6,6546,11.4,22.5],[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",10.73,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",66.04,0.113,22.4,6282,11.5,22.3],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",10.86,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",66.04,0.143,6.87,11577,37.2,6.88],[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",10.73,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",66.04,0.144,6.84,11312,37.4,6.84],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",10.86,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",66.04,0.148,6.71,11585,38.1,6.72],[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",10.73,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",66.04,0.149,6.76,11320,37.8,6.77],[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",10.73,\"float32\",\"pytorch\",\"None\",\"None\",66.04,0.39,26.5,43555,10.0,25.6],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",10.86,\"float32\",\"pytorch\",\"None\",\"None\",66.04,0.39,26.5,44079,10.0,25.6],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",10.86,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",66.04,0.429,15.1,7245,17.3,14.8],[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",10.73,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",66.04,0.43,14.7,6980,17.8,14.4],[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",10.73,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",66.04,0.434,14.7,6980,17.7,14.5],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",10.86,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",66.04,0.439,14.7,7245,17.7,14.5],[\"huggyllama\\u002fllama-65b\",\"\ud83e\udd99 LLaMA\",65.29,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",62.79,0.239,4.14,68608,61.8,4.14],[\"huggyllama\\u002fllama-65b\",\"\ud83e\udd99 LLaMA\",65.29,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",62.79,0.241,4.04,68608,63.3,4.04],[\"huggyllama\\u002fllama-65b\",\"\ud83e\udd99 LLaMA\",65.29,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",62.79,0.296,16.3,39328,15.9,16.1],[\"huggyllama\\u002fllama-65b\",\"\ud83e\udd99 LLaMA\",65.29,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",62.79,0.296,13.7,37862,18.9,13.5],[\"huggyllama\\u002fllama-65b\",\"\ud83e\udd99 LLaMA\",65.29,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",62.79,0.575,15.2,37465,17.4,14.7],[\"huggyllama\\u002fllama-65b\",\"\ud83e\udd99 LLaMA\",65.29,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",62.79,2.5,6.57,40249,41.3,6.2],[\"huggyllama\\u002fllama-65b\",\"\ud83e\udd99 LLaMA\",65.29,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",62.79,2.5,6.57,40249,41.3,6.2],[\"huggingface\\u002fllama-65b\",\"\ud83e\udd99 LLaMA\",65.29,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",61.19,0.239,4.09,68608,62.5,4.1],[\"huggingface\\u002fllama-65b\",\"\ud83e\udd99 LLaMA\",65.29,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",61.19,0.295,15.8,39328,16.4,15.6],[\"huggingface\\u002fllama-65b\",\"\ud83e\udd99 LLaMA\",65.29,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",61.19,0.295,13.7,37862,18.9,13.5],[\"huggingface\\u002fllama-65b\",\"\ud83e\udd99 LLaMA\",65.29,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",61.19,0.577,14.9,37465,17.7,14.5],[\"huggingface\\u002fllama-30b\",\"\ud83e\udd99 LLaMA\",32.53,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",56.94,0.153,21.6,19768,12.0,21.3],[\"huggingface\\u002fllama-30b\",\"\ud83e\udd99 LLaMA\",32.53,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",56.94,0.155,21.6,20957,12.0,21.3],[\"huggingface\\u002fllama-30b\",\"\ud83e\udd99 LLaMA\",32.53,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",56.94,0.178,5.57,34766,46.0,5.57],[\"huggingface\\u002fllama-30b\",\"\ud83e\udd99 LLaMA\",32.53,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",56.94,0.241,22.0,19528,11.8,21.7],[\"huggingface\\u002fllama-30b\",\"\ud83e\udd99 LLaMA\",32.53,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",56.94,0.302,19.8,19494,13.2,19.4],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",6.06,\"bfloat16\",\"pytorch\",\"None\",\"None\",56.69,0.0275,39.8,15525,6.44,39.8],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",56.69,0.029,39.8,15525,6.43,39.8],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"None\",56.69,0.0312,38.5,15525,6.66,38.4],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",56.69,0.0359,36.6,7476,7.01,36.5],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",56.69,0.0379,36.2,8202,7.09,36.1],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",56.69,0.0548,37.8,7378,6.8,37.6],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",56.69,0.0627,34.3,7378,7.49,34.2],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",56.69,0.0965,10.4,10093,24.6,10.4],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",56.69,0.101,10.1,10093,25.3,10.1],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",56.69,0.143,25.2,7524,10.2,25.1],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",6.06,\"float32\",\"pytorch\",\"None\",\"None\",56.69,0.212,41.5,30980,6.35,40.3],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",56.69,0.235,22.0,7811,11.8,21.7],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",56.69,0.238,22.6,7811,11.5,22.3],[\"meta-llama\\u002fLlama-2-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"bfloat16\",\"pytorch\",\"None\",\"None\",55.69,0.0505,33.6,27089,7.65,33.5],[\"meta-llama\\u002fLlama-2-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",55.69,0.0667,31.9,8420,8.06,31.8],[\"meta-llama\\u002fLlama-2-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",55.69,0.0681,32.1,9334,8.02,31.9],[\"meta-llama\\u002fLlama-2-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",55.69,0.105,32.6,8267,7.93,32.3],[\"meta-llama\\u002fLlama-2-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",55.69,0.119,8.36,14514,30.6,8.37],[\"meta-llama\\u002fLlama-2-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",55.69,0.132,28.8,8267,8.98,28.5],[\"meta-llama\\u002fLlama-2-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",55.69,0.307,23.8,8279,11.0,23.3],[\"meta-llama\\u002fLlama-2-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float32\",\"pytorch\",\"None\",\"None\",55.69,0.454,24.5,53918,10.9,23.5],[\"codellama\\u002fCodeLlama-34b-hf\",\"\ud83e\udd99 LLaMA\",33.74,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",55.28,0.241,24.8,18941,10.5,24.4],[\"codellama\\u002fCodeLlama-34b-hf\",\"\ud83e\udd99 LLaMA\",33.74,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",55.28,0.305,23.2,18937,11.3,22.7],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",6.06,\"bfloat16\",\"pytorch\",\"None\",\"None\",54.08,0.0276,39.1,12315,6.55,39.1],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",54.08,0.0284,39.5,12315,6.49,39.4],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"None\",54.08,0.0298,38.0,12315,6.74,38.0],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",54.08,0.0368,36.8,4266,6.97,36.7],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",54.08,0.0379,37.1,4992,6.92,37.0],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",54.08,0.0553,38.8,4169,6.63,38.6],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",54.08,0.0646,33.6,4168,7.64,33.5],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",54.08,0.0956,10.4,6883,24.6,10.4],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",54.08,0.0996,10.0,6883,25.5,10.0],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",54.08,0.144,27.3,4314,9.49,27.0],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",6.06,\"float32\",\"pytorch\",\"None\",\"None\",54.08,0.213,41.3,24537,6.39,40.1],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",54.08,0.238,22.6,4602,11.5,22.3],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",6.06,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",54.08,0.239,22.0,4602,11.8,21.7],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",13.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",53.67,0.0499,34.8,27089,7.37,34.7],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",53.67,0.0517,31.8,27089,8.08,31.7],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",53.67,0.0675,31.8,8420,8.1,31.6],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",53.67,0.0685,31.7,9334,8.11,31.6],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",53.67,0.105,33.4,8267,7.74,33.1],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",53.67,0.119,8.28,14514,30.9,8.28],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",53.67,0.122,8.25,14514,31.0,8.26],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",53.67,0.132,29.2,8267,8.86,28.9],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",53.67,0.307,23.0,8279,11.4,22.5],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",13.0,\"float32\",\"pytorch\",\"None\",\"None\",53.67,0.452,24.5,53918,10.9,23.5],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",53.67,0.503,18.2,8754,14.5,17.7],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",53.67,0.51,18.9,8754,14.0,18.3],[\"TigerResearch\\u002ftigerbot-13b-base\",\"\ud83e\udd99 LLaMA\",13.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",53.42,0.0654,20.4,27641,12.6,20.3],[\"TigerResearch\\u002ftigerbot-13b-base\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",53.42,0.0695,19.6,27641,13.1,19.5],[\"TigerResearch\\u002ftigerbot-13b-base\",\"\ud83e\udd99 LLaMA\",13.0,\"float32\",\"pytorch\",\"None\",\"None\",53.42,0.507,21.1,55017,12.6,20.3],[\"TencentARC\\u002fLLaMA-Pro-8B\",\"\ud83e\udd99 LLaMA\",8.36,\"bfloat16\",\"pytorch\",\"None\",\"None\",51.67,0.0373,33.9,17479,7.56,33.9],[\"TencentARC\\u002fLLaMA-Pro-8B\",\"\ud83e\udd99 LLaMA\",8.36,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",51.67,0.077,33.7,5558,7.64,33.5],[\"TencentARC\\u002fLLaMA-Pro-8B\",\"\ud83e\udd99 LLaMA\",8.36,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",51.67,0.0886,29.4,5557,8.77,29.2],[\"TencentARC\\u002fLLaMA-Pro-8B\",\"\ud83e\udd99 LLaMA\",8.36,\"float32\",\"pytorch\",\"None\",\"None\",51.67,0.298,33.4,34944,7.94,32.2],[\"huggingface\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"bfloat16\",\"pytorch\",\"None\",\"None\",51.36,0.051,33.8,27047,7.6,33.7],[\"huggingface\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",51.36,0.0673,31.4,8379,8.19,31.3],[\"huggingface\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",51.36,0.0683,32.3,9292,7.96,32.2],[\"huggingface\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",51.36,0.105,33.0,8225,7.83,32.7],[\"huggingface\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",51.36,0.122,7.99,14472,32.0,8.0],[\"huggingface\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",51.36,0.132,28.8,8225,8.97,28.5],[\"huggingface\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",51.36,0.307,23.4,8237,11.2,22.9],[\"huggingface\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float32\",\"pytorch\",\"None\",\"None\",51.36,0.453,24.5,53834,10.9,23.5],[\"huggyllama\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"bfloat16\",\"pytorch\",\"None\",\"None\",51.33,0.0498,34.6,27047,7.41,34.5],[\"huggyllama\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",51.33,0.052,32.6,27047,7.87,32.5],[\"huggyllama\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",51.33,0.0665,31.4,8379,8.18,31.3],[\"huggyllama\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",51.33,0.0685,32.2,9292,7.98,32.1],[\"huggyllama\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",51.33,0.105,32.1,8225,8.05,31.8],[\"huggyllama\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",51.33,0.119,8.31,14472,30.8,8.31],[\"huggyllama\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",51.33,0.122,7.16,14472,35.7,7.17],[\"huggyllama\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",51.33,0.132,29.3,8225,8.82,29.0],[\"huggyllama\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float32\",\"pytorch\",\"None\",\"None\",51.33,0.454,24.5,53834,10.9,23.5],[\"huggyllama\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",51.33,0.5,18.9,8713,14.0,18.3],[\"huggyllama\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",51.33,0.515,18.5,8713,14.3,17.9],[\"chargoddard\\u002fllama-2-26b-trenchcoat-stack\",\"\ud83e\udd99 LLaMA\",25.7,\"bfloat16\",\"pytorch\",\"None\",\"None\",51.13,0.1,17.3,53514,14.8,17.3],[\"chargoddard\\u002fllama-2-26b-trenchcoat-stack\",\"\ud83e\udd99 LLaMA\",25.7,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",51.13,0.105,15.6,53514,16.4,15.6],[\"chargoddard\\u002fllama-2-26b-trenchcoat-stack\",\"\ud83e\udd99 LLaMA\",25.7,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",51.13,0.134,16.0,16955,16.0,16.0],[\"chargoddard\\u002fllama-2-26b-trenchcoat-stack\",\"\ud83e\udd99 LLaMA\",25.7,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",51.13,0.134,15.7,16035,16.3,15.7],[\"chargoddard\\u002fllama-2-26b-trenchcoat-stack\",\"\ud83e\udd99 LLaMA\",25.7,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",51.13,0.211,16.3,15869,15.8,16.2],[\"chargoddard\\u002fllama-2-26b-trenchcoat-stack\",\"\ud83e\udd99 LLaMA\",25.7,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",51.13,0.242,4.13,28365,62.0,4.13],[\"chargoddard\\u002fllama-2-26b-trenchcoat-stack\",\"\ud83e\udd99 LLaMA\",25.7,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",51.13,0.242,4.03,28365,63.4,4.04],[\"chargoddard\\u002fllama-2-26b-trenchcoat-stack\",\"\ud83e\udd99 LLaMA\",25.7,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",51.13,0.265,14.9,15869,17.4,14.7],[\"chargoddard\\u002fllama-2-26b-trenchcoat-stack\",\"\ud83e\udd99 LLaMA\",25.7,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",51.13,0.995,9.44,16846,28.0,9.14],[\"chargoddard\\u002fllama-2-26b-trenchcoat-stack\",\"\ud83e\udd99 LLaMA\",25.7,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",51.13,1.0,9.24,16846,28.6,8.95],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"bfloat16\",\"pytorch\",\"None\",\"None\",50.97,0.0311,41.5,14089,6.18,41.4],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"None\",50.97,0.0322,42.0,14089,6.1,42.0],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",50.97,0.0398,40.2,4649,6.38,40.1],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",50.97,0.041,39.8,5375,6.45,39.7],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",50.97,0.0632,40.2,4553,6.4,40.0],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",50.97,0.0721,36.5,4552,7.05,36.3],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",50.97,0.0975,10.2,7719,25.0,10.2],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",50.97,0.167,28.9,4561,8.99,28.5],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float32\",\"pytorch\",\"None\",\"None\",50.97,0.238,41.6,28167,6.37,40.2],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",6.18,\"bfloat16\",\"pytorch\",\"None\",\"None\",50.27,0.0276,38.1,12542,6.72,38.1],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",6.18,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",50.27,0.0286,39.5,12542,6.48,39.5],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",6.18,\"float16\",\"pytorch\",\"None\",\"None\",50.27,0.0293,38.1,12542,6.73,38.0],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",6.18,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",50.27,0.036,36.4,4493,7.05,36.3],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",6.18,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",50.27,0.0383,35.8,5219,7.16,35.8],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",6.18,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",50.27,0.0548,37.4,4397,6.87,37.3],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",6.18,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",50.27,0.063,34.0,4396,7.55,33.9],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",6.18,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",50.27,0.097,10.3,7111,24.9,10.3],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",6.18,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",50.27,0.105,9.73,7111,26.3,9.73],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",6.18,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",50.27,0.148,27.4,4520,9.46,27.1],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",6.18,\"float32\",\"pytorch\",\"None\",\"None\",50.27,0.214,41.1,24949,6.42,39.9],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",6.18,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",50.27,0.235,22.8,4807,11.4,22.5],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",6.18,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",50.27,0.238,21.8,4807,11.9,21.5],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",13.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",49.5,0.0518,33.8,27415,7.6,33.7],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",49.5,0.0539,32.0,27415,8.01,32.0],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",49.5,0.0681,31.6,8747,8.15,31.4],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",49.5,0.0701,32.1,9660,8.01,32.0],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",49.5,0.107,32.9,8593,7.85,32.6],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",49.5,0.122,8.25,14841,31.0,8.26],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",49.5,0.124,8.04,14841,31.8,8.05],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",49.5,0.133,29.7,8593,8.72,29.4],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",49.5,0.308,23.6,8605,11.1,23.1],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",13.0,\"float32\",\"pytorch\",\"None\",\"None\",49.5,0.455,24.3,54569,11.0,23.3],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",49.5,0.498,18.8,9081,14.1,18.2],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",49.5,0.504,18.1,9081,14.6,17.5],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"bfloat16\",\"pytorch\",\"None\",\"None\",48.64,0.0301,42.4,14056,6.05,42.3],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",48.64,0.0319,41.1,14056,6.24,41.0],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"None\",48.64,0.0321,42.9,14056,5.97,42.9],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",48.64,0.0397,40.5,4616,6.34,40.4],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",48.64,0.0408,39.7,5342,6.46,39.6],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",48.64,0.0629,41.3,4519,6.24,41.0],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",48.64,0.071,36.0,4519,7.15,35.8],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",48.64,0.0953,10.3,7686,24.8,10.3],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",48.64,0.0964,10.3,7686,24.8,10.3],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",48.64,0.165,28.7,4527,9.03,28.3],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float32\",\"pytorch\",\"None\",\"None\",48.64,0.238,41.8,28100,6.34,40.4],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",48.64,0.271,23.6,4795,11.1,23.1],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",48.64,0.273,22.8,4795,11.5,22.3],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",7.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",47.93,0.0303,42.5,14056,6.03,42.5],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",47.93,0.0319,39.0,14056,6.57,39.0],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",47.93,0.0398,39.1,4616,6.57,39.0],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",47.93,0.041,39.7,5342,6.46,39.6],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",47.93,0.0613,39.6,4519,6.5,39.4],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",47.93,0.0716,37.0,4519,6.97,36.7],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",47.93,0.0933,10.5,7686,24.4,10.5],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",47.93,0.0955,10.3,7686,24.8,10.3],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",7.0,\"float32\",\"pytorch\",\"None\",\"None\",47.93,0.238,41.7,28100,6.35,40.3],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",47.93,0.268,23.4,4795,11.2,22.9],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",47.93,0.268,23.0,4796,11.4,22.5],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",0.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",47.64,0.031,43.3,14653,5.92,43.2],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",47.64,0.0326,42.6,14653,6.01,42.6],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",47.64,0.0329,40.5,14653,6.33,40.4],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",47.64,0.0405,39.2,5213,6.55,39.1],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",47.64,0.0418,38.9,5939,6.59,38.8],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",47.64,0.0619,38.8,5117,6.63,38.6],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",47.64,0.0714,36.6,5116,7.03,36.4],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",47.64,0.0956,10.5,8283,24.4,10.5],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",47.64,0.0991,10.1,8283,25.4,10.1],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",47.64,0.169,29.2,5124,8.89,28.8],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",47.64,0.242,41.0,29295,6.46,39.6],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",47.64,0.268,23.2,5393,11.3,22.7],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",47.64,0.27,22.8,5393,11.5,22.3],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",13.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",47.26,0.0504,33.5,27047,7.66,33.4],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",47.26,0.0522,32.5,27047,7.9,32.4],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",47.26,0.0661,31.3,8379,8.22,31.1],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",47.26,0.0679,31.5,9292,8.16,31.4],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",47.26,0.105,32.9,8225,7.85,32.6],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",47.26,0.12,8.25,14472,31.0,8.26],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",47.26,0.126,8.02,14472,31.9,8.03],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",47.26,0.132,29.1,8225,8.89,28.8],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",47.26,0.307,23.2,8237,11.3,22.7],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",13.0,\"float32\",\"pytorch\",\"None\",\"None\",47.26,0.451,24.5,53834,10.9,23.5],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",47.26,0.498,18.9,8713,14.0,18.3],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",13.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",47.26,0.507,18.5,8713,14.3,17.9],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",7.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",47.09,0.0301,42.4,14056,6.05,42.3],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",47.09,0.0323,41.4,14056,6.19,41.4],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",47.09,0.0407,39.7,4616,6.47,39.6],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",47.09,0.0408,38.0,5342,6.75,37.9],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",47.09,0.0617,41.2,4519,6.25,41.0],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",47.09,0.0735,35.6,4519,7.24,35.4],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",47.09,0.0936,10.5,7686,24.3,10.5],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",47.09,0.0966,10.5,7686,24.4,10.5],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",47.09,0.168,28.7,4527,9.04,28.3],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",7.0,\"float32\",\"pytorch\",\"None\",\"None\",47.09,0.238,41.8,28100,6.34,40.4],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",47.09,0.268,23.0,4796,11.4,22.5],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",47.09,0.273,23.4,4795,11.2,22.9],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",6.87,\"bfloat16\",\"pytorch\",\"None\",\"None\",46.64,0.0309,43.1,14354,5.95,43.0],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",6.87,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",46.64,0.0325,40.7,14354,6.29,40.7],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",6.87,\"float16\",\"pytorch\",\"None\",\"None\",46.64,0.0327,41.5,14354,6.18,41.4],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",6.87,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",46.64,0.0399,39.4,4914,6.52,39.3],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",6.87,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",46.64,0.0417,39.5,5640,6.49,39.4],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",6.87,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",46.64,0.0619,41.1,4818,6.26,40.9],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",6.87,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",46.64,0.0725,35.6,4817,7.23,35.4],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",6.87,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",46.64,0.0951,10.3,7984,24.8,10.3],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",6.87,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",46.64,0.0985,9.96,7984,25.7,9.96],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",6.87,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",46.64,0.168,28.8,4825,9.02,28.4],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",6.87,\"float32\",\"pytorch\",\"None\",\"None\",46.64,0.239,41.5,28696,6.39,40.1],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",6.87,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",46.64,0.268,23.2,5094,11.3,22.7],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",6.87,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",46.64,0.274,22.8,5094,11.5,22.3],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"None\",45.65,0.0325,42.2,14056,6.07,42.2],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",45.65,0.0402,38.1,4616,6.73,38.0],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",45.65,0.0409,39.5,5342,6.5,39.4],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",45.65,0.0617,40.2,4519,6.41,39.9],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",45.65,0.0728,36.3,4519,7.09,36.1],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",45.65,0.095,10.5,7686,24.5,10.4],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",45.65,0.166,28.6,4527,9.1,28.1],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",6.74,\"float32\",\"pytorch\",\"None\",\"None\",45.65,0.238,41.7,28100,6.36,40.3],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",6.61,\"bfloat16\",\"pytorch\",\"None\",\"None\",45.62,0.0302,44.0,14056,5.82,44.0],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",6.61,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",45.62,0.0318,40.5,14056,6.33,40.4],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",6.61,\"float16\",\"pytorch\",\"None\",\"None\",45.62,0.032,42.4,14056,6.04,42.4],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",6.61,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",45.62,0.0399,39.2,4616,6.54,39.1],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",6.61,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",45.62,0.0408,39.7,5342,6.46,39.6],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",6.61,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",45.62,0.0611,41.5,4519,6.2,41.3],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",6.61,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",45.62,0.0708,37.3,4519,6.91,37.0],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",6.61,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",45.62,0.0971,10.4,7686,24.7,10.4],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",6.61,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",45.62,0.0989,10.2,7686,25.1,10.2],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",6.61,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",45.62,0.166,28.8,4527,9.02,28.4],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",6.61,\"float32\",\"pytorch\",\"None\",\"None\",45.62,0.237,41.8,28100,6.34,40.4],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",6.61,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",45.62,0.268,22.8,4796,11.5,22.3],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",6.61,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",45.62,0.269,23.4,4795,11.2,22.9],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",3.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",44.75,0.0181,54.0,6401,4.74,54.0],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"None\",44.75,0.0184,55.0,6401,4.66,54.9],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",44.75,0.0187,51.9,6401,4.93,51.9],[\"KnutJaegersberg\\u002fQwen-1_8B-Llamafied\",\"\ud83e\udd99 LLaMA\",1.84,\"float16\",\"pytorch\",\"None\",\"None\",44.75,0.0196,51.9,4106,4.93,51.9],[\"KnutJaegersberg\\u002fQwen-1_8B-Llamafied\",\"\ud83e\udd99 LLaMA\",1.84,\"bfloat16\",\"pytorch\",\"None\",\"None\",44.75,0.0196,51.6,4106,4.96,51.6],[\"KnutJaegersberg\\u002fQwen-1_8B-Llamafied\",\"\ud83e\udd99 LLaMA\",1.84,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",44.75,0.0205,52.7,2666,4.86,52.7],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",44.75,0.0213,52.6,2971,4.87,52.6],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",44.75,0.0215,50.4,2432,5.08,50.4],[\"KnutJaegersberg\\u002fQwen-1_8B-Llamafied\",\"\ud83e\udd99 LLaMA\",1.84,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",44.75,0.0217,50.7,2279,5.05,50.7],[\"KnutJaegersberg\\u002fQwen-1_8B-Llamafied\",\"\ud83e\udd99 LLaMA\",1.84,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",44.75,0.023,45.8,2278,5.59,45.8],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",44.75,0.0313,52.8,2377,4.86,52.7],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",44.75,0.0351,49.0,2377,5.24,48.9],[\"KnutJaegersberg\\u002fQwen-1_8B-Llamafied\",\"\ud83e\udd99 LLaMA\",1.84,\"float32\",\"pytorch\",\"None\",\"None\",44.75,0.0602,54.8,8039,4.71,54.4],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",44.75,0.0716,13.6,3687,18.8,13.6],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",44.75,0.0733,13.7,3687,18.7,13.7],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",44.75,0.0755,38.6,2384,6.69,38.3],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",3.0,\"float32\",\"pytorch\",\"None\",\"None\",44.75,0.111,60.6,12793,4.32,59.3],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",44.75,0.125,30.2,2513,8.57,29.9],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",44.75,0.128,29.1,2513,8.88,28.8],[\"abhinand\\u002ftamil-llama-7b-base-v0.1\",\"\ud83e\udd99 LLaMA\",7.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",44.52,0.0317,42.9,14351,5.97,42.9],[\"abhinand\\u002ftamil-llama-7b-base-v0.1\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",44.52,0.0332,40.7,14351,6.29,40.7],[\"abhinand\\u002ftamil-llama-7b-base-v0.1\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",44.52,0.0412,39.7,4911,6.47,39.6],[\"abhinand\\u002ftamil-llama-7b-base-v0.1\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",44.52,0.0427,39.5,5637,6.5,39.4],[\"abhinand\\u002ftamil-llama-7b-base-v0.1\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",44.52,0.0633,41.1,4815,6.26,40.9],[\"abhinand\\u002ftamil-llama-7b-base-v0.1\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",44.52,0.0728,35.2,4814,7.31,35.0],[\"abhinand\\u002ftamil-llama-7b-base-v0.1\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",44.52,0.0937,10.7,7981,24.0,10.7],[\"abhinand\\u002ftamil-llama-7b-base-v0.1\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",44.52,0.1,10.2,7981,25.0,10.2],[\"abhinand\\u002ftamil-llama-7b-base-v0.1\",\"\ud83e\udd99 LLaMA\",7.0,\"float32\",\"pytorch\",\"None\",\"None\",44.52,0.239,41.5,28691,6.39,40.1],[\"abhinand\\u002ftamil-llama-7b-base-v0.1\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",44.52,0.269,23.0,5091,11.4,22.5],[\"abhinand\\u002ftamil-llama-7b-base-v0.1\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",44.52,0.27,23.6,5091,11.1,23.1],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",7.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",44.26,0.0303,42.2,14056,6.07,42.2],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",44.26,0.0321,40.0,14056,6.4,40.0],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"None\",44.26,0.0328,42.5,14056,6.03,42.5],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",44.26,0.04,37.4,4616,6.85,37.4],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",44.26,0.0409,39.4,5342,6.52,39.3],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",44.26,0.0619,39.9,4519,6.45,39.7],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",44.26,0.0714,37.1,4519,6.94,36.9],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",44.26,0.0965,10.4,7686,24.6,10.4],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",44.26,0.0994,10.2,7686,25.2,10.2],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",44.26,0.167,28.7,4527,9.04,28.3],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",7.0,\"float32\",\"pytorch\",\"None\",\"None\",44.26,0.237,41.7,28100,6.35,40.3],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",44.26,0.272,23.6,4795,11.1,23.1],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",44.26,0.273,22.8,4796,11.5,22.3],[\"codellama\\u002fCodeLlama-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"bfloat16\",\"pytorch\",\"None\",\"None\",43.35,0.0499,33.8,27341,7.59,33.7],[\"codellama\\u002fCodeLlama-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",43.35,0.0525,31.9,27341,8.05,31.8],[\"codellama\\u002fCodeLlama-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",43.35,0.0667,32.5,8672,7.92,32.3],[\"codellama\\u002fCodeLlama-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",43.35,0.0697,31.9,9586,8.07,31.7],[\"codellama\\u002fCodeLlama-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",43.35,0.105,33.2,8519,7.79,32.9],[\"codellama\\u002fCodeLlama-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",43.35,0.119,8.39,14766,30.5,8.39],[\"codellama\\u002fCodeLlama-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",43.35,0.119,8.28,14766,30.9,8.28],[\"codellama\\u002fCodeLlama-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",43.35,0.135,29.3,8519,8.82,29.0],[\"codellama\\u002fCodeLlama-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float32\",\"pytorch\",\"None\",\"None\",43.35,0.454,24.5,54421,10.9,23.5],[\"codellama\\u002fCodeLlama-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",43.35,0.496,19.0,9005,13.9,18.4],[\"codellama\\u002fCodeLlama-13b-hf\",\"\ud83e\udd99 LLaMA\",13.02,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",43.35,0.503,18.2,9005,14.5,17.7],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",7.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",43.27,0.0306,41.8,15101,6.13,41.8],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",43.27,0.0325,40.7,15101,6.3,40.6],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",43.27,0.0411,39.4,5660,6.51,39.3],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",43.27,0.0413,39.1,6386,6.57,39.0],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",43.27,0.062,41.1,5564,6.27,40.8],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",43.27,0.0723,36.6,5563,7.04,36.4],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",43.27,0.0941,10.5,8731,24.5,10.4],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",43.27,0.0991,10.2,8731,25.0,10.2],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",43.27,0.168,29.0,5571,8.96,28.6],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",7.0,\"float32\",\"pytorch\",\"None\",\"None\",43.27,0.241,41.1,30189,6.45,39.7],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",43.27,0.268,22.8,5841,11.5,22.3],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",43.27,0.269,23.0,5841,11.4,22.5],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",7.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",42.31,0.0301,42.9,14056,5.97,42.9],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",42.31,0.0318,40.9,14056,6.27,40.8],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"None\",42.31,0.0334,42.6,14056,6.01,42.6],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",42.31,0.0397,40.0,4616,6.42,39.9],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",42.31,0.0411,39.0,5342,6.58,38.9],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",42.31,0.0614,41.5,4519,6.2,41.3],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",42.31,0.0724,37.0,4519,6.97,36.7],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",42.31,0.0954,10.4,7686,24.7,10.4],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",42.31,0.0986,10.2,7686,25.1,10.2],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",42.31,0.166,28.7,4527,9.06,28.3],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",7.0,\"float32\",\"pytorch\",\"None\",\"None\",42.31,0.238,41.7,28100,6.36,40.3],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",42.31,0.268,23.6,4796,11.1,23.1],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",42.31,0.271,22.8,4795,11.5,22.3],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",3.02,\"bfloat16\",\"pytorch\",\"None\",\"None\",41.44,0.0181,53.9,6401,4.75,53.9],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",3.02,\"float16\",\"pytorch\",\"None\",\"None\",41.44,0.0185,53.2,6401,4.81,53.2],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",3.02,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",41.44,0.0187,51.6,6401,4.96,51.6],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",3.02,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",41.44,0.0206,52.3,2432,4.9,52.2],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",3.02,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",41.44,0.021,52.3,2971,4.9,52.2],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",3.02,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",41.44,0.0309,55.9,2377,4.59,55.8],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",3.02,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",41.44,0.035,49.4,2377,5.19,49.3],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",3.02,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",41.44,0.0722,13.5,3687,19.0,13.5],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",3.02,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",41.44,0.0726,13.7,3687,18.7,13.7],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",3.02,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",41.44,0.0755,38.9,2384,6.64,38.6],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",3.02,\"float32\",\"pytorch\",\"None\",\"None\",41.44,0.111,61.3,12793,4.27,60.0],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",3.02,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",41.44,0.122,30.5,2513,8.49,30.2],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",3.02,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",41.44,0.124,30.3,2513,8.53,30.0],[\"NucleusAI\\u002fnucleus-22B-token-500B\",\"\ud83e\udd99 LLaMA\",21.83,\"bfloat16\",\"pytorch\",\"None\",\"None\",41.33,0.0769,28.1,44887,9.16,27.9],[\"NucleusAI\\u002fnucleus-22B-token-500B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",41.33,0.0865,28.0,44887,9.21,27.8],[\"NucleusAI\\u002fnucleus-22B-token-500B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",41.33,0.102,31.6,13546,8.16,31.4],[\"NucleusAI\\u002fnucleus-22B-token-500B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",41.33,0.103,32.0,14729,8.08,31.7],[\"NucleusAI\\u002fnucleus-22B-token-500B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",41.33,0.119,8.31,23464,30.8,8.31],[\"NucleusAI\\u002fnucleus-22B-token-500B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",41.33,0.121,8.31,23464,30.8,8.31],[\"NucleusAI\\u002fnucleus-22B-token-500B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",41.33,0.161,32.7,13306,7.96,32.2],[\"NucleusAI\\u002fnucleus-22B-token-500B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",41.33,0.201,29.5,13283,8.83,29.0],[\"NucleusAI\\u002fnucleus-22B-token-500B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",41.33,0.818,17.7,14203,15.2,16.8],[\"NucleusAI\\u002fnucleus-22B-token-500B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",41.33,0.824,17.8,14203,15.1,17.0],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",40.97,0.0314,44.1,14056,5.81,44.1],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",0.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",40.97,0.0315,41.7,14056,6.14,41.7],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",40.97,0.0316,41.0,14056,6.25,41.0],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",40.97,0.0407,39.8,4616,6.44,39.8],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",40.97,0.0408,39.9,5342,6.43,39.8],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",40.97,0.0614,40.7,4519,6.33,40.4],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",40.97,0.071,36.6,4519,7.03,36.4],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",40.97,0.0958,10.4,7686,24.6,10.4],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",40.97,0.0973,10.3,7686,24.9,10.3],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",40.97,0.163,29.0,4527,8.95,28.6],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",40.97,0.24,41.7,28100,6.35,40.3],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",40.97,0.263,23.0,4796,11.4,22.5],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",40.97,0.27,23.2,4796,11.3,22.7],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",40.28,0.0221,43.8,7283,5.84,43.8],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",3.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",40.28,0.0225,47.7,7280,5.37,47.7],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"None\",40.28,0.023,46.4,7280,5.51,46.5],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",40.28,0.0813,12.6,4035,20.3,12.6],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",40.28,0.0821,12.3,4039,20.8,12.3],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",3.0,\"float32\",\"pytorch\",\"None\",\"None\",40.28,0.13,57.4,14582,4.57,56.0],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",40.28,0.147,26.3,2669,9.85,26.0],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",40.28,0.149,27.0,2666,9.61,26.6],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"bfloat16\",\"pytorch\",\"None\",\"None\",39.81,0.0301,43.1,14291,5.94,43.1],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",39.81,0.0321,40.7,14291,6.3,40.6],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"None\",39.81,0.0329,43.4,14291,5.91,43.3],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",39.81,0.0405,39.1,4851,6.56,39.0],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",39.81,0.0409,39.6,5577,6.48,39.5],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",39.81,0.0612,40.1,4755,6.42,39.9],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",39.81,0.0712,37.0,4754,6.97,36.7],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",39.81,0.0949,10.5,7921,24.3,10.5],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",39.81,0.0988,10.3,7921,24.9,10.3],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",39.81,0.167,29.4,4762,8.83,29.0],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float32\",\"pytorch\",\"None\",\"None\",39.81,0.238,41.7,28570,6.35,40.3],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",39.81,0.267,23.2,5031,11.3,22.7],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",6.74,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",39.81,0.271,23.0,5031,11.4,22.5],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",38.26,0.0225,42.9,7283,5.96,43.0],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"None\",38.26,0.023,46.8,7280,5.47,46.8],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",3.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",38.26,0.0231,47.0,7280,5.44,47.1],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",38.26,0.0825,12.1,4039,21.1,12.1],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",38.26,0.0828,12.4,4035,20.7,12.4],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",3.0,\"float32\",\"pytorch\",\"None\",\"None\",38.26,0.13,57.0,14582,4.6,55.7],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",38.26,0.147,26.7,2666,9.71,26.4],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",38.26,0.147,26.5,2669,9.79,26.1],[\"Devio\\u002ftest-22B\",\"\ud83e\udd99 LLaMA\",21.83,\"bfloat16\",\"pytorch\",\"None\",\"None\",37.71,0.077,28.1,44887,9.17,27.9],[\"Devio\\u002ftest-22B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",37.71,0.0848,28.0,44887,9.2,27.8],[\"Devio\\u002ftest-22B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",37.71,0.102,30.8,13546,8.39,30.5],[\"Devio\\u002ftest-22B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",37.71,0.103,32.1,14729,8.05,31.8],[\"Devio\\u002ftest-22B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",37.71,0.119,8.33,23464,30.7,8.34],[\"Devio\\u002ftest-22B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",37.71,0.123,7.97,23464,32.1,7.98],[\"Devio\\u002ftest-22B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",37.71,0.161,32.7,13306,7.97,32.1],[\"Devio\\u002ftest-22B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",37.71,0.202,29.0,13283,9.0,28.4],[\"Devio\\u002ftest-22B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",37.71,0.818,17.6,14203,15.3,16.7],[\"Devio\\u002ftest-22B\",\"\ud83e\udd99 LLaMA\",21.83,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",37.71,0.825,17.7,14203,15.2,16.8],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",36.42,0.0171,57.2,2279,4.48,57.1],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",1.1,\"bfloat16\",\"pytorch\",\"None\",\"None\",36.42,0.0179,53.6,2279,4.78,53.6],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"None\",36.42,0.0181,53.9,2279,4.75,53.9],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",36.42,0.0195,48.4,851,5.29,48.4],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",36.42,0.0202,51.1,1248,5.01,51.1],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",36.42,0.0202,50.1,852,5.11,50.1],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",36.42,0.0206,50.8,877,5.04,50.8],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",1.1,\"float32\",\"pytorch\",\"None\",\"None\",36.42,0.0432,56.8,4492,4.53,56.5],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",36.42,0.0443,37.0,869,6.94,36.9],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",36.42,0.0483,32.2,924,7.97,32.1],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",36.42,0.0492,31.4,924,8.18,31.3],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",36.42,0.0651,15.2,1380,16.9,15.1],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",36.42,0.0701,14.3,1380,17.9,14.3],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",1.3,\"bfloat16\",\"pytorch\",\"None\",\"None\",35.71,0.018,56.2,2991,4.56,56.1],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",1.3,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",35.71,0.0182,53.6,2991,4.78,53.6],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",1.3,\"float16\",\"pytorch\",\"None\",\"None\",35.71,0.0185,54.8,2991,4.67,54.8],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",1.3,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",35.71,0.0201,55.0,1157,4.66,54.9],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",1.3,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",35.71,0.0206,52.8,1544,4.85,52.8],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",1.3,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",35.71,0.0206,52.4,1182,4.89,52.4],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",1.3,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",35.71,0.0211,48.1,1156,5.32,48.1],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",1.3,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",35.71,0.0463,38.4,1159,6.69,38.3],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",1.3,\"float32\",\"pytorch\",\"None\",\"None\",35.71,0.0483,60.0,5969,4.3,59.5],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",1.3,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",35.71,0.0557,30.2,1226,8.51,30.1],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",1.3,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",35.71,0.0558,30.7,1226,8.36,30.6],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",1.3,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",35.71,0.0714,13.9,1741,18.5,13.8],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",1.3,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",35.71,0.0731,13.9,1741,18.5,13.8],[\"bn22\\u002ftinyllama_frankenmerge\",\"\ud83e\udd99 LLaMA\",1.54,\"bfloat16\",\"pytorch\",\"None\",\"None\",34.64,0.026,37.0,3168,6.92,37.0],[\"bn22\\u002ftinyllama_frankenmerge\",\"\ud83e\udd99 LLaMA\",1.54,\"float16\",\"pytorch\",\"None\",\"None\",34.64,0.0266,37.3,3168,6.86,37.3],[\"bn22\\u002ftinyllama_frankenmerge\",\"\ud83e\udd99 LLaMA\",1.54,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",34.64,0.0284,33.2,1091,7.71,33.2],[\"bn22\\u002ftinyllama_frankenmerge\",\"\ud83e\udd99 LLaMA\",1.54,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",34.64,0.0286,36.2,1092,7.07,36.2],[\"bn22\\u002ftinyllama_frankenmerge\",\"\ud83e\udd99 LLaMA\",1.54,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",34.64,0.0296,34.4,1490,7.44,34.4],[\"bn22\\u002ftinyllama_frankenmerge\",\"\ud83e\udd99 LLaMA\",1.54,\"float32\",\"pytorch\",\"None\",\"None\",34.64,0.0595,42.4,6279,6.07,42.2],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",1.03,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",34.37,0.0172,57.8,2279,4.43,57.8],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",1.03,\"bfloat16\",\"pytorch\",\"None\",\"None\",34.37,0.0186,51.8,2279,4.94,51.8],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",1.03,\"float16\",\"pytorch\",\"None\",\"None\",34.37,0.0187,53.2,2279,4.81,53.2],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",1.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",34.37,0.0199,52.6,852,4.87,52.6],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",1.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",34.37,0.02,51.4,1248,4.98,51.4],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",1.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",34.37,0.0206,45.9,851,5.58,45.9],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",1.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",34.37,0.0209,50.4,877,5.08,50.4],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",1.03,\"float32\",\"pytorch\",\"None\",\"None\",34.37,0.0442,58.1,4492,4.43,57.8],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",1.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",34.37,0.0445,38.8,869,6.61,38.7],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",1.03,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",34.37,0.0483,32.4,924,7.92,32.3],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",1.03,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",34.37,0.0485,31.1,924,8.24,31.1],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",1.03,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",34.37,0.0662,14.9,1380,17.2,14.9],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",1.03,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",34.37,0.0675,14.5,1380,17.7,14.5],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",33.72,0.0169,57.7,2279,4.44,57.7],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"None\",33.72,0.0178,53.8,2279,4.76,53.8],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",1.1,\"bfloat16\",\"pytorch\",\"None\",\"None\",33.72,0.0182,52.0,2279,4.92,52.0],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",33.72,0.0204,50.6,1248,5.06,50.6],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",33.72,0.0205,47.5,851,5.39,47.5],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",33.72,0.0208,51.4,852,4.98,51.4],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",33.72,0.021,48.1,877,5.32,48.1],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",1.1,\"float32\",\"pytorch\",\"None\",\"None\",33.72,0.0432,57.6,4492,4.47,57.3],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",33.72,0.0448,38.3,869,6.69,38.3],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",33.72,0.0486,31.6,924,8.11,31.6],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",33.72,0.0487,32.6,924,7.88,32.5],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",33.72,0.066,14.8,1380,17.3,14.8],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",33.72,0.0688,14.5,1380,17.7,14.5],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",7.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",32.44,0.0282,38.6,12365,6.64,38.6],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",32.44,0.0288,40.2,12357,6.38,40.1],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"None\",32.44,0.0316,38.7,12365,6.62,38.7],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",32.44,0.0374,37.2,4117,6.9,37.1],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",32.44,0.0386,36.7,4843,6.99,36.6],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",32.44,0.0554,37.2,4021,6.92,37.0],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",32.44,0.0641,34.0,4020,7.56,33.9],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",32.44,0.0958,10.5,6791,24.5,10.4],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",32.44,0.102,9.85,6800,26.0,9.85],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",32.44,0.15,26.8,4158,9.68,26.4],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",7.0,\"float32\",\"pytorch\",\"None\",\"None\",32.44,0.216,41.7,24722,6.33,40.4],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",32.44,0.24,21.4,4451,12.1,21.2],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",32.44,0.246,22.4,4451,11.6,22.1],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",31.86,0.0168,58.0,2279,4.42,57.9],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",1.1,\"bfloat16\",\"pytorch\",\"None\",\"None\",31.86,0.0178,54.8,2279,4.67,54.8],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"None\",31.86,0.018,52.1,2279,4.91,52.1],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",31.86,0.0196,47.8,851,5.36,47.8],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",31.86,0.02,50.1,1248,5.11,50.1],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",31.86,0.0201,52.4,852,4.89,52.4],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",31.86,0.0218,48.9,877,5.24,48.9],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",1.1,\"float32\",\"pytorch\",\"None\",\"None\",31.86,0.0434,57.3,4492,4.49,57.0],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",31.86,0.0451,37.9,869,6.77,37.8],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",31.86,0.0487,32.4,924,7.91,32.4],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",31.86,0.0496,31.2,924,8.23,31.1],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",31.86,0.0651,15.1,1380,17.0,15.1],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",31.86,0.0693,14.5,1380,17.7,14.5],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",0.46,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",30.17,0.0182,52.8,1020,4.85,52.8],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",0.46,\"float16\",\"pytorch\",\"None\",\"None\",30.17,0.0188,51.0,1020,5.02,51.0],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",0.46,\"bfloat16\",\"pytorch\",\"None\",\"None\",30.17,0.0196,49.2,1020,5.2,49.2],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",0.46,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",30.17,0.0213,44.0,489,5.81,44.1],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",0.46,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",30.17,0.0216,47.4,768,5.4,47.4],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",0.46,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",30.17,0.0217,48.9,489,5.24,48.9],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",0.46,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",30.17,0.0225,46.2,499,5.54,46.2],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",0.46,\"float32\",\"pytorch\",\"None\",\"None\",30.17,0.023,55.0,1936,4.66,54.9],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",0.46,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",30.17,0.0447,29.4,504,8.71,29.4],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",0.46,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",30.17,0.0464,28.3,504,9.05,28.3],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",0.46,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",30.17,0.0483,35.4,490,7.25,35.3],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",0.46,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",30.17,0.0724,13.8,662,18.6,13.8],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",0.46,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",30.17,0.0743,13.1,662,19.5,13.1],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",7.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",30.15,0.0307,43.1,14089,5.94,43.1],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",30.15,0.0328,40.8,14089,6.28,40.8],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",30.15,0.0413,40.2,4649,6.39,40.1],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",30.15,0.0417,40.2,5375,6.38,40.1],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",30.15,0.0635,40.1,4553,6.42,39.9],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",30.15,0.0725,36.7,4552,7.01,36.5],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",30.15,0.0935,10.6,7719,24.1,10.6],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",30.15,0.1,10.2,7719,25.1,10.2],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",30.15,0.166,28.3,4561,9.19,27.9],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",7.0,\"float32\",\"pytorch\",\"None\",\"None\",30.15,0.238,41.7,28167,6.35,40.3],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",30.15,0.269,23.2,4829,11.3,22.7],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",30.15,0.273,22.6,4829,11.6,22.1],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",3.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",29.53,0.0154,78.7,7311,3.26,78.5],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"None\",29.53,0.0163,82.5,7311,3.11,82.3],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",29.53,0.0165,79.4,7311,3.23,79.3],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",29.53,0.0203,76.3,2637,3.36,76.2],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",29.53,0.0208,76.6,3360,3.35,76.4],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",29.53,0.0308,82.0,2543,3.14,81.5],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",29.53,0.0362,70.2,2543,3.67,69.8],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",29.53,0.0486,20.4,4126,12.5,20.5],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",29.53,0.0491,20.7,4126,12.3,20.8],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",29.53,0.0845,58.1,2584,4.47,57.3],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",3.0,\"float32\",\"pytorch\",\"None\",\"None\",29.53,0.121,80.4,14612,3.29,77.8],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",29.53,0.134,45.6,2776,5.72,44.8],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",29.53,0.134,45.2,2776,5.77,44.4],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"None\",29.44,0.00784,118.0,500,2.18,117.0],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",29.44,0.00789,120.0,500,2.14,120.0],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",0.22,\"bfloat16\",\"pytorch\",\"None\",\"None\",29.44,0.00855,109.0,500,2.35,109.0],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",29.44,0.00919,109.0,275,2.34,109.0],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",29.44,0.00921,109.0,553,2.35,109.0],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",29.44,0.00938,105.0,283,2.43,105.0],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",29.44,0.0096,98.5,275,2.6,98.5],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",0.22,\"float32\",\"pytorch\",\"None\",\"None\",29.44,0.0108,126.0,932,2.04,125.0],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",29.44,0.0185,70.1,281,3.66,69.9],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",29.44,0.0188,67.8,281,3.78,67.7],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",29.44,0.02,82.0,275,3.13,81.8],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",29.44,0.0306,32.2,348,7.94,32.2],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",29.44,0.0312,31.4,348,8.14,31.4],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",29.23,0.00778,121.0,500,2.11,121.0],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"None\",29.23,0.00789,116.0,500,2.21,116.0],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",0.22,\"bfloat16\",\"pytorch\",\"None\",\"None\",29.23,0.00816,114.0,500,2.24,114.0],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",29.23,0.00908,102.0,275,2.5,102.0],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",29.23,0.00926,112.0,275,2.28,112.0],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",29.23,0.00938,107.0,553,2.39,107.0],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",29.23,0.00991,108.0,283,2.38,108.0],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",0.22,\"float32\",\"pytorch\",\"None\",\"None\",29.23,0.0103,122.0,932,2.1,122.0],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",29.23,0.0182,69.5,281,3.69,69.4],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",29.23,0.0188,67.8,281,3.78,67.7],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",29.23,0.0203,82.8,275,3.1,82.6],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",29.23,0.0305,32.1,348,7.98,32.1],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",29.23,0.0319,31.2,348,8.2,31.2],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",0.1,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.97,0.00491,192.0,269,1.33,192.0],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",0.1,\"float16\",\"pytorch\",\"None\",\"None\",28.97,0.00495,185.0,269,1.38,186.0],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",0.1,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.97,0.00517,181.0,269,1.42,180.0],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",0.1,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.97,0.00559,177.0,190,1.45,177.0],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",0.1,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.97,0.00567,173.0,396,1.48,173.0],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",0.1,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.97,0.00567,158.0,189,1.62,158.0],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",0.1,\"float32\",\"pytorch\",\"None\",\"None\",28.97,0.00572,199.0,456,1.29,198.0],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",0.1,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.97,0.00598,170.0,195,1.51,170.0],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",0.1,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",28.97,0.0113,112.0,192,2.28,112.0],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",0.1,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.97,0.0114,110.0,192,2.33,110.0],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",0.1,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.97,0.0126,130.0,190,1.97,130.0],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",0.1,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",28.97,0.0185,53.8,214,4.76,53.8],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",0.1,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.97,0.019,51.1,214,5.01,51.1],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",0.06,\"float16\",\"pytorch\",\"None\",\"None\",28.7,0.00452,196.0,180,1.3,197.0],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",0.06,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.7,0.00474,190.0,180,1.34,191.0],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",0.06,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.7,0.00483,190.0,180,1.34,191.0],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",0.06,\"float32\",\"pytorch\",\"None\",\"None\",28.7,0.00484,204.0,288,1.25,205.0],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",0.06,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.7,0.00523,186.0,279,1.38,186.0],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",0.06,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.7,0.00526,189.0,143,1.36,188.0],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",0.06,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.7,0.00528,175.0,143,1.47,174.0],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",0.06,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.7,0.00533,185.0,145,1.39,184.0],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",0.06,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",28.7,0.011,113.0,144,2.26,113.0],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",0.06,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.7,0.0114,107.0,144,2.39,107.0],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",0.06,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.7,0.0115,139.0,143,1.85,138.0],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",0.06,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",28.7,0.0186,53.2,155,4.81,53.2],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",0.06,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.7,0.0191,52.6,155,4.87,52.6],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",1.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.44,0.00443,283.0,2229,0.905,283.0],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",1.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.44,0.00449,268.0,2229,0.956,268.0],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",1.0,\"float16\",\"pytorch\",\"None\",\"None\",28.44,0.00458,278.0,2229,0.921,278.0],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",1.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.44,0.00548,265.0,1128,0.968,264.0],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",1.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.44,0.00559,274.0,1850,0.938,273.0],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",1.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.44,0.00827,275.0,1050,0.935,274.0],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",1.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.44,0.00972,252.0,1050,1.02,251.0],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",1.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",28.44,0.0124,78.5,1432,3.26,78.5],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",1.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.44,0.013,78.0,1432,3.28,78.0],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",1.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.44,0.0215,202.0,1225,1.28,200.0],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",1.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.44,0.0338,165.0,1315,1.58,162.0],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",1.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",28.44,0.034,158.0,1315,1.64,156.0],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",1.0,\"float32\",\"pytorch\",\"None\",\"None\",28.44,0.0347,270.0,4446,0.978,262.0],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",0.08,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.17,0.00454,193.0,232,1.32,194.0],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",0.08,\"float16\",\"pytorch\",\"None\",\"None\",28.17,0.00459,195.0,232,1.31,195.0],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",0.08,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.17,0.00468,195.0,232,1.31,195.0],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",0.08,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.17,0.00528,172.0,145,1.49,172.0],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",0.08,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.17,0.00531,185.0,351,1.39,184.0],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",0.08,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.17,0.00534,193.0,145,1.33,192.0],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",0.08,\"float32\",\"pytorch\",\"None\",\"None\",28.17,0.00537,214.0,387,1.2,213.0],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",0.08,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.17,0.00537,185.0,150,1.39,184.0],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",0.08,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.17,0.0112,112.0,147,2.29,112.0],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",0.08,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",28.17,0.0116,111.0,147,2.3,111.0],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",0.08,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.17,0.0117,139.0,145,1.84,139.0],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",0.08,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.17,0.0181,53.6,171,4.78,53.6],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",0.08,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",28.17,0.0183,52.8,171,4.85,52.8]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83e\udd99 LLaMA\",\"marker\":{\"color\":\"#6A76FC\",\"size\":[69165,19971,21328,35851,35836,19654,19654,21798,21798,75184,25989,27347,41854,41869,25673,25673,27816,27816,69165,21328,19971,35836,35851,19654,19654,21798,21798,69753,69737,40008,38539,38138,41781,41781,70040,39553,37649,37141,37141,21781,22045,21773,22037,6411,6675,7358,7622,6282,6546,6546,6282,11577,11312,11585,11320,43555,44079,7245,6980,6980,7245,68608,68608,39328,37862,37465,40249,40249,68608,39328,37862,37465,19768,20957,34766,19528,19494,15525,15525,15525,7476,8202,7378,7378,10093,10093,7524,30980,7811,7811,27089,8420,9334,8267,14514,8267,8279,53918,18941,18937,12315,12315,12315,4266,4992,4169,4168,6883,6883,4314,24537,4602,4602,27089,27089,8420,9334,8267,14514,14514,8267,8279,53918,8754,8754,27641,27641,55017,17479,5558,5557,34944,27047,8379,9292,8225,14472,8225,8237,53834,27047,27047,8379,9292,8225,14472,14472,8225,53834,8713,8713,53514,53514,16955,16035,15869,28365,28365,15869,16846,16846,14089,14089,4649,5375,4553,4552,7719,4561,28167,12542,12542,12542,4493,5219,4397,4396,7111,7111,4520,24949,4807,4807,27415,27415,8747,9660,8593,14841,14841,8593,8605,54569,9081,9081,14056,14056,14056,4616,5342,4519,4519,7686,7686,4527,28100,4795,4795,14056,14056,4616,5342,4519,4519,7686,7686,28100,4795,4796,14653,14653,14653,5213,5939,5117,5116,8283,8283,5124,29295,5393,5393,27047,27047,8379,9292,8225,14472,14472,8225,8237,53834,8713,8713,14056,14056,4616,5342,4519,4519,7686,7686,4527,28100,4796,4795,14354,14354,14354,4914,5640,4818,4817,7984,7984,4825,28696,5094,5094,14056,4616,5342,4519,4519,7686,4527,28100,14056,14056,14056,4616,5342,4519,4519,7686,7686,4527,28100,4796,4795,6401,6401,6401,4106,4106,2666,2971,2432,2279,2278,2377,2377,8039,3687,3687,2384,12793,2513,2513,14351,14351,4911,5637,4815,4814,7981,7981,28691,5091,5091,14056,14056,14056,4616,5342,4519,4519,7686,7686,4527,28100,4795,4796,27341,27341,8672,9586,8519,14766,14766,8519,54421,9005,9005,15101,15101,5660,6386,5564,5563,8731,8731,5571,30189,5841,5841,14056,14056,14056,4616,5342,4519,4519,7686,7686,4527,28100,4796,4795,6401,6401,6401,2432,2971,2377,2377,3687,3687,2384,12793,2513,2513,44887,44887,13546,14729,23464,23464,13306,13283,14203,14203,14056,14056,14056,4616,5342,4519,4519,7686,7686,4527,28100,4796,4796,7283,7280,7280,4035,4039,14582,2669,2666,14291,14291,14291,4851,5577,4755,4754,7921,7921,4762,28570,5031,5031,7283,7280,7280,4039,4035,14582,2666,2669,44887,44887,13546,14729,23464,23464,13306,13283,14203,14203,2279,2279,2279,851,1248,852,877,4492,869,924,924,1380,1380,2991,2991,2991,1157,1544,1182,1156,1159,5969,1226,1226,1741,1741,3168,3168,1091,1092,1490,6279,2279,2279,2279,852,1248,851,877,4492,869,924,924,1380,1380,2279,2279,2279,1248,851,852,877,4492,869,924,924,1380,1380,12365,12357,12365,4117,4843,4021,4020,6791,6800,4158,24722,4451,4451,2279,2279,2279,851,1248,852,877,4492,869,924,924,1380,1380,1020,1020,1020,489,768,489,499,1936,504,504,490,662,662,14089,14089,4649,5375,4553,4552,7719,7719,4561,28167,4829,4829,7311,7311,7311,2637,3360,2543,2543,4126,4126,2584,14612,2776,2776,500,500,500,275,553,283,275,932,281,281,275,348,348,500,500,500,275,275,553,283,932,281,281,275,348,348,269,269,269,190,396,189,456,195,192,192,190,214,214,180,180,180,288,279,143,143,145,144,144,143,155,155,2229,2229,2229,1128,1850,1050,1050,1432,1432,1225,1315,1315,4446,232,232,232,145,351,145,387,150,147,147,145,171,171],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83e\udd99 LLaMA\",\"showlegend\":true,\"x\":[13.9,13.2,13.2,47.3,46.8,12.9,14.2,23.4,22.9,14.1,13.4,13.2,47.0,47.2,12.6,14.3,23.4,22.9,13.9,13.5,13.2,47.5,47.7,12.9,14.4,23.3,23.0,75.7,75.4,21.1,21.3,22.8,42.5,43.3,64.0,17.8,19.6,17.1,19.0,10.0,10.1,9.65,9.83,10.5,10.6,10.6,10.4,10.2,10.2,11.4,11.5,37.2,37.4,38.1,37.8,10.0,10.0,17.3,17.8,17.7,17.7,61.8,63.3,15.9,18.9,17.4,41.3,41.3,62.5,16.4,18.9,17.7,12.0,12.0,46.0,11.8,13.2,6.44,6.43,6.66,7.01,7.09,6.8,7.49,24.6,25.3,10.2,6.35,11.8,11.5,7.65,8.06,8.02,7.93,30.6,8.98,11.0,10.9,10.5,11.3,6.55,6.49,6.74,6.97,6.92,6.63,7.64,24.6,25.5,9.49,6.39,11.5,11.8,7.37,8.08,8.1,8.11,7.74,30.9,31.0,8.86,11.4,10.9,14.5,14.0,12.6,13.1,12.6,7.56,7.64,8.77,7.94,7.6,8.19,7.96,7.83,32.0,8.97,11.2,10.9,7.41,7.87,8.18,7.98,8.05,30.8,35.7,8.82,10.9,14.0,14.3,14.8,16.4,16.0,16.3,15.8,62.0,63.4,17.4,28.0,28.6,6.18,6.1,6.38,6.45,6.4,7.05,25.0,8.99,6.37,6.72,6.48,6.73,7.05,7.16,6.87,7.55,24.9,26.3,9.46,6.42,11.4,11.9,7.6,8.01,8.15,8.01,7.85,31.0,31.8,8.72,11.1,11.0,14.1,14.6,6.05,6.24,5.97,6.34,6.46,6.24,7.15,24.8,24.8,9.03,6.34,11.1,11.5,6.03,6.57,6.57,6.46,6.5,6.97,24.4,24.8,6.35,11.2,11.4,5.92,6.01,6.33,6.55,6.59,6.63,7.03,24.4,25.4,8.89,6.46,11.3,11.5,7.66,7.9,8.22,8.16,7.85,31.0,31.9,8.89,11.3,10.9,14.0,14.3,6.05,6.19,6.47,6.75,6.25,7.24,24.3,24.4,9.04,6.34,11.4,11.2,5.95,6.29,6.18,6.52,6.49,6.26,7.23,24.8,25.7,9.02,6.39,11.3,11.5,6.07,6.73,6.5,6.41,7.09,24.5,9.1,6.36,5.82,6.33,6.04,6.54,6.46,6.2,6.91,24.7,25.1,9.02,6.34,11.5,11.2,4.74,4.66,4.93,4.93,4.96,4.86,4.87,5.08,5.05,5.59,4.86,5.24,4.71,18.8,18.7,6.69,4.32,8.57,8.88,5.97,6.29,6.47,6.5,6.26,7.31,24.0,25.0,6.39,11.4,11.1,6.07,6.4,6.03,6.85,6.52,6.45,6.94,24.6,25.2,9.04,6.35,11.1,11.5,7.59,8.05,7.92,8.07,7.79,30.5,30.9,8.82,10.9,13.9,14.5,6.13,6.3,6.51,6.57,6.27,7.04,24.5,25.0,8.96,6.45,11.5,11.4,5.97,6.27,6.01,6.42,6.58,6.2,6.97,24.7,25.1,9.06,6.36,11.1,11.5,4.75,4.81,4.96,4.9,4.9,4.59,5.19,19.0,18.7,6.64,4.27,8.49,8.53,9.16,9.21,8.16,8.08,30.8,30.8,7.96,8.83,15.2,15.1,5.81,6.14,6.25,6.44,6.43,6.33,7.03,24.6,24.9,8.95,6.35,11.4,11.3,5.84,5.37,5.51,20.3,20.8,4.57,9.85,9.61,5.94,6.3,5.91,6.56,6.48,6.42,6.97,24.3,24.9,8.83,6.35,11.3,11.4,5.96,5.47,5.44,21.1,20.7,4.6,9.71,9.79,9.17,9.2,8.39,8.05,30.7,32.1,7.97,9.0,15.3,15.2,4.48,4.78,4.75,5.29,5.01,5.11,5.04,4.53,6.94,7.97,8.18,16.9,17.9,4.56,4.78,4.67,4.66,4.85,4.89,5.32,6.69,4.3,8.51,8.36,18.5,18.5,6.92,6.86,7.71,7.07,7.44,6.07,4.43,4.94,4.81,4.87,4.98,5.58,5.08,4.43,6.61,7.92,8.24,17.2,17.7,4.44,4.76,4.92,5.06,5.39,4.98,5.32,4.47,6.69,8.11,7.88,17.3,17.7,6.64,6.38,6.62,6.9,6.99,6.92,7.56,24.5,26.0,9.68,6.33,12.1,11.6,4.42,4.67,4.91,5.36,5.11,4.89,5.24,4.49,6.77,7.91,8.23,17.0,17.7,4.85,5.02,5.2,5.81,5.4,5.24,5.54,4.66,8.71,9.05,7.25,18.6,19.5,5.94,6.28,6.39,6.38,6.42,7.01,24.1,25.1,9.19,6.35,11.3,11.6,3.26,3.11,3.23,3.36,3.35,3.14,3.67,12.5,12.3,4.47,3.29,5.72,5.77,2.18,2.14,2.35,2.34,2.35,2.43,2.6,2.04,3.66,3.78,3.13,7.94,8.14,2.11,2.21,2.24,2.5,2.28,2.39,2.38,2.1,3.69,3.78,3.1,7.98,8.2,1.33,1.38,1.42,1.45,1.48,1.62,1.29,1.51,2.28,2.33,1.97,4.76,5.01,1.3,1.34,1.34,1.25,1.38,1.36,1.47,1.39,2.26,2.39,1.85,4.81,4.87,0.905,0.956,0.921,0.968,0.938,0.935,1.02,3.26,3.28,1.28,1.58,1.64,0.978,1.32,1.31,1.31,1.49,1.39,1.33,1.2,1.39,2.29,2.3,1.84,4.78,4.85],\"xaxis\":\"x\",\"y\":[70.95,70.95,70.95,70.95,70.95,70.95,70.95,70.95,70.95,70.81,70.81,70.81,70.81,70.81,70.81,70.81,70.81,70.81,69.42,69.42,69.42,69.42,69.42,69.42,69.42,69.42,69.42,69.38,69.38,69.38,69.38,69.38,69.38,69.38,67.87,67.87,67.87,67.87,67.87,66.04,66.04,66.04,66.04,66.04,66.04,66.04,66.04,66.04,66.04,66.04,66.04,66.04,66.04,66.04,66.04,66.04,66.04,66.04,66.04,66.04,66.04,62.79,62.79,62.79,62.79,62.79,62.79,62.79,61.19,61.19,61.19,61.19,56.94,56.94,56.94,56.94,56.94,56.69,56.69,56.69,56.69,56.69,56.69,56.69,56.69,56.69,56.69,56.69,56.69,56.69,55.69,55.69,55.69,55.69,55.69,55.69,55.69,55.69,55.28,55.28,54.08,54.08,54.08,54.08,54.08,54.08,54.08,54.08,54.08,54.08,54.08,54.08,54.08,53.67,53.67,53.67,53.67,53.67,53.67,53.67,53.67,53.67,53.67,53.67,53.67,53.42,53.42,53.42,51.67,51.67,51.67,51.67,51.36,51.36,51.36,51.36,51.36,51.36,51.36,51.36,51.33,51.33,51.33,51.33,51.33,51.33,51.33,51.33,51.33,51.33,51.33,51.13,51.13,51.13,51.13,51.13,51.13,51.13,51.13,51.13,51.13,50.97,50.97,50.97,50.97,50.97,50.97,50.97,50.97,50.97,50.27,50.27,50.27,50.27,50.27,50.27,50.27,50.27,50.27,50.27,50.27,50.27,50.27,49.5,49.5,49.5,49.5,49.5,49.5,49.5,49.5,49.5,49.5,49.5,49.5,48.64,48.64,48.64,48.64,48.64,48.64,48.64,48.64,48.64,48.64,48.64,48.64,48.64,47.93,47.93,47.93,47.93,47.93,47.93,47.93,47.93,47.93,47.93,47.93,47.64,47.64,47.64,47.64,47.64,47.64,47.64,47.64,47.64,47.64,47.64,47.64,47.64,47.26,47.26,47.26,47.26,47.26,47.26,47.26,47.26,47.26,47.26,47.26,47.26,47.09,47.09,47.09,47.09,47.09,47.09,47.09,47.09,47.09,47.09,47.09,47.09,46.64,46.64,46.64,46.64,46.64,46.64,46.64,46.64,46.64,46.64,46.64,46.64,46.64,45.65,45.65,45.65,45.65,45.65,45.65,45.65,45.65,45.62,45.62,45.62,45.62,45.62,45.62,45.62,45.62,45.62,45.62,45.62,45.62,45.62,44.75,44.75,44.75,44.75,44.75,44.75,44.75,44.75,44.75,44.75,44.75,44.75,44.75,44.75,44.75,44.75,44.75,44.75,44.75,44.52,44.52,44.52,44.52,44.52,44.52,44.52,44.52,44.52,44.52,44.52,44.26,44.26,44.26,44.26,44.26,44.26,44.26,44.26,44.26,44.26,44.26,44.26,44.26,43.35,43.35,43.35,43.35,43.35,43.35,43.35,43.35,43.35,43.35,43.35,43.27,43.27,43.27,43.27,43.27,43.27,43.27,43.27,43.27,43.27,43.27,43.27,42.31,42.31,42.31,42.31,42.31,42.31,42.31,42.31,42.31,42.31,42.31,42.31,42.31,41.44,41.44,41.44,41.44,41.44,41.44,41.44,41.44,41.44,41.44,41.44,41.44,41.44,41.33,41.33,41.33,41.33,41.33,41.33,41.33,41.33,41.33,41.33,40.97,40.97,40.97,40.97,40.97,40.97,40.97,40.97,40.97,40.97,40.97,40.97,40.97,40.28,40.28,40.28,40.28,40.28,40.28,40.28,40.28,39.81,39.81,39.81,39.81,39.81,39.81,39.81,39.81,39.81,39.81,39.81,39.81,39.81,38.26,38.26,38.26,38.26,38.26,38.26,38.26,38.26,37.71,37.71,37.71,37.71,37.71,37.71,37.71,37.71,37.71,37.71,36.42,36.42,36.42,36.42,36.42,36.42,36.42,36.42,36.42,36.42,36.42,36.42,36.42,35.71,35.71,35.71,35.71,35.71,35.71,35.71,35.71,35.71,35.71,35.71,35.71,35.71,34.64,34.64,34.64,34.64,34.64,34.64,34.37,34.37,34.37,34.37,34.37,34.37,34.37,34.37,34.37,34.37,34.37,34.37,34.37,33.72,33.72,33.72,33.72,33.72,33.72,33.72,33.72,33.72,33.72,33.72,33.72,33.72,32.44,32.44,32.44,32.44,32.44,32.44,32.44,32.44,32.44,32.44,32.44,32.44,32.44,31.86,31.86,31.86,31.86,31.86,31.86,31.86,31.86,31.86,31.86,31.86,31.86,31.86,30.17,30.17,30.17,30.17,30.17,30.17,30.17,30.17,30.17,30.17,30.17,30.17,30.17,30.15,30.15,30.15,30.15,30.15,30.15,30.15,30.15,30.15,30.15,30.15,30.15,29.53,29.53,29.53,29.53,29.53,29.53,29.53,29.53,29.53,29.53,29.53,29.53,29.53,29.44,29.44,29.44,29.44,29.44,29.44,29.44,29.44,29.44,29.44,29.44,29.44,29.44,29.23,29.23,29.23,29.23,29.23,29.23,29.23,29.23,29.23,29.23,29.23,29.23,29.23,28.97,28.97,28.97,28.97,28.97,28.97,28.97,28.97,28.97,28.97,28.97,28.97,28.97,28.7,28.7,28.7,28.7,28.7,28.7,28.7,28.7,28.7,28.7,28.7,28.7,28.7,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.17,28.17,28.17,28.17,28.17,28.17,28.17,28.17,28.17,28.17,28.17,28.17,28.17],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Deci\\u002fDeciLM-7B\",\"\ud83d\udd35 deci\",7.04,\"bfloat16\",\"pytorch\",\"None\",\"None\",61.55,0.0341,38.9,14290,6.59,38.8],[\"Deci\\u002fDeciLM-7B\",\"\ud83d\udd35 deci\",7.04,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",61.55,0.0353,39.1,14290,6.57,39.0],[\"Deci\\u002fDeciLM-7B\",\"\ud83d\udd35 deci\",7.04,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",61.55,0.065,38.5,4281,6.7,38.2],[\"Deci\\u002fDeciLM-7B\",\"\ud83d\udd35 deci\",7.04,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",61.55,0.0765,34.7,4281,7.42,34.5],[\"Deci\\u002fDeciLM-7B\",\"\ud83d\udd35 deci\",7.04,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",61.55,0.102,10.2,7514,25.1,10.2],[\"Deci\\u002fDeciLM-7B\",\"\ud83d\udd35 deci\",7.04,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",61.55,0.104,10.2,7514,25.1,10.2],[\"Deci\\u002fDeciLM-7B\",\"\ud83d\udd35 deci\",7.04,\"float32\",\"pytorch\",\"None\",\"None\",61.55,0.255,41.2,28538,6.44,39.8],[\"Deci\\u002fDeciLM-7B\",\"\ud83d\udd35 deci\",7.04,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",61.55,0.284,22.2,4884,11.8,21.7],[\"Deci\\u002fDeciLM-7B\",\"\ud83d\udd35 deci\",7.04,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",61.55,0.284,21.8,4884,12.0,21.3]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83d\udd35 deci\",\"marker\":{\"color\":\"#FE00CE\",\"size\":[14290,14290,4281,4281,7514,7514,28538,4884,4884],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83d\udd35 deci\",\"showlegend\":true,\"x\":[6.59,6.57,6.7,7.42,25.1,25.1,6.44,11.8,12.0],\"xaxis\":\"x\",\"y\":[61.55,61.55,61.55,61.55,61.55,61.55,61.55,61.55,61.55],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"microsoft\\u002fphi-2\",\"phi\",2.78,\"bfloat16\",\"pytorch\",\"None\",\"None\",61.33,0.0354,27.2,6339,9.42,27.2],[\"microsoft\\u002fphi-2\",\"phi\",2.78,\"float16\",\"pytorch\",\"None\",\"None\",61.33,0.04,27.1,6339,9.45,27.1],[\"microsoft\\u002fphi-2\",\"phi\",2.78,\"float32\",\"pytorch\",\"None\",\"None\",61.33,0.107,31.7,12603,8.15,31.4],[\"microsoft\\u002fphi-1_5\",\"phi\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",47.69,0.0203,52.5,1280,4.88,52.5],[\"microsoft\\u002fphi-1_5\",\"phi\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",47.69,0.0208,49.1,1280,5.21,49.1],[\"microsoft\\u002fphi-1_5\",\"phi\",0.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",47.69,0.0268,36.4,3329,7.04,36.4],[\"microsoft\\u002fphi-1_5\",\"phi\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",47.69,0.0273,36.4,3329,7.04,36.4],[\"microsoft\\u002fphi-1_5\",\"phi\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",47.69,0.0549,41.1,6550,6.26,40.9]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"phi\",\"marker\":{\"color\":\"#0DF9FF\",\"size\":[6339,6339,12603,1280,1280,3329,3329,6550],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"phi\",\"showlegend\":true,\"x\":[9.42,9.45,8.15,4.88,5.21,7.04,7.04,6.26],\"xaxis\":\"x\",\"y\":[61.33,61.33,61.33,47.69,47.69,47.69,47.69,47.69],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"internlm\\u002finternlm-20b\",\"\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f\",20.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",59.55,0.0803,18.8,41901,13.7,18.7],[\"internlm\\u002finternlm-20b\",\"\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f\",20.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",59.55,0.104,17.3,13828,14.8,17.3],[\"internlm\\u002finternlm-20b\",\"\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f\",20.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",59.55,0.163,18.0,13668,14.4,17.8],[\"internlm\\u002finternlm-20b\",\"\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f\",20.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",59.55,0.19,5.21,23038,49.1,5.21],[\"internlm\\u002finternlm-20b\",\"\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f\",20.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",59.55,0.203,15.9,13667,16.2,15.8],[\"internlm\\u002finternlm-20b\",\"\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f\",20.0,\"float32\",\"pytorch\",\"None\",\"None\",59.55,0.699,15.8,83163,16.8,15.2],[\"internlm\\u002finternlm-20b\",\"\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f\",20.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",59.55,0.749,10.5,14401,24.9,10.3]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f\",\"marker\":{\"color\":\"#F6F926\",\"size\":[41901,13828,13668,23038,13667,83163,14401],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83e\uddd1\u200d\ud83c\udf93 InternLM \u4e66\u751f\",\"showlegend\":true,\"x\":[13.7,14.8,14.4,49.1,16.2,16.8,24.9],\"xaxis\":\"x\",\"y\":[59.55,59.55,59.55,59.55,59.55,59.55,59.55],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"tiiuae\\u002ffalcon-40b\",\"\ud83e\udd85 Falcon\",40.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",58.07,0.137,8.04,43877,31.8,8.05],[\"tiiuae\\u002ffalcon-40b\",\"\ud83e\udd85 Falcon\",40.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",58.07,0.193,15.3,24846,16.9,15.1],[\"tiiuae\\u002ffalcon-40b\",\"\ud83e\udd85 Falcon\",40.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",58.07,0.296,17.7,24289,14.7,17.4],[\"tiiuae\\u002ffalcon-40b\",\"\ud83e\udd85 Falcon\",40.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",58.07,0.367,16.5,24290,15.9,16.1],[\"tiiuae\\u002ffalcon-40b\",\"\ud83e\udd85 Falcon\",40.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",58.07,1.63,8.23,26201,32.6,7.85],[\"tiiuae\\u002ffalcon-7b\",\"\ud83e\udd85 Falcon\",7.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",44.17,0.0345,40.5,13945,6.32,40.5],[\"tiiuae\\u002ffalcon-7b\",\"\ud83e\udd85 Falcon\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",44.17,0.0645,15.5,7296,16.6,15.4],[\"tiiuae\\u002ffalcon-7b\",\"\ud83e\udd85 Falcon\",7.0,\"float32\",\"pytorch\",\"None\",\"None\",44.17,0.247,31.9,27839,8.24,31.1],[\"tiiuae\\u002ffalcon-7b\",\"\ud83e\udd85 Falcon\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",44.17,0.272,26.5,4927,9.9,25.9],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",1.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",37.07,0.015,66.8,2834,3.84,66.7],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",1.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",37.07,0.0155,70.1,1080,3.66,69.9],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",1.0,\"float16\",\"pytorch\",\"None\",\"None\",37.07,0.017,65.1,2834,3.94,65.0],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",1.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",37.07,0.0183,65.7,1045,3.9,65.6],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",1.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",37.07,0.0193,60.1,1045,4.26,60.1],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",1.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",37.07,0.0362,53.9,1047,4.77,53.7],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",1.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",37.07,0.0473,21.4,1629,11.9,21.5],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",1.0,\"float32\",\"pytorch\",\"None\",\"None\",37.07,0.0521,72.2,5659,3.58,71.5],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",1.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",37.07,0.0578,39.0,1097,6.6,38.8]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83e\udd85 Falcon\",\"marker\":{\"color\":\"#FF9616\",\"size\":[43877,24846,24289,24290,26201,13945,7296,27839,4927,2834,1080,2834,1045,1045,1047,1629,5659,1097],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83e\udd85 Falcon\",\"showlegend\":true,\"x\":[31.8,16.9,14.7,15.9,32.6,6.32,16.6,8.24,9.9,3.84,3.66,3.94,3.9,4.26,4.77,11.9,3.58,6.6],\"xaxis\":\"x\",\"y\":[58.07,58.07,58.07,58.07,58.07,44.17,44.17,44.17,44.17,37.07,37.07,37.07,37.07,37.07,37.07,37.07,37.07,37.07],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"mosaicml\\u002fmpt-30b\",\"\ud83e\uddf1 MPT\",30.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",52.77,0.0895,11.5,31849,22.3,11.5],[\"mosaicml\\u002fmpt-30b\",\"\ud83e\uddf1 MPT\",30.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",52.77,0.213,39.6,17553,6.65,38.5],[\"mosaicml\\u002fmpt-30b\",\"\ud83e\uddf1 MPT\",30.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",52.77,0.264,35.0,17553,7.55,33.9],[\"mosaicml\\u002fmpt-30b\",\"\ud83e\uddf1 MPT\",30.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",52.77,1.14,14.6,19236,18.6,13.8],[\"mosaicml\\u002fmpt-7b\",\"\ud83e\uddf1 MPT\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",44.28,0.0557,62.2,4308,4.16,61.5],[\"mosaicml\\u002fmpt-7b\",\"\ud83e\uddf1 MPT\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",44.28,0.0612,17.1,7406,15.0,17.1],[\"mosaicml\\u002fmpt-7b\",\"\ud83e\uddf1 MPT\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",44.28,0.0649,56.0,4308,4.61,55.5],[\"mosaicml\\u002fmpt-7b\",\"\ud83e\uddf1 MPT\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",44.28,0.25,34.0,4721,7.74,33.1],[\"team-lucid\\u002fmptk-1b\",\"\ud83e\uddf1 MPT\",1.31,\"bfloat16\",\"pytorch\",\"None\",\"None\",20.84,0.0116,87.3,2834,2.93,87.4],[\"team-lucid\\u002fmptk-1b\",\"\ud83e\uddf1 MPT\",1.31,\"float16\",\"pytorch\",\"None\",\"None\",20.84,0.012,85.3,2834,3.0,85.3],[\"team-lucid\\u002fmptk-1b\",\"\ud83e\uddf1 MPT\",1.31,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",20.84,0.0161,84.2,1046,3.05,83.9],[\"team-lucid\\u002fmptk-1b\",\"\ud83e\uddf1 MPT\",1.31,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",20.84,0.0174,76.6,1046,3.35,76.4],[\"team-lucid\\u002fmptk-1b\",\"\ud83e\uddf1 MPT\",1.31,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",20.84,0.0434,22.8,1628,11.2,22.9],[\"team-lucid\\u002fmptk-1b\",\"\ud83e\uddf1 MPT\",1.31,\"float32\",\"pytorch\",\"None\",\"None\",20.84,0.0508,93.4,5657,2.78,92.1],[\"team-lucid\\u002fmptk-1b\",\"\ud83e\uddf1 MPT\",1.31,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",20.84,0.0533,47.0,1099,5.48,46.7]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83e\uddf1 MPT\",\"marker\":{\"color\":\"#479B55\",\"size\":[31849,17553,17553,19236,4308,7406,4308,4721,2834,2834,1046,1046,1628,5657,1099],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83e\uddf1 MPT\",\"showlegend\":true,\"x\":[22.3,6.65,7.55,18.6,4.16,15.0,4.61,7.74,2.93,3.0,3.05,3.35,11.2,2.78,5.48],\"xaxis\":\"x\",\"y\":[52.77,52.77,52.77,52.77,44.28,44.28,44.28,44.28,20.84,20.84,20.84,20.84,20.84,20.84,20.84],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"stabilityai\\u002fstablelm-3b-4e1t\",\"\ud83d\udd34 StableLM-Epoch\",2.8,\"float16\",\"pytorch\",\"None\",\"None\",46.58,0.0228,42.4,5972,6.03,42.5],[\"stabilityai\\u002fstablelm-3b-4e1t\",\"\ud83d\udd34 StableLM-Epoch\",2.8,\"bfloat16\",\"pytorch\",\"None\",\"None\",46.58,0.0234,42.6,5972,6.01,42.6],[\"stabilityai\\u002fstablelm-3b-4e1t\",\"\ud83d\udd34 StableLM-Epoch\",2.8,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",46.58,0.0276,40.5,2230,6.32,40.5],[\"stabilityai\\u002fstablelm-3b-4e1t\",\"\ud83d\udd34 StableLM-Epoch\",2.8,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",46.58,0.0371,38.2,2192,6.71,38.2],[\"stabilityai\\u002fstablelm-3b-4e1t\",\"\ud83d\udd34 StableLM-Epoch\",2.8,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",46.58,0.0762,29.9,2195,8.62,29.7],[\"stabilityai\\u002fstablelm-3b-4e1t\",\"\ud83d\udd34 StableLM-Epoch\",2.8,\"float32\",\"pytorch\",\"None\",\"None\",46.58,0.0944,45.8,12068,5.66,45.2]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83d\udd34 StableLM-Epoch\",\"marker\":{\"color\":\"#EEA6FB\",\"size\":[5972,5972,2230,2192,2195,12068],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83d\udd34 StableLM-Epoch\",\"showlegend\":true,\"x\":[6.03,6.01,6.32,6.71,8.62,5.66],\"xaxis\":\"x\",\"y\":[46.58,46.58,46.58,46.58,46.58,46.58],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"stabilityai\\u002fstablelm-base-alpha-7b-v2\",\"\ud83d\udd34 StableLM-Alpha\",6.89,\"bfloat16\",\"pytorch\",\"None\",\"None\",46.18,0.0286,43.5,14353,5.89,43.5],[\"stabilityai\\u002fstablelm-base-alpha-7b-v2\",\"\ud83d\udd34 StableLM-Alpha\",6.89,\"float16\",\"pytorch\",\"None\",\"None\",46.18,0.0303,43.1,14353,5.95,43.0],[\"stabilityai\\u002fstablelm-base-alpha-7b-v2\",\"\ud83d\udd34 StableLM-Alpha\",6.89,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",46.18,0.0611,41.7,4816,6.17,41.5],[\"stabilityai\\u002fstablelm-base-alpha-7b-v2\",\"\ud83d\udd34 StableLM-Alpha\",6.89,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",46.18,0.0643,15.4,7911,16.7,15.3],[\"stabilityai\\u002fstablelm-base-alpha-7b-v2\",\"\ud83d\udd34 StableLM-Alpha\",6.89,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",46.18,0.0707,38.9,4815,6.63,38.6],[\"stabilityai\\u002fstablelm-base-alpha-7b-v2\",\"\ud83d\udd34 StableLM-Alpha\",6.89,\"float32\",\"pytorch\",\"None\",\"None\",46.18,0.239,43.4,28695,6.11,41.9],[\"stabilityai\\u002fstablelm-base-alpha-7b-v2\",\"\ud83d\udd34 StableLM-Alpha\",6.89,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",46.18,0.266,27.4,5398,9.56,26.8]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83d\udd34 StableLM-Alpha\",\"marker\":{\"color\":\"#DC587D\",\"size\":[14353,14353,4816,7911,4815,28695,5398],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83d\udd34 StableLM-Alpha\",\"showlegend\":true,\"x\":[5.89,5.95,6.17,16.7,6.63,6.11,9.56],\"xaxis\":\"x\",\"y\":[46.18,46.18,46.18,46.18,46.18,46.18,46.18],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"AI-Sweden-Models\\u002fgpt-sw3-40b\",\"GPT-2\",39.93,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",43.42,0.0988,16.5,80221,15.6,16.4],[\"AI-Sweden-Models\\u002fgpt-sw3-40b\",\"GPT-2\",39.93,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",43.42,0.101,16.6,80221,15.5,16.5],[\"AI-Sweden-Models\\u002fgpt-sw3-40b\",\"GPT-2\",39.93,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",43.42,0.165,23.4,23570,11.1,23.1],[\"Writer\\u002fpalmyra-large\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",42.09,0.0578,27.2,41812,9.42,27.2],[\"Writer\\u002fpalmyra-large\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",42.09,0.0582,27.2,41812,9.42,27.2],[\"Writer\\u002fpalmyra-large\",\"GPT-2\",0.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",42.09,0.0674,27.2,41812,9.43,27.1],[\"Writer\\u002fpalmyra-large\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",42.09,0.072,28.7,41812,8.96,28.6],[\"Writer\\u002fpalmyra-large\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",42.09,0.0758,27.1,41812,9.49,27.0],[\"Writer\\u002fpalmyra-large\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",42.09,0.0951,35.2,12620,7.34,34.9],[\"Writer\\u002fpalmyra-large\",\"GPT-2\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",42.09,0.652,14.2,83432,18.6,13.8],[\"AI-Sweden-Models\\u002fgpt-sw3-20b\",\"GPT-2\",20.92,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",40.71,0.0572,27.1,41981,9.46,27.1],[\"AI-Sweden-Models\\u002fgpt-sw3-20b\",\"GPT-2\",20.92,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",40.71,0.0573,27.0,41981,9.49,27.0],[\"AI-Sweden-Models\\u002fgpt-sw3-20b\",\"GPT-2\",20.92,\"bfloat16\",\"pytorch\",\"None\",\"None\",40.71,0.0663,27.3,41981,9.42,27.2],[\"AI-Sweden-Models\\u002fgpt-sw3-20b\",\"GPT-2\",20.92,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",40.71,0.092,35.0,12789,7.37,34.7],[\"AI-Sweden-Models\\u002fgpt-sw3-20b\",\"GPT-2\",20.92,\"float32\",\"pytorch\",\"None\",\"None\",40.71,0.673,14.1,83770,18.8,13.6],[\"AI-Sweden-Models\\u002fgpt-sw3-6.7b-v2\",\"GPT-2\",7.11,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",39.49,0.0252,54.5,14110,4.71,54.4],[\"AI-Sweden-Models\\u002fgpt-sw3-6.7b-v2\",\"GPT-2\",7.11,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",39.49,0.0252,53.9,14110,4.76,53.8],[\"AI-Sweden-Models\\u002fgpt-sw3-6.7b-v2\",\"GPT-2\",7.11,\"bfloat16\",\"pytorch\",\"None\",\"None\",39.49,0.0269,55.6,14110,4.62,55.4],[\"AI-Sweden-Models\\u002fgpt-sw3-6.7b-v2\",\"GPT-2\",7.11,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",39.49,0.036,58.1,4709,4.43,57.8],[\"AI-Sweden-Models\\u002fgpt-sw3-6.7b-v2\",\"GPT-2\",7.11,\"float32\",\"pytorch\",\"None\",\"None\",39.49,0.231,35.5,28075,7.42,34.5],[\"cerebras\\u002fCerebras-GPT-13B\",\"GPT-2\",13.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",37.4,0.0426,36.3,26723,7.06,36.3],[\"cerebras\\u002fCerebras-GPT-13B\",\"GPT-2\",13.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",37.4,0.0434,36.8,26723,6.96,36.8],[\"cerebras\\u002fCerebras-GPT-13B\",\"GPT-2\",13.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",37.4,0.0471,36.8,26723,6.97,36.7],[\"cerebras\\u002fCerebras-GPT-13B\",\"GPT-2\",13.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",37.4,0.0642,44.9,8352,5.74,44.6],[\"cerebras\\u002fCerebras-GPT-13B\",\"GPT-2\",13.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",37.4,0.309,34.6,8169,7.67,33.4],[\"cerebras\\u002fCerebras-GPT-13B\",\"GPT-2\",13.0,\"float32\",\"pytorch\",\"None\",\"None\",37.4,0.412,22.2,53269,11.9,21.5],[\"AI-Sweden-Models\\u002fgpt-sw3-6.7b\",\"GPT-2\",7.11,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",37.23,0.0251,54.3,14110,4.73,54.1],[\"AI-Sweden-Models\\u002fgpt-sw3-6.7b\",\"GPT-2\",7.11,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",37.23,0.0253,54.4,14110,4.72,54.2],[\"AI-Sweden-Models\\u002fgpt-sw3-6.7b\",\"GPT-2\",7.11,\"bfloat16\",\"pytorch\",\"None\",\"None\",37.23,0.027,54.5,14110,4.71,54.4],[\"AI-Sweden-Models\\u002fgpt-sw3-6.7b\",\"GPT-2\",7.11,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",37.23,0.036,54.8,4709,4.69,54.6],[\"AI-Sweden-Models\\u002fgpt-sw3-6.7b\",\"GPT-2\",7.11,\"float32\",\"pytorch\",\"None\",\"None\",37.23,0.238,35.6,28075,7.4,34.6],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",6.7,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",36.27,0.0263,53.8,13997,4.77,53.7],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",6.7,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",36.27,0.0264,54.6,13997,4.7,54.5],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",6.7,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",36.27,0.0272,63.3,13997,4.06,63.1],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",6.7,\"bfloat16\",\"pytorch\",\"None\",\"None\",36.27,0.028,55.2,13997,4.65,55.1],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",6.7,\"float16\",\"pytorch\",\"None\",\"None\",36.27,0.029,55.1,13997,4.66,54.9],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",6.7,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",36.27,0.037,55.6,4596,4.63,55.3],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",6.7,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",36.27,0.161,42.5,4483,6.16,41.6],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",6.7,\"float32\",\"pytorch\",\"None\",\"None\",36.27,0.229,35.5,27850,7.41,34.5],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",35.18,0.0201,72.4,10607,3.54,72.3],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",35.18,0.0201,70.8,10607,3.62,70.7],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",35.18,0.021,84.4,10607,3.04,84.2],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",0.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",35.18,0.0214,72.2,10607,3.55,72.1],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",35.18,0.0229,71.4,10607,3.59,71.3],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",35.18,0.0288,76.8,3590,3.35,76.4],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",35.18,0.122,55.4,3561,4.72,54.2],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",35.18,0.173,47.1,21105,5.58,45.9],[\"gpt2-xl\",\"GPT-2\",1.61,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",34.38,0.0271,35.6,3561,7.2,35.6],[\"gpt2-xl\",\"GPT-2\",1.61,\"bfloat16\",\"pytorch\",\"None\",\"None\",34.38,0.0316,31.8,3561,8.06,31.8],[\"gpt2-xl\",\"GPT-2\",1.61,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",34.38,0.0319,31.2,3561,8.19,31.3],[\"gpt2-xl\",\"GPT-2\",1.61,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",34.38,0.032,31.2,3561,8.19,31.3],[\"gpt2-xl\",\"GPT-2\",1.61,\"float16\",\"pytorch\",\"None\",\"None\",34.38,0.0342,30.9,3561,8.27,31.0],[\"gpt2-xl\",\"GPT-2\",1.61,\"float32\",\"pytorch\",\"None\",\"None\",34.38,0.0647,33.3,7061,7.72,33.2],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",1.44,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",34.31,0.0113,78.2,2999,3.27,78.3],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",1.44,\"bfloat16\",\"pytorch\",\"None\",\"None\",34.31,0.0139,70.4,2999,3.63,70.5],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",1.44,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",34.31,0.014,72.6,2999,3.52,72.7],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",1.44,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",34.31,0.0143,71.2,2999,3.59,71.3],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",1.44,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",34.31,0.0144,75.4,1245,3.39,75.5],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",1.44,\"float16\",\"pytorch\",\"None\",\"None\",34.31,0.0147,70.4,2999,3.63,70.5],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",1.44,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",34.31,0.0413,57.8,1212,4.45,57.5],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",1.44,\"float32\",\"pytorch\",\"None\",\"None\",34.31,0.0535,72.6,5888,3.56,71.9],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",2.7,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",33.25,0.0163,62.3,5799,4.11,62.3],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",2.7,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",33.25,0.0189,55.7,5799,4.6,55.7],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",2.7,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",33.25,0.019,55.0,5799,4.66,54.9],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",2.7,\"bfloat16\",\"pytorch\",\"None\",\"None\",33.25,0.019,54.4,5799,4.71,54.4],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",2.7,\"float16\",\"pytorch\",\"None\",\"None\",33.25,0.0196,53.9,5799,4.75,53.9],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",2.7,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",33.25,0.0224,57.4,2119,4.46,57.4],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",2.7,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",33.25,0.076,43.1,2067,6.0,42.7],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",2.7,\"float32\",\"pytorch\",\"None\",\"None\",33.25,0.0938,56.8,11488,4.58,55.9],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",1.3,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",31.3,0.0115,86.4,2943,2.96,86.5],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",1.3,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",31.3,0.0142,72.0,2943,3.55,72.1],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",1.3,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",31.3,0.0146,77.0,1188,3.32,77.1],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",1.3,\"bfloat16\",\"pytorch\",\"None\",\"None\",31.3,0.0147,68.2,2943,3.75,68.3],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",1.3,\"float16\",\"pytorch\",\"None\",\"None\",31.3,0.0151,70.4,2943,3.64,70.3],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",1.3,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",31.3,0.0153,67.6,2943,3.79,67.5],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",1.3,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",31.3,0.0425,57.4,1156,4.48,57.1],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",1.3,\"float32\",\"pytorch\",\"None\",\"None\",31.3,0.0524,73.1,5776,3.54,72.3],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",0.47,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",30.41,0.0113,78.5,951,3.26,78.5],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",0.47,\"float16\",\"pytorch\",\"None\",\"None\",30.41,0.0133,71.0,951,3.6,71.1],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",0.47,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",30.41,0.0136,70.8,951,3.61,70.9],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",0.47,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",30.41,0.014,69.7,951,3.67,69.8],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",0.47,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",30.41,0.0142,75.4,525,3.39,75.5],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",0.47,\"bfloat16\",\"pytorch\",\"None\",\"None\",30.41,0.0149,69.5,951,3.68,69.6],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",0.47,\"float32\",\"pytorch\",\"None\",\"None\",30.41,0.0209,71.8,1792,3.57,71.7],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",0.47,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",30.41,0.0295,56.7,517,4.53,56.5],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",0.38,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",30.4,0.0138,68.5,844,3.73,68.6],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",0.38,\"float16\",\"pytorch\",\"None\",\"None\",30.4,0.0155,62.8,844,4.08,62.7],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",0.38,\"bfloat16\",\"pytorch\",\"None\",\"None\",30.4,0.0158,61.2,844,4.19,61.1],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",0.38,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",30.4,0.0161,61.9,844,4.14,61.8],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",0.38,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",30.4,0.0162,61.4,844,4.17,61.4],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",0.38,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",30.4,0.0168,64.9,407,3.95,64.8],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",0.38,\"float32\",\"pytorch\",\"None\",\"None\",30.4,0.0206,62.8,1656,4.08,62.7],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",0.38,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",30.4,0.0309,51.6,399,4.97,51.5],[\"cerebras\\u002fCerebras-GPT-256M\",\"GPT-2\",0.26,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",29.38,0.00683,130.0,665,1.97,130.0],[\"cerebras\\u002fCerebras-GPT-256M\",\"GPT-2\",0.26,\"float16\",\"pytorch\",\"None\",\"None\",29.38,0.00825,120.0,665,2.14,120.0],[\"cerebras\\u002fCerebras-GPT-256M\",\"GPT-2\",0.26,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",29.38,0.00836,115.0,664,2.22,115.0],[\"cerebras\\u002fCerebras-GPT-256M\",\"GPT-2\",0.26,\"bfloat16\",\"pytorch\",\"None\",\"None\",29.38,0.0085,114.0,665,2.25,114.0],[\"cerebras\\u002fCerebras-GPT-256M\",\"GPT-2\",0.26,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",29.38,0.0085,114.0,664,2.24,114.0],[\"cerebras\\u002fCerebras-GPT-256M\",\"GPT-2\",0.26,\"float32\",\"pytorch\",\"None\",\"None\",29.38,0.0127,123.0,1257,2.08,123.0],[\"gpt2\",\"GPT-2\",0.14,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",28.53,0.00695,131.0,328,1.95,131.0],[\"gpt2\",\"GPT-2\",0.14,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.53,0.00804,121.0,328,2.11,121.0],[\"gpt2\",\"GPT-2\",0.14,\"float16\",\"pytorch\",\"None\",\"None\",28.53,0.00807,120.0,328,2.13,120.0],[\"gpt2\",\"GPT-2\",0.14,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.53,0.00827,118.0,328,2.17,118.0],[\"gpt2\",\"GPT-2\",0.14,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.53,0.00859,125.0,200,2.05,125.0],[\"gpt2\",\"GPT-2\",0.14,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.53,0.00867,115.0,328,2.23,115.0],[\"gpt2\",\"GPT-2\",0.14,\"float32\",\"pytorch\",\"None\",\"None\",28.53,0.00945,120.0,621,2.13,120.0],[\"gpt2\",\"GPT-2\",0.14,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.53,0.0161,98.5,197,2.61,98.1],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",0.19,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",28.45,0.00554,155.0,404,1.66,154.0],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",0.19,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.45,0.00673,137.0,404,1.87,137.0],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",0.19,\"float16\",\"pytorch\",\"None\",\"None\",28.45,0.00693,139.0,404,1.84,139.0],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",0.19,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.45,0.0071,136.0,392,1.88,136.0],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",0.19,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.45,0.00715,135.0,392,1.9,135.0],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",0.19,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.45,0.00718,143.0,268,1.79,143.0],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",0.19,\"float32\",\"pytorch\",\"None\",\"None\",28.45,0.00813,133.0,721,1.93,133.0],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",0.19,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.45,0.0146,109.0,266,2.36,108.0],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",0.14,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",28.4,0.00696,132.0,328,1.94,132.0],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",0.14,\"float16\",\"pytorch\",\"None\",\"None\",28.4,0.00816,120.0,328,2.14,120.0],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",0.14,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.4,0.00817,120.0,328,2.13,120.0],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",0.14,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.4,0.0083,119.0,328,2.15,119.0],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",0.14,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.4,0.00847,120.0,328,2.13,120.0],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",0.14,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.4,0.00858,122.0,200,2.1,122.0],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",0.14,\"float32\",\"pytorch\",\"None\",\"None\",28.4,0.00967,118.0,621,2.18,117.0],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",0.14,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.4,0.0157,100.0,197,2.57,99.6],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",0.11,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",27.75,0.00514,180.0,332,1.43,179.0],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",0.11,\"float16\",\"pytorch\",\"None\",\"None\",27.75,0.00601,162.0,332,1.58,162.0],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",0.11,\"bfloat16\",\"pytorch\",\"None\",\"None\",27.75,0.00613,153.0,332,1.68,152.0],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",0.11,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",27.75,0.00615,158.0,323,1.62,158.0],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",0.11,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",27.75,0.00625,158.0,323,1.62,158.0],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",0.11,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",27.75,0.0064,167.0,219,1.54,166.0],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",0.11,\"float32\",\"pytorch\",\"None\",\"None\",27.75,0.00707,157.0,592,1.63,157.0],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",0.11,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",27.75,0.0125,131.0,219,1.95,131.0],[\"ai-forever\\u002fmGPT\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",27.61,0.0135,71.8,3147,3.56,71.9],[\"ai-forever\\u002fmGPT\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",27.61,0.0156,64.4,3147,3.98,64.3],[\"ai-forever\\u002fmGPT\",\"GPT-2\",0.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",27.61,0.0159,61.6,3147,4.16,61.5],[\"ai-forever\\u002fmGPT\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",27.61,0.0162,66.9,1392,3.83,66.8],[\"ai-forever\\u002fmGPT\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",27.61,0.0162,63.3,3147,4.05,63.2],[\"ai-forever\\u002fmGPT\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",27.61,0.0165,63.0,3147,4.07,62.9],[\"ai-forever\\u002fmGPT\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",27.61,0.0433,51.2,1360,5.02,51.0]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"GPT-2\",\"marker\":{\"color\":\"#D626FF\",\"size\":[80221,80221,23570,41812,41812,41812,41812,41812,12620,83432,41981,41981,41981,12789,83770,14110,14110,14110,4709,28075,26723,26723,26723,8352,8169,53269,14110,14110,14110,4709,28075,13997,13997,13997,13997,13997,4596,4483,27850,10607,10607,10607,10607,10607,3590,3561,21105,3561,3561,3561,3561,3561,7061,2999,2999,2999,2999,1245,2999,1212,5888,5799,5799,5799,5799,5799,2119,2067,11488,2943,2943,1188,2943,2943,2943,1156,5776,951,951,951,951,525,951,1792,517,844,844,844,844,844,407,1656,399,665,665,664,665,664,1257,328,328,328,328,200,328,621,197,404,404,404,392,392,268,721,266,328,328,328,328,328,200,621,197,332,332,332,323,323,219,592,219,3147,3147,3147,1392,3147,3147,1360],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"GPT-2\",\"showlegend\":true,\"x\":[15.6,15.5,11.1,9.42,9.42,9.43,8.96,9.49,7.34,18.6,9.46,9.49,9.42,7.37,18.8,4.71,4.76,4.62,4.43,7.42,7.06,6.96,6.97,5.74,7.67,11.9,4.73,4.72,4.71,4.69,7.4,4.77,4.7,4.06,4.65,4.66,4.63,6.16,7.41,3.54,3.62,3.04,3.55,3.59,3.35,4.72,5.58,7.2,8.06,8.19,8.19,8.27,7.72,3.27,3.63,3.52,3.59,3.39,3.63,4.45,3.56,4.11,4.6,4.66,4.71,4.75,4.46,6.0,4.58,2.96,3.55,3.32,3.75,3.64,3.79,4.48,3.54,3.26,3.6,3.61,3.67,3.39,3.68,3.57,4.53,3.73,4.08,4.19,4.14,4.17,3.95,4.08,4.97,1.97,2.14,2.22,2.25,2.24,2.08,1.95,2.11,2.13,2.17,2.05,2.23,2.13,2.61,1.66,1.87,1.84,1.88,1.9,1.79,1.93,2.36,1.94,2.14,2.13,2.15,2.13,2.1,2.18,2.57,1.43,1.58,1.68,1.62,1.62,1.54,1.63,1.95,3.56,3.98,4.16,3.83,4.05,4.07,5.02],\"xaxis\":\"x\",\"y\":[43.42,43.42,43.42,42.09,42.09,42.09,42.09,42.09,42.09,42.09,40.71,40.71,40.71,40.71,40.71,39.49,39.49,39.49,39.49,39.49,37.4,37.4,37.4,37.4,37.4,37.4,37.23,37.23,37.23,37.23,37.23,36.27,36.27,36.27,36.27,36.27,36.27,36.27,36.27,35.18,35.18,35.18,35.18,35.18,35.18,35.18,35.18,34.38,34.38,34.38,34.38,34.38,34.38,34.31,34.31,34.31,34.31,34.31,34.31,34.31,34.31,33.25,33.25,33.25,33.25,33.25,33.25,33.25,33.25,31.3,31.3,31.3,31.3,31.3,31.3,31.3,31.3,30.41,30.41,30.41,30.41,30.41,30.41,30.41,30.41,30.4,30.4,30.4,30.4,30.4,30.4,30.4,30.4,29.38,29.38,29.38,29.38,29.38,29.38,28.53,28.53,28.53,28.53,28.53,28.53,28.53,28.53,28.45,28.45,28.45,28.45,28.45,28.45,28.45,28.45,28.4,28.4,28.4,28.4,28.4,28.4,28.4,28.4,27.75,27.75,27.75,27.75,27.75,27.75,27.75,27.75,27.61,27.61,27.61,27.61,27.61,27.61,27.61],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Salesforce\\u002fcodegen-16B-nl\",\"\u267e\ufe0f CodeGen\",16.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",42.59,0.061,26.3,33100,9.77,26.2],[\"Salesforce\\u002fcodegen-16B-nl\",\"\u267e\ufe0f CodeGen\",16.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",42.59,0.0787,12.4,17713,20.6,12.4],[\"Salesforce\\u002fcodegen-16B-nl\",\"\u267e\ufe0f CodeGen\",16.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",42.59,0.0806,26.3,10609,9.76,26.2],[\"Salesforce\\u002fcodegen-16B-nl\",\"\u267e\ufe0f CodeGen\",16.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",42.59,0.0812,26.3,12226,9.79,26.1],[\"Salesforce\\u002fcodegen-16B-nl\",\"\u267e\ufe0f CodeGen\",16.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",42.59,0.0842,11.9,17707,21.6,11.9],[\"Salesforce\\u002fcodegen-16B-nl\",\"\u267e\ufe0f CodeGen\",16.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",42.59,0.128,25.8,10305,10.0,25.6],[\"Salesforce\\u002fcodegen-16B-nl\",\"\u267e\ufe0f CodeGen\",16.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",42.59,0.154,24.8,10304,10.5,24.4],[\"Salesforce\\u002fcodegen-16B-nl\",\"\u267e\ufe0f CodeGen\",16.0,\"float32\",\"pytorch\",\"None\",\"None\",42.59,0.572,20.6,66008,13.0,19.7],[\"Salesforce\\u002fcodegen-16B-nl\",\"\u267e\ufe0f CodeGen\",16.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",42.59,0.621,19.2,11264,13.9,18.4],[\"Salesforce\\u002fcodegen-16B-nl\",\"\u267e\ufe0f CodeGen\",16.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",42.59,0.63,20.1,11264,13.3,19.2],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",40.0,0.0348,29.2,14859,8.75,29.3],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",6.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",40.0,0.0377,27.0,14855,9.48,27.0],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"None\",40.0,0.0385,26.9,14855,9.51,26.9],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",40.0,0.0457,27.2,5158,9.44,27.1],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",40.0,0.0474,27.1,6235,9.45,27.1],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",40.0,0.0685,27.2,5020,9.44,27.1],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",40.0,0.0771,26.0,5020,9.9,25.9],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",40.0,0.0786,12.9,8220,19.9,12.9],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",40.0,0.0816,12.3,8216,20.8,12.3],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",40.0,0.176,23.4,5023,11.1,23.1],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",6.0,\"float32\",\"pytorch\",\"None\",\"None\",40.0,0.256,28.6,29527,9.19,27.9],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",40.0,0.284,19.6,5351,13.3,19.2],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",40.0,0.286,20.9,5351,12.5,20.5],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",32.43,0.034,28.9,14859,8.84,29.0],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",6.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",32.43,0.0383,26.4,14855,9.71,26.4],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"None\",32.43,0.04,26.7,14855,9.6,26.7],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",32.43,0.0455,26.4,5158,9.7,26.4],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",32.43,0.0474,27.1,6235,9.45,27.1],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",32.43,0.0682,26.8,5020,9.58,26.7],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",32.43,0.0758,12.9,8220,19.8,12.9],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",32.43,0.0771,25.5,5020,10.1,25.3],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",32.43,0.0808,12.5,8216,20.5,12.5],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",32.43,0.173,23.4,5023,11.1,23.1],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",6.0,\"float32\",\"pytorch\",\"None\",\"None\",32.43,0.256,29.0,29527,9.06,28.3],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",32.43,0.285,21.1,5351,12.4,20.6],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",32.43,0.29,19.6,5351,13.3,19.2]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\u267e\ufe0f CodeGen\",\"marker\":{\"color\":\"#6E899C\",\"size\":[33100,17713,10609,12226,17707,10305,10304,66008,11264,11264,14859,14855,14855,5158,6235,5020,5020,8220,8216,5023,29527,5351,5351,14859,14855,14855,5158,6235,5020,8220,5020,8216,5023,29527,5351,5351],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\u267e\ufe0f CodeGen\",\"showlegend\":true,\"x\":[9.77,20.6,9.76,9.79,21.6,10.0,10.5,13.0,13.9,13.3,8.75,9.48,9.51,9.44,9.45,9.44,9.9,19.9,20.8,11.1,9.19,13.3,12.5,8.84,9.71,9.6,9.7,9.45,9.58,19.8,10.1,20.5,11.1,9.06,12.4,13.3],\"xaxis\":\"x\",\"y\":[42.59,42.59,42.59,42.59,42.59,42.59,42.59,42.59,42.59,42.59,40.0,40.0,40.0,40.0,40.0,40.0,40.0,40.0,40.0,40.0,40.0,40.0,40.0,32.43,32.43,32.43,32.43,32.43,32.43,32.43,32.43,32.43,32.43,32.43,32.43,32.43],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"RWKV\\u002frwkv-raven-14b\",\"\ud83d\udc26\u200d\u2b1b RWKV\",14.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",42.09,0.0548,23.2,28350,11.1,23.1],[\"RWKV\\u002frwkv-raven-14b\",\"\ud83d\udc26\u200d\u2b1b RWKV\",14.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",42.09,0.139,7.41,14963,34.5,7.42],[\"RWKV\\u002frwkv-raven-14b\",\"\ud83d\udc26\u200d\u2b1b RWKV\",14.0,\"float32\",\"pytorch\",\"None\",\"None\",42.09,0.49,23.0,56690,11.6,22.1],[\"RWKV\\u002frwkv-raven-14b\",\"\ud83d\udc26\u200d\u2b1b RWKV\",14.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",42.09,0.538,14.1,9149,18.6,13.8],[\"RWKV\\u002frwkv-4-14b-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",14.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",39.92,0.055,24.3,28350,10.6,24.2],[\"RWKV\\u002frwkv-4-14b-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",14.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",39.92,0.136,7.57,14963,33.8,7.57],[\"RWKV\\u002frwkv-4-14b-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",14.0,\"float32\",\"pytorch\",\"None\",\"None\",39.92,0.517,22.8,56690,11.7,21.9],[\"RWKV\\u002frwkv-4-14b-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",14.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",39.92,0.538,13.9,9149,18.8,13.6],[\"RWKV\\u002frwkv-4-7b-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",7.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",37.95,0.0339,29.9,14829,8.55,29.9],[\"RWKV\\u002frwkv-4-7b-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",7.0,\"float16\",\"pytorch\",\"None\",\"None\",37.95,0.0354,30.2,14829,8.49,30.2],[\"RWKV\\u002frwkv-4-7b-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",37.95,0.107,9.41,7875,27.2,9.41],[\"RWKV\\u002frwkv-4-7b-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",7.0,\"float32\",\"pytorch\",\"None\",\"None\",37.95,0.241,30.8,29649,8.53,30.0],[\"RWKV\\u002frwkv-4-7b-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",37.95,0.272,17.5,5081,14.9,17.2],[\"RWKV\\u002frwkv-4-3b-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",3.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",35.25,0.0325,29.1,6007,8.78,29.2],[\"RWKV\\u002frwkv-4-3b-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",3.0,\"float16\",\"pytorch\",\"None\",\"None\",35.25,0.0343,29.3,6007,8.73,29.3],[\"RWKV\\u002frwkv-4-3b-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",3.0,\"float32\",\"pytorch\",\"None\",\"None\",35.25,0.102,31.8,12171,8.12,31.5],[\"RWKV\\u002frwkv-4-3b-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",35.25,0.109,9.27,3358,27.6,9.28],[\"RWKV\\u002frwkv-4-3b-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",35.25,0.119,17.6,2195,14.6,17.5],[\"RWKV\\u002frwkv-4-1b5-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",1.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",33.25,0.025,39.1,3066,6.55,39.1],[\"RWKV\\u002frwkv-4-1b5-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",1.0,\"float16\",\"pytorch\",\"None\",\"None\",33.25,0.0253,39.0,3066,6.57,39.0],[\"RWKV\\u002frwkv-4-1b5-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",1.0,\"float32\",\"pytorch\",\"None\",\"None\",33.25,0.0562,40.6,6124,6.34,40.4],[\"RWKV\\u002frwkv-4-1b5-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",1.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",33.25,0.064,23.2,1255,11.1,23.1],[\"RWKV\\u002frwkv-4-1b5-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",1.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",33.25,0.0832,12.1,1759,21.1,12.1],[\"RWKV\\u002frwkv-4-430m-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",0.43,\"float16\",\"pytorch\",\"None\",\"None\",30.45,0.0237,41.1,896,6.23,41.1],[\"RWKV\\u002frwkv-4-430m-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",0.43,\"bfloat16\",\"pytorch\",\"None\",\"None\",30.45,0.024,39.3,895,6.51,39.3],[\"RWKV\\u002frwkv-4-430m-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",0.43,\"float32\",\"pytorch\",\"None\",\"None\",30.45,0.0248,41.3,1784,6.19,41.4],[\"RWKV\\u002frwkv-4-430m-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",0.43,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",30.45,0.0529,23.4,428,11.0,23.3],[\"RWKV\\u002frwkv-4-430m-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",0.43,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",30.45,0.0813,12.4,569,20.6,12.4],[\"RWKV\\u002frwkv-4-169m-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",0.17,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.64,0.0126,78.5,381,3.26,78.5],[\"RWKV\\u002frwkv-4-169m-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",0.17,\"float16\",\"pytorch\",\"None\",\"None\",28.64,0.013,78.5,381,3.26,78.5],[\"RWKV\\u002frwkv-4-169m-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",0.17,\"float32\",\"pytorch\",\"None\",\"None\",28.64,0.0137,80.2,740,3.19,80.3],[\"RWKV\\u002frwkv-4-169m-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",0.17,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.64,0.0274,45.9,241,5.59,45.8],[\"RWKV\\u002frwkv-4-169m-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",0.17,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.64,0.0411,24.5,283,10.4,24.6],[\"beomi\\u002fKoRWKV-6B\",\"\ud83d\udc26\u200d\u2b1b RWKV\",6.53,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.19,0.0296,34.2,13112,7.49,34.2],[\"beomi\\u002fKoRWKV-6B\",\"\ud83d\udc26\u200d\u2b1b RWKV\",6.53,\"float16\",\"pytorch\",\"None\",\"None\",28.19,0.0316,33.9,13112,7.55,33.9],[\"beomi\\u002fKoRWKV-6B\",\"\ud83d\udc26\u200d\u2b1b RWKV\",6.53,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.19,0.0943,10.4,7030,24.6,10.4],[\"beomi\\u002fKoRWKV-6B\",\"\ud83d\udc26\u200d\u2b1b RWKV\",6.53,\"float32\",\"pytorch\",\"None\",\"None\",28.19,0.211,35.8,26214,7.34,34.9],[\"beomi\\u002fKoRWKV-6B\",\"\ud83d\udc26\u200d\u2b1b RWKV\",6.53,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.19,0.25,20.6,4633,12.7,20.2]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83d\udc26\u200d\u2b1b RWKV\",\"marker\":{\"color\":\"#00B5F7\",\"size\":[28350,14963,56690,9149,28350,14963,56690,9149,14829,14829,7875,29649,5081,6007,6007,12171,3358,2195,3066,3066,6124,1255,1759,896,895,1784,428,569,381,381,740,241,283,13112,13112,7030,26214,4633],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83d\udc26\u200d\u2b1b RWKV\",\"showlegend\":true,\"x\":[11.1,34.5,11.6,18.6,10.6,33.8,11.7,18.8,8.55,8.49,27.2,8.53,14.9,8.78,8.73,8.12,27.6,14.6,6.55,6.57,6.34,11.1,21.1,6.23,6.51,6.19,11.0,20.6,3.26,3.26,3.19,5.59,10.4,7.49,7.55,24.6,7.34,12.7],\"xaxis\":\"x\",\"y\":[42.09,42.09,42.09,42.09,39.92,39.92,39.92,39.92,37.95,37.95,37.95,37.95,37.95,35.25,35.25,35.25,35.25,35.25,33.25,33.25,33.25,33.25,33.25,30.45,30.45,30.45,30.45,30.45,28.64,28.64,28.64,28.64,28.64,28.19,28.19,28.19,28.19,28.19],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"EleutherAI\\u002fgpt-neox-20b\",\"GPT-NeoX\",20.74,\"bfloat16\",\"pytorch\",\"None\",\"None\",41.69,0.0776,25.0,42420,10.3,24.9],[\"EleutherAI\\u002fgpt-neox-20b\",\"GPT-NeoX\",20.74,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",41.69,0.0778,28.9,42420,8.9,28.8],[\"EleutherAI\\u002fgpt-neox-20b\",\"GPT-NeoX\",20.74,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",41.69,0.0928,10.8,22498,23.8,10.8],[\"EleutherAI\\u002fgpt-neox-20b\",\"GPT-NeoX\",20.74,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",41.69,0.0972,10.5,22497,24.5,10.4],[\"EleutherAI\\u002fgpt-neox-20b\",\"GPT-NeoX\",20.74,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",41.69,0.102,27.6,13222,9.34,27.4],[\"EleutherAI\\u002fgpt-neox-20b\",\"GPT-NeoX\",20.74,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",41.69,0.104,27.7,14841,9.32,27.5],[\"EleutherAI\\u002fgpt-neox-20b\",\"GPT-NeoX\",20.74,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",41.69,0.171,25.9,12919,10.0,25.6],[\"EleutherAI\\u002fgpt-neox-20b\",\"GPT-NeoX\",20.74,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",41.69,0.206,23.8,12920,10.9,23.5],[\"EleutherAI\\u002fgpt-neox-20b\",\"GPT-NeoX\",20.74,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",41.69,0.826,15.5,14172,17.2,14.9],[\"EleutherAI\\u002fgpt-neox-20b\",\"GPT-NeoX\",20.74,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",41.69,0.826,15.4,14172,17.4,14.7],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",41.49,0.0281,48.8,14429,5.26,48.7],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",41.49,0.0305,46.4,14429,5.52,46.4],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",7.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",41.49,0.0311,40.8,14429,6.28,40.8],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"None\",41.49,0.0329,40.0,14429,6.4,40.0],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",41.49,0.0395,42.5,5027,6.04,42.4],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",41.49,0.0409,40.7,6105,6.3,40.6],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",41.49,0.0623,15.6,7990,16.4,15.6],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",41.49,0.0639,39.2,4891,6.57,39.0],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",41.49,0.0676,15.1,7990,17.0,15.1],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",41.49,0.0725,36.8,4891,7.0,36.6],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",41.49,0.163,33.6,4980,7.74,33.1],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",7.0,\"float32\",\"pytorch\",\"None\",\"None\",41.49,0.247,38.3,28712,6.91,37.0],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",41.49,0.277,27.9,5370,9.43,27.1],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",41.49,0.284,26.0,5370,10.1,25.3],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",41.25,0.0297,48.3,14429,5.31,48.2],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",6.65,\"bfloat16\",\"pytorch\",\"None\",\"None\",41.25,0.0309,41.1,14429,6.23,41.1],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"None\",41.25,0.0329,40.5,14429,6.32,40.5],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",41.25,0.0338,45.0,14429,5.7,44.9],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",41.25,0.0406,42.1,5027,6.1,42.0],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",41.25,0.0408,41.9,6105,6.12,41.8],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",41.25,0.0618,15.8,7990,16.2,15.8],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",41.25,0.0634,40.3,4891,6.39,40.1],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",41.25,0.0675,14.8,7990,17.3,14.8],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",41.25,0.0718,37.4,4891,6.88,37.2],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",41.25,0.163,33.2,4980,7.83,32.7],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",6.65,\"float32\",\"pytorch\",\"None\",\"None\",41.25,0.247,38.4,28712,6.89,37.2],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",41.25,0.277,25.7,5370,10.2,25.1],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",41.25,0.28,27.8,5370,9.45,27.1],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",39.7,0.0451,43.3,24619,5.94,43.1],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",12.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",39.7,0.0453,36.6,24619,7.01,36.5],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",39.7,0.0605,36.7,8106,7.0,36.6],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",39.7,0.0617,37.8,9453,6.8,37.6],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",39.7,0.0718,13.8,13360,18.6,13.8],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",39.7,0.0771,13.2,13360,19.4,13.2],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",39.7,0.0964,36.0,7892,7.18,35.7],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",39.7,0.119,33.4,7893,7.76,33.0],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",12.0,\"float32\",\"pytorch\",\"None\",\"None\",39.7,0.404,25.5,49074,10.4,24.6],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",39.7,0.449,22.8,8652,11.6,22.1],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",39.7,0.456,23.8,8652,11.2,22.9],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",6.9,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",39.3,0.0276,48.7,14404,5.27,48.6],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",6.9,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",39.3,0.0299,47.1,14404,5.44,47.1],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",6.9,\"bfloat16\",\"pytorch\",\"None\",\"None\",39.3,0.0299,40.0,14403,6.41,39.9],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",6.9,\"float16\",\"pytorch\",\"None\",\"None\",39.3,0.0315,40.4,14403,6.34,40.4],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",6.9,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",39.3,0.0394,42.4,5002,6.05,42.3],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",6.9,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",39.3,0.0401,42.8,6080,6.0,42.7],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",6.9,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",39.3,0.0637,15.6,7965,16.4,15.6],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",6.9,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",39.3,0.0661,15.1,7965,17.0,15.1],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",6.9,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",39.3,0.0728,37.7,4866,6.84,37.4],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",6.9,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",39.3,0.162,33.4,4952,7.8,32.8],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",6.9,\"float32\",\"pytorch\",\"None\",\"None\",39.3,0.247,38.6,28662,6.86,37.3],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",6.9,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",39.3,0.276,27.9,5343,9.43,27.1],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",6.9,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",39.3,0.277,26.0,5343,10.1,25.3],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",38.82,0.0446,43.5,24619,5.9,43.4],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",12.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",38.82,0.0454,35.6,24619,7.21,35.5],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",38.82,0.0604,36.8,8106,6.99,36.6],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",38.82,0.0614,37.9,9453,6.79,37.7],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",38.82,0.0737,13.9,13360,18.5,13.8],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",38.82,0.0748,13.3,13360,19.3,13.3],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",38.82,0.0955,35.7,7892,7.25,35.3],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",38.82,0.119,33.2,7893,7.79,32.9],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",12.0,\"float32\",\"pytorch\",\"None\",\"None\",38.82,0.406,25.5,49074,10.4,24.6],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",38.82,0.446,22.8,8652,11.6,22.1],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",38.82,0.457,23.8,8652,11.2,22.9],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",10.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",38.59,0.0416,43.0,22366,5.97,42.9],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",10.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",38.59,0.0428,36.5,22366,7.02,36.5],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",10.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",38.59,0.057,37.5,8755,6.86,37.3],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",10.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",38.59,0.0571,36.5,7475,7.04,36.4],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",10.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",38.59,0.0712,13.9,12106,18.5,13.8],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",10.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",38.59,0.0747,13.4,12106,19.1,13.4],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",10.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",38.59,0.0872,35.3,7285,7.31,35.0],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",10.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",38.59,0.108,33.0,7284,7.84,32.7],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",10.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",38.59,0.26,28.7,7424,9.14,28.0],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",10.0,\"float32\",\"pytorch\",\"None\",\"None\",38.59,0.39,27.6,44535,9.62,26.6],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",10.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",38.59,0.425,24.5,7972,10.8,23.7],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",10.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",38.59,0.425,23.4,7971,11.3,22.7],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",38.54,0.0205,48.9,6068,5.24,48.9],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",38.54,0.0232,43.4,6068,5.89,43.5],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",38.54,0.0253,41.6,3061,6.16,41.6],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",38.54,0.0253,40.3,6068,6.35,40.3],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",38.54,0.0254,42.7,2388,6.0,42.7],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"None\",38.54,0.0277,39.5,6068,6.49,39.4],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",38.54,0.0375,39.7,2340,6.46,39.6],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",38.54,0.0401,37.6,2334,6.82,37.5],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",38.54,0.0636,15.5,3605,16.5,15.5],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",38.54,0.0669,14.8,3605,17.3,14.8],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",38.54,0.0796,33.2,2338,7.77,32.9],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"float32\",\"pytorch\",\"None\",\"None\",38.54,0.106,41.9,12028,6.2,41.3],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",38.54,0.118,27.0,2457,9.56,26.8],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",38.54,0.123,25.2,2457,10.2,25.1],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",38.06,0.0275,48.8,14404,5.26,48.7],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",38.06,0.0296,45.3,14404,5.66,45.2],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",6.65,\"bfloat16\",\"pytorch\",\"None\",\"None\",38.06,0.03,41.5,14403,6.17,41.5],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"None\",38.06,0.0315,40.4,14403,6.34,40.4],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",38.06,0.0394,40.0,5002,6.41,39.9],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",38.06,0.0401,40.9,6080,6.28,40.8],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",38.06,0.0632,39.7,4866,6.49,39.4],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",38.06,0.0638,15.6,7965,16.4,15.6],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",38.06,0.0671,15.2,7965,16.9,15.1],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",38.06,0.071,37.8,4866,6.82,37.5],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",38.06,0.162,33.9,4952,7.69,33.3],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",6.65,\"float32\",\"pytorch\",\"None\",\"None\",38.06,0.246,38.3,28662,6.91,37.0],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",38.06,0.275,27.9,5343,9.43,27.1],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",38.06,0.276,25.7,5343,10.2,25.1],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",37.31,0.0295,33.9,8666,7.56,33.9],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",37.31,0.0319,30.7,8666,8.34,30.7],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",4.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",37.31,0.036,28.2,8666,9.07,28.2],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"None\",37.31,0.0367,27.7,8666,9.25,27.7],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",37.31,0.0375,28.9,3911,8.85,28.9],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",37.31,0.0382,27.7,3237,9.25,27.7],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",37.31,0.0534,26.7,3191,9.6,26.7],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",37.31,0.0577,25.5,3182,10.1,25.3],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",37.31,0.0915,10.5,5049,24.5,10.4],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",37.31,0.101,10.1,5052,25.3,10.1],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",37.31,0.114,22.6,3184,11.4,22.5],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",4.0,\"float32\",\"pytorch\",\"None\",\"None\",37.31,0.157,29.1,17177,8.93,28.7],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",37.31,0.174,18.5,3333,14.0,18.3],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",37.31,0.175,17.3,3333,14.9,17.2],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",37.09,0.0202,49.6,6052,5.16,49.6],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",37.09,0.0236,43.1,6052,5.94,43.1],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"bfloat16\",\"pytorch\",\"None\",\"None\",37.09,0.0247,40.3,6052,6.34,40.4],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",37.09,0.0254,42.6,3045,6.01,42.6],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",37.09,0.0255,42.5,2372,6.03,42.5],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"None\",37.09,0.0256,41.3,6052,6.21,41.2],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",37.09,0.0359,40.3,2324,6.36,40.3],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",37.09,0.0404,37.9,2318,6.77,37.8],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",37.09,0.0636,15.4,3588,16.7,15.3],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",37.09,0.0667,14.9,3589,17.2,14.9],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",37.09,0.0783,34.0,2321,7.57,33.8],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"float32\",\"pytorch\",\"None\",\"None\",37.09,0.106,41.6,11995,6.24,41.0],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",37.09,0.121,25.0,2439,10.3,24.9],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",37.09,0.122,27.2,2439,9.5,26.9],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",36.72,0.0204,48.9,6052,5.24,48.9],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",36.72,0.0236,44.0,6052,5.82,44.0],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"bfloat16\",\"pytorch\",\"None\",\"None\",36.72,0.0245,40.7,6052,6.29,40.7],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",36.72,0.0251,42.9,3045,5.97,42.9],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",36.72,0.0256,42.2,2372,6.07,42.2],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"None\",36.72,0.026,40.4,6052,6.34,40.4],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",36.72,0.0359,39.8,2324,6.44,39.8],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",36.72,0.04,37.4,2318,6.86,37.3],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",36.72,0.0636,15.4,3588,16.7,15.3],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",36.72,0.0674,15.1,3589,17.0,15.1],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",36.72,0.0776,34.2,2321,7.54,34.0],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"float32\",\"pytorch\",\"None\",\"None\",36.72,0.105,41.4,11995,6.27,40.8],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",36.72,0.118,26.9,2439,9.61,26.6],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",36.72,0.123,25.6,2439,10.1,25.3],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",36.15,0.021,47.6,6068,5.38,47.6],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",36.15,0.022,44.6,6068,5.74,44.6],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",4.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",36.15,0.0249,40.2,6068,6.36,40.3],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",36.15,0.0255,41.9,3061,6.11,41.9],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",36.15,0.0256,42.4,2388,6.05,42.3],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"None\",36.15,0.0266,40.1,6068,6.39,40.1],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",36.15,0.037,40.5,2340,6.33,40.4],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",36.15,0.0393,37.8,2334,6.79,37.7],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",36.15,0.0619,15.5,3605,16.5,15.5],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",36.15,0.0674,14.8,3605,17.3,14.8],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",36.15,0.0791,34.4,2338,7.49,34.2],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",4.0,\"float32\",\"pytorch\",\"None\",\"None\",36.15,0.106,41.1,12028,6.31,40.6],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",36.15,0.121,27.0,2457,9.57,26.8],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",4.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",36.15,0.122,25.6,2457,10.1,25.3],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",35.0,0.0154,62.0,3147,4.13,62.0],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",35.0,0.0158,61.7,3147,4.15,61.7],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"bfloat16\",\"pytorch\",\"None\",\"None\",35.0,0.0184,53.2,3147,4.81,53.2],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"float16\",\"pytorch\",\"None\",\"None\",35.0,0.0189,53.7,3147,4.77,53.7],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",35.0,0.019,55.4,1931,4.62,55.4],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",35.0,0.0198,54.8,1393,4.67,54.8],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",35.0,0.0218,52.6,1358,4.87,52.6],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",35.0,0.0228,48.8,1358,5.25,48.8],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",35.0,0.0441,43.9,1360,5.85,43.8],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",35.0,0.0476,20.4,1942,12.5,20.5],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",35.0,0.0559,19.6,1941,13.1,19.5],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",35.0,0.059,36.8,1428,6.99,36.6],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"float32\",\"pytorch\",\"None\",\"None\",35.0,0.0604,54.0,6186,4.78,53.6],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",35.0,0.0607,32.9,1428,7.8,32.8],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",34.46,0.0154,63.8,3147,4.02,63.7],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",34.46,0.016,61.6,3147,4.16,61.5],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",34.46,0.0191,54.8,1931,4.67,54.8],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"float16\",\"pytorch\",\"None\",\"None\",34.46,0.0194,53.5,3147,4.79,53.4],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",34.46,0.0197,55.6,1393,4.61,55.5],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"bfloat16\",\"pytorch\",\"None\",\"None\",34.46,0.0198,54.1,3147,4.73,54.1],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",34.46,0.0222,53.9,1358,4.75,53.9],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",34.46,0.0235,49.5,1358,5.17,49.5],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",34.46,0.0448,44.8,1360,5.73,44.7],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",34.46,0.0472,20.7,1942,12.3,20.8],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",34.46,0.0514,19.9,1941,12.9,19.8],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"float32\",\"pytorch\",\"None\",\"None\",34.46,0.0543,52.5,6186,4.91,52.1],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",34.46,0.0632,36.4,1428,7.06,36.3],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",34.46,0.0635,33.5,1428,7.67,33.4],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",34.42,0.0295,46.9,16060,5.47,46.8],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",34.42,0.0309,45.3,16060,5.66,45.2],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",0.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",34.42,0.0317,40.9,16060,6.27,40.8],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",34.42,0.0333,39.6,16060,6.47,39.6],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",34.42,0.0406,41.1,6659,6.25,41.0],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",34.42,0.0418,40.7,7737,6.31,40.6],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",34.42,0.0619,16.0,9622,16.0,16.0],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",34.42,0.0636,39.2,6523,6.56,39.0],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",34.42,0.0674,14.9,9622,17.2,14.9],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",34.42,0.0758,37.0,6523,6.98,36.7],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",34.42,0.26,37.0,31976,7.16,35.8],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",34.42,0.282,25.7,7001,10.2,25.1],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",34.42,0.288,27.5,7001,9.55,26.8],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",7.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",34.37,0.0267,68.0,16428,3.78,67.7],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",34.37,0.0275,78.7,16428,3.27,78.3],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",34.37,0.0277,71.2,16428,3.61,70.9],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"None\",34.37,0.0288,66.4,16428,3.87,66.1],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",34.37,0.0322,31.2,9183,8.21,31.2],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",34.37,0.0339,29.0,9183,8.83,29.0],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",34.37,0.0355,79.9,6002,3.23,79.3],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",34.37,0.0357,79.9,7615,3.23,79.3],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",34.37,0.0591,75.2,5699,3.45,74.2],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",34.37,0.0741,73.3,5699,3.55,72.1],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",7.0,\"float32\",\"pytorch\",\"None\",\"None\",34.37,0.269,43.1,32575,6.19,41.4],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",34.37,0.291,41.8,6764,6.39,40.1],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",34.37,0.299,43.1,6764,6.21,41.2],[\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"GPT-NeoX\",13.06,\"bfloat16\",\"pytorch\",\"None\",\"None\",33.33,0.0507,32.6,26825,7.88,32.5],[\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"GPT-NeoX\",13.06,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",33.33,0.0515,38.9,26825,6.6,38.8],[\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"GPT-NeoX\",13.06,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",33.33,0.0684,33.6,8454,7.67,33.4],[\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"GPT-NeoX\",13.06,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",33.33,0.0686,34.0,9802,7.57,33.8],[\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"GPT-NeoX\",13.06,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",33.33,0.0766,12.6,14315,20.4,12.5],[\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"GPT-NeoX\",13.06,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",33.33,0.0871,11.9,14313,21.6,11.9],[\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"GPT-NeoX\",13.06,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",33.33,0.109,31.7,8240,8.16,31.4],[\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"GPT-NeoX\",13.06,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",33.33,0.133,29.7,8240,8.72,29.4],[\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"GPT-NeoX\",13.06,\"float32\",\"pytorch\",\"None\",\"None\",33.33,0.445,23.0,53472,11.5,22.3],[\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"GPT-NeoX\",13.06,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",33.33,0.496,20.7,9009,12.8,20.0],[\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"GPT-NeoX\",13.06,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",33.33,0.505,21.4,9009,12.4,20.6],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",32.78,0.0107,93.1,2243,2.75,93.1],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",32.78,0.0122,88.5,2243,2.89,88.6],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"float16\",\"pytorch\",\"None\",\"None\",32.78,0.0125,78.2,2243,3.27,78.3],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",32.78,0.0131,79.7,1623,3.21,79.8],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"bfloat16\",\"pytorch\",\"None\",\"None\",32.78,0.0131,75.4,2243,3.39,75.5],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",32.78,0.0132,81.2,1085,3.15,81.3],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",32.78,0.0154,76.1,1050,3.37,76.0],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",32.78,0.0154,72.4,1050,3.54,72.3],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",32.78,0.0291,64.2,1076,4.0,64.0],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",32.78,0.0323,30.3,1448,8.45,30.3],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",32.78,0.0335,29.1,1450,8.8,29.1],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"float32\",\"pytorch\",\"None\",\"None\",32.78,0.038,79.2,4411,3.26,78.5],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",32.78,0.0397,54.3,1135,4.74,54.0],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",32.78,0.0408,50.1,1135,5.13,49.9],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",32.23,0.0233,43.3,8277,5.91,43.3],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",32.23,0.0261,41.7,8277,6.14,41.7],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",3.95,\"bfloat16\",\"pytorch\",\"None\",\"None\",32.23,0.0274,37.3,8277,6.86,37.3],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"None\",32.23,0.03,36.2,8277,7.07,36.2],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",32.23,0.0301,37.8,4056,6.78,37.8],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",32.23,0.0306,37.0,3315,6.92,37.0],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",32.23,0.0447,35.7,3242,7.19,35.6],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",32.23,0.0482,33.6,3238,7.64,33.5],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",32.23,0.0704,13.9,4819,18.5,13.8],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",32.23,0.0743,13.4,4816,19.1,13.4],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",32.23,0.101,30.5,3235,8.46,30.3],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",3.95,\"float32\",\"pytorch\",\"None\",\"None\",32.23,0.15,37.4,16325,6.96,36.8],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",32.23,0.165,24.5,3362,10.6,24.2],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",32.23,0.171,22.6,3359,11.5,22.3],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",32.14,0.023,43.0,8277,5.95,43.0],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",32.14,0.0249,40.6,8277,6.3,40.6],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",3.95,\"bfloat16\",\"pytorch\",\"None\",\"None\",32.14,0.0278,36.2,8277,7.08,36.2],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"None\",32.14,0.0284,35.8,8277,7.16,35.8],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",32.14,0.0297,37.2,3315,6.88,37.2],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",32.14,0.0303,37.1,4056,6.9,37.1],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",32.14,0.045,35.6,3242,7.21,35.5],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",32.14,0.0481,32.7,3238,7.85,32.6],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",32.14,0.0731,13.9,4819,18.4,13.9],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",32.14,0.0761,13.1,4816,19.5,13.1],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",32.14,0.103,30.1,3235,8.56,29.9],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",3.95,\"float32\",\"pytorch\",\"None\",\"None\",32.14,0.15,36.5,16325,7.14,35.9],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",32.14,0.17,22.6,3359,11.5,22.3],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",3.95,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",32.14,0.171,25.0,3362,10.4,24.6],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",31.55,0.0157,63.9,1028,4.01,63.8],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",31.55,0.0164,59.2,1028,4.33,59.1],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"None\",31.55,0.0181,55.3,1028,4.63,55.3],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",31.55,0.0186,56.0,869,4.57,56.0],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"bfloat16\",\"pytorch\",\"None\",\"None\",31.55,0.0193,53.6,1028,4.78,53.6],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",31.55,0.0195,54.5,601,4.7,54.5],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",31.55,0.0202,52.8,590,4.85,52.8],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",31.55,0.0203,49.0,590,5.22,49.0],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"float32\",\"pytorch\",\"None\",\"None\",31.55,0.0231,53.1,1948,4.82,53.1],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",31.55,0.0324,35.9,603,7.13,35.9],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",31.55,0.0351,44.7,592,5.75,44.5],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",31.55,0.0356,34.3,603,7.47,34.3],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",31.55,0.0471,20.2,742,12.6,20.3],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",31.55,0.0506,20.1,743,12.8,20.0],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",31.5,0.0143,93.8,7829,2.73,93.8],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",31.5,0.0154,90.1,7829,2.85,89.8],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",31.5,0.0154,79.2,7829,3.24,79.0],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"None\",31.5,0.0172,79.2,7829,3.24,79.0],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",31.5,0.0201,81.5,3195,3.15,81.3],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",31.5,0.0205,79.7,4271,3.22,79.5],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",31.5,0.0314,78.0,3060,3.3,77.6],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",31.5,0.0319,31.1,4609,8.24,31.1],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",31.5,0.0351,29.2,4609,8.76,29.2],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",31.5,0.0365,73.5,3060,3.51,72.9],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",31.5,0.0825,65.6,3280,3.97,64.5],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"float32\",\"pytorch\",\"None\",\"None\",31.5,0.127,74.1,15380,3.57,71.7],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",31.5,0.138,54.6,3516,4.81,53.2],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",31.5,0.138,50.3,3516,5.21,49.1],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",31.29,0.0153,65.6,1028,3.91,65.5],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",31.29,0.0158,59.7,1028,4.29,59.7],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",31.29,0.0186,55.9,869,4.58,55.9],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",0.51,\"bfloat16\",\"pytorch\",\"None\",\"None\",31.29,0.0191,55.1,1028,4.65,55.1],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",31.29,0.0195,55.6,601,4.61,55.5],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",31.29,0.0201,53.1,590,4.82,53.1],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",31.29,0.0203,49.1,590,5.21,49.1],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",0.51,\"float32\",\"pytorch\",\"None\",\"None\",31.29,0.0237,54.3,1948,4.72,54.2],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",31.29,0.0332,36.4,603,7.04,36.4],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",31.29,0.0346,44.4,592,5.77,44.4],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",31.29,0.0362,33.7,603,7.61,33.6],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",31.29,0.0484,20.2,742,12.6,20.3],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",31.29,0.0545,19.2,743,13.4,19.1],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",29.38,0.00793,128.0,458,2.01,127.0],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",29.38,0.00807,118.0,458,2.17,118.0],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"None\",29.38,0.00925,109.0,458,2.35,109.0],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"bfloat16\",\"pytorch\",\"None\",\"None\",29.38,0.00928,106.0,458,2.41,106.0],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",29.38,0.00966,107.0,521,2.39,107.0],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",29.38,0.00978,107.0,320,2.4,107.0],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",29.38,0.0103,95.5,315,2.68,95.5],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"float32\",\"pytorch\",\"None\",\"None\",29.38,0.0106,108.0,828,2.37,108.0],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",29.38,0.0106,102.0,315,2.5,102.0],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",29.38,0.0165,68.9,320,3.72,68.8],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",29.38,0.0172,87.9,317,2.92,87.7],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",29.38,0.0186,65.1,320,3.94,65.0],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",29.38,0.0244,39.4,363,6.49,39.4],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",29.38,0.0253,39.4,363,6.51,39.3],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",29.15,0.00391,243.0,126,1.05,244.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",29.15,0.00431,220.0,126,1.16,221.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"None\",29.15,0.00482,207.0,126,1.23,208.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"bfloat16\",\"pytorch\",\"None\",\"None\",29.15,0.00484,204.0,126,1.25,205.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",29.15,0.00499,198.0,186,1.29,198.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",29.15,0.00512,204.0,119,1.26,203.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",29.15,0.00521,182.0,118,1.41,182.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",29.15,0.00534,206.0,221,1.25,205.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",29.15,0.00535,192.0,118,1.34,191.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",29.15,0.00844,134.0,118,1.92,133.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",29.15,0.00905,126.0,118,2.04,125.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",29.15,0.00911,167.0,118,1.54,166.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",29.15,0.0124,78.7,121,3.25,78.8],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",29.15,0.0129,75.9,121,3.37,76.0],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",29.02,0.0079,129.0,458,1.98,129.0],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",29.02,0.00844,115.0,458,2.22,115.0],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"None\",29.02,0.00918,108.0,458,2.38,108.0],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",29.02,0.00968,106.0,521,2.41,106.0],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"bfloat16\",\"pytorch\",\"None\",\"None\",29.02,0.00969,107.0,458,2.4,107.0],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",29.02,0.00975,106.0,320,2.42,106.0],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"float32\",\"pytorch\",\"None\",\"None\",29.02,0.0103,108.0,828,2.37,108.0],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",29.02,0.0103,103.0,315,2.49,103.0],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",29.02,0.0105,92.4,315,2.77,92.4],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",29.02,0.0163,68.9,320,3.72,68.8],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",29.02,0.0179,65.9,320,3.89,65.8],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",29.02,0.018,82.8,317,3.1,82.6],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",29.02,0.024,40.1,363,6.38,40.1],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",29.02,0.0256,39.1,363,6.56,39.0],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.88,0.0155,63.4,1976,4.04,63.4],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",28.88,0.0173,58.1,1976,4.41,58.0],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.88,0.0191,55.0,1387,4.66,54.9],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",28.88,0.0196,50.4,1976,5.08,50.4],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.88,0.02,53.8,968,4.76,53.8],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.88,0.02,52.4,1976,4.89,52.4],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.88,0.0201,49.8,965,5.14,49.8],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.88,0.0204,54.5,983,4.7,54.5],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",28.88,0.0353,53.9,3836,4.77,53.7],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.88,0.0361,44.5,962,5.77,44.4],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",28.88,0.0425,36.0,998,7.13,35.9],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.88,0.0444,33.3,998,7.7,33.2],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",28.88,0.0483,20.1,1305,12.7,20.2],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.88,0.0507,19.8,1303,13.0,19.7],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.85,0.00389,245.0,126,1.04,246.0],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",28.85,0.00413,224.0,126,1.14,225.0],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.85,0.00474,204.0,126,1.25,205.0],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.85,0.00493,201.0,186,1.27,202.0],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"None\",28.85,0.00494,206.0,126,1.24,206.0],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.85,0.005,199.0,119,1.28,200.0],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",28.85,0.0051,204.0,221,1.26,203.0],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.85,0.00513,185.0,118,1.39,184.0],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.85,0.00515,196.0,118,1.31,195.0],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.85,0.00875,163.0,118,1.57,163.0],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",28.85,0.00893,131.0,118,1.96,131.0],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.85,0.0092,126.0,118,2.03,126.0],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",28.85,0.0123,79.7,121,3.21,79.8],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.85,0.0131,73.5,121,3.48,73.6],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.81,0.0041,238.0,126,1.07,239.0],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",28.81,0.00417,224.0,126,1.14,225.0],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"None\",28.81,0.00468,209.0,126,1.22,210.0],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.81,0.0048,204.0,126,1.25,205.0],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.81,0.00483,206.0,186,1.24,206.0],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.81,0.00516,202.0,119,1.27,202.0],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",28.81,0.00526,202.0,221,1.27,202.0],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.81,0.0053,195.0,118,1.32,194.0],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.81,0.00552,176.0,118,1.46,175.0],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.81,0.00875,166.0,118,1.55,165.0],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",28.81,0.00877,131.0,118,1.95,131.0],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.81,0.00906,126.0,118,2.03,126.0],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",28.81,0.0123,78.0,121,3.28,78.0],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.81,0.0127,75.9,121,3.37,76.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.61,0.00391,243.0,126,1.05,244.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",28.61,0.00413,218.0,126,1.17,219.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"None\",28.61,0.00472,207.0,126,1.23,208.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.61,0.00492,199.0,186,1.28,200.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.61,0.00496,201.0,119,1.27,202.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.61,0.00513,195.0,118,1.32,194.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.61,0.0053,204.0,126,1.26,203.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.61,0.00532,183.0,118,1.4,183.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",28.61,0.00545,207.0,221,1.24,206.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",28.61,0.00845,133.0,118,1.93,133.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.61,0.00881,166.0,118,1.55,165.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.61,0.00917,128.0,118,2.01,127.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",28.61,0.0125,78.0,121,3.28,78.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.61,0.0127,75.9,121,3.37,76.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.6,0.00387,241.0,126,1.06,242.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",28.6,0.00419,220.0,126,1.16,221.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"None\",28.6,0.0047,207.0,126,1.23,208.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.6,0.00481,204.0,126,1.25,205.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.6,0.00482,204.0,186,1.25,205.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.6,0.00499,198.0,119,1.29,198.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.6,0.00516,193.0,118,1.33,192.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",28.6,0.00545,209.0,221,1.23,208.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.6,0.00545,182.0,118,1.41,182.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",28.6,0.00854,134.0,118,1.91,134.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.6,0.00893,167.0,118,1.54,166.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.6,0.00917,125.0,118,2.05,125.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",28.6,0.0121,79.9,121,3.2,80.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.6,0.0128,74.6,121,3.43,74.6],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.44,0.00394,238.0,216,1.07,239.0],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",28.44,0.00421,220.0,216,1.16,221.0],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"float16\",\"pytorch\",\"None\",\"None\",28.44,0.00476,204.0,216,1.25,205.0],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.44,0.00486,201.0,216,1.27,202.0],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.44,0.00492,201.0,321,1.27,202.0],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.44,0.005,198.0,186,1.29,198.0],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.44,0.00529,181.0,184,1.42,180.0],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.44,0.00545,185.0,184,1.39,184.0],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"float32\",\"pytorch\",\"None\",\"None\",28.44,0.00549,207.0,397,1.24,206.0],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",28.44,0.00834,133.0,185,1.93,133.0],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.44,0.00888,165.0,184,1.56,164.0],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.44,0.00903,128.0,185,2.01,127.0],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",28.44,0.0121,78.5,193,3.26,78.5],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.44,0.013,75.4,193,3.39,75.5],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.27,0.00389,248.0,126,1.03,249.0],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",28.27,0.00429,220.0,126,1.16,221.0],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"None\",28.27,0.00472,207.0,126,1.23,208.0],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.27,0.00476,204.0,126,1.25,205.0],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.27,0.0049,202.0,186,1.26,203.0],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",28.27,0.00499,206.0,221,1.24,206.0],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.27,0.00499,199.0,119,1.28,200.0],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.27,0.00513,183.0,118,1.4,183.0],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.27,0.00521,193.0,118,1.33,192.0],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",28.27,0.00821,134.0,118,1.92,133.0],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.27,0.00895,158.0,118,1.62,158.0],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.27,0.00931,128.0,118,2.01,127.0],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",28.27,0.012,78.9,121,3.24,79.0],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.27,0.0129,75.7,121,3.38,75.7]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"GPT-NeoX\",\"marker\":{\"color\":\"#B68E00\",\"size\":[42420,42420,22498,22497,13222,14841,12919,12920,14172,14172,14429,14429,14429,14429,5027,6105,7990,4891,7990,4891,4980,28712,5370,5370,14429,14429,14429,14429,5027,6105,7990,4891,7990,4891,4980,28712,5370,5370,24619,24619,8106,9453,13360,13360,7892,7893,49074,8652,8652,14404,14404,14403,14403,5002,6080,7965,7965,4866,4952,28662,5343,5343,24619,24619,8106,9453,13360,13360,7892,7893,49074,8652,8652,22366,22366,8755,7475,12106,12106,7285,7284,7424,44535,7972,7971,6068,6068,3061,6068,2388,6068,2340,2334,3605,3605,2338,12028,2457,2457,14404,14404,14403,14403,5002,6080,4866,7965,7965,4866,4952,28662,5343,5343,8666,8666,8666,8666,3911,3237,3191,3182,5049,5052,3184,17177,3333,3333,6052,6052,6052,3045,2372,6052,2324,2318,3588,3589,2321,11995,2439,2439,6052,6052,6052,3045,2372,6052,2324,2318,3588,3589,2321,11995,2439,2439,6068,6068,6068,3061,2388,6068,2340,2334,3605,3605,2338,12028,2457,2457,3147,3147,3147,3147,1931,1393,1358,1358,1360,1942,1941,1428,6186,1428,3147,3147,1931,3147,1393,3147,1358,1358,1360,1942,1941,6186,1428,1428,16060,16060,16060,16060,6659,7737,9622,6523,9622,6523,31976,7001,7001,16428,16428,16428,16428,9183,9183,6002,7615,5699,5699,32575,6764,6764,26825,26825,8454,9802,14315,14313,8240,8240,53472,9009,9009,2243,2243,2243,1623,2243,1085,1050,1050,1076,1448,1450,4411,1135,1135,8277,8277,8277,8277,4056,3315,3242,3238,4819,4816,3235,16325,3362,3359,8277,8277,8277,8277,3315,4056,3242,3238,4819,4816,3235,16325,3359,3362,1028,1028,1028,869,1028,601,590,590,1948,603,592,603,742,743,7829,7829,7829,7829,3195,4271,3060,4609,4609,3060,3280,15380,3516,3516,1028,1028,869,1028,601,590,590,1948,603,592,603,742,743,458,458,458,458,521,320,315,828,315,320,317,320,363,363,126,126,126,126,186,119,118,221,118,118,118,118,121,121,458,458,458,521,458,320,828,315,315,320,320,317,363,363,1976,1976,1387,1976,968,1976,965,983,3836,962,998,998,1305,1303,126,126,126,186,126,119,221,118,118,118,118,118,121,121,126,126,126,126,186,119,221,118,118,118,118,118,121,121,126,126,126,186,119,118,126,118,221,118,118,118,121,121,126,126,126,126,186,119,118,221,118,118,118,118,121,121,216,216,216,216,321,186,184,184,397,185,184,185,193,193,126,126,126,126,186,221,119,118,118,118,118,118,121,121],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"GPT-NeoX\",\"showlegend\":true,\"x\":[10.3,8.9,23.8,24.5,9.34,9.32,10.0,10.9,17.2,17.4,5.26,5.52,6.28,6.4,6.04,6.3,16.4,6.57,17.0,7.0,7.74,6.91,9.43,10.1,5.31,6.23,6.32,5.7,6.1,6.12,16.2,6.39,17.3,6.88,7.83,6.89,10.2,9.45,5.94,7.01,7.0,6.8,18.6,19.4,7.18,7.76,10.4,11.6,11.2,5.27,5.44,6.41,6.34,6.05,6.0,16.4,17.0,6.84,7.8,6.86,9.43,10.1,5.9,7.21,6.99,6.79,18.5,19.3,7.25,7.79,10.4,11.6,11.2,5.97,7.02,6.86,7.04,18.5,19.1,7.31,7.84,9.14,9.62,10.8,11.3,5.24,5.89,6.16,6.35,6.0,6.49,6.46,6.82,16.5,17.3,7.77,6.2,9.56,10.2,5.26,5.66,6.17,6.34,6.41,6.28,6.49,16.4,16.9,6.82,7.69,6.91,9.43,10.2,7.56,8.34,9.07,9.25,8.85,9.25,9.6,10.1,24.5,25.3,11.4,8.93,14.0,14.9,5.16,5.94,6.34,6.01,6.03,6.21,6.36,6.77,16.7,17.2,7.57,6.24,10.3,9.5,5.24,5.82,6.29,5.97,6.07,6.34,6.44,6.86,16.7,17.0,7.54,6.27,9.61,10.1,5.38,5.74,6.36,6.11,6.05,6.39,6.33,6.79,16.5,17.3,7.49,6.31,9.57,10.1,4.13,4.15,4.81,4.77,4.62,4.67,4.87,5.25,5.85,12.5,13.1,6.99,4.78,7.8,4.02,4.16,4.67,4.79,4.61,4.73,4.75,5.17,5.73,12.3,12.9,4.91,7.06,7.67,5.47,5.66,6.27,6.47,6.25,6.31,16.0,6.56,17.2,6.98,7.16,10.2,9.55,3.78,3.27,3.61,3.87,8.21,8.83,3.23,3.23,3.45,3.55,6.19,6.39,6.21,7.88,6.6,7.67,7.57,20.4,21.6,8.16,8.72,11.5,12.8,12.4,2.75,2.89,3.27,3.21,3.39,3.15,3.37,3.54,4.0,8.45,8.8,3.26,4.74,5.13,5.91,6.14,6.86,7.07,6.78,6.92,7.19,7.64,18.5,19.1,8.46,6.96,10.6,11.5,5.95,6.3,7.08,7.16,6.88,6.9,7.21,7.85,18.4,19.5,8.56,7.14,11.5,10.4,4.01,4.33,4.63,4.57,4.78,4.7,4.85,5.22,4.82,7.13,5.75,7.47,12.6,12.8,2.73,2.85,3.24,3.24,3.15,3.22,3.3,8.24,8.76,3.51,3.97,3.57,4.81,5.21,3.91,4.29,4.58,4.65,4.61,4.82,5.21,4.72,7.04,5.77,7.61,12.6,13.4,2.01,2.17,2.35,2.41,2.39,2.4,2.68,2.37,2.5,3.72,2.92,3.94,6.49,6.51,1.05,1.16,1.23,1.25,1.29,1.26,1.41,1.25,1.34,1.92,2.04,1.54,3.25,3.37,1.98,2.22,2.38,2.41,2.4,2.42,2.37,2.49,2.77,3.72,3.89,3.1,6.38,6.56,4.04,4.41,4.66,5.08,4.76,4.89,5.14,4.7,4.77,5.77,7.13,7.7,12.7,13.0,1.04,1.14,1.25,1.27,1.24,1.28,1.26,1.39,1.31,1.57,1.96,2.03,3.21,3.48,1.07,1.14,1.22,1.25,1.24,1.27,1.27,1.32,1.46,1.55,1.95,2.03,3.28,3.37,1.05,1.17,1.23,1.28,1.27,1.32,1.26,1.4,1.24,1.93,1.55,2.01,3.28,3.37,1.06,1.16,1.23,1.25,1.25,1.29,1.33,1.23,1.41,1.91,1.54,2.05,3.2,3.43,1.07,1.16,1.25,1.27,1.27,1.29,1.42,1.39,1.24,1.93,1.56,2.01,3.26,3.39,1.03,1.16,1.23,1.25,1.26,1.24,1.28,1.4,1.33,1.92,1.62,2.01,3.24,3.38],\"xaxis\":\"x\",\"y\":[41.69,41.69,41.69,41.69,41.69,41.69,41.69,41.69,41.69,41.69,41.49,41.49,41.49,41.49,41.49,41.49,41.49,41.49,41.49,41.49,41.49,41.49,41.49,41.49,41.25,41.25,41.25,41.25,41.25,41.25,41.25,41.25,41.25,41.25,41.25,41.25,41.25,41.25,39.7,39.7,39.7,39.7,39.7,39.7,39.7,39.7,39.7,39.7,39.7,39.3,39.3,39.3,39.3,39.3,39.3,39.3,39.3,39.3,39.3,39.3,39.3,39.3,38.82,38.82,38.82,38.82,38.82,38.82,38.82,38.82,38.82,38.82,38.82,38.59,38.59,38.59,38.59,38.59,38.59,38.59,38.59,38.59,38.59,38.59,38.59,38.54,38.54,38.54,38.54,38.54,38.54,38.54,38.54,38.54,38.54,38.54,38.54,38.54,38.54,38.06,38.06,38.06,38.06,38.06,38.06,38.06,38.06,38.06,38.06,38.06,38.06,38.06,38.06,37.31,37.31,37.31,37.31,37.31,37.31,37.31,37.31,37.31,37.31,37.31,37.31,37.31,37.31,37.09,37.09,37.09,37.09,37.09,37.09,37.09,37.09,37.09,37.09,37.09,37.09,37.09,37.09,36.72,36.72,36.72,36.72,36.72,36.72,36.72,36.72,36.72,36.72,36.72,36.72,36.72,36.72,36.15,36.15,36.15,36.15,36.15,36.15,36.15,36.15,36.15,36.15,36.15,36.15,36.15,36.15,35.0,35.0,35.0,35.0,35.0,35.0,35.0,35.0,35.0,35.0,35.0,35.0,35.0,35.0,34.46,34.46,34.46,34.46,34.46,34.46,34.46,34.46,34.46,34.46,34.46,34.46,34.46,34.46,34.42,34.42,34.42,34.42,34.42,34.42,34.42,34.42,34.42,34.42,34.42,34.42,34.42,34.37,34.37,34.37,34.37,34.37,34.37,34.37,34.37,34.37,34.37,34.37,34.37,34.37,33.33,33.33,33.33,33.33,33.33,33.33,33.33,33.33,33.33,33.33,33.33,32.78,32.78,32.78,32.78,32.78,32.78,32.78,32.78,32.78,32.78,32.78,32.78,32.78,32.78,32.23,32.23,32.23,32.23,32.23,32.23,32.23,32.23,32.23,32.23,32.23,32.23,32.23,32.23,32.14,32.14,32.14,32.14,32.14,32.14,32.14,32.14,32.14,32.14,32.14,32.14,32.14,32.14,31.55,31.55,31.55,31.55,31.55,31.55,31.55,31.55,31.55,31.55,31.55,31.55,31.55,31.55,31.5,31.5,31.5,31.5,31.5,31.5,31.5,31.5,31.5,31.5,31.5,31.5,31.5,31.5,31.29,31.29,31.29,31.29,31.29,31.29,31.29,31.29,31.29,31.29,31.29,31.29,31.29,29.38,29.38,29.38,29.38,29.38,29.38,29.38,29.38,29.38,29.38,29.38,29.38,29.38,29.38,29.15,29.15,29.15,29.15,29.15,29.15,29.15,29.15,29.15,29.15,29.15,29.15,29.15,29.15,29.02,29.02,29.02,29.02,29.02,29.02,29.02,29.02,29.02,29.02,29.02,29.02,29.02,29.02,28.88,28.88,28.88,28.88,28.88,28.88,28.88,28.88,28.88,28.88,28.88,28.88,28.88,28.88,28.85,28.85,28.85,28.85,28.85,28.85,28.85,28.85,28.85,28.85,28.85,28.85,28.85,28.85,28.81,28.81,28.81,28.81,28.81,28.81,28.81,28.81,28.81,28.81,28.81,28.81,28.81,28.81,28.61,28.61,28.61,28.61,28.61,28.61,28.61,28.61,28.61,28.61,28.61,28.61,28.61,28.61,28.6,28.6,28.6,28.6,28.6,28.6,28.6,28.6,28.6,28.6,28.6,28.6,28.6,28.6,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.27,28.27,28.27,28.27,28.27,28.27,28.27,28.27,28.27,28.27,28.27,28.27,28.27,28.27],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",6.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",40.1,0.0318,33.5,12726,7.65,33.5],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",6.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",40.1,0.0318,32.3,12721,7.92,32.3],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",6.0,\"float16\",\"pytorch\",\"None\",\"None\",40.1,0.0332,31.0,12721,8.25,31.0],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",6.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",40.1,0.0406,31.2,5593,8.21,31.2],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",6.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",40.1,0.0412,30.8,4515,8.31,30.8],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",6.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",40.1,0.0571,31.2,4376,8.22,31.1],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",6.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",40.1,0.0647,29.0,4376,8.84,29.0],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",6.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",40.1,0.0833,11.7,7092,21.9,11.7],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",6.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",40.1,0.0881,11.5,7088,22.3,11.5],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",6.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",40.1,0.148,25.2,4381,10.2,25.1],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",6.0,\"float32\",\"pytorch\",\"None\",\"None\",40.1,0.213,33.4,25284,7.84,32.7],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",6.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",40.1,0.242,20.7,4693,12.5,20.5],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",6.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",40.1,0.243,21.6,4693,12.0,21.3]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"GPT-J\",\"marker\":{\"color\":\"#C9FBE5\",\"size\":[12726,12721,12721,5593,4515,4376,4376,7092,7088,4381,25284,4693,4693],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"GPT-J\",\"showlegend\":true,\"x\":[7.65,7.92,8.25,8.21,8.31,8.22,8.84,21.9,22.3,10.2,7.84,12.5,12.0],\"xaxis\":\"x\",\"y\":[40.1,40.1,40.1,40.1,40.1,40.1,40.1,40.1,40.1,40.1,40.1,40.1,40.1],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"bigscience\\u002fbloom-7b1\",\"\ud83c\udf38 Bloom\",7.07,\"bfloat16\",\"pytorch\",\"None\",\"None\",39.18,0.0281,53.1,14651,4.83,53.0],[\"bigscience\\u002fbloom-7b1\",\"\ud83c\udf38 Bloom\",7.07,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",39.18,0.0364,54.4,5846,4.73,54.1],[\"bigscience\\u002fbloom-7b1\",\"\ud83c\udf38 Bloom\",7.07,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",39.18,0.0365,54.3,6923,4.74,54.0],[\"bigscience\\u002fbloom-7b1\",\"\ud83c\udf38 Bloom\",7.07,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",39.18,0.0557,17.5,8616,14.7,17.4],[\"bigscience\\u002fbloom-7b1\",\"\ud83c\udf38 Bloom\",7.07,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",39.18,0.0573,51.6,5709,5.0,51.2],[\"bigscience\\u002fbloom-7b1\",\"\ud83c\udf38 Bloom\",7.07,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",39.18,0.059,17.1,8615,15.0,17.1],[\"bigscience\\u002fbloom-7b1\",\"\ud83c\udf38 Bloom\",7.07,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",39.18,0.0675,47.8,5709,5.4,47.4],[\"bigscience\\u002fbloom-7b1\",\"\ud83c\udf38 Bloom\",7.07,\"float32\",\"pytorch\",\"None\",\"None\",39.18,0.257,46.0,29292,5.8,44.1],[\"bigscience\\u002fbloom-7b1\",\"\ud83c\udf38 Bloom\",7.07,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",39.18,0.257,31.2,6120,8.44,30.3],[\"bigscience\\u002fbloom-7b1\",\"\ud83c\udf38 Bloom\",7.07,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",39.18,0.258,31.8,6120,8.29,30.9],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",3.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",36.07,0.0177,55.4,6365,4.62,55.4],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",3.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",36.07,0.0193,53.6,6365,4.78,53.6],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",3.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",36.07,0.0201,55.6,3591,4.61,55.5],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",3.0,\"float16\",\"pytorch\",\"None\",\"None\",36.07,0.0202,53.5,6365,4.79,53.4],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",3.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",36.07,0.0203,55.0,2918,4.66,54.9],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",3.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",36.07,0.0312,53.1,2869,4.83,53.0],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",3.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",36.07,0.0357,49.7,2863,5.17,49.5],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",3.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",36.07,0.0561,17.6,4056,14.6,17.5],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",36.07,0.058,17.2,4059,14.9,17.2],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",3.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",36.07,0.0669,43.7,2867,5.91,43.3],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",3.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",36.07,0.107,32.0,2966,8.09,31.6],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",3.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",36.07,0.11,30.8,2966,8.38,30.5],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",3.0,\"float32\",\"pytorch\",\"None\",\"None\",36.07,0.113,54.8,12734,4.76,53.8],[\"TurkuNLP\\u002fgpt3-finnish-13B\",\"\ud83c\udf38 Bloom\",13.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",32.95,0.0495,40.7,27362,6.31,40.6],[\"TurkuNLP\\u002fgpt3-finnish-13B\",\"\ud83c\udf38 Bloom\",13.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",32.95,0.0663,41.7,10325,6.18,41.4],[\"TurkuNLP\\u002fgpt3-finnish-13B\",\"\ud83c\udf38 Bloom\",13.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",32.95,0.067,41.5,8977,6.22,41.2],[\"TurkuNLP\\u002fgpt3-finnish-13B\",\"\ud83c\udf38 Bloom\",13.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",32.95,0.0739,13.0,14853,19.7,13.0],[\"TurkuNLP\\u002fgpt3-finnish-13B\",\"\ud83c\udf38 Bloom\",13.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",32.95,0.0785,12.8,14857,20.0,12.8],[\"TurkuNLP\\u002fgpt3-finnish-13B\",\"\ud83c\udf38 Bloom\",13.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",32.95,0.106,40.0,8762,6.49,39.4],[\"TurkuNLP\\u002fgpt3-finnish-13B\",\"\ud83c\udf38 Bloom\",13.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",32.95,0.133,37.3,8762,6.97,36.7],[\"TurkuNLP\\u002fgpt3-finnish-13B\",\"\ud83c\udf38 Bloom\",13.0,\"float32\",\"pytorch\",\"None\",\"None\",32.95,0.461,25.9,54714,10.3,24.9],[\"TurkuNLP\\u002fgpt3-finnish-13B\",\"\ud83c\udf38 Bloom\",13.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",32.95,0.491,24.1,9440,11.1,23.1],[\"TurkuNLP\\u002fgpt3-finnish-13B\",\"\ud83c\udf38 Bloom\",13.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",32.95,0.51,24.3,9440,11.0,23.3],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",29.11,0.0135,69.5,1939,3.68,69.6],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",29.11,0.0151,67.3,1939,3.81,67.2],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",29.11,0.0152,68.0,1939,3.77,67.9],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",29.11,0.0155,70.4,962,3.64,70.3],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",29.11,0.0156,71.4,1365,3.59,71.3],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",29.11,0.0163,60.7,942,4.22,60.7],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",29.11,0.0164,66.9,945,3.83,66.8],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",29.11,0.0301,55.2,943,4.65,55.1],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",29.11,0.0354,39.2,972,6.54,39.1],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",29.11,0.036,38.9,972,6.59,38.8],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",29.11,0.0373,70.8,3893,3.64,70.3],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",29.11,0.0454,21.6,1262,11.8,21.7],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",29.11,0.0469,20.7,1259,12.3,20.8],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",27.95,0.0072,134.0,480,1.92,133.0],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",27.95,0.00752,134.0,480,1.91,134.0],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",0.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",27.95,0.00784,128.0,480,2.0,128.0],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",27.95,0.00786,132.0,553,1.94,132.0],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",27.95,0.00803,129.0,352,1.99,129.0],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",27.95,0.00843,126.0,347,2.03,126.0],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",27.95,0.00856,115.0,347,2.23,115.0],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",27.95,0.011,135.0,934,1.9,135.0],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",27.95,0.0149,76.8,350,3.33,76.9],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",27.95,0.0157,106.0,350,2.42,106.0],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",27.95,0.0163,75.7,350,3.39,75.5],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",27.95,0.023,41.7,392,6.13,41.8],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",27.95,0.0238,41.8,393,6.12,41.8]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83c\udf38 Bloom\",\"marker\":{\"color\":\"#FF0092\",\"size\":[14651,5846,6923,8616,5709,8615,5709,29292,6120,6120,6365,6365,3591,6365,2918,2869,2863,4056,4059,2867,2966,2966,12734,27362,10325,8977,14853,14857,8762,8762,54714,9440,9440,1939,1939,1939,962,1365,942,945,943,972,972,3893,1262,1259,480,480,480,553,352,347,347,934,350,350,350,392,393],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83c\udf38 Bloom\",\"showlegend\":true,\"x\":[4.83,4.73,4.74,14.7,5.0,15.0,5.4,5.8,8.44,8.29,4.62,4.78,4.61,4.79,4.66,4.83,5.17,14.6,14.9,5.91,8.09,8.38,4.76,6.31,6.18,6.22,19.7,20.0,6.49,6.97,10.3,11.1,11.0,3.68,3.81,3.77,3.64,3.59,4.22,3.83,4.65,6.54,6.59,3.64,11.8,12.3,1.92,1.91,2.0,1.94,1.99,2.03,2.23,1.9,3.33,2.42,3.39,6.13,6.12],\"xaxis\":\"x\",\"y\":[39.18,39.18,39.18,39.18,39.18,39.18,39.18,39.18,39.18,39.18,36.07,36.07,36.07,36.07,36.07,36.07,36.07,36.07,36.07,36.07,36.07,36.07,36.07,32.95,32.95,32.95,32.95,32.95,32.95,32.95,32.95,32.95,32.95,29.11,29.11,29.11,29.11,29.11,29.11,29.11,29.11,29.11,29.11,29.11,29.11,29.11,27.95,27.95,27.95,27.95,27.95,27.95,27.95,27.95,27.95,27.95,27.95,27.95,27.95],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"facebook\\u002fxglm-7.5B\",\"XGLM\",7.5,\"bfloat16\",\"pytorch\",\"None\",\"None\",36.38,0.0308,52.5,15548,4.89,52.4],[\"facebook\\u002fxglm-7.5B\",\"XGLM\",7.5,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",36.38,0.0402,52.1,6149,4.93,51.9],[\"facebook\\u002fxglm-7.5B\",\"XGLM\",7.5,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",36.38,0.0411,53.0,7228,4.85,52.8],[\"facebook\\u002fxglm-7.5B\",\"XGLM\",7.5,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",36.38,0.0612,50.0,6011,5.16,49.6],[\"facebook\\u002fxglm-7.5B\",\"XGLM\",7.5,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",36.38,0.0703,43.5,6011,5.93,43.2],[\"facebook\\u002fxglm-7.5B\",\"XGLM\",7.5,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",36.38,0.0823,12.4,9110,20.7,12.4],[\"facebook\\u002fxglm-7.5B\",\"XGLM\",7.5,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",36.38,0.284,25.7,6318,10.2,25.1],[\"facebook\\u002fxglm-7.5B\",\"XGLM\",7.5,\"float32\",\"pytorch\",\"None\",\"None\",36.38,0.286,39.3,31087,6.78,37.8],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",5.08,\"bfloat16\",\"pytorch\",\"None\",\"None\",34.31,0.0292,36.6,9526,6.99,36.6],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",5.08,\"float16\",\"pytorch\",\"None\",\"None\",34.31,0.033,35.3,9526,7.26,35.3],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",5.08,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",34.31,0.0361,36.6,3636,7.0,36.6],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",5.08,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",34.31,0.0382,36.7,4715,6.98,36.7],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",5.08,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",34.31,0.0567,34.4,3564,7.48,34.2],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",5.08,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",34.31,0.0594,30.0,3564,8.55,29.9],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",5.08,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",34.31,0.121,25.6,3570,10.1,25.3],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",5.08,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",34.31,0.134,8.15,5503,31.4,8.15],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",5.08,\"float32\",\"pytorch\",\"None\",\"None\",34.31,0.167,37.4,19042,6.99,36.6],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",5.08,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",34.31,0.19,17.2,3737,15.0,17.1],[\"facebook\\u002fxglm-564M\",\"XGLM\",0.56,\"bfloat16\",\"pytorch\",\"None\",\"None\",29.55,0.0155,75.7,1325,3.39,75.5],[\"facebook\\u002fxglm-564M\",\"XGLM\",0.56,\"float16\",\"pytorch\",\"None\",\"None\",29.55,0.0158,74.6,1325,3.44,74.4],[\"facebook\\u002fxglm-564M\",\"XGLM\",0.56,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",29.55,0.0162,74.1,1156,3.46,74.0],[\"facebook\\u002fxglm-564M\",\"XGLM\",0.56,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",29.55,0.0164,71.4,886,3.59,71.3],[\"facebook\\u002fxglm-564M\",\"XGLM\",0.56,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",29.55,0.0177,68.7,877,3.73,68.6],[\"facebook\\u002fxglm-564M\",\"XGLM\",0.56,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",29.55,0.0179,58.4,877,4.39,58.3],[\"facebook\\u002fxglm-564M\",\"XGLM\",0.56,\"float32\",\"pytorch\",\"None\",\"None\",29.55,0.0282,73.7,2644,3.49,73.4],[\"facebook\\u002fxglm-564M\",\"XGLM\",0.56,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",29.55,0.0392,49.7,878,5.17,49.5],[\"facebook\\u002fxglm-564M\",\"XGLM\",0.56,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",29.55,0.0401,33.6,890,7.64,33.5],[\"facebook\\u002fxglm-564M\",\"XGLM\",0.56,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",29.55,0.0626,16.6,1023,15.5,16.5]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"XGLM\",\"marker\":{\"color\":\"#22FFA7\",\"size\":[15548,6149,7228,6011,6011,9110,6318,31087,9526,9526,3636,4715,3564,3564,3570,5503,19042,3737,1325,1325,1156,886,877,877,2644,878,890,1023],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"XGLM\",\"showlegend\":true,\"x\":[4.89,4.93,4.85,5.16,5.93,20.7,10.2,6.78,6.99,7.26,7.0,6.98,7.48,8.55,10.1,31.4,6.99,15.0,3.39,3.44,3.46,3.59,3.73,4.39,3.49,5.17,7.64,15.5],\"xaxis\":\"x\",\"y\":[36.38,36.38,36.38,36.38,36.38,36.38,36.38,36.38,34.31,34.31,34.31,34.31,34.31,34.31,34.31,34.31,34.31,34.31,29.55,29.55,29.55,29.55,29.55,29.55,29.55,29.55,29.55,29.55],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",2.72,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",36.2,0.017,56.7,5782,4.52,56.6],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",2.72,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",36.2,0.0219,46.6,5782,5.49,46.6],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",2.72,\"bfloat16\",\"pytorch\",\"None\",\"None\",36.2,0.0222,44.4,5787,5.76,44.4],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",2.72,\"float16\",\"pytorch\",\"None\",\"None\",36.2,0.0241,43.9,5787,5.83,43.9],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",2.72,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",36.2,0.0257,45.5,2129,5.63,45.5],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",2.72,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",36.2,0.0264,46.4,2803,5.52,46.4],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",2.72,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",36.2,0.0366,44.9,2080,5.72,44.8],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",2.72,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",36.2,0.0402,41.2,2073,6.23,41.1],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",2.72,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",36.2,0.0797,32.7,2077,7.87,32.5],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",2.72,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",36.2,0.0822,12.1,3336,21.1,12.1],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",2.72,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",36.2,0.0864,11.8,3341,21.8,11.7],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",2.72,\"float32\",\"pytorch\",\"None\",\"None\",36.2,0.104,50.3,11555,5.17,49.5],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",2.72,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",36.2,0.117,25.2,2164,10.2,25.1],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",2.72,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",36.2,0.121,24.5,2169,10.5,24.4],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",1.37,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",33.58,0.0129,75.4,2943,3.39,75.5],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",1.37,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",33.58,0.0158,63.9,2943,4.01,63.8],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",1.37,\"float16\",\"pytorch\",\"None\",\"None\",33.58,0.0172,59.7,2946,4.29,59.7],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",1.37,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",33.58,0.0182,60.9,1732,4.21,60.8],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",1.37,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",33.58,0.0187,60.9,1193,4.21,60.8],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",1.37,\"bfloat16\",\"pytorch\",\"None\",\"None\",33.58,0.019,59.7,2946,4.29,59.7],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",1.37,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",33.58,0.0227,60.6,1157,4.23,60.5],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",1.37,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",33.58,0.0239,52.8,1157,4.85,52.8],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",1.37,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",33.58,0.0467,44.0,1160,5.84,43.8],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",1.37,\"float32\",\"pytorch\",\"None\",\"None\",33.58,0.0567,62.3,5775,4.15,61.7],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",1.37,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",33.58,0.0606,32.7,1206,7.85,32.6],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",1.37,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",33.58,0.0637,16.1,1736,15.9,16.1],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",1.37,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",33.58,0.065,32.4,1210,7.95,32.2],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",1.37,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",33.58,0.0676,15.6,1740,16.4,15.6],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",0.15,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",29.47,0.00641,147.0,363,1.74,147.0],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",0.15,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",29.47,0.00791,122.0,363,2.1,122.0],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",0.15,\"float16\",\"pytorch\",\"None\",\"None\",29.47,0.00838,121.0,363,2.12,121.0],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",0.15,\"bfloat16\",\"pytorch\",\"None\",\"None\",29.47,0.00875,115.0,363,2.22,115.0],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",0.15,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",29.47,0.00928,115.0,436,2.22,115.0],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",0.15,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",29.47,0.00953,114.0,235,2.25,114.0],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",0.15,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",29.47,0.00962,103.0,230,2.49,103.0],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",0.15,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",29.47,0.00975,116.0,230,2.2,116.0],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",0.15,\"float32\",\"pytorch\",\"None\",\"None\",29.47,0.00981,123.0,658,2.08,123.0],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",0.15,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",29.47,0.0196,64.2,233,3.99,64.2],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",0.15,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",29.47,0.0208,85.9,230,2.99,85.6],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",0.15,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",29.47,0.021,59.9,233,4.28,59.8],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",0.15,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",29.47,0.0324,31.2,271,8.2,31.2],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",0.15,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",29.47,0.0325,31.3,270,8.18,31.3],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",29.28,0.0135,75.4,3045,3.39,75.5],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",29.28,0.0157,65.7,3045,3.9,65.6],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",29.28,0.0172,61.4,3048,4.17,61.4],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",29.28,0.0172,58.9,3048,4.35,58.9],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",29.28,0.0189,59.9,1834,4.28,59.8],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",29.28,0.0195,59.3,1296,4.32,59.3],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",29.28,0.0224,57.6,1260,4.45,57.5],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",29.28,0.0233,51.9,1260,4.93,51.9],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",29.28,0.0471,43.4,1262,5.93,43.2],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",29.28,0.0553,63.8,5980,4.06,63.1],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",29.28,0.062,33.1,1308,7.77,32.9],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",29.28,0.0633,15.8,1838,16.2,15.8],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",29.28,0.0642,15.6,1842,16.4,15.6],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",29.28,0.066,32.2,1312,7.98,32.1],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",29.14,0.0047,193.0,77,1.32,194.0],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",29.14,0.00526,182.0,112,1.41,182.0],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",29.14,0.00542,182.0,77,1.41,182.0],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",0.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",29.14,0.00571,175.0,77,1.47,174.0],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",29.14,0.00643,173.0,77,1.48,173.0],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",29.14,0.0132,93.8,75,2.73,93.8],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",29.14,0.0138,90.7,75,2.82,90.8],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",29.14,0.0214,47.0,76,5.45,47.0],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",29.14,0.0217,45.4,76,5.64,45.4],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.44,0.00444,212.0,182,1.2,213.0],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",28.44,0.00536,177.0,182,1.45,177.0],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"None\",28.44,0.00584,176.0,182,1.46,175.0],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",0.03,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.44,0.00608,171.0,182,1.5,171.0],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",28.44,0.00613,183.0,320,1.4,183.0],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.44,0.0064,170.0,146,1.51,170.0],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.44,0.0065,170.0,281,1.51,170.0],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.44,0.00674,151.0,144,1.7,151.0],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.44,0.00684,163.0,144,1.57,163.0],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",28.44,0.0136,93.8,145,2.73,93.8],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.44,0.0137,123.0,144,2.08,123.0],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.44,0.0138,91.1,145,2.81,91.1],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",28.44,0.0208,46.8,156,5.47,46.8],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.44,0.0216,45.5,156,5.62,45.6],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.41,0.00256,354.0,197,0.723,354.0],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",28.41,0.00311,313.0,197,0.818,313.0],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"None\",28.41,0.00332,309.0,197,0.829,309.0],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",0.03,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.41,0.00337,300.0,197,0.853,300.0],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.41,0.00359,302.0,360,0.847,302.0],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.41,0.0036,301.0,159,0.851,301.0],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.41,0.00371,272.0,154,0.942,272.0],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.41,0.00375,293.0,154,0.873,293.0],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",28.41,0.004,324.0,366,0.79,324.0],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",28.41,0.00698,176.0,154,1.46,175.0],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.41,0.00745,228.0,154,1.13,227.0],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.41,0.00749,172.0,155,1.49,172.0],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",28.41,0.0109,89.5,166,2.86,89.5],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",0.03,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.41,0.0112,88.9,168,2.88,88.9],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",0.01,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.31,0.00446,220.0,112,1.16,221.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",0.01,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",28.31,0.00531,183.0,112,1.4,183.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",0.01,\"float32\",\"pytorch\",\"None\",\"None\",28.31,0.00564,186.0,183,1.38,186.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",0.01,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.31,0.00583,175.0,112,1.47,174.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",0.01,\"float16\",\"pytorch\",\"None\",\"None\",28.31,0.00591,178.0,112,1.44,178.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",0.01,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.31,0.00631,169.0,170,1.52,168.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",0.01,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.31,0.00635,153.0,102,1.68,152.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",0.01,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.31,0.00647,168.0,103,1.53,167.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",0.01,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.31,0.00651,169.0,102,1.52,168.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",0.01,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",28.31,0.0129,93.8,102,2.73,93.8],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",0.01,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.31,0.0137,92.1,102,2.78,92.1],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",0.01,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.31,0.0139,126.0,102,2.04,125.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",0.01,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",28.31,0.0209,46.4,105,5.52,46.4],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",0.01,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.31,0.022,45.5,105,5.62,45.6],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.19,0.00452,218.0,87,1.17,219.0],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"None\",28.19,0.00536,180.0,87,1.43,179.0],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",28.19,0.00546,185.0,132,1.39,184.0],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",28.19,0.00582,178.0,87,1.44,178.0],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",0.0,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.19,0.00591,171.0,87,1.5,171.0],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.19,0.00634,169.0,84,1.52,168.0],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.19,0.00643,149.0,84,1.72,149.0],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.19,0.00651,167.0,84,1.54,166.0],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.4bit\",28.19,0.0128,95.1,84,2.69,95.2],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.19,0.0137,92.7,84,2.76,92.8],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.19,0.0142,128.0,84,2.0,128.0],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"BetterTransformer\",\"BnB.8bit\",28.19,0.021,46.4,85,5.52,46.4],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.19,0.0212,46.4,85,5.52,46.4]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"GPT-Neo\",\"marker\":{\"color\":\"#E3EE9E\",\"size\":[5782,5782,5787,5787,2129,2803,2080,2073,2077,3336,3341,11555,2164,2169,2943,2943,2946,1732,1193,2946,1157,1157,1160,5775,1206,1736,1210,1740,363,363,363,363,436,235,230,230,658,233,230,233,271,270,3045,3045,3048,3048,1834,1296,1260,1260,1262,5980,1308,1838,1842,1312,77,112,77,77,77,75,75,76,76,182,182,182,182,320,146,281,144,144,145,144,145,156,156,197,197,197,197,360,159,154,154,366,154,154,155,166,168,112,112,183,112,112,170,102,103,102,102,102,102,105,105,87,87,132,87,87,84,84,84,84,84,84,85,85],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"GPT-Neo\",\"showlegend\":true,\"x\":[4.52,5.49,5.76,5.83,5.63,5.52,5.72,6.23,7.87,21.1,21.8,5.17,10.2,10.5,3.39,4.01,4.29,4.21,4.21,4.29,4.23,4.85,5.84,4.15,7.85,15.9,7.95,16.4,1.74,2.1,2.12,2.22,2.22,2.25,2.49,2.2,2.08,3.99,2.99,4.28,8.2,8.18,3.39,3.9,4.17,4.35,4.28,4.32,4.45,4.93,5.93,4.06,7.77,16.2,16.4,7.98,1.32,1.41,1.41,1.47,1.48,2.73,2.82,5.45,5.64,1.2,1.45,1.46,1.5,1.4,1.51,1.51,1.7,1.57,2.73,2.08,2.81,5.47,5.62,0.723,0.818,0.829,0.853,0.847,0.851,0.942,0.873,0.79,1.46,1.13,1.49,2.86,2.88,1.16,1.4,1.38,1.47,1.44,1.52,1.68,1.53,1.52,2.73,2.78,2.04,5.52,5.62,1.17,1.43,1.39,1.44,1.5,1.52,1.72,1.54,2.69,2.76,2.0,5.52,5.52],\"xaxis\":\"x\",\"y\":[36.2,36.2,36.2,36.2,36.2,36.2,36.2,36.2,36.2,36.2,36.2,36.2,36.2,36.2,33.58,33.58,33.58,33.58,33.58,33.58,33.58,33.58,33.58,33.58,33.58,33.58,33.58,33.58,29.47,29.47,29.47,29.47,29.47,29.47,29.47,29.47,29.47,29.47,29.47,29.47,29.47,29.47,29.28,29.28,29.28,29.28,29.28,29.28,29.28,29.28,29.28,29.28,29.28,29.28,29.28,29.28,29.14,29.14,29.14,29.14,29.14,29.14,29.14,29.14,29.14,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.44,28.41,28.41,28.41,28.41,28.41,28.41,28.41,28.41,28.41,28.41,28.41,28.41,28.41,28.41,28.31,28.31,28.31,28.31,28.31,28.31,28.31,28.31,28.31,28.31,28.31,28.31,28.31,28.31,28.19,28.19,28.19,28.19,28.19,28.19,28.19,28.19,28.19,28.19,28.19,28.19,28.19],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",0.16,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",29.41,0.0078,116.0,446,2.21,116.0],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",0.16,\"float16\",\"pytorch\",\"None\",\"None\",29.41,0.0089,112.0,446,2.29,112.0],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",0.16,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",29.41,0.00962,111.0,470,2.31,111.0],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",0.16,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",29.41,0.00997,109.0,269,2.34,109.0],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",0.16,\"bfloat16\",\"pytorch\",\"None\",\"None\",29.41,0.0104,111.0,446,2.31,111.0],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",0.16,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",29.41,0.0105,103.0,264,2.49,103.0],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",0.16,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",29.41,0.0108,92.1,263,2.78,92.1],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",0.16,\"float32\",\"pytorch\",\"None\",\"None\",29.41,0.0115,109.0,804,2.36,108.0],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",0.16,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",29.41,0.0226,79.2,264,3.24,79.0],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",0.16,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",29.41,0.0229,55.2,269,4.64,55.2],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",0.16,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",29.41,0.0232,55.4,269,4.62,55.4],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",0.16,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",29.41,0.0335,29.3,320,8.74,29.3],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",0.16,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",29.41,0.0373,27.7,320,9.25,27.7],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",1.12,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"None\",28.49,0.00938,97.3,2329,2.63,97.3],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",1.12,\"float16\",\"pytorch\",\"None\",\"None\",28.49,0.011,86.4,2329,2.96,86.5],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",1.12,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV2\",28.49,0.0115,93.4,1395,2.74,93.4],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",1.12,\"bfloat16\",\"pytorch\",\"None\",\"None\",28.49,0.0117,83.9,2329,3.05,83.9],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",1.12,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",28.49,0.0118,91.1,856,2.81,91.1],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",1.12,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMV\",28.49,0.0144,87.9,838,2.91,88.0],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",1.12,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",28.49,0.0151,79.2,838,3.24,79.0],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",1.12,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",28.49,0.0306,67.8,890,3.79,67.5],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",1.12,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",28.49,0.0421,23.8,1312,10.7,23.9],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",1.12,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",28.49,0.0428,23.4,1311,10.9,23.5],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",1.12,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.4bit\",28.49,0.0464,48.7,970,5.29,48.4],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",1.12,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",28.49,0.0472,47.0,970,5.48,46.7],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",1.12,\"float32\",\"pytorch\",\"None\",\"None\",28.49,0.0476,88.5,4628,2.93,87.4]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\u2b50 StarCoder\",\"marker\":{\"color\":\"#86CE00\",\"size\":[446,446,470,269,446,264,263,804,264,269,269,320,320,2329,2329,1395,2329,856,838,838,890,1312,1311,970,970,4628],\"sizemode\":\"area\",\"sizeref\":209.425,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\u2b50 StarCoder\",\"showlegend\":true,\"x\":[2.21,2.29,2.31,2.34,2.31,2.49,2.78,2.36,3.24,4.64,4.62,8.74,9.25,2.63,2.96,2.74,3.05,2.81,2.91,3.24,3.79,10.7,10.9,5.29,5.48,2.93],\"xaxis\":\"x\",\"y\":[29.41,29.41,29.41,29.41,29.41,29.41,29.41,29.41,29.41,29.41,29.41,29.41,29.41,28.49,28.49,28.49,28.49,28.49,28.49,28.49,28.49,28.49,28.49,28.49,28.49,28.49],\"yaxis\":\"y\",\"type\":\"scattergl\"}],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Time To Generate 256 Tokens (s)\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Open LLM Score (%)\"}},\"legend\":{\"title\":{\"text\":\"LLM Architecture\"},\"tracegroupgap\":0,\"itemsizing\":\"constant\"},\"margin\":{\"t\":60},\"title\":{\"text\":\"Latency vs. Score vs. Memory\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":600}}"
                },
                "show_label": false,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "plot",
                "elem_classes": [],
                "name": "plot",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "12c46a60185f5c11b6b6b6f9adbe826c",
            "api_info": {
                "properties": {
                    "type": {
                        "enum": [
                            "altair",
                            "bokeh",
                            "plotly",
                            "matplotlib"
                        ],
                        "title": "Type",
                        "type": "string"
                    },
                    "plot": {
                        "title": "Plot",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "plot"
                ],
                "title": "PlotData",
                "type": "object"
            },
            "example_inputs": null
        },
        {
            "id": 40,
            "type": "tabitem",
            "props": {
                "label": "BetterTransformer \ud83d\udcc8",
                "id": 2,
                "name": "tabitem"
            },
            "skip_api": true,
            "component_class_id": "f72faf2b8abb615823b199272218625a"
        },
        {
            "id": 41,
            "type": "html",
            "props": {
                "value": "\ud83d\udc46 Hover over the points \ud83d\udc46 for additional information.",
                "show_label": true,
                "visible": true,
                "elem_id": "text",
                "elem_classes": [],
                "name": "html",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "0ef1a4eade65d710fcd9fc56f6586b0e",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "<p>Hello</p>"
        },
        {
            "id": 42,
            "type": "plot",
            "props": {
                "value": {
                    "type": "plotly",
                    "plot": "{\"data\":[{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"Salesforce\\u002fcodegen-16B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",16.0,42.59,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0842,0.0787,11.9,12.4,11.9,12.4,6.989999999999995,4.200000000000003],[\"EleutherAI\\u002fgpt-neox-20b\",\"GPT-NeoX\",\"float16\",\"pytorch\",20.74,41.69,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0972,0.0928,10.5,10.8,10.4,10.8,4.739999999999995,2.8599999999999994],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0676,0.0623,15.1,15.6,15.1,15.6,8.510000000000005,3.3100000000000023],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0675,0.0618,14.8,15.8,14.8,15.8,9.219999999999999,6.760000000000005],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",\"float16\",\"pytorch\",6.0,40.1,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0881,0.0833,11.5,11.7,11.5,11.7,5.760000000000005,1.7399999999999949],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,40.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0816,0.0786,12.3,12.9,12.3,12.9,3.819999999999993,4.8799999999999955],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",12.0,39.7,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0771,0.0718,13.2,13.8,13.2,13.8,7.3799999999999955,4.549999999999997],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.9,39.3,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0661,0.0637,15.1,15.6,15.1,15.6,3.769999999999996,3.3100000000000023],[\"bigscience\\u002fbloom-7b1\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",7.07,39.18,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.059,0.0557,17.1,17.5,17.1,17.4,5.920000000000002,2.3400000000000034],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",\"float16\",\"pytorch\",12.0,38.82,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0748,0.0737,13.3,13.9,13.3,13.8,1.4899999999999949,4.510000000000005],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",\"float16\",\"pytorch\",10.0,38.59,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0747,0.0712,13.4,13.9,13.4,13.8,4.920000000000002,3.730000000000004],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0669,0.0636,14.8,15.5,14.8,15.5,5.189999999999998,4.730000000000004],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0671,0.0638,15.2,15.6,15.1,15.6,5.170000000000002,2.6299999999999955],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.101,0.0915,10.1,10.5,10.1,10.4,10.379999999999995,3.9599999999999937],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0667,0.0636,14.9,15.4,14.9,15.3,4.8700000000000045,3.3599999999999994],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0674,0.0636,15.1,15.4,15.1,15.3,5.969999999999999,1.9899999999999949],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0864,0.0822,11.8,12.1,11.7,12.1,5.109999999999999,2.5400000000000063],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0674,0.0619,14.8,15.5,14.8,15.5,8.89,4.730000000000004],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",3.0,36.07,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.058,0.0561,17.2,17.6,17.2,17.5,3.3900000000000006,2.3299999999999983],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0559,0.0476,19.6,20.4,19.5,20.5,17.439999999999998,4.079999999999998],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0514,0.0472,19.9,20.7,19.8,20.8,8.900000000000006,4.019999999999996],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0674,0.0619,14.9,16.0,14.9,16.0,8.89,7.3799999999999955],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0339,0.0322,29.0,31.2,29.0,31.2,5.280000000000001,7.590000000000003],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0676,0.0637,15.6,16.1,15.6,16.1,6.1200000000000045,3.2099999999999937],[\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"GPT-NeoX\",\"float16\",\"pytorch\",13.06,33.33,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0871,0.0766,11.9,12.6,11.9,12.5,13.709999999999994,5.8799999999999955],[\"TurkuNLP\\u002fgpt3-finnish-13B\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",13.0,32.95,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0785,0.0739,12.8,13.0,12.8,13.0,6.219999999999999,1.5600000000000023],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0335,0.0323,29.1,30.3,29.1,30.3,3.719999999999999,4.1200000000000045],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,32.43,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0808,0.0758,12.5,12.9,12.5,12.9,6.599999999999994,3.200000000000003],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0743,0.0704,13.4,13.9,13.4,13.8,5.540000000000006,3.730000000000004],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0761,0.0731,13.1,13.9,13.1,13.9,4.099999999999994,6.109999999999999],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0506,0.0471,20.1,20.2,20.0,20.3,7.430000000000007,0.5],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0351,0.0319,29.2,31.1,29.2,31.1,10.030000000000001,6.510000000000005],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.29,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0545,0.0484,19.2,20.2,19.1,20.3,12.599999999999994,5.209999999999994],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0324,0.0325,31.2,31.3,31.2,31.3,-0.3100000000000023,0.3199999999999932],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0253,0.0244,39.4,39.4,39.3,39.4,3.6899999999999977,0.0],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0642,0.0633,15.6,15.8,15.6,15.8,1.4200000000000017,1.2800000000000011],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0129,0.0124,75.9,78.7,76.0,78.8,4.030000000000001,3.6899999999999977],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.14,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0217,0.0214,45.4,47.0,45.4,47.0,1.4000000000000057,3.519999999999996],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0469,0.0454,20.7,21.6,20.8,21.7,3.299999999999997,4.349999999999994],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0256,0.024,39.1,40.1,39.0,40.1,6.670000000000002,2.5600000000000023],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0507,0.0483,19.8,20.1,19.7,20.2,4.969999999999999,1.519999999999996],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0131,0.0123,73.5,79.7,73.6,79.8,6.5,8.439999999999998],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0127,0.0123,75.9,78.0,76.0,78.0,3.25,2.769999999999996],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0127,0.0125,75.9,78.0,76.0,78.0,1.5999999999999943,2.769999999999996],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0128,0.0121,74.6,79.9,74.6,80.0,5.790000000000006,7.099999999999994],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.013,0.0121,75.4,78.5,75.5,78.5,7.439999999999998,4.109999999999999],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0216,0.0208,45.5,46.8,45.6,46.8,3.8499999999999943,2.8599999999999994],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0112,0.0109,88.9,89.5,88.9,89.5,2.75,0.6700000000000017],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.022,0.0209,45.5,46.4,45.6,46.4,5.260000000000005,1.980000000000004],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0129,0.012,75.7,78.9,75.7,79.0,7.5,4.230000000000004],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,28.19,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0212,0.021,46.4,46.4,46.4,46.4,0.9500000000000028,0.0],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,27.95,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0238,0.023,41.8,41.7,41.8,41.8,3.480000000000004,-0.23999999999999488]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[18]}\",\"legendgroup\":\"BnB.8bit\",\"marker\":{\"color\":\"#FD3216\"},\"name\":\"BnB.8bit\",\"notched\":false,\"offsetgroup\":\"BnB.8bit\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-J\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83c\udf38 Bloom\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[6.989999999999995,4.739999999999995,8.510000000000005,9.219999999999999,5.760000000000005,3.819999999999993,7.3799999999999955,3.769999999999996,5.920000000000002,1.4899999999999949,4.920000000000002,5.189999999999998,5.170000000000002,10.379999999999995,4.8700000000000045,5.969999999999999,5.109999999999999,8.89,3.3900000000000006,17.439999999999998,8.900000000000006,8.89,5.280000000000001,6.1200000000000045,13.709999999999994,6.219999999999999,3.719999999999999,6.599999999999994,5.540000000000006,4.099999999999994,7.430000000000007,10.030000000000001,12.599999999999994,-0.3100000000000023,3.6899999999999977,1.4200000000000017,4.030000000000001,1.4000000000000057,3.299999999999997,6.670000000000002,4.969999999999999,6.5,3.25,1.5999999999999943,5.790000000000006,7.439999999999998,3.8499999999999943,2.75,5.260000000000005,7.5,0.9500000000000028,3.480000000000004],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"Salesforce\\u002fcodegen-16B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",16.0,42.59,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.621,0.63,19.2,20.1,18.4,19.2,-1.4300000000000068,4.689999999999998],[\"EleutherAI\\u002fgpt-neox-20b\",\"GPT-NeoX\",\"float16\",\"pytorch\",20.74,41.69,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.826,0.826,15.4,15.5,14.7,14.9,0.0,0.6500000000000057],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.284,0.277,26.0,27.9,25.3,27.1,2.530000000000001,7.310000000000002],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.277,0.28,25.7,27.8,25.1,27.1,-1.0699999999999932,8.170000000000002],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",\"float16\",\"pytorch\",6.0,40.1,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.242,0.243,20.7,21.6,20.5,21.3,-0.4099999999999966,4.349999999999994],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,40.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.284,0.286,19.6,20.9,19.2,20.5,-0.7000000000000028,6.6299999999999955],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",12.0,39.7,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.449,0.456,22.8,23.8,22.1,22.9,-1.5400000000000063,4.390000000000001],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.9,39.3,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.277,0.276,26.0,27.9,25.3,27.1,0.35999999999999943,7.310000000000002],[\"bigscience\\u002fbloom-7b1\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",7.07,39.18,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.257,0.258,31.2,31.8,30.3,30.9,-0.39000000000000057,1.9200000000000017],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",\"float16\",\"pytorch\",12.0,38.82,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.446,0.457,22.8,23.8,22.1,22.9,-2.4099999999999966,4.390000000000001],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",\"float16\",\"pytorch\",10.0,38.59,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.425,0.425,23.4,24.5,22.7,23.7,0.0,4.700000000000003],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.123,0.118,25.2,27.0,25.1,26.8,4.239999999999995,7.140000000000001],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.276,0.275,25.7,27.9,25.1,27.1,0.35999999999999943,8.560000000000002],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.175,0.174,17.3,18.5,17.2,18.3,0.5699999999999932,6.939999999999998],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.121,0.122,25.0,27.2,24.9,26.9,-0.8199999999999932,8.799999999999997],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.123,0.118,25.6,26.9,25.3,26.6,4.239999999999995,5.079999999999998],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.121,0.117,24.5,25.2,24.4,25.1,3.4200000000000017,2.8599999999999994],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.122,0.121,25.6,27.0,25.3,26.8,0.8299999999999983,5.469999999999999],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",3.0,36.07,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.11,0.107,30.8,32.0,30.5,31.6,2.799999999999997,3.9000000000000057],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0607,0.059,32.9,36.8,32.8,36.6,2.8799999999999955,11.849999999999994],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0635,0.0632,33.5,36.4,33.4,36.3,0.46999999999999886,8.659999999999997],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.282,0.288,25.7,27.5,25.1,26.8,-2.0799999999999983,7.0],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.291,0.299,41.8,43.1,40.1,41.2,-2.680000000000007,3.1099999999999994],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.065,0.0606,32.4,32.7,32.2,32.6,7.260000000000005,0.9300000000000068],[\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"GPT-NeoX\",\"float16\",\"pytorch\",13.06,33.33,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.496,0.505,20.7,21.4,20.0,20.6,-1.7800000000000011,3.3799999999999955],[\"TurkuNLP\\u002fgpt3-finnish-13B\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",13.0,32.95,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.491,0.51,24.1,24.3,23.1,23.3,-3.730000000000004,0.8299999999999983],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0408,0.0397,50.1,54.3,49.9,54.0,2.769999999999996,8.379999999999995],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,32.43,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.29,0.285,19.6,21.1,19.2,20.6,1.75,7.650000000000006],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.171,0.165,22.6,24.5,22.3,24.2,3.6400000000000006,8.409999999999997],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.17,0.171,22.6,25.0,22.3,24.6,-0.5799999999999983,10.620000000000005],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0356,0.0324,34.3,35.9,34.3,35.9,9.879999999999995,4.659999999999997],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.138,0.138,50.3,54.6,49.1,53.2,0.0,8.549999999999997],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.29,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0362,0.0332,33.7,36.4,33.6,36.4,9.040000000000006,8.010000000000005],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.021,0.0196,59.9,64.2,59.8,64.2,7.140000000000001,7.180000000000007],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0186,0.0165,65.1,68.9,65.0,68.8,12.730000000000004,5.840000000000003],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.066,0.062,32.2,33.1,32.1,32.9,6.450000000000003,2.799999999999997],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.00905,0.00844,126.0,134.0,125.0,133.0,7.230000000000004,6.349999999999994],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.14,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0132,0.0138,93.8,90.7,93.8,90.8,-4.349999999999994,-3.299999999999997],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.036,0.0354,38.9,39.2,38.8,39.1,1.6899999999999977,0.769999999999996],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0179,0.0163,65.9,68.9,65.8,68.8,9.819999999999993,4.549999999999997],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0444,0.0425,33.3,36.0,33.2,35.9,4.469999999999999,8.11],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0092,0.00893,126.0,131.0,126.0,131.0,3.019999999999996,3.969999999999999],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.00906,0.00877,126.0,131.0,126.0,131.0,3.3100000000000023,3.969999999999999],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.00917,0.00845,128.0,133.0,127.0,133.0,8.519999999999996,3.9099999999999966],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.00917,0.00854,125.0,134.0,125.0,134.0,7.3799999999999955,7.200000000000003],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.00903,0.00834,128.0,133.0,127.0,133.0,8.269999999999996,3.9099999999999966],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0138,0.0136,91.1,93.8,91.1,93.8,1.4699999999999989,2.9599999999999937],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.00749,0.00698,172.0,176.0,172.0,175.0,7.310000000000002,2.3299999999999983],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0137,0.0129,92.1,93.8,92.1,93.8,6.200000000000003,1.8499999999999943],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.00931,0.00821,128.0,134.0,127.0,133.0,13.400000000000006,4.689999999999998],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,28.19,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0137,0.0128,92.7,95.1,92.8,95.2,7.030000000000001,2.5900000000000034],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,27.95,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0163,0.0149,75.7,76.8,75.5,76.9,9.400000000000006,1.4500000000000028]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[18]}\",\"legendgroup\":\"BnB.4bit\",\"marker\":{\"color\":\"#00FE35\"},\"name\":\"BnB.4bit\",\"notched\":false,\"offsetgroup\":\"BnB.4bit\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-J\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83c\udf38 Bloom\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-1.4300000000000068,0.0,2.530000000000001,-1.0699999999999932,-0.4099999999999966,-0.7000000000000028,-1.5400000000000063,0.35999999999999943,-0.39000000000000057,-2.4099999999999966,0.0,4.239999999999995,0.35999999999999943,0.5699999999999932,-0.8199999999999932,4.239999999999995,3.4200000000000017,0.8299999999999983,2.799999999999997,2.8799999999999955,0.46999999999999886,-2.0799999999999983,-2.680000000000007,7.260000000000005,-1.7800000000000011,-3.730000000000004,2.769999999999996,1.75,3.6400000000000006,-0.5799999999999983,9.879999999999995,0.0,9.040000000000006,7.140000000000001,12.730000000000004,6.450000000000003,7.230000000000004,-4.349999999999994,1.6899999999999977,9.819999999999993,4.469999999999999,3.019999999999996,3.3100000000000023,8.519999999999996,7.3799999999999955,8.269999999999996,1.4699999999999989,7.310000000000002,6.200000000000003,13.400000000000006,7.030000000000001,9.400000000000006],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"Writer\\u002fpalmyra-large\",\"GPT-2\",\"float16\",\"pytorch\",0.0,42.09,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0758,0.072,27.1,28.7,27.0,28.6,5.280000000000001,5.900000000000006],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0329,0.0305,40.0,46.4,40.0,46.4,7.8700000000000045,16.0],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0329,0.0338,40.5,45.0,40.5,44.9,-2.6599999999999966,11.11],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",\"float16\",\"pytorch\",6.0,40.1,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0332,0.0318,31.0,33.5,31.0,33.5,4.400000000000006,8.060000000000002],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,40.0,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0385,0.0348,26.9,29.2,26.9,29.3,10.629999999999995,8.549999999999997],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.9,39.3,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0315,0.0299,40.4,47.1,40.4,47.1,5.349999999999994,16.58],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0277,0.0232,39.5,43.4,39.4,43.5,19.400000000000006,9.870000000000005],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0315,0.0296,40.4,45.3,40.4,45.2,6.420000000000002,12.129999999999995],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0367,0.0319,27.7,30.7,27.7,30.7,15.049999999999997,10.829999999999998],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0256,0.0236,41.3,43.1,41.2,43.1,8.469999999999999,4.359999999999999],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.026,0.0236,40.4,44.0,40.4,44.0,10.170000000000002,8.909999999999997],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",\"float16\",\"pytorch\",6.7,36.27,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.029,0.0272,55.1,63.3,54.9,63.1,6.6200000000000045,14.879999999999995],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0241,0.0219,43.9,46.6,43.9,46.6,10.049999999999997,6.150000000000006],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0266,0.022,40.1,44.6,40.1,44.6,20.909999999999997,11.219999999999999],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",3.0,36.07,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0202,0.0177,53.5,55.4,53.4,55.4,14.120000000000005,3.549999999999997],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",\"float16\",\"pytorch\",0.0,35.18,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0229,0.021,71.4,84.4,71.3,84.2,9.049999999999997,18.209999999999994],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0189,0.0158,53.7,61.7,53.7,61.7,19.620000000000005,14.900000000000006],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0194,0.016,53.5,61.6,53.4,61.5,21.25,15.14],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0333,0.0309,39.6,45.3,39.6,45.2,7.769999999999996,14.39],[\"gpt2-xl\",\"GPT-2\",\"float16\",\"pytorch\",1.61,34.38,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0342,0.0271,30.9,35.6,31.0,35.6,26.200000000000003,15.209999999999994],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0288,0.0277,66.4,71.2,66.1,70.9,3.969999999999999,7.230000000000004],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",\"float16\",\"pytorch\",1.44,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0147,0.0113,70.4,78.2,70.5,78.3,30.090000000000003,11.079999999999998],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0172,0.0158,59.7,63.9,59.7,63.8,8.86,7.040000000000006],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",\"float16\",\"pytorch\",2.7,33.25,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0196,0.0163,53.9,62.3,53.9,62.3,20.25,15.579999999999998],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0125,0.0122,78.2,88.5,78.3,88.6,2.4599999999999937,13.170000000000002],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,32.43,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.04,0.034,26.7,28.9,26.7,29.0,17.650000000000006,8.239999999999995],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.03,0.0261,36.2,41.7,36.2,41.7,14.939999999999998,15.189999999999998],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0284,0.0249,35.8,40.6,35.8,40.6,14.060000000000002,13.409999999999997],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0181,0.0164,55.3,59.2,55.3,59.1,10.370000000000005,7.049999999999997],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0172,0.0154,79.2,90.1,79.0,89.8,11.689999999999998,13.760000000000005],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",\"float16\",\"pytorch\",1.3,31.3,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0151,0.0115,70.4,86.4,70.3,86.5,31.30000000000001,22.730000000000004],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",\"float16\",\"pytorch\",0.47,30.41,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0133,0.0113,71.0,78.5,71.1,78.5,17.700000000000003,10.560000000000002],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",\"float16\",\"pytorch\",0.38,30.4,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0155,0.0138,62.8,68.5,62.7,68.6,12.319999999999993,9.079999999999998],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00838,0.00791,121.0,122.0,121.0,122.0,5.939999999999998,0.8299999999999983],[\"cerebras\\u002fCerebras-GPT-256M\",\"GPT-2\",\"float16\",\"pytorch\",0.26,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00825,0.00683,120.0,130.0,120.0,130.0,20.790000000000006,8.329999999999998],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00925,0.00807,109.0,118.0,109.0,118.0,14.620000000000005,8.260000000000005],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0172,0.0157,61.4,65.7,61.4,65.6,9.549999999999997,7.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00482,0.00431,207.0,220.0,208.0,221.0,11.829999999999998,6.280000000000001],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.14,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00542,0.00643,182.0,173.0,182.0,173.0,-15.709999999999994,-4.950000000000003],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0151,0.0135,67.3,69.5,67.2,69.6,11.849999999999994,3.269999999999996],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00918,0.00844,108.0,115.0,108.0,115.0,8.769999999999996,6.480000000000004],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0196,0.0173,50.4,58.1,50.4,58.0,13.290000000000006,15.280000000000001],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00494,0.00413,206.0,224.0,206.0,225.0,19.61,8.739999999999995],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00468,0.00417,209.0,224.0,210.0,225.0,12.230000000000004,7.180000000000007],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00472,0.00413,207.0,218.0,208.0,219.0,14.290000000000006,5.310000000000002],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0047,0.00419,207.0,220.0,208.0,221.0,12.170000000000002,6.280000000000001],[\"gpt2\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.53,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00807,0.00695,120.0,131.0,120.0,131.0,16.120000000000005,9.170000000000002],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",\"float16\",\"pytorch\",0.19,28.45,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00693,0.00554,139.0,155.0,139.0,154.0,25.090000000000003,11.510000000000005],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00476,0.00421,204.0,220.0,205.0,221.0,13.060000000000002,7.840000000000003],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00584,0.00536,176.0,177.0,175.0,177.0,8.959999999999994,0.5699999999999932],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00332,0.00311,309.0,313.0,309.0,313.0,6.75,1.2900000000000063],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.4,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00816,0.00696,120.0,132.0,120.0,132.0,17.239999999999995,10.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00591,0.00531,178.0,183.0,178.0,183.0,11.299999999999997,2.8100000000000023],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00472,0.00429,207.0,220.0,208.0,221.0,10.019999999999996,6.280000000000001],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,28.19,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00582,0.00536,178.0,180.0,178.0,179.0,8.579999999999998,1.1200000000000045],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,27.95,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00752,0.0072,134.0,134.0,134.0,133.0,4.439999999999998,0.0],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",\"float16\",\"pytorch\",0.11,27.75,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00601,0.00514,162.0,180.0,162.0,179.0,16.930000000000007,11.11],[\"ai-forever\\u002fmGPT\",\"GPT-2\",\"float16\",\"pytorch\",0.0,27.61,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0156,0.0135,64.4,71.8,64.3,71.9,15.560000000000002,11.489999999999995]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[18]}\",\"legendgroup\":\"None\",\"marker\":{\"color\":\"#6A76FC\"},\"name\":\"None\",\"notched\":false,\"offsetgroup\":\"None\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"GPT-2\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-J\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-2\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"GPT-NeoX\",\"GPT-2\",\"GPT-Neo\",\"GPT-2\",\"GPT-NeoX\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"GPT-2\",\"GPT-2\",\"GPT-Neo\",\"GPT-2\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"GPT-2\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-2\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83c\udf38 Bloom\",\"GPT-2\",\"GPT-2\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[5.280000000000001,7.8700000000000045,-2.6599999999999966,4.400000000000006,10.629999999999995,5.349999999999994,19.400000000000006,6.420000000000002,15.049999999999997,8.469999999999999,10.170000000000002,6.6200000000000045,10.049999999999997,20.909999999999997,14.120000000000005,9.049999999999997,19.620000000000005,21.25,7.769999999999996,26.200000000000003,3.969999999999999,30.090000000000003,8.86,20.25,2.4599999999999937,17.650000000000006,14.939999999999998,14.060000000000002,10.370000000000005,11.689999999999998,31.30000000000001,17.700000000000003,12.319999999999993,5.939999999999998,20.790000000000006,14.620000000000005,9.549999999999997,11.829999999999998,-15.709999999999994,11.849999999999994,8.769999999999996,13.290000000000006,19.61,12.230000000000004,14.290000000000006,12.170000000000002,16.120000000000005,25.090000000000003,13.060000000000002,8.959999999999994,6.75,17.239999999999995,11.299999999999997,10.019999999999996,8.579999999999998,4.439999999999998,16.930000000000007,15.560000000000002],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"}],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"LLM Architecture\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Prefill Speedup (%)\"}},\"legend\":{\"title\":{\"text\":\"Quantization Scheme\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"boxmode\":\"group\",\"title\":{\"text\":\"Prefill Latency Speedup per Architecture, Compared To Non-Optimized Model\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":600}}"
                },
                "show_label": false,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "plot",
                "elem_classes": [],
                "name": "plot",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "12c46a60185f5c11b6b6b6f9adbe826c",
            "api_info": {
                "properties": {
                    "type": {
                        "enum": [
                            "altair",
                            "bokeh",
                            "plotly",
                            "matplotlib"
                        ],
                        "title": "Type",
                        "type": "string"
                    },
                    "plot": {
                        "title": "Plot",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "plot"
                ],
                "title": "PlotData",
                "type": "object"
            },
            "example_inputs": null
        },
        {
            "id": 43,
            "type": "plot",
            "props": {
                "value": {
                    "type": "plotly",
                    "plot": "{\"data\":[{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"Salesforce\\u002fcodegen-16B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",16.0,42.59,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0842,0.0787,11.9,12.4,11.9,12.4,6.989999999999995,4.200000000000003],[\"EleutherAI\\u002fgpt-neox-20b\",\"GPT-NeoX\",\"float16\",\"pytorch\",20.74,41.69,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0972,0.0928,10.5,10.8,10.4,10.8,4.739999999999995,2.8599999999999994],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0676,0.0623,15.1,15.6,15.1,15.6,8.510000000000005,3.3100000000000023],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0675,0.0618,14.8,15.8,14.8,15.8,9.219999999999999,6.760000000000005],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",\"float16\",\"pytorch\",6.0,40.1,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0881,0.0833,11.5,11.7,11.5,11.7,5.760000000000005,1.7399999999999949],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,40.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0816,0.0786,12.3,12.9,12.3,12.9,3.819999999999993,4.8799999999999955],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",12.0,39.7,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0771,0.0718,13.2,13.8,13.2,13.8,7.3799999999999955,4.549999999999997],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.9,39.3,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0661,0.0637,15.1,15.6,15.1,15.6,3.769999999999996,3.3100000000000023],[\"bigscience\\u002fbloom-7b1\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",7.07,39.18,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.059,0.0557,17.1,17.5,17.1,17.4,5.920000000000002,2.3400000000000034],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",\"float16\",\"pytorch\",12.0,38.82,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0748,0.0737,13.3,13.9,13.3,13.8,1.4899999999999949,4.510000000000005],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",\"float16\",\"pytorch\",10.0,38.59,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0747,0.0712,13.4,13.9,13.4,13.8,4.920000000000002,3.730000000000004],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0669,0.0636,14.8,15.5,14.8,15.5,5.189999999999998,4.730000000000004],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0671,0.0638,15.2,15.6,15.1,15.6,5.170000000000002,2.6299999999999955],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.101,0.0915,10.1,10.5,10.1,10.4,10.379999999999995,3.9599999999999937],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0667,0.0636,14.9,15.4,14.9,15.3,4.8700000000000045,3.3599999999999994],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0674,0.0636,15.1,15.4,15.1,15.3,5.969999999999999,1.9899999999999949],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0864,0.0822,11.8,12.1,11.7,12.1,5.109999999999999,2.5400000000000063],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0674,0.0619,14.8,15.5,14.8,15.5,8.89,4.730000000000004],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",3.0,36.07,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.058,0.0561,17.2,17.6,17.2,17.5,3.3900000000000006,2.3299999999999983],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0559,0.0476,19.6,20.4,19.5,20.5,17.439999999999998,4.079999999999998],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0514,0.0472,19.9,20.7,19.8,20.8,8.900000000000006,4.019999999999996],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0674,0.0619,14.9,16.0,14.9,16.0,8.89,7.3799999999999955],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0339,0.0322,29.0,31.2,29.0,31.2,5.280000000000001,7.590000000000003],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0676,0.0637,15.6,16.1,15.6,16.1,6.1200000000000045,3.2099999999999937],[\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"GPT-NeoX\",\"float16\",\"pytorch\",13.06,33.33,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0871,0.0766,11.9,12.6,11.9,12.5,13.709999999999994,5.8799999999999955],[\"TurkuNLP\\u002fgpt3-finnish-13B\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",13.0,32.95,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0785,0.0739,12.8,13.0,12.8,13.0,6.219999999999999,1.5600000000000023],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0335,0.0323,29.1,30.3,29.1,30.3,3.719999999999999,4.1200000000000045],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,32.43,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0808,0.0758,12.5,12.9,12.5,12.9,6.599999999999994,3.200000000000003],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0743,0.0704,13.4,13.9,13.4,13.8,5.540000000000006,3.730000000000004],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0761,0.0731,13.1,13.9,13.1,13.9,4.099999999999994,6.109999999999999],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0506,0.0471,20.1,20.2,20.0,20.3,7.430000000000007,0.5],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0351,0.0319,29.2,31.1,29.2,31.1,10.030000000000001,6.510000000000005],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.29,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0545,0.0484,19.2,20.2,19.1,20.3,12.599999999999994,5.209999999999994],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0324,0.0325,31.2,31.3,31.2,31.3,-0.3100000000000023,0.3199999999999932],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0253,0.0244,39.4,39.4,39.3,39.4,3.6899999999999977,0.0],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0642,0.0633,15.6,15.8,15.6,15.8,1.4200000000000017,1.2800000000000011],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0129,0.0124,75.9,78.7,76.0,78.8,4.030000000000001,3.6899999999999977],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.14,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0217,0.0214,45.4,47.0,45.4,47.0,1.4000000000000057,3.519999999999996],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0469,0.0454,20.7,21.6,20.8,21.7,3.299999999999997,4.349999999999994],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0256,0.024,39.1,40.1,39.0,40.1,6.670000000000002,2.5600000000000023],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0507,0.0483,19.8,20.1,19.7,20.2,4.969999999999999,1.519999999999996],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0131,0.0123,73.5,79.7,73.6,79.8,6.5,8.439999999999998],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0127,0.0123,75.9,78.0,76.0,78.0,3.25,2.769999999999996],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0127,0.0125,75.9,78.0,76.0,78.0,1.5999999999999943,2.769999999999996],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0128,0.0121,74.6,79.9,74.6,80.0,5.790000000000006,7.099999999999994],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.013,0.0121,75.4,78.5,75.5,78.5,7.439999999999998,4.109999999999999],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0216,0.0208,45.5,46.8,45.6,46.8,3.8499999999999943,2.8599999999999994],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0112,0.0109,88.9,89.5,88.9,89.5,2.75,0.6700000000000017],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.022,0.0209,45.5,46.4,45.6,46.4,5.260000000000005,1.980000000000004],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0129,0.012,75.7,78.9,75.7,79.0,7.5,4.230000000000004],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,28.19,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0212,0.021,46.4,46.4,46.4,46.4,0.9500000000000028,0.0],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,27.95,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"BetterTransformer\",0.0238,0.023,41.8,41.7,41.8,41.8,3.480000000000004,-0.23999999999999488]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[18]}\",\"legendgroup\":\"BnB.8bit\",\"marker\":{\"color\":\"#FD3216\"},\"name\":\"BnB.8bit\",\"notched\":false,\"offsetgroup\":\"BnB.8bit\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-J\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83c\udf38 Bloom\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[4.200000000000003,2.8599999999999994,3.3100000000000023,6.760000000000005,1.7399999999999949,4.8799999999999955,4.549999999999997,3.3100000000000023,2.3400000000000034,4.510000000000005,3.730000000000004,4.730000000000004,2.6299999999999955,3.9599999999999937,3.3599999999999994,1.9899999999999949,2.5400000000000063,4.730000000000004,2.3299999999999983,4.079999999999998,4.019999999999996,7.3799999999999955,7.590000000000003,3.2099999999999937,5.8799999999999955,1.5600000000000023,4.1200000000000045,3.200000000000003,3.730000000000004,6.109999999999999,0.5,6.510000000000005,5.209999999999994,0.3199999999999932,0.0,1.2800000000000011,3.6899999999999977,3.519999999999996,4.349999999999994,2.5600000000000023,1.519999999999996,8.439999999999998,2.769999999999996,2.769999999999996,7.099999999999994,4.109999999999999,2.8599999999999994,0.6700000000000017,1.980000000000004,4.230000000000004,0.0,-0.23999999999999488],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"Salesforce\\u002fcodegen-16B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",16.0,42.59,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.621,0.63,19.2,20.1,18.4,19.2,-1.4300000000000068,4.689999999999998],[\"EleutherAI\\u002fgpt-neox-20b\",\"GPT-NeoX\",\"float16\",\"pytorch\",20.74,41.69,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.826,0.826,15.4,15.5,14.7,14.9,0.0,0.6500000000000057],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.284,0.277,26.0,27.9,25.3,27.1,2.530000000000001,7.310000000000002],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.277,0.28,25.7,27.8,25.1,27.1,-1.0699999999999932,8.170000000000002],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",\"float16\",\"pytorch\",6.0,40.1,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.242,0.243,20.7,21.6,20.5,21.3,-0.4099999999999966,4.349999999999994],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,40.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.284,0.286,19.6,20.9,19.2,20.5,-0.7000000000000028,6.6299999999999955],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",12.0,39.7,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.449,0.456,22.8,23.8,22.1,22.9,-1.5400000000000063,4.390000000000001],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.9,39.3,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.277,0.276,26.0,27.9,25.3,27.1,0.35999999999999943,7.310000000000002],[\"bigscience\\u002fbloom-7b1\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",7.07,39.18,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.257,0.258,31.2,31.8,30.3,30.9,-0.39000000000000057,1.9200000000000017],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",\"float16\",\"pytorch\",12.0,38.82,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.446,0.457,22.8,23.8,22.1,22.9,-2.4099999999999966,4.390000000000001],[\"matsuo-lab\\u002fweblab-10b\",\"GPT-NeoX\",\"float16\",\"pytorch\",10.0,38.59,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.425,0.425,23.4,24.5,22.7,23.7,0.0,4.700000000000003],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.123,0.118,25.2,27.0,25.1,26.8,4.239999999999995,7.140000000000001],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.276,0.275,25.7,27.9,25.1,27.1,0.35999999999999943,8.560000000000002],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.175,0.174,17.3,18.5,17.2,18.3,0.5699999999999932,6.939999999999998],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.121,0.122,25.0,27.2,24.9,26.9,-0.8199999999999932,8.799999999999997],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.123,0.118,25.6,26.9,25.3,26.6,4.239999999999995,5.079999999999998],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.121,0.117,24.5,25.2,24.4,25.1,3.4200000000000017,2.8599999999999994],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.122,0.121,25.6,27.0,25.3,26.8,0.8299999999999983,5.469999999999999],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",3.0,36.07,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.11,0.107,30.8,32.0,30.5,31.6,2.799999999999997,3.9000000000000057],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0607,0.059,32.9,36.8,32.8,36.6,2.8799999999999955,11.849999999999994],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0635,0.0632,33.5,36.4,33.4,36.3,0.46999999999999886,8.659999999999997],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.282,0.288,25.7,27.5,25.1,26.8,-2.0799999999999983,7.0],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.291,0.299,41.8,43.1,40.1,41.2,-2.680000000000007,3.1099999999999994],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.065,0.0606,32.4,32.7,32.2,32.6,7.260000000000005,0.9300000000000068],[\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"GPT-NeoX\",\"float16\",\"pytorch\",13.06,33.33,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.496,0.505,20.7,21.4,20.0,20.6,-1.7800000000000011,3.3799999999999955],[\"TurkuNLP\\u002fgpt3-finnish-13B\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",13.0,32.95,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.491,0.51,24.1,24.3,23.1,23.3,-3.730000000000004,0.8299999999999983],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0408,0.0397,50.1,54.3,49.9,54.0,2.769999999999996,8.379999999999995],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,32.43,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.29,0.285,19.6,21.1,19.2,20.6,1.75,7.650000000000006],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.171,0.165,22.6,24.5,22.3,24.2,3.6400000000000006,8.409999999999997],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.17,0.171,22.6,25.0,22.3,24.6,-0.5799999999999983,10.620000000000005],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0356,0.0324,34.3,35.9,34.3,35.9,9.879999999999995,4.659999999999997],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.138,0.138,50.3,54.6,49.1,53.2,0.0,8.549999999999997],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.29,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0362,0.0332,33.7,36.4,33.6,36.4,9.040000000000006,8.010000000000005],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.021,0.0196,59.9,64.2,59.8,64.2,7.140000000000001,7.180000000000007],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0186,0.0165,65.1,68.9,65.0,68.8,12.730000000000004,5.840000000000003],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.066,0.062,32.2,33.1,32.1,32.9,6.450000000000003,2.799999999999997],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.00905,0.00844,126.0,134.0,125.0,133.0,7.230000000000004,6.349999999999994],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.14,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0132,0.0138,93.8,90.7,93.8,90.8,-4.349999999999994,-3.299999999999997],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.036,0.0354,38.9,39.2,38.8,39.1,1.6899999999999977,0.769999999999996],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0179,0.0163,65.9,68.9,65.8,68.8,9.819999999999993,4.549999999999997],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0444,0.0425,33.3,36.0,33.2,35.9,4.469999999999999,8.11],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0092,0.00893,126.0,131.0,126.0,131.0,3.019999999999996,3.969999999999999],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.00906,0.00877,126.0,131.0,126.0,131.0,3.3100000000000023,3.969999999999999],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.00917,0.00845,128.0,133.0,127.0,133.0,8.519999999999996,3.9099999999999966],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.00917,0.00854,125.0,134.0,125.0,134.0,7.3799999999999955,7.200000000000003],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.00903,0.00834,128.0,133.0,127.0,133.0,8.269999999999996,3.9099999999999966],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0138,0.0136,91.1,93.8,91.1,93.8,1.4699999999999989,2.9599999999999937],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.00749,0.00698,172.0,176.0,172.0,175.0,7.310000000000002,2.3299999999999983],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0137,0.0129,92.1,93.8,92.1,93.8,6.200000000000003,1.8499999999999943],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.00931,0.00821,128.0,134.0,127.0,133.0,13.400000000000006,4.689999999999998],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,28.19,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0137,0.0128,92.7,95.1,92.8,95.2,7.030000000000001,2.5900000000000034],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,27.95,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"BetterTransformer\",0.0163,0.0149,75.7,76.8,75.5,76.9,9.400000000000006,1.4500000000000028]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[18]}\",\"legendgroup\":\"BnB.4bit\",\"marker\":{\"color\":\"#00FE35\"},\"name\":\"BnB.4bit\",\"notched\":false,\"offsetgroup\":\"BnB.4bit\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-J\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83c\udf38 Bloom\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[4.689999999999998,0.6500000000000057,7.310000000000002,8.170000000000002,4.349999999999994,6.6299999999999955,4.390000000000001,7.310000000000002,1.9200000000000017,4.390000000000001,4.700000000000003,7.140000000000001,8.560000000000002,6.939999999999998,8.799999999999997,5.079999999999998,2.8599999999999994,5.469999999999999,3.9000000000000057,11.849999999999994,8.659999999999997,7.0,3.1099999999999994,0.9300000000000068,3.3799999999999955,0.8299999999999983,8.379999999999995,7.650000000000006,8.409999999999997,10.620000000000005,4.659999999999997,8.549999999999997,8.010000000000005,7.180000000000007,5.840000000000003,2.799999999999997,6.349999999999994,-3.299999999999997,0.769999999999996,4.549999999999997,8.11,3.969999999999999,3.969999999999999,3.9099999999999966,7.200000000000003,3.9099999999999966,2.9599999999999937,2.3299999999999983,1.8499999999999943,4.689999999999998,2.5900000000000034,1.4500000000000028],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"Writer\\u002fpalmyra-large\",\"GPT-2\",\"float16\",\"pytorch\",0.0,42.09,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0758,0.072,27.1,28.7,27.0,28.6,5.280000000000001,5.900000000000006],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0329,0.0305,40.0,46.4,40.0,46.4,7.8700000000000045,16.0],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0329,0.0338,40.5,45.0,40.5,44.9,-2.6599999999999966,11.11],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",\"float16\",\"pytorch\",6.0,40.1,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0332,0.0318,31.0,33.5,31.0,33.5,4.400000000000006,8.060000000000002],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,40.0,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0385,0.0348,26.9,29.2,26.9,29.3,10.629999999999995,8.549999999999997],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.9,39.3,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0315,0.0299,40.4,47.1,40.4,47.1,5.349999999999994,16.58],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0277,0.0232,39.5,43.4,39.4,43.5,19.400000000000006,9.870000000000005],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0315,0.0296,40.4,45.3,40.4,45.2,6.420000000000002,12.129999999999995],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0367,0.0319,27.7,30.7,27.7,30.7,15.049999999999997,10.829999999999998],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0256,0.0236,41.3,43.1,41.2,43.1,8.469999999999999,4.359999999999999],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.026,0.0236,40.4,44.0,40.4,44.0,10.170000000000002,8.909999999999997],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",\"float16\",\"pytorch\",6.7,36.27,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.029,0.0272,55.1,63.3,54.9,63.1,6.6200000000000045,14.879999999999995],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0241,0.0219,43.9,46.6,43.9,46.6,10.049999999999997,6.150000000000006],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0266,0.022,40.1,44.6,40.1,44.6,20.909999999999997,11.219999999999999],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",3.0,36.07,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0202,0.0177,53.5,55.4,53.4,55.4,14.120000000000005,3.549999999999997],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",\"float16\",\"pytorch\",0.0,35.18,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0229,0.021,71.4,84.4,71.3,84.2,9.049999999999997,18.209999999999994],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0189,0.0158,53.7,61.7,53.7,61.7,19.620000000000005,14.900000000000006],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0194,0.016,53.5,61.6,53.4,61.5,21.25,15.14],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0333,0.0309,39.6,45.3,39.6,45.2,7.769999999999996,14.39],[\"gpt2-xl\",\"GPT-2\",\"float16\",\"pytorch\",1.61,34.38,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0342,0.0271,30.9,35.6,31.0,35.6,26.200000000000003,15.209999999999994],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0288,0.0277,66.4,71.2,66.1,70.9,3.969999999999999,7.230000000000004],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",\"float16\",\"pytorch\",1.44,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0147,0.0113,70.4,78.2,70.5,78.3,30.090000000000003,11.079999999999998],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0172,0.0158,59.7,63.9,59.7,63.8,8.86,7.040000000000006],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",\"float16\",\"pytorch\",2.7,33.25,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0196,0.0163,53.9,62.3,53.9,62.3,20.25,15.579999999999998],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0125,0.0122,78.2,88.5,78.3,88.6,2.4599999999999937,13.170000000000002],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,32.43,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.04,0.034,26.7,28.9,26.7,29.0,17.650000000000006,8.239999999999995],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.03,0.0261,36.2,41.7,36.2,41.7,14.939999999999998,15.189999999999998],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0284,0.0249,35.8,40.6,35.8,40.6,14.060000000000002,13.409999999999997],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0181,0.0164,55.3,59.2,55.3,59.1,10.370000000000005,7.049999999999997],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0172,0.0154,79.2,90.1,79.0,89.8,11.689999999999998,13.760000000000005],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",\"float16\",\"pytorch\",1.3,31.3,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0151,0.0115,70.4,86.4,70.3,86.5,31.30000000000001,22.730000000000004],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",\"float16\",\"pytorch\",0.47,30.41,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0133,0.0113,71.0,78.5,71.1,78.5,17.700000000000003,10.560000000000002],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",\"float16\",\"pytorch\",0.38,30.4,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0155,0.0138,62.8,68.5,62.7,68.6,12.319999999999993,9.079999999999998],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00838,0.00791,121.0,122.0,121.0,122.0,5.939999999999998,0.8299999999999983],[\"cerebras\\u002fCerebras-GPT-256M\",\"GPT-2\",\"float16\",\"pytorch\",0.26,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00825,0.00683,120.0,130.0,120.0,130.0,20.790000000000006,8.329999999999998],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00925,0.00807,109.0,118.0,109.0,118.0,14.620000000000005,8.260000000000005],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0172,0.0157,61.4,65.7,61.4,65.6,9.549999999999997,7.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00482,0.00431,207.0,220.0,208.0,221.0,11.829999999999998,6.280000000000001],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.14,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00542,0.00643,182.0,173.0,182.0,173.0,-15.709999999999994,-4.950000000000003],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0151,0.0135,67.3,69.5,67.2,69.6,11.849999999999994,3.269999999999996],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00918,0.00844,108.0,115.0,108.0,115.0,8.769999999999996,6.480000000000004],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0196,0.0173,50.4,58.1,50.4,58.0,13.290000000000006,15.280000000000001],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00494,0.00413,206.0,224.0,206.0,225.0,19.61,8.739999999999995],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00468,0.00417,209.0,224.0,210.0,225.0,12.230000000000004,7.180000000000007],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00472,0.00413,207.0,218.0,208.0,219.0,14.290000000000006,5.310000000000002],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0047,0.00419,207.0,220.0,208.0,221.0,12.170000000000002,6.280000000000001],[\"gpt2\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.53,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00807,0.00695,120.0,131.0,120.0,131.0,16.120000000000005,9.170000000000002],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",\"float16\",\"pytorch\",0.19,28.45,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00693,0.00554,139.0,155.0,139.0,154.0,25.090000000000003,11.510000000000005],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00476,0.00421,204.0,220.0,205.0,221.0,13.060000000000002,7.840000000000003],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00584,0.00536,176.0,177.0,175.0,177.0,8.959999999999994,0.5699999999999932],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00332,0.00311,309.0,313.0,309.0,313.0,6.75,1.2900000000000063],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.4,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00816,0.00696,120.0,132.0,120.0,132.0,17.239999999999995,10.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00591,0.00531,178.0,183.0,178.0,183.0,11.299999999999997,2.8100000000000023],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00472,0.00429,207.0,220.0,208.0,221.0,10.019999999999996,6.280000000000001],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,28.19,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00582,0.00536,178.0,180.0,178.0,179.0,8.579999999999998,1.1200000000000045],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,27.95,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00752,0.0072,134.0,134.0,134.0,133.0,4.439999999999998,0.0],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",\"float16\",\"pytorch\",0.11,27.75,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.00601,0.00514,162.0,180.0,162.0,179.0,16.930000000000007,11.11],[\"ai-forever\\u002fmGPT\",\"GPT-2\",\"float16\",\"pytorch\",0.0,27.61,\"float16\",\"pytorch\",\"None\",\"None\",\"BetterTransformer\",0.0156,0.0135,64.4,71.8,64.3,71.9,15.560000000000002,11.489999999999995]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs) BetterTransformer:\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[18]}\",\"legendgroup\":\"None\",\"marker\":{\"color\":\"#6A76FC\"},\"name\":\"None\",\"notched\":false,\"offsetgroup\":\"None\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"GPT-2\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-J\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-2\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"GPT-NeoX\",\"GPT-2\",\"GPT-Neo\",\"GPT-2\",\"GPT-NeoX\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"GPT-2\",\"GPT-2\",\"GPT-Neo\",\"GPT-2\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"GPT-2\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-2\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83c\udf38 Bloom\",\"GPT-2\",\"GPT-2\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[5.900000000000006,16.0,11.11,8.060000000000002,8.549999999999997,16.58,9.870000000000005,12.129999999999995,10.829999999999998,4.359999999999999,8.909999999999997,14.879999999999995,6.150000000000006,11.219999999999999,3.549999999999997,18.209999999999994,14.900000000000006,15.14,14.39,15.209999999999994,7.230000000000004,11.079999999999998,7.040000000000006,15.579999999999998,13.170000000000002,8.239999999999995,15.189999999999998,13.409999999999997,7.049999999999997,13.760000000000005,22.730000000000004,10.560000000000002,9.079999999999998,0.8299999999999983,8.329999999999998,8.260000000000005,7.0,6.280000000000001,-4.950000000000003,3.269999999999996,6.480000000000004,15.280000000000001,8.739999999999995,7.180000000000007,5.310000000000002,6.280000000000001,9.170000000000002,11.510000000000005,7.840000000000003,0.5699999999999932,1.2900000000000063,10.0,2.8100000000000023,6.280000000000001,1.1200000000000045,0.0,11.11,11.489999999999995],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"}],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"LLM Architecture\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Decode Speedup (%)\"}},\"legend\":{\"title\":{\"text\":\"Quantization Scheme\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"boxmode\":\"group\",\"title\":{\"text\":\"Decode Throughput Speedup per Architecture, Compared To Non-Optimized Model\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":600}}"
                },
                "show_label": false,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "plot",
                "elem_classes": [],
                "name": "plot",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "12c46a60185f5c11b6b6b6f9adbe826c",
            "api_info": {
                "properties": {
                    "type": {
                        "enum": [
                            "altair",
                            "bokeh",
                            "plotly",
                            "matplotlib"
                        ],
                        "title": "Type",
                        "type": "string"
                    },
                    "plot": {
                        "title": "Plot",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "plot"
                ],
                "title": "PlotData",
                "type": "object"
            },
            "example_inputs": null
        },
        {
            "id": 44,
            "type": "tabitem",
            "props": {
                "label": "FlashAttentionV2 \ud83d\udcc8",
                "id": 3,
                "name": "tabitem"
            },
            "skip_api": true,
            "component_class_id": "f72faf2b8abb615823b199272218625a"
        },
        {
            "id": 45,
            "type": "html",
            "props": {
                "value": "\ud83d\udc46 Hover over the points \ud83d\udc46 for additional information.",
                "show_label": true,
                "visible": true,
                "elem_id": "text",
                "elem_classes": [],
                "name": "html",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "0ef1a4eade65d710fcd9fc56f6586b0e",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "<p>Hello</p>"
        },
        {
            "id": 46,
            "type": "plot",
            "props": {
                "value": {
                    "type": "plotly",
                    "plot": "{\"data\":[{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"cloudyu\\u002fMixtral_7Bx2_MoE\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",12.88,73.43,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.542,0.554,11.3,11.5,11.1,11.2,-2.1700000000000017,1.769999999999996],[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.24,72.25,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.289,0.299,20.7,20.7,20.3,20.3,-3.3400000000000034,0.0],[\"chargoddard\\u002fYi-34B-Llama\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",34.39,70.95,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",1.24,1.25,11.5,11.8,10.9,11.2,-0.7999999999999972,2.6099999999999994],[\"01-ai\\u002fYi-34B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",34.39,70.81,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",1.24,1.25,11.5,11.8,10.9,11.2,-0.7999999999999972,2.6099999999999994],[\"01-ai\\u002fYi-34B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",34.39,69.42,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",1.24,1.25,11.5,11.7,11.0,11.1,-0.7999999999999972,1.7399999999999949],[\"deepseek-ai\\u002fdeepseek-llm-67b-base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",67.0,69.38,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",2.58,2.57,6.27,6.39,5.91,6.02,0.39000000000000057,1.9099999999999966],[\"itsliupeng\\u002fMixtral-8x7B-v0.1-top3\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",46.7,69.09,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.801,0.809,7.97,8.02,7.8,7.85,-0.9899999999999949,0.6299999999999955],[\"cloudyu\\u002fMixtral_7Bx4_MOE_24B\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",24.15,68.85,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.553,0.562,10.4,10.5,10.2,10.3,-1.5999999999999943,0.9599999999999937],[\"mistralai\\u002fMixtral-8x7B-v0.1\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",46.7,68.42,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.561,0.569,9.62,9.85,9.45,9.66,-1.4099999999999966,2.3900000000000006],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",10.86,66.04,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.429,0.439,15.1,14.7,14.8,14.5,-2.280000000000001,-2.6500000000000057],[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",10.73,66.04,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.43,0.434,14.7,14.7,14.4,14.5,-0.9200000000000017,0.0],[\"cloudyu\\u002fMixtral_7Bx2_MoE_13B\",null,\"float16\",\"pytorch\",12.88,65.14,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.542,0.555,11.4,11.5,11.2,11.2,-2.3400000000000034,0.8799999999999955],[\"Delcos\\u002fStarling-LM-11B-alpha\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",11.39,63.66,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.459,0.475,13.1,13.0,12.9,12.7,-3.3700000000000045,-0.7600000000000051],[\"huggyllama\\u002fllama-65b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",65.29,62.79,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",2.5,2.5,6.57,6.57,6.2,6.2,0.0,0.0],[\"Deci\\u002fDeciLM-7B\",\"\ud83d\udd35 deci\",\"float16\",\"pytorch\",7.04,61.55,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.284,0.284,21.8,22.2,21.3,21.7,0.0,1.8299999999999983],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.24,60.97,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.29,0.299,20.9,20.2,20.5,19.8,-3.010000000000005,-3.3499999999999943],[\"Walmart-the-bag\\u002fInfluxient-4x13B\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",38.5,60.57,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.857,0.868,8.17,8.17,7.98,7.98,-1.269999999999996,0.0],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.0,58.05,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.295,0.3,20.6,20.6,20.2,20.2,-1.6700000000000017,0.0],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,56.69,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.235,0.238,22.0,22.6,21.7,22.3,-1.2600000000000051,2.730000000000004],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,54.08,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.239,0.238,22.0,22.6,21.7,22.3,0.4200000000000017,2.730000000000004],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.0,53.67,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.51,0.503,18.9,18.2,18.3,17.7,1.3900000000000006,-3.700000000000003],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.24,52.59,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.29,0.299,20.9,20.7,20.5,20.3,-3.010000000000005,-0.9599999999999937],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.96,51.64,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.29,0.302,20.7,20.4,20.3,20.0,-3.969999999999999,-1.4500000000000028],[\"huggyllama\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.02,51.33,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.5,0.515,18.9,18.5,18.3,17.9,-2.9099999999999966,-2.1200000000000045],[\"chargoddard\\u002fllama-2-26b-trenchcoat-stack\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",25.7,51.13,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.995,1.0,9.44,9.24,9.14,8.95,-0.5,-2.1200000000000045],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.18,50.27,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.238,0.235,21.8,22.8,21.5,22.5,1.2800000000000011,4.590000000000003],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.0,49.5,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.498,0.504,18.8,18.1,18.2,17.5,-1.1899999999999977,-3.719999999999999],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,48.64,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.271,0.273,23.6,22.8,23.1,22.3,-0.730000000000004,-3.3900000000000006],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,47.93,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.268,0.268,23.4,23.0,22.9,22.5,0.0,-1.7099999999999937],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,47.64,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.268,0.27,23.2,22.8,22.7,22.3,-0.7399999999999949,-1.7199999999999989],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.0,47.26,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.498,0.507,18.9,18.5,18.3,17.9,-1.7800000000000011,-2.1200000000000045],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,47.09,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.273,0.268,23.4,23.0,22.9,22.5,1.8700000000000045,-1.7099999999999937],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.87,46.64,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.268,0.274,23.2,22.8,22.7,22.3,-2.1899999999999977,-1.7199999999999989],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.61,45.62,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.269,0.268,23.4,22.8,22.9,22.3,0.37000000000000455,-2.5600000000000023],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,44.75,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.125,0.128,30.2,29.1,29.9,28.8,-2.3400000000000034,-3.6400000000000006],[\"abhinand\\u002ftamil-llama-7b-base-v0.1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.52,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.27,0.269,23.6,23.0,23.1,22.5,0.37000000000000455,-2.5400000000000063],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.26,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.272,0.273,23.6,22.8,23.1,22.3,-0.37000000000000455,-3.3900000000000006],[\"codellama\\u002fCodeLlama-13b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.02,43.35,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.496,0.503,19.0,18.2,18.4,17.7,-1.3900000000000006,-4.209999999999994],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,43.27,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.269,0.268,23.0,22.8,22.5,22.3,0.37000000000000455,-0.8700000000000045],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,42.31,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.268,0.271,23.6,22.8,23.1,22.3,-1.1099999999999994,-3.3900000000000006],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.02,41.44,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.122,0.124,30.5,30.3,30.2,30.0,-1.6099999999999994,-0.6599999999999966],[\"NucleusAI\\u002fnucleus-22B-token-500B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",21.83,41.33,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.818,0.824,17.7,17.8,16.8,17.0,-0.730000000000004,0.5600000000000023],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,40.97,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.263,0.27,23.0,23.2,22.5,22.7,-2.5900000000000034,0.8700000000000045],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,40.28,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.149,0.147,27.0,26.3,26.6,26.0,1.3599999999999994,-2.5900000000000034],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,39.81,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.267,0.271,23.2,23.0,22.7,22.5,-1.480000000000004,-0.8599999999999994],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,38.26,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.147,0.147,26.7,26.5,26.4,26.1,0.0,-0.75],[\"Devio\\u002ftest-22B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",21.83,37.71,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.818,0.825,17.6,17.7,16.7,16.8,-0.8499999999999943,0.5699999999999932],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,36.42,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0492,0.0483,31.4,32.2,31.3,32.1,1.8599999999999994,2.549999999999997],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.3,35.71,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0558,0.0557,30.7,30.2,30.6,30.1,0.18000000000000682,-1.6299999999999955],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.03,34.37,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0485,0.0483,31.1,32.4,31.1,32.3,0.4099999999999966,4.180000000000007],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,33.72,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0486,0.0487,31.6,32.6,31.6,32.5,-0.20999999999999375,3.1599999999999966],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,32.44,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.24,0.246,21.4,22.4,21.2,22.1,-2.4399999999999977,4.670000000000002],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,31.86,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0496,0.0487,31.2,32.4,31.1,32.4,1.8499999999999943,3.8499999999999943],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.46,30.17,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0464,0.0447,28.3,29.4,28.3,29.4,3.799999999999997,3.8900000000000006],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,30.15,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.269,0.273,23.2,22.6,22.7,22.1,-1.4699999999999989,-2.5900000000000034],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,29.53,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.134,0.134,45.6,45.2,44.8,44.4,0.0,-0.8799999999999955],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.44,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0188,0.0185,67.8,70.1,67.7,69.9,1.6200000000000045,3.3900000000000006],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",0.16,29.41,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0229,0.0232,55.2,55.4,55.2,55.4,-1.2900000000000063,0.35999999999999943],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.23,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0188,0.0182,67.8,69.5,67.7,69.4,3.299999999999997,2.510000000000005],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.1,28.97,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0114,0.0113,110.0,112.0,110.0,112.0,0.8799999999999955,1.8199999999999932],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.06,28.7,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0114,0.011,107.0,113.0,107.0,113.0,3.6400000000000006,5.609999999999999],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",1.12,28.49,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0472,0.0464,47.0,48.7,46.7,48.4,1.7199999999999989,3.6200000000000045],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",2.01,28.45,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0372,0.0367,54.1,54.5,53.9,54.2,1.3599999999999994,0.7399999999999949],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.0,28.44,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0338,0.034,165.0,158.0,162.0,156.0,-0.5900000000000034,-4.239999999999995],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.08,28.17,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0112,0.0116,112.0,111.0,112.0,111.0,-3.450000000000003,-0.8900000000000006],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,27.73,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0241,0.0239,53.3,51.4,53.3,51.4,0.8400000000000034,-3.5600000000000023]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[18]}\",\"legendgroup\":\"BnB.4bit\",\"marker\":{\"color\":\"#FD3216\"},\"name\":\"BnB.4bit\",\"notched\":false,\"offsetgroup\":\"BnB.4bit\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u24c2\ufe0f Mixtral\",\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u24c2\ufe0f Mixtral\",\"\u24c2\ufe0f Mixtral\",\"\u24c2\ufe0f Mixtral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",null,\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83d\udd35 deci\",\"\u24c2\ufe0f Mistral\",\"\u24c2\ufe0f Mixtral\",\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u24c2\ufe0f Mistral\",\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"\u24c2\ufe0f Mixtral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u24c2\ufe0f Mistral\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-2.1700000000000017,-3.3400000000000034,-0.7999999999999972,-0.7999999999999972,-0.7999999999999972,0.39000000000000057,-0.9899999999999949,-1.5999999999999943,-1.4099999999999966,-2.280000000000001,-0.9200000000000017,-2.3400000000000034,-3.3700000000000045,0.0,0.0,-3.010000000000005,-1.269999999999996,-1.6700000000000017,-1.2600000000000051,0.4200000000000017,1.3900000000000006,-3.010000000000005,-3.969999999999999,-2.9099999999999966,-0.5,1.2800000000000011,-1.1899999999999977,-0.730000000000004,0.0,-0.7399999999999949,-1.7800000000000011,1.8700000000000045,-2.1899999999999977,0.37000000000000455,-2.3400000000000034,0.37000000000000455,-0.37000000000000455,-1.3900000000000006,0.37000000000000455,-1.1099999999999994,-1.6099999999999994,-0.730000000000004,-2.5900000000000034,1.3599999999999994,-1.480000000000004,0.0,-0.8499999999999943,1.8599999999999994,0.18000000000000682,0.4099999999999966,-0.20999999999999375,-2.4399999999999977,1.8499999999999943,3.799999999999997,-1.4699999999999989,0.0,1.6200000000000045,-1.2900000000000063,3.299999999999997,0.8799999999999955,3.6400000000000006,1.7199999999999989,1.3599999999999994,-0.5900000000000034,-3.450000000000003,0.8400000000000034],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.24,72.25,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.101,0.103,9.88,9.7,9.88,9.7,-1.9399999999999977,-1.8199999999999932],[\"chargoddard\\u002fYi-34B-Llama\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",34.39,70.95,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.185,0.187,5.41,5.47,5.41,5.47,-1.0699999999999932,1.1099999999999994],[\"01-ai\\u002fYi-34B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",34.39,70.81,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.189,0.186,5.43,5.45,5.42,5.45,1.6099999999999994,0.37000000000000455],[\"01-ai\\u002fYi-34B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",34.39,69.42,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.184,0.182,5.37,5.39,5.37,5.39,1.0999999999999943,0.37000000000000455],[\"deepseek-ai\\u002fdeepseek-llm-67b-base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",67.0,69.38,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.295,0.296,3.38,3.4,3.38,3.4,-0.3400000000000034,0.5900000000000034],[\"itsliupeng\\u002fMixtral-8x7B-v0.1-top3\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",46.7,69.09,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.229,0.227,4.54,4.59,4.54,4.6,0.8799999999999955,1.0999999999999943],[\"cloudyu\\u002fMixtral_7Bx4_MOE_24B\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",24.15,68.85,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.177,0.181,5.92,5.73,5.91,5.73,-2.2099999999999937,-3.2099999999999937],[\"mistralai\\u002fMixtral-8x7B-v0.1\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",46.7,68.42,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.188,0.185,5.57,5.41,5.57,5.41,1.6200000000000045,-2.8700000000000045],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",10.86,66.04,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.148,0.143,6.71,6.87,6.72,6.88,3.5,2.3799999999999955],[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",10.73,66.04,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.149,0.144,6.76,6.84,6.77,6.84,3.469999999999999,1.1800000000000068],[\"cloudyu\\u002fMixtral_7Bx2_MoE_13B\",null,\"float16\",\"pytorch\",12.88,65.14,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.173,0.174,6.0,5.94,6.0,5.94,-0.5699999999999932,-1.0],[\"Delcos\\u002fStarling-LM-11B-alpha\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",11.39,63.66,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.165,0.165,6.13,6.13,6.12,6.12,0.0,0.0],[\"huggyllama\\u002fllama-65b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",65.29,62.79,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.239,0.241,4.14,4.04,4.14,4.04,-0.8299999999999983,-2.4200000000000017],[\"Deci\\u002fDeciLM-7B\",\"\ud83d\udd35 deci\",\"float16\",\"pytorch\",7.04,61.55,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.104,0.102,10.2,10.2,10.2,10.2,1.9599999999999937,0.0],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.24,60.97,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.102,0.104,9.85,9.77,9.85,9.77,-1.9200000000000017,-0.8100000000000023],[\"Walmart-the-bag\\u002fInfluxient-4x13B\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",38.5,60.57,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.219,0.224,4.8,4.74,4.8,4.74,-2.230000000000004,-1.25],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.0,58.05,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.104,0.102,9.81,10.0,9.81,10.0,1.9599999999999937,1.9399999999999977],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,56.69,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.101,0.0965,10.1,10.4,10.1,10.4,4.659999999999997,2.969999999999999],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,54.08,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0996,0.0956,10.0,10.4,10.0,10.4,4.180000000000007,4.0],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.0,53.67,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.122,0.119,8.25,8.28,8.26,8.28,2.519999999999996,0.35999999999999943],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.24,52.59,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.102,0.103,9.73,9.81,9.73,9.81,-0.9699999999999989,0.8199999999999932],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.96,51.64,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.105,0.104,9.85,9.85,9.85,9.85,0.9599999999999937,0.0],[\"huggyllama\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.02,51.33,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.119,0.122,8.31,7.16,8.31,7.17,-2.4599999999999937,-13.840000000000003],[\"chargoddard\\u002fllama-2-26b-trenchcoat-stack\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",25.7,51.13,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.242,0.242,4.13,4.03,4.13,4.04,0.0,-2.4200000000000017],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.18,50.27,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.105,0.097,9.73,10.3,9.73,10.3,8.25,5.859999999999999],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.0,49.5,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.122,0.124,8.25,8.04,8.26,8.05,-1.6099999999999994,-2.549999999999997],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,48.64,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0964,0.0953,10.3,10.3,10.3,10.3,1.1500000000000057,0.0],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,47.93,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0933,0.0955,10.5,10.3,10.5,10.3,-2.299999999999997,-1.9000000000000057],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,47.64,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0956,0.0991,10.5,10.1,10.5,10.1,-3.530000000000001,-3.8100000000000023],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.0,47.26,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.12,0.126,8.25,8.02,8.26,8.03,-4.760000000000005,-2.7900000000000063],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,47.09,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0936,0.0966,10.5,10.5,10.5,10.5,-3.1099999999999994,0.0],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.87,46.64,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0951,0.0985,10.3,9.96,10.3,9.96,-3.450000000000003,-3.299999999999997],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.61,45.62,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0971,0.0989,10.4,10.2,10.4,10.2,-1.8199999999999932,-1.9200000000000017],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,44.75,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0733,0.0716,13.7,13.6,13.7,13.6,2.3700000000000045,-0.730000000000004],[\"abhinand\\u002ftamil-llama-7b-base-v0.1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.52,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0937,0.1,10.7,10.2,10.7,10.2,-6.299999999999997,-4.670000000000002],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.26,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0965,0.0994,10.4,10.2,10.4,10.2,-2.9200000000000017,-1.9200000000000017],[\"codellama\\u002fCodeLlama-13b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.02,43.35,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.119,0.119,8.39,8.28,8.39,8.28,0.0,-1.3100000000000023],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,43.27,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0991,0.0941,10.2,10.5,10.2,10.4,5.310000000000002,2.9399999999999977],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,42.31,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0986,0.0954,10.2,10.4,10.2,10.4,3.3499999999999943,1.9599999999999937],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.02,41.44,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0726,0.0722,13.7,13.5,13.7,13.5,0.5499999999999972,-1.4599999999999937],[\"NucleusAI\\u002fnucleus-22B-token-500B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",21.83,41.33,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.119,0.121,8.31,8.31,8.31,8.31,-1.6500000000000057,0.0],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,40.97,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0958,0.0973,10.4,10.3,10.4,10.3,-1.5400000000000063,-0.9599999999999937],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,40.28,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0813,0.0821,12.6,12.3,12.6,12.3,-0.9699999999999989,-2.3799999999999955],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,39.81,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0949,0.0988,10.5,10.3,10.5,10.3,-3.950000000000003,-1.9000000000000057],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,38.26,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0828,0.0825,12.4,12.1,12.4,12.1,0.35999999999999943,-2.4200000000000017],[\"Devio\\u002ftest-22B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",21.83,37.71,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.119,0.123,8.33,7.97,8.34,7.98,-3.25,-4.319999999999993],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,36.42,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0701,0.0651,14.3,15.2,14.3,15.1,7.680000000000007,6.290000000000006],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.3,35.71,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0731,0.0714,13.9,13.9,13.8,13.8,2.3799999999999955,0.0],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.03,34.37,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0675,0.0662,14.5,14.9,14.5,14.9,1.9599999999999937,2.760000000000005],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,33.72,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0688,0.066,14.5,14.8,14.5,14.8,4.239999999999995,2.069999999999993],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,32.44,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.102,0.0958,9.85,10.5,9.85,10.4,6.469999999999999,6.599999999999994],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,31.86,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0693,0.0651,14.5,15.1,14.5,15.1,6.450000000000003,4.140000000000001],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.46,30.17,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0743,0.0724,13.1,13.8,13.1,13.8,2.6200000000000045,5.340000000000003],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,30.15,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0935,0.1,10.6,10.2,10.6,10.2,-6.5,-3.769999999999996],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,29.53,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0491,0.0486,20.7,20.4,20.8,20.5,1.0300000000000011,-1.4500000000000028],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.44,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0312,0.0306,31.4,32.2,31.4,32.2,1.9599999999999937,2.549999999999997],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",0.16,29.41,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0373,0.0335,27.7,29.3,27.7,29.3,11.340000000000003,5.780000000000001],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.23,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0319,0.0305,31.2,32.1,31.2,32.1,4.590000000000003,2.8799999999999955],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.1,28.97,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.019,0.0185,51.1,53.8,51.1,53.8,2.700000000000003,5.280000000000001],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.06,28.7,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0191,0.0186,52.6,53.2,52.6,53.2,2.6899999999999977,1.1400000000000006],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",1.12,28.49,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0428,0.0421,23.4,23.8,23.5,23.9,1.6599999999999966,1.7099999999999937],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",2.01,28.45,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0361,0.0343,29.4,29.6,29.4,29.6,5.25,0.6800000000000068],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.0,28.44,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.013,0.0124,78.0,78.5,78.0,78.5,4.840000000000003,0.6400000000000006],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.08,28.17,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0181,0.0183,53.6,52.8,53.6,52.8,-1.0900000000000034,-1.4899999999999949],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,27.73,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0396,0.0377,25.6,25.6,25.6,25.6,5.040000000000006,0.0]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[18]}\",\"legendgroup\":\"BnB.8bit\",\"marker\":{\"color\":\"#00FE35\"},\"name\":\"BnB.8bit\",\"notched\":false,\"offsetgroup\":\"BnB.8bit\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u24c2\ufe0f Mixtral\",\"\u24c2\ufe0f Mixtral\",\"\u24c2\ufe0f Mixtral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",null,\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83d\udd35 deci\",\"\u24c2\ufe0f Mistral\",\"\u24c2\ufe0f Mixtral\",\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u24c2\ufe0f Mistral\",\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"\u24c2\ufe0f Mixtral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u24c2\ufe0f Mistral\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-1.9399999999999977,-1.0699999999999932,1.6099999999999994,1.0999999999999943,-0.3400000000000034,0.8799999999999955,-2.2099999999999937,1.6200000000000045,3.5,3.469999999999999,-0.5699999999999932,0.0,-0.8299999999999983,1.9599999999999937,-1.9200000000000017,-2.230000000000004,1.9599999999999937,4.659999999999997,4.180000000000007,2.519999999999996,-0.9699999999999989,0.9599999999999937,-2.4599999999999937,0.0,8.25,-1.6099999999999994,1.1500000000000057,-2.299999999999997,-3.530000000000001,-4.760000000000005,-3.1099999999999994,-3.450000000000003,-1.8199999999999932,2.3700000000000045,-6.299999999999997,-2.9200000000000017,0.0,5.310000000000002,3.3499999999999943,0.5499999999999972,-1.6500000000000057,-1.5400000000000063,-0.9699999999999989,-3.950000000000003,0.35999999999999943,-3.25,7.680000000000007,2.3799999999999955,1.9599999999999937,4.239999999999995,6.469999999999999,6.450000000000003,2.6200000000000045,-6.5,1.0300000000000011,1.9599999999999937,11.340000000000003,4.590000000000003,2.700000000000003,2.6899999999999977,1.6599999999999966,5.25,4.840000000000003,-1.0900000000000034,5.040000000000006],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.0,58.05,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0368,0.0423,34.4,33.7,34.4,33.7,-13.0,-2.030000000000001],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,56.69,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0312,0.029,38.5,39.8,38.4,39.8,7.590000000000003,3.3799999999999955],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,54.08,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0298,0.0284,38.0,39.5,38.0,39.4,4.930000000000007,3.950000000000003],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.18,50.27,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0293,0.0286,38.1,39.5,38.0,39.5,2.450000000000003,3.6700000000000017],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,48.64,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0321,0.0319,42.9,41.1,42.9,41.0,0.6299999999999955,-4.200000000000003],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,47.64,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0326,0.0329,42.6,40.5,42.6,40.4,-0.9099999999999966,-4.930000000000007],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.87,46.64,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0327,0.0325,41.5,40.7,41.4,40.7,0.6200000000000045,-1.9300000000000068],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.61,45.62,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.032,0.0318,42.4,40.5,42.4,40.4,0.6299999999999955,-4.480000000000004],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0184,0.0187,55.0,51.9,54.9,51.9,-1.5999999999999943,-5.640000000000001],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.26,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0328,0.0321,42.5,40.0,42.5,40.0,2.180000000000007,-5.8799999999999955],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,42.31,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0334,0.0318,42.6,40.9,42.6,40.8,5.030000000000001,-3.989999999999995],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0329,0.0281,40.0,48.8,40.0,48.7,17.08,22.0],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.02,41.44,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0185,0.0187,53.2,51.6,53.2,51.6,-1.0699999999999932,-3.010000000000005],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0329,0.0297,40.5,48.3,40.5,48.2,10.769999999999996,19.260000000000005],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,40.97,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0314,0.0316,44.1,41.0,44.1,41.0,-0.6299999999999955,-7.030000000000001],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,40.28,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.023,0.0221,46.4,43.8,46.5,43.8,4.069999999999993,-5.599999999999994],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,39.81,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0329,0.0321,43.4,40.7,43.3,40.6,2.489999999999995,-6.219999999999999],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.9,39.3,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0315,0.0276,40.4,48.7,40.4,48.6,14.129999999999995,20.540000000000006],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0277,0.0205,39.5,48.9,39.4,48.9,35.120000000000005,23.799999999999997],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,38.26,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.023,0.0225,46.8,42.9,46.8,43.0,2.219999999999999,-8.329999999999998],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0315,0.0275,40.4,48.8,40.4,48.7,14.549999999999997,20.790000000000006],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0367,0.0295,27.7,33.9,27.7,33.9,24.409999999999997,22.379999999999995],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0256,0.0202,41.3,49.6,41.2,49.6,26.730000000000004,20.099999999999994],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.026,0.0204,40.4,48.9,40.4,48.9,27.450000000000003,21.040000000000006],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,36.42,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0181,0.0171,53.9,57.2,53.9,57.1,5.849999999999994,6.1200000000000045],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0241,0.017,43.9,56.7,43.9,56.6,41.75999999999999,29.159999999999997],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0266,0.021,40.1,47.6,40.1,47.6,26.67,18.700000000000003],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.3,35.71,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0185,0.0182,54.8,53.6,54.8,53.6,1.6500000000000057,-2.1899999999999977],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0189,0.0154,53.7,62.0,53.7,62.0,22.730000000000004,15.459999999999994],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0194,0.0154,53.5,63.8,53.4,63.7,25.97,19.25],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0333,0.0295,39.6,46.9,39.6,46.8,12.879999999999995,18.430000000000007],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.03,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0187,0.0172,53.2,57.8,53.2,57.8,8.719999999999999,8.650000000000006],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0288,0.0275,66.4,78.7,66.1,78.3,4.730000000000004,18.519999999999996],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,33.72,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0178,0.0169,53.8,57.7,53.8,57.7,5.329999999999998,7.25],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0172,0.0129,59.7,75.4,59.7,75.5,33.33000000000001,26.299999999999997],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0125,0.0107,78.2,93.1,78.3,93.1,16.819999999999993,19.049999999999997],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,32.44,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0316,0.0288,38.7,40.2,38.7,40.1,9.719999999999999,3.8799999999999955],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.03,0.0233,36.2,43.3,36.2,43.3,28.75999999999999,19.61],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0284,0.023,35.8,43.0,35.8,43.0,23.480000000000004,20.11],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,31.86,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.018,0.0168,52.1,58.0,52.1,57.9,7.140000000000001,11.319999999999993],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0181,0.0157,55.3,63.9,55.3,63.8,15.290000000000006,15.549999999999997],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0172,0.0143,79.2,93.8,79.0,93.8,20.28,18.430000000000007],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.46,30.17,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0188,0.0182,51.0,52.8,51.0,52.8,3.299999999999997,3.530000000000001],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,29.53,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0163,0.0165,82.5,79.4,82.3,79.3,-1.2099999999999937,-3.760000000000005],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00838,0.00641,121.0,147.0,121.0,147.0,30.72999999999999,21.489999999999995],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.44,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00784,0.00789,118.0,120.0,117.0,120.0,-0.6299999999999955,1.6899999999999977],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",0.16,29.41,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0089,0.0078,112.0,116.0,112.0,116.0,14.099999999999994,3.569999999999993],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00925,0.00793,109.0,128.0,109.0,127.0,16.650000000000006,17.430000000000007],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0172,0.0135,61.4,75.4,61.4,75.5,27.409999999999997,22.799999999999997],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.23,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00789,0.00778,116.0,121.0,116.0,121.0,1.4099999999999966,4.310000000000002],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00482,0.00391,207.0,243.0,208.0,244.0,23.269999999999996,17.39],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.14,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00542,0.0047,182.0,193.0,182.0,194.0,15.319999999999993,6.040000000000006],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00918,0.0079,108.0,129.0,108.0,129.0,16.200000000000003,19.439999999999998],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.1,28.97,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00495,0.00491,185.0,192.0,186.0,192.0,0.8100000000000023,3.780000000000001],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0196,0.0155,50.4,63.4,50.4,63.4,26.450000000000003,25.790000000000006],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00494,0.00389,206.0,245.0,206.0,246.0,26.989999999999995,18.930000000000007],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00468,0.0041,209.0,238.0,210.0,239.0,14.150000000000006,13.879999999999995],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.06,28.7,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00452,0.00483,196.0,190.0,197.0,191.0,-6.420000000000002,-3.0600000000000023],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00472,0.00391,207.0,243.0,208.0,244.0,20.72,17.39],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0047,0.00387,207.0,241.0,208.0,242.0,21.450000000000003,16.430000000000007],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",1.12,28.49,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.011,0.00938,86.4,97.3,86.5,97.3,17.269999999999996,12.620000000000005],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",2.01,28.45,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0246,0.0248,79.9,78.2,79.8,78.0,-0.8100000000000023,-2.1299999999999955],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.0,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00458,0.00449,278.0,268.0,278.0,268.0,2.0,-3.5999999999999943],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00476,0.00394,204.0,238.0,205.0,239.0,20.810000000000002,16.67],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00584,0.00444,176.0,212.0,175.0,213.0,31.53,20.450000000000003],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00332,0.00256,309.0,354.0,309.0,354.0,29.689999999999998,14.560000000000002],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00591,0.00446,178.0,220.0,178.0,221.0,32.50999999999999,23.599999999999994],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00472,0.00389,207.0,248.0,208.0,249.0,21.340000000000003,19.810000000000002],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,28.19,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00582,0.00452,178.0,218.0,178.0,219.0,28.75999999999999,22.47],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.08,28.17,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00459,0.00468,195.0,195.0,195.0,195.0,-1.9200000000000017,0.0],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,27.73,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.011,0.0111,90.4,83.9,90.5,83.9,-0.9000000000000057,-7.189999999999998]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[18]}\",\"legendgroup\":\"None\",\"marker\":{\"color\":\"#6A76FC\"},\"name\":\"None\",\"notched\":false,\"offsetgroup\":\"None\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"\u2b50 StarCoder\",\"\u24c2\ufe0f Mixtral\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\u24c2\ufe0f Mistral\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-13.0,7.590000000000003,4.930000000000007,2.450000000000003,0.6299999999999955,-0.9099999999999966,0.6200000000000045,0.6299999999999955,-1.5999999999999943,2.180000000000007,5.030000000000001,17.08,-1.0699999999999932,10.769999999999996,-0.6299999999999955,4.069999999999993,2.489999999999995,14.129999999999995,35.120000000000005,2.219999999999999,14.549999999999997,24.409999999999997,26.730000000000004,27.450000000000003,5.849999999999994,41.75999999999999,26.67,1.6500000000000057,22.730000000000004,25.97,12.879999999999995,8.719999999999999,4.730000000000004,5.329999999999998,33.33000000000001,16.819999999999993,9.719999999999999,28.75999999999999,23.480000000000004,7.140000000000001,15.290000000000006,20.28,3.299999999999997,-1.2099999999999937,30.72999999999999,-0.6299999999999955,14.099999999999994,16.650000000000006,27.409999999999997,1.4099999999999966,23.269999999999996,15.319999999999993,16.200000000000003,0.8100000000000023,26.450000000000003,26.989999999999995,14.150000000000006,-6.420000000000002,20.72,21.450000000000003,17.269999999999996,-0.8100000000000023,2.0,20.810000000000002,31.53,29.689999999999998,32.50999999999999,21.340000000000003,28.75999999999999,-1.9200000000000017,-0.9000000000000057],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"}],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"LLM Architecture\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Prefill Speedup (%)\"}},\"legend\":{\"title\":{\"text\":\"Quantization Scheme\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"boxmode\":\"group\",\"title\":{\"text\":\"Prefill Latency Speedup per Architecture, Compared To Non-Optimized Model\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":600}}"
                },
                "show_label": false,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "plot",
                "elem_classes": [],
                "name": "plot",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "12c46a60185f5c11b6b6b6f9adbe826c",
            "api_info": {
                "properties": {
                    "type": {
                        "enum": [
                            "altair",
                            "bokeh",
                            "plotly",
                            "matplotlib"
                        ],
                        "title": "Type",
                        "type": "string"
                    },
                    "plot": {
                        "title": "Plot",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "plot"
                ],
                "title": "PlotData",
                "type": "object"
            },
            "example_inputs": null
        },
        {
            "id": 47,
            "type": "plot",
            "props": {
                "value": {
                    "type": "plotly",
                    "plot": "{\"data\":[{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"cloudyu\\u002fMixtral_7Bx2_MoE\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",12.88,73.43,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.542,0.554,11.3,11.5,11.1,11.2,-2.1700000000000017,1.769999999999996],[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.24,72.25,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.289,0.299,20.7,20.7,20.3,20.3,-3.3400000000000034,0.0],[\"chargoddard\\u002fYi-34B-Llama\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",34.39,70.95,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",1.24,1.25,11.5,11.8,10.9,11.2,-0.7999999999999972,2.6099999999999994],[\"01-ai\\u002fYi-34B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",34.39,70.81,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",1.24,1.25,11.5,11.8,10.9,11.2,-0.7999999999999972,2.6099999999999994],[\"01-ai\\u002fYi-34B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",34.39,69.42,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",1.24,1.25,11.5,11.7,11.0,11.1,-0.7999999999999972,1.7399999999999949],[\"deepseek-ai\\u002fdeepseek-llm-67b-base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",67.0,69.38,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",2.58,2.57,6.27,6.39,5.91,6.02,0.39000000000000057,1.9099999999999966],[\"itsliupeng\\u002fMixtral-8x7B-v0.1-top3\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",46.7,69.09,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.801,0.809,7.97,8.02,7.8,7.85,-0.9899999999999949,0.6299999999999955],[\"cloudyu\\u002fMixtral_7Bx4_MOE_24B\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",24.15,68.85,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.553,0.562,10.4,10.5,10.2,10.3,-1.5999999999999943,0.9599999999999937],[\"mistralai\\u002fMixtral-8x7B-v0.1\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",46.7,68.42,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.561,0.569,9.62,9.85,9.45,9.66,-1.4099999999999966,2.3900000000000006],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",10.86,66.04,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.429,0.439,15.1,14.7,14.8,14.5,-2.280000000000001,-2.6500000000000057],[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",10.73,66.04,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.43,0.434,14.7,14.7,14.4,14.5,-0.9200000000000017,0.0],[\"cloudyu\\u002fMixtral_7Bx2_MoE_13B\",null,\"float16\",\"pytorch\",12.88,65.14,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.542,0.555,11.4,11.5,11.2,11.2,-2.3400000000000034,0.8799999999999955],[\"Delcos\\u002fStarling-LM-11B-alpha\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",11.39,63.66,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.459,0.475,13.1,13.0,12.9,12.7,-3.3700000000000045,-0.7600000000000051],[\"huggyllama\\u002fllama-65b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",65.29,62.79,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",2.5,2.5,6.57,6.57,6.2,6.2,0.0,0.0],[\"Deci\\u002fDeciLM-7B\",\"\ud83d\udd35 deci\",\"float16\",\"pytorch\",7.04,61.55,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.284,0.284,21.8,22.2,21.3,21.7,0.0,1.8299999999999983],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.24,60.97,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.29,0.299,20.9,20.2,20.5,19.8,-3.010000000000005,-3.3499999999999943],[\"Walmart-the-bag\\u002fInfluxient-4x13B\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",38.5,60.57,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.857,0.868,8.17,8.17,7.98,7.98,-1.269999999999996,0.0],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.0,58.05,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.295,0.3,20.6,20.6,20.2,20.2,-1.6700000000000017,0.0],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,56.69,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.235,0.238,22.0,22.6,21.7,22.3,-1.2600000000000051,2.730000000000004],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,54.08,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.239,0.238,22.0,22.6,21.7,22.3,0.4200000000000017,2.730000000000004],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.0,53.67,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.51,0.503,18.9,18.2,18.3,17.7,1.3900000000000006,-3.700000000000003],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.24,52.59,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.29,0.299,20.9,20.7,20.5,20.3,-3.010000000000005,-0.9599999999999937],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.96,51.64,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.29,0.302,20.7,20.4,20.3,20.0,-3.969999999999999,-1.4500000000000028],[\"huggyllama\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.02,51.33,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.5,0.515,18.9,18.5,18.3,17.9,-2.9099999999999966,-2.1200000000000045],[\"chargoddard\\u002fllama-2-26b-trenchcoat-stack\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",25.7,51.13,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.995,1.0,9.44,9.24,9.14,8.95,-0.5,-2.1200000000000045],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.18,50.27,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.238,0.235,21.8,22.8,21.5,22.5,1.2800000000000011,4.590000000000003],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.0,49.5,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.498,0.504,18.8,18.1,18.2,17.5,-1.1899999999999977,-3.719999999999999],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,48.64,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.271,0.273,23.6,22.8,23.1,22.3,-0.730000000000004,-3.3900000000000006],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,47.93,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.268,0.268,23.4,23.0,22.9,22.5,0.0,-1.7099999999999937],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,47.64,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.268,0.27,23.2,22.8,22.7,22.3,-0.7399999999999949,-1.7199999999999989],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.0,47.26,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.498,0.507,18.9,18.5,18.3,17.9,-1.7800000000000011,-2.1200000000000045],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,47.09,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.273,0.268,23.4,23.0,22.9,22.5,1.8700000000000045,-1.7099999999999937],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.87,46.64,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.268,0.274,23.2,22.8,22.7,22.3,-2.1899999999999977,-1.7199999999999989],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.61,45.62,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.269,0.268,23.4,22.8,22.9,22.3,0.37000000000000455,-2.5600000000000023],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,44.75,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.125,0.128,30.2,29.1,29.9,28.8,-2.3400000000000034,-3.6400000000000006],[\"abhinand\\u002ftamil-llama-7b-base-v0.1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.52,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.27,0.269,23.6,23.0,23.1,22.5,0.37000000000000455,-2.5400000000000063],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.26,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.272,0.273,23.6,22.8,23.1,22.3,-0.37000000000000455,-3.3900000000000006],[\"codellama\\u002fCodeLlama-13b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.02,43.35,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.496,0.503,19.0,18.2,18.4,17.7,-1.3900000000000006,-4.209999999999994],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,43.27,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.269,0.268,23.0,22.8,22.5,22.3,0.37000000000000455,-0.8700000000000045],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,42.31,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.268,0.271,23.6,22.8,23.1,22.3,-1.1099999999999994,-3.3900000000000006],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.02,41.44,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.122,0.124,30.5,30.3,30.2,30.0,-1.6099999999999994,-0.6599999999999966],[\"NucleusAI\\u002fnucleus-22B-token-500B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",21.83,41.33,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.818,0.824,17.7,17.8,16.8,17.0,-0.730000000000004,0.5600000000000023],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,40.97,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.263,0.27,23.0,23.2,22.5,22.7,-2.5900000000000034,0.8700000000000045],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,40.28,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.149,0.147,27.0,26.3,26.6,26.0,1.3599999999999994,-2.5900000000000034],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,39.81,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.267,0.271,23.2,23.0,22.7,22.5,-1.480000000000004,-0.8599999999999994],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,38.26,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.147,0.147,26.7,26.5,26.4,26.1,0.0,-0.75],[\"Devio\\u002ftest-22B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",21.83,37.71,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.818,0.825,17.6,17.7,16.7,16.8,-0.8499999999999943,0.5699999999999932],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,36.42,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0492,0.0483,31.4,32.2,31.3,32.1,1.8599999999999994,2.549999999999997],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.3,35.71,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0558,0.0557,30.7,30.2,30.6,30.1,0.18000000000000682,-1.6299999999999955],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.03,34.37,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0485,0.0483,31.1,32.4,31.1,32.3,0.4099999999999966,4.180000000000007],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,33.72,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0486,0.0487,31.6,32.6,31.6,32.5,-0.20999999999999375,3.1599999999999966],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,32.44,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.24,0.246,21.4,22.4,21.2,22.1,-2.4399999999999977,4.670000000000002],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,31.86,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0496,0.0487,31.2,32.4,31.1,32.4,1.8499999999999943,3.8499999999999943],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.46,30.17,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0464,0.0447,28.3,29.4,28.3,29.4,3.799999999999997,3.8900000000000006],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,30.15,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.269,0.273,23.2,22.6,22.7,22.1,-1.4699999999999989,-2.5900000000000034],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,29.53,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.134,0.134,45.6,45.2,44.8,44.4,0.0,-0.8799999999999955],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.44,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0188,0.0185,67.8,70.1,67.7,69.9,1.6200000000000045,3.3900000000000006],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",0.16,29.41,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0229,0.0232,55.2,55.4,55.2,55.4,-1.2900000000000063,0.35999999999999943],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.23,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0188,0.0182,67.8,69.5,67.7,69.4,3.299999999999997,2.510000000000005],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.1,28.97,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0114,0.0113,110.0,112.0,110.0,112.0,0.8799999999999955,1.8199999999999932],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.06,28.7,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0114,0.011,107.0,113.0,107.0,113.0,3.6400000000000006,5.609999999999999],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",1.12,28.49,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0472,0.0464,47.0,48.7,46.7,48.4,1.7199999999999989,3.6200000000000045],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",2.01,28.45,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0372,0.0367,54.1,54.5,53.9,54.2,1.3599999999999994,0.7399999999999949],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.0,28.44,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0338,0.034,165.0,158.0,162.0,156.0,-0.5900000000000034,-4.239999999999995],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.08,28.17,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0112,0.0116,112.0,111.0,112.0,111.0,-3.450000000000003,-0.8900000000000006],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,27.73,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",\"FlashAttentionV2\",0.0241,0.0239,53.3,51.4,53.3,51.4,0.8400000000000034,-3.5600000000000023]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[18]}\",\"legendgroup\":\"BnB.4bit\",\"marker\":{\"color\":\"#FD3216\"},\"name\":\"BnB.4bit\",\"notched\":false,\"offsetgroup\":\"BnB.4bit\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u24c2\ufe0f Mixtral\",\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u24c2\ufe0f Mixtral\",\"\u24c2\ufe0f Mixtral\",\"\u24c2\ufe0f Mixtral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",null,\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83d\udd35 deci\",\"\u24c2\ufe0f Mistral\",\"\u24c2\ufe0f Mixtral\",\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u24c2\ufe0f Mistral\",\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"\u24c2\ufe0f Mixtral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u24c2\ufe0f Mistral\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[1.769999999999996,0.0,2.6099999999999994,2.6099999999999994,1.7399999999999949,1.9099999999999966,0.6299999999999955,0.9599999999999937,2.3900000000000006,-2.6500000000000057,0.0,0.8799999999999955,-0.7600000000000051,0.0,1.8299999999999983,-3.3499999999999943,0.0,0.0,2.730000000000004,2.730000000000004,-3.700000000000003,-0.9599999999999937,-1.4500000000000028,-2.1200000000000045,-2.1200000000000045,4.590000000000003,-3.719999999999999,-3.3900000000000006,-1.7099999999999937,-1.7199999999999989,-2.1200000000000045,-1.7099999999999937,-1.7199999999999989,-2.5600000000000023,-3.6400000000000006,-2.5400000000000063,-3.3900000000000006,-4.209999999999994,-0.8700000000000045,-3.3900000000000006,-0.6599999999999966,0.5600000000000023,0.8700000000000045,-2.5900000000000034,-0.8599999999999994,-0.75,0.5699999999999932,2.549999999999997,-1.6299999999999955,4.180000000000007,3.1599999999999966,4.670000000000002,3.8499999999999943,3.8900000000000006,-2.5900000000000034,-0.8799999999999955,3.3900000000000006,0.35999999999999943,2.510000000000005,1.8199999999999932,5.609999999999999,3.6200000000000045,0.7399999999999949,-4.239999999999995,-0.8900000000000006,-3.5600000000000023],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.24,72.25,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.101,0.103,9.88,9.7,9.88,9.7,-1.9399999999999977,-1.8199999999999932],[\"chargoddard\\u002fYi-34B-Llama\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",34.39,70.95,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.185,0.187,5.41,5.47,5.41,5.47,-1.0699999999999932,1.1099999999999994],[\"01-ai\\u002fYi-34B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",34.39,70.81,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.189,0.186,5.43,5.45,5.42,5.45,1.6099999999999994,0.37000000000000455],[\"01-ai\\u002fYi-34B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",34.39,69.42,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.184,0.182,5.37,5.39,5.37,5.39,1.0999999999999943,0.37000000000000455],[\"deepseek-ai\\u002fdeepseek-llm-67b-base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",67.0,69.38,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.295,0.296,3.38,3.4,3.38,3.4,-0.3400000000000034,0.5900000000000034],[\"itsliupeng\\u002fMixtral-8x7B-v0.1-top3\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",46.7,69.09,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.229,0.227,4.54,4.59,4.54,4.6,0.8799999999999955,1.0999999999999943],[\"cloudyu\\u002fMixtral_7Bx4_MOE_24B\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",24.15,68.85,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.177,0.181,5.92,5.73,5.91,5.73,-2.2099999999999937,-3.2099999999999937],[\"mistralai\\u002fMixtral-8x7B-v0.1\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",46.7,68.42,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.188,0.185,5.57,5.41,5.57,5.41,1.6200000000000045,-2.8700000000000045],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",10.86,66.04,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.148,0.143,6.71,6.87,6.72,6.88,3.5,2.3799999999999955],[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",10.73,66.04,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.149,0.144,6.76,6.84,6.77,6.84,3.469999999999999,1.1800000000000068],[\"cloudyu\\u002fMixtral_7Bx2_MoE_13B\",null,\"float16\",\"pytorch\",12.88,65.14,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.173,0.174,6.0,5.94,6.0,5.94,-0.5699999999999932,-1.0],[\"Delcos\\u002fStarling-LM-11B-alpha\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",11.39,63.66,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.165,0.165,6.13,6.13,6.12,6.12,0.0,0.0],[\"huggyllama\\u002fllama-65b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",65.29,62.79,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.239,0.241,4.14,4.04,4.14,4.04,-0.8299999999999983,-2.4200000000000017],[\"Deci\\u002fDeciLM-7B\",\"\ud83d\udd35 deci\",\"float16\",\"pytorch\",7.04,61.55,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.104,0.102,10.2,10.2,10.2,10.2,1.9599999999999937,0.0],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.24,60.97,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.102,0.104,9.85,9.77,9.85,9.77,-1.9200000000000017,-0.8100000000000023],[\"Walmart-the-bag\\u002fInfluxient-4x13B\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",38.5,60.57,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.219,0.224,4.8,4.74,4.8,4.74,-2.230000000000004,-1.25],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.0,58.05,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.104,0.102,9.81,10.0,9.81,10.0,1.9599999999999937,1.9399999999999977],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,56.69,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.101,0.0965,10.1,10.4,10.1,10.4,4.659999999999997,2.969999999999999],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,54.08,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0996,0.0956,10.0,10.4,10.0,10.4,4.180000000000007,4.0],[\"TheBloke\\u002fLlama-2-13B-fp16\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.0,53.67,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.122,0.119,8.25,8.28,8.26,8.28,2.519999999999996,0.35999999999999943],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.24,52.59,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.102,0.103,9.73,9.81,9.73,9.81,-0.9699999999999989,0.8199999999999932],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.96,51.64,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.105,0.104,9.85,9.85,9.85,9.85,0.9599999999999937,0.0],[\"huggyllama\\u002fllama-13b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.02,51.33,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.119,0.122,8.31,7.16,8.31,7.17,-2.4599999999999937,-13.840000000000003],[\"chargoddard\\u002fllama-2-26b-trenchcoat-stack\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",25.7,51.13,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.242,0.242,4.13,4.03,4.13,4.04,0.0,-2.4200000000000017],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.18,50.27,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.105,0.097,9.73,10.3,9.73,10.3,8.25,5.859999999999999],[\"abhinand\\u002ftamil-llama-13b-base-v0.1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.0,49.5,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.122,0.124,8.25,8.04,8.26,8.05,-1.6099999999999994,-2.549999999999997],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,48.64,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0964,0.0953,10.3,10.3,10.3,10.3,1.1500000000000057,0.0],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,47.93,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0933,0.0955,10.5,10.3,10.5,10.3,-2.299999999999997,-1.9000000000000057],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,47.64,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0956,0.0991,10.5,10.1,10.5,10.1,-3.530000000000001,-3.8100000000000023],[\"openlm-research\\u002fopen_llama_13b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.0,47.26,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.12,0.126,8.25,8.02,8.26,8.03,-4.760000000000005,-2.7900000000000063],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,47.09,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0936,0.0966,10.5,10.5,10.5,10.5,-3.1099999999999994,0.0],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.87,46.64,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0951,0.0985,10.3,9.96,10.3,9.96,-3.450000000000003,-3.299999999999997],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.61,45.62,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0971,0.0989,10.4,10.2,10.4,10.2,-1.8199999999999932,-1.9200000000000017],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,44.75,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0733,0.0716,13.7,13.6,13.7,13.6,2.3700000000000045,-0.730000000000004],[\"abhinand\\u002ftamil-llama-7b-base-v0.1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.52,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0937,0.1,10.7,10.2,10.7,10.2,-6.299999999999997,-4.670000000000002],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.26,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0965,0.0994,10.4,10.2,10.4,10.2,-2.9200000000000017,-1.9200000000000017],[\"codellama\\u002fCodeLlama-13b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",13.02,43.35,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.119,0.119,8.39,8.28,8.39,8.28,0.0,-1.3100000000000023],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,43.27,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0991,0.0941,10.2,10.5,10.2,10.4,5.310000000000002,2.9399999999999977],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,42.31,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0986,0.0954,10.2,10.4,10.2,10.4,3.3499999999999943,1.9599999999999937],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.02,41.44,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0726,0.0722,13.7,13.5,13.7,13.5,0.5499999999999972,-1.4599999999999937],[\"NucleusAI\\u002fnucleus-22B-token-500B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",21.83,41.33,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.119,0.121,8.31,8.31,8.31,8.31,-1.6500000000000057,0.0],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,40.97,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0958,0.0973,10.4,10.3,10.4,10.3,-1.5400000000000063,-0.9599999999999937],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,40.28,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0813,0.0821,12.6,12.3,12.6,12.3,-0.9699999999999989,-2.3799999999999955],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,39.81,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0949,0.0988,10.5,10.3,10.5,10.3,-3.950000000000003,-1.9000000000000057],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,38.26,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0828,0.0825,12.4,12.1,12.4,12.1,0.35999999999999943,-2.4200000000000017],[\"Devio\\u002ftest-22B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",21.83,37.71,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.119,0.123,8.33,7.97,8.34,7.98,-3.25,-4.319999999999993],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,36.42,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0701,0.0651,14.3,15.2,14.3,15.1,7.680000000000007,6.290000000000006],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.3,35.71,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0731,0.0714,13.9,13.9,13.8,13.8,2.3799999999999955,0.0],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.03,34.37,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0675,0.0662,14.5,14.9,14.5,14.9,1.9599999999999937,2.760000000000005],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,33.72,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0688,0.066,14.5,14.8,14.5,14.8,4.239999999999995,2.069999999999993],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,32.44,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.102,0.0958,9.85,10.5,9.85,10.4,6.469999999999999,6.599999999999994],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,31.86,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0693,0.0651,14.5,15.1,14.5,15.1,6.450000000000003,4.140000000000001],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.46,30.17,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0743,0.0724,13.1,13.8,13.1,13.8,2.6200000000000045,5.340000000000003],[\"uukuguy\\u002fOrca-2-7b-f16\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,30.15,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0935,0.1,10.6,10.2,10.6,10.2,-6.5,-3.769999999999996],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,29.53,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0491,0.0486,20.7,20.4,20.8,20.5,1.0300000000000011,-1.4500000000000028],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.44,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0312,0.0306,31.4,32.2,31.4,32.2,1.9599999999999937,2.549999999999997],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",0.16,29.41,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0373,0.0335,27.7,29.3,27.7,29.3,11.340000000000003,5.780000000000001],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.23,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0319,0.0305,31.2,32.1,31.2,32.1,4.590000000000003,2.8799999999999955],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.1,28.97,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.019,0.0185,51.1,53.8,51.1,53.8,2.700000000000003,5.280000000000001],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.06,28.7,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0191,0.0186,52.6,53.2,52.6,53.2,2.6899999999999977,1.1400000000000006],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",1.12,28.49,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0428,0.0421,23.4,23.8,23.5,23.9,1.6599999999999966,1.7099999999999937],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",2.01,28.45,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0361,0.0343,29.4,29.6,29.4,29.6,5.25,0.6800000000000068],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.0,28.44,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.013,0.0124,78.0,78.5,78.0,78.5,4.840000000000003,0.6400000000000006],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.08,28.17,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0181,0.0183,53.6,52.8,53.6,52.8,-1.0900000000000034,-1.4899999999999949],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,27.73,\"float16\",\"pytorch\",\"None\",\"BnB.8bit\",\"FlashAttentionV2\",0.0396,0.0377,25.6,25.6,25.6,25.6,5.040000000000006,0.0]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[18]}\",\"legendgroup\":\"BnB.8bit\",\"marker\":{\"color\":\"#00FE35\"},\"name\":\"BnB.8bit\",\"notched\":false,\"offsetgroup\":\"BnB.8bit\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u24c2\ufe0f Mixtral\",\"\u24c2\ufe0f Mixtral\",\"\u24c2\ufe0f Mixtral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",null,\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83d\udd35 deci\",\"\u24c2\ufe0f Mistral\",\"\u24c2\ufe0f Mixtral\",\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u24c2\ufe0f Mistral\",\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"\u24c2\ufe0f Mixtral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\u24c2\ufe0f Mistral\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-1.8199999999999932,1.1099999999999994,0.37000000000000455,0.37000000000000455,0.5900000000000034,1.0999999999999943,-3.2099999999999937,-2.8700000000000045,2.3799999999999955,1.1800000000000068,-1.0,0.0,-2.4200000000000017,0.0,-0.8100000000000023,-1.25,1.9399999999999977,2.969999999999999,4.0,0.35999999999999943,0.8199999999999932,0.0,-13.840000000000003,-2.4200000000000017,5.859999999999999,-2.549999999999997,0.0,-1.9000000000000057,-3.8100000000000023,-2.7900000000000063,0.0,-3.299999999999997,-1.9200000000000017,-0.730000000000004,-4.670000000000002,-1.9200000000000017,-1.3100000000000023,2.9399999999999977,1.9599999999999937,-1.4599999999999937,0.0,-0.9599999999999937,-2.3799999999999955,-1.9000000000000057,-2.4200000000000017,-4.319999999999993,6.290000000000006,0.0,2.760000000000005,2.069999999999993,6.599999999999994,4.140000000000001,5.340000000000003,-3.769999999999996,-1.4500000000000028,2.549999999999997,5.780000000000001,2.8799999999999955,5.280000000000001,1.1400000000000006,1.7099999999999937,0.6800000000000068,0.6400000000000006,-1.4899999999999949,0.0],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.0,58.05,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0368,0.0423,34.4,33.7,34.4,33.7,-13.0,-2.030000000000001],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,56.69,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0312,0.029,38.5,39.8,38.4,39.8,7.590000000000003,3.3799999999999955],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,54.08,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0298,0.0284,38.0,39.5,38.0,39.4,4.930000000000007,3.950000000000003],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.18,50.27,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0293,0.0286,38.1,39.5,38.0,39.5,2.450000000000003,3.6700000000000017],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,48.64,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0321,0.0319,42.9,41.1,42.9,41.0,0.6299999999999955,-4.200000000000003],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,47.64,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0326,0.0329,42.6,40.5,42.6,40.4,-0.9099999999999966,-4.930000000000007],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.87,46.64,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0327,0.0325,41.5,40.7,41.4,40.7,0.6200000000000045,-1.9300000000000068],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.61,45.62,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.032,0.0318,42.4,40.5,42.4,40.4,0.6299999999999955,-4.480000000000004],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0184,0.0187,55.0,51.9,54.9,51.9,-1.5999999999999943,-5.640000000000001],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.26,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0328,0.0321,42.5,40.0,42.5,40.0,2.180000000000007,-5.8799999999999955],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,42.31,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0334,0.0318,42.6,40.9,42.6,40.8,5.030000000000001,-3.989999999999995],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0329,0.0281,40.0,48.8,40.0,48.7,17.08,22.0],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.02,41.44,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0185,0.0187,53.2,51.6,53.2,51.6,-1.0699999999999932,-3.010000000000005],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0329,0.0297,40.5,48.3,40.5,48.2,10.769999999999996,19.260000000000005],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,40.97,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0314,0.0316,44.1,41.0,44.1,41.0,-0.6299999999999955,-7.030000000000001],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,40.28,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.023,0.0221,46.4,43.8,46.5,43.8,4.069999999999993,-5.599999999999994],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,39.81,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0329,0.0321,43.4,40.7,43.3,40.6,2.489999999999995,-6.219999999999999],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.9,39.3,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0315,0.0276,40.4,48.7,40.4,48.6,14.129999999999995,20.540000000000006],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0277,0.0205,39.5,48.9,39.4,48.9,35.120000000000005,23.799999999999997],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,38.26,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.023,0.0225,46.8,42.9,46.8,43.0,2.219999999999999,-8.329999999999998],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0315,0.0275,40.4,48.8,40.4,48.7,14.549999999999997,20.790000000000006],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0367,0.0295,27.7,33.9,27.7,33.9,24.409999999999997,22.379999999999995],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0256,0.0202,41.3,49.6,41.2,49.6,26.730000000000004,20.099999999999994],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.026,0.0204,40.4,48.9,40.4,48.9,27.450000000000003,21.040000000000006],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,36.42,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0181,0.0171,53.9,57.2,53.9,57.1,5.849999999999994,6.1200000000000045],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0241,0.017,43.9,56.7,43.9,56.6,41.75999999999999,29.159999999999997],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0266,0.021,40.1,47.6,40.1,47.6,26.67,18.700000000000003],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.3,35.71,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0185,0.0182,54.8,53.6,54.8,53.6,1.6500000000000057,-2.1899999999999977],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0189,0.0154,53.7,62.0,53.7,62.0,22.730000000000004,15.459999999999994],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0194,0.0154,53.5,63.8,53.4,63.7,25.97,19.25],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0333,0.0295,39.6,46.9,39.6,46.8,12.879999999999995,18.430000000000007],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.03,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0187,0.0172,53.2,57.8,53.2,57.8,8.719999999999999,8.650000000000006],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0288,0.0275,66.4,78.7,66.1,78.3,4.730000000000004,18.519999999999996],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,33.72,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0178,0.0169,53.8,57.7,53.8,57.7,5.329999999999998,7.25],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0172,0.0129,59.7,75.4,59.7,75.5,33.33000000000001,26.299999999999997],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0125,0.0107,78.2,93.1,78.3,93.1,16.819999999999993,19.049999999999997],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,32.44,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0316,0.0288,38.7,40.2,38.7,40.1,9.719999999999999,3.8799999999999955],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.03,0.0233,36.2,43.3,36.2,43.3,28.75999999999999,19.61],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0284,0.023,35.8,43.0,35.8,43.0,23.480000000000004,20.11],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,31.86,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.018,0.0168,52.1,58.0,52.1,57.9,7.140000000000001,11.319999999999993],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0181,0.0157,55.3,63.9,55.3,63.8,15.290000000000006,15.549999999999997],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0172,0.0143,79.2,93.8,79.0,93.8,20.28,18.430000000000007],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.46,30.17,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0188,0.0182,51.0,52.8,51.0,52.8,3.299999999999997,3.530000000000001],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,29.53,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0163,0.0165,82.5,79.4,82.3,79.3,-1.2099999999999937,-3.760000000000005],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00838,0.00641,121.0,147.0,121.0,147.0,30.72999999999999,21.489999999999995],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.44,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00784,0.00789,118.0,120.0,117.0,120.0,-0.6299999999999955,1.6899999999999977],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",0.16,29.41,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0089,0.0078,112.0,116.0,112.0,116.0,14.099999999999994,3.569999999999993],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00925,0.00793,109.0,128.0,109.0,127.0,16.650000000000006,17.430000000000007],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0172,0.0135,61.4,75.4,61.4,75.5,27.409999999999997,22.799999999999997],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.23,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00789,0.00778,116.0,121.0,116.0,121.0,1.4099999999999966,4.310000000000002],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00482,0.00391,207.0,243.0,208.0,244.0,23.269999999999996,17.39],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.14,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00542,0.0047,182.0,193.0,182.0,194.0,15.319999999999993,6.040000000000006],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00918,0.0079,108.0,129.0,108.0,129.0,16.200000000000003,19.439999999999998],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.1,28.97,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00495,0.00491,185.0,192.0,186.0,192.0,0.8100000000000023,3.780000000000001],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0196,0.0155,50.4,63.4,50.4,63.4,26.450000000000003,25.790000000000006],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00494,0.00389,206.0,245.0,206.0,246.0,26.989999999999995,18.930000000000007],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00468,0.0041,209.0,238.0,210.0,239.0,14.150000000000006,13.879999999999995],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.06,28.7,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00452,0.00483,196.0,190.0,197.0,191.0,-6.420000000000002,-3.0600000000000023],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00472,0.00391,207.0,243.0,208.0,244.0,20.72,17.39],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0047,0.00387,207.0,241.0,208.0,242.0,21.450000000000003,16.430000000000007],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",1.12,28.49,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.011,0.00938,86.4,97.3,86.5,97.3,17.269999999999996,12.620000000000005],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",\"float16\",\"pytorch\",2.01,28.45,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.0246,0.0248,79.9,78.2,79.8,78.0,-0.8100000000000023,-2.1299999999999955],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.0,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00458,0.00449,278.0,268.0,278.0,268.0,2.0,-3.5999999999999943],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00476,0.00394,204.0,238.0,205.0,239.0,20.810000000000002,16.67],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00584,0.00444,176.0,212.0,175.0,213.0,31.53,20.450000000000003],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00332,0.00256,309.0,354.0,309.0,354.0,29.689999999999998,14.560000000000002],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00591,0.00446,178.0,220.0,178.0,221.0,32.50999999999999,23.599999999999994],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00472,0.00389,207.0,248.0,208.0,249.0,21.340000000000003,19.810000000000002],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,28.19,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00582,0.00452,178.0,218.0,178.0,219.0,28.75999999999999,22.47],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.08,28.17,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.00459,0.00468,195.0,195.0,195.0,195.0,-1.9200000000000017,0.0],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,27.73,\"float16\",\"pytorch\",\"None\",\"None\",\"FlashAttentionV2\",0.011,0.0111,90.4,83.9,90.5,83.9,-0.9000000000000057,-7.189999999999998]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs) FlashAttentionV2:\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[18]}\",\"legendgroup\":\"None\",\"marker\":{\"color\":\"#6A76FC\"},\"name\":\"None\",\"notched\":false,\"offsetgroup\":\"None\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"\u2b50 StarCoder\",\"\u24c2\ufe0f Mixtral\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\u24c2\ufe0f Mistral\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-2.030000000000001,3.3799999999999955,3.950000000000003,3.6700000000000017,-4.200000000000003,-4.930000000000007,-1.9300000000000068,-4.480000000000004,-5.640000000000001,-5.8799999999999955,-3.989999999999995,22.0,-3.010000000000005,19.260000000000005,-7.030000000000001,-5.599999999999994,-6.219999999999999,20.540000000000006,23.799999999999997,-8.329999999999998,20.790000000000006,22.379999999999995,20.099999999999994,21.040000000000006,6.1200000000000045,29.159999999999997,18.700000000000003,-2.1899999999999977,15.459999999999994,19.25,18.430000000000007,8.650000000000006,18.519999999999996,7.25,26.299999999999997,19.049999999999997,3.8799999999999955,19.61,20.11,11.319999999999993,15.549999999999997,18.430000000000007,3.530000000000001,-3.760000000000005,21.489999999999995,1.6899999999999977,3.569999999999993,17.430000000000007,22.799999999999997,4.310000000000002,17.39,6.040000000000006,19.439999999999998,3.780000000000001,25.790000000000006,18.930000000000007,13.879999999999995,-3.0600000000000023,17.39,16.430000000000007,12.620000000000005,-2.1299999999999955,-3.5999999999999943,16.67,20.450000000000003,14.560000000000002,23.599999999999994,19.810000000000002,22.47,0.0,-7.189999999999998],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"}],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"LLM Architecture\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Decode Speedup (%)\"}},\"legend\":{\"title\":{\"text\":\"Quantization Scheme\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"boxmode\":\"group\",\"title\":{\"text\":\"Decode Throughput Speedup per Architecture, Compared To Non-Optimized Model\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":600}}"
                },
                "show_label": false,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "plot",
                "elem_classes": [],
                "name": "plot",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "12c46a60185f5c11b6b6b6f9adbe826c",
            "api_info": {
                "properties": {
                    "type": {
                        "enum": [
                            "altair",
                            "bokeh",
                            "plotly",
                            "matplotlib"
                        ],
                        "title": "Type",
                        "type": "string"
                    },
                    "plot": {
                        "title": "Plot",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "plot"
                ],
                "title": "PlotData",
                "type": "object"
            },
            "example_inputs": null
        },
        {
            "id": 48,
            "type": "tabitem",
            "props": {
                "label": "Custom Quantization Kernels \ud83d\udcc8",
                "id": 4,
                "name": "tabitem"
            },
            "skip_api": true,
            "component_class_id": "f72faf2b8abb615823b199272218625a"
        },
        {
            "id": 49,
            "type": "html",
            "props": {
                "value": "\ud83d\udc46 Hover over the points \ud83d\udc46 for additional information.",
                "show_label": true,
                "visible": true,
                "elem_id": "text",
                "elem_classes": [],
                "name": "html",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "0ef1a4eade65d710fcd9fc56f6586b0e",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "<p>Hello</p>"
        },
        {
            "id": 50,
            "type": "plot",
            "props": {
                "value": {
                    "type": "plotly",
                    "plot": "{\"data\":[{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.0,58.05,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0368,0.0464,34.4,32.1,-20.689999999999998,-6.689999999999998],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,56.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0312,0.0359,38.5,36.6,-13.090000000000003,-4.939999999999998],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,54.08,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0298,0.0368,38.0,36.8,-19.019999999999996,-3.1599999999999966],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,50.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0322,0.0398,42.0,40.2,-19.099999999999994,-4.290000000000006],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.18,50.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0293,0.036,38.1,36.4,-18.61,-4.459999999999994],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,48.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0321,0.0397,42.9,40.5,-19.14,-5.590000000000003],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,47.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0326,0.0405,42.6,39.2,-19.510000000000005,-7.980000000000004],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.87,46.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0327,0.0399,41.5,39.4,-18.049999999999997,-5.060000000000002],[\"stabilityai\\u002fstablelm-3b-4e1t\",\"\ud83d\udd34 StableLM-Epoch\",\"float16\",\"pytorch\",2.8,46.58,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0228,0.0276,42.4,40.5,-17.39,-4.480000000000004],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,45.65,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0325,0.0402,42.2,38.1,-19.150000000000006,-9.719999999999999],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.61,45.62,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.032,0.0399,42.4,39.2,-19.799999999999997,-7.549999999999997],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0184,0.0215,55.0,50.4,-14.420000000000002,-8.36],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.26,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0328,0.04,42.5,37.4,-18.0,-12.0],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,42.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0334,0.0397,42.6,40.0,-15.870000000000005,-6.099999999999994],[\"Writer\\u002fpalmyra-large\",\"GPT-2\",\"float16\",\"pytorch\",0.0,42.09,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0758,0.0951,27.1,35.2,-20.290000000000006,29.889999999999986],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0329,0.0395,40.0,42.5,-16.709999999999994,6.25],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.02,41.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0185,0.0206,53.2,52.3,-10.189999999999998,-1.6899999999999977],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0329,0.0406,40.5,42.1,-18.97,3.950000000000003],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,40.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0314,0.0407,44.1,39.8,-22.849999999999994,-9.75],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",\"float16\",\"pytorch\",6.0,40.1,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0332,0.0412,31.0,30.8,-19.42,-0.6500000000000057],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,40.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0385,0.0457,26.9,27.2,-15.75,1.1200000000000045],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,39.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0329,0.0405,43.4,39.1,-18.769999999999996,-9.909999999999997],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.9,39.3,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0315,0.0394,40.4,42.4,-20.049999999999997,4.950000000000003],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0277,0.0254,39.5,42.7,9.060000000000002,8.099999999999994],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0315,0.0394,40.4,40.0,-20.049999999999997,-0.9899999999999949],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0367,0.0382,27.7,27.7,-3.930000000000007,0.0],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0256,0.0255,41.3,42.5,0.39000000000000057,2.9099999999999966],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",\"float16\",\"pytorch\",1.0,37.07,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.017,0.0155,65.1,70.1,9.680000000000007,7.680000000000007],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.026,0.0256,40.4,42.2,1.5600000000000023,4.459999999999994],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,36.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0181,0.0206,53.9,50.8,-12.14,-5.75],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",\"float16\",\"pytorch\",6.7,36.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.029,0.037,55.1,55.6,-21.620000000000005,0.9099999999999966],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0241,0.0257,43.9,45.5,-6.230000000000004,3.6400000000000006],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0266,0.0256,40.1,42.4,3.9099999999999966,5.739999999999995],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",3.0,36.07,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0202,0.0203,53.5,55.0,-0.4899999999999949,2.799999999999997],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.3,35.71,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0185,0.0206,54.8,52.4,-10.189999999999998,-4.3799999999999955],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",\"float16\",\"pytorch\",0.0,35.18,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0229,0.0288,71.4,76.8,-20.489999999999995,7.560000000000002],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0189,0.0198,53.7,54.8,-4.549999999999997,2.049999999999997],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0194,0.0197,53.5,55.6,-1.519999999999996,3.930000000000007],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0333,0.0406,39.6,41.1,-17.980000000000004,3.7900000000000063],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.03,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0187,0.0209,53.2,50.4,-10.530000000000001,-5.260000000000005],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0288,0.0355,66.4,79.9,-18.870000000000005,20.33],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",\"float16\",\"pytorch\",1.44,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0147,0.0144,70.4,75.4,2.0799999999999983,7.099999999999994],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",\"float16\",\"pytorch\",5.08,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.033,0.0361,35.3,36.6,-8.590000000000003,3.680000000000007],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,33.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0178,0.021,53.8,48.1,-15.239999999999995,-10.590000000000003],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0172,0.0187,59.7,60.9,-8.019999999999996,2.010000000000005],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",\"float16\",\"pytorch\",2.7,33.25,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0196,0.0224,53.9,57.4,-12.5,6.489999999999995],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0125,0.0132,78.2,81.2,-5.299999999999997,3.8400000000000034],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,32.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0316,0.0374,38.7,37.2,-15.510000000000005,-3.8799999999999955],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,32.43,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.04,0.0455,26.7,26.4,-12.090000000000003,-1.1200000000000045],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.03,0.0306,36.2,37.0,-1.9599999999999937,2.2099999999999937],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0284,0.0297,35.8,37.2,-4.3799999999999955,3.9099999999999966],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,31.86,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.018,0.0218,52.1,48.9,-17.430000000000007,-6.140000000000001],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0181,0.0195,55.3,54.5,-7.180000000000007,-1.4500000000000028],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0172,0.0201,79.2,81.5,-14.430000000000007,2.9000000000000057],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",\"float16\",\"pytorch\",1.3,31.3,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0151,0.0146,70.4,77.0,3.4200000000000017,9.379999999999995],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",\"float16\",\"pytorch\",0.47,30.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0133,0.0142,71.0,75.4,-6.340000000000003,6.200000000000003],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",\"float16\",\"pytorch\",0.38,30.4,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0155,0.0168,62.8,64.9,-7.739999999999995,3.3400000000000034],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.46,30.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0188,0.0225,51.0,46.2,-16.439999999999998,-9.409999999999997],[\"facebook\\u002fxglm-564M\",\"XGLM\",\"float16\",\"pytorch\",0.56,29.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0158,0.0164,74.6,71.4,-3.6599999999999966,-4.290000000000006],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,29.53,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0163,0.0203,82.5,76.3,-19.700000000000003,-7.519999999999996],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00838,0.00953,121.0,114.0,-12.069999999999993,-5.790000000000006],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00784,0.00938,118.0,105.0,-16.42,-11.019999999999996],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",0.16,29.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0089,0.00997,112.0,109.0,-10.730000000000004,-2.680000000000007],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00925,0.00978,109.0,107.0,-5.420000000000002,-1.8299999999999983],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0172,0.0195,61.4,59.3,-11.790000000000006,-3.4200000000000017],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00789,0.00991,116.0,108.0,-20.379999999999995,-6.900000000000006],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00482,0.00512,207.0,204.0,-5.859999999999999,-1.4500000000000028],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0151,0.0155,67.3,70.4,-2.5799999999999983,4.609999999999999],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00918,0.00975,108.0,106.0,-5.849999999999994,-1.8499999999999943],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.1,28.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00495,0.00598,185.0,170.0,-17.22,-8.11],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0196,0.0204,50.4,54.5,-3.9200000000000017,8.129999999999995],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00494,0.005,206.0,199.0,-1.2000000000000028,-3.4000000000000057],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00468,0.00516,209.0,202.0,-9.299999999999997,-3.3499999999999943],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.06,28.7,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00452,0.00533,196.0,185.0,-15.200000000000003,-5.609999999999999],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00472,0.00496,207.0,201.0,-4.840000000000003,-2.9000000000000057],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0047,0.00499,207.0,198.0,-5.810000000000002,-4.349999999999994],[\"gpt2\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.53,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00807,0.00859,120.0,125.0,-6.049999999999997,4.170000000000002],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",1.12,28.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.011,0.0118,86.4,91.1,-6.780000000000001,5.439999999999998],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",\"float16\",\"pytorch\",0.19,28.45,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00693,0.00718,139.0,143.0,-3.480000000000004,2.8799999999999955],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.0,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00458,0.00548,278.0,265.0,-16.42,-4.680000000000007],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00476,0.005,204.0,198.0,-4.799999999999997,-2.9399999999999977],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00584,0.0064,176.0,170.0,-8.75,-3.4099999999999966],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00332,0.0036,309.0,301.0,-7.780000000000001,-2.5900000000000034],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.4,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00816,0.00858,120.0,122.0,-4.900000000000006,1.6700000000000017],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00591,0.00647,178.0,168.0,-8.659999999999997,-5.6200000000000045],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00472,0.00499,207.0,199.0,-5.409999999999997,-3.8599999999999994],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,28.19,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00582,0.00651,178.0,167.0,-10.599999999999994,-6.180000000000007],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.08,28.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00459,0.00537,195.0,185.0,-14.530000000000001,-5.1299999999999955],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,27.95,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00752,0.00803,134.0,129.0,-6.349999999999994,-3.730000000000004],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",\"float16\",\"pytorch\",0.11,27.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00601,0.0064,162.0,167.0,-6.090000000000003,3.0900000000000034],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,27.73,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.011,0.0129,90.4,85.9,-14.730000000000004,-4.980000000000004],[\"ai-forever\\u002fmGPT\",\"GPT-2\",\"float16\",\"pytorch\",0.0,27.61,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0156,0.0162,64.4,66.9,-3.700000000000003,3.8799999999999955]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\",\"legendgroup\":\"GPTQ.4bit+ExllamaV1\",\"marker\":{\"color\":\"#FD3216\"},\"name\":\"GPTQ.4bit+ExllamaV1\",\"notched\":false,\"offsetgroup\":\"GPTQ.4bit+ExllamaV1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83d\udd34 StableLM-Epoch\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-J\",\"\u267e\ufe0f CodeGen\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd85 Falcon\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-2\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"GPT-2\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"\u2b50 StarCoder\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-2\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\ud83c\udf38 Bloom\",\"GPT-2\",\"\u24c2\ufe0f Mistral\",\"GPT-2\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-20.689999999999998,-13.090000000000003,-19.019999999999996,-19.099999999999994,-18.61,-19.14,-19.510000000000005,-18.049999999999997,-17.39,-19.150000000000006,-19.799999999999997,-14.420000000000002,-18.0,-15.870000000000005,-20.290000000000006,-16.709999999999994,-10.189999999999998,-18.97,-22.849999999999994,-19.42,-15.75,-18.769999999999996,-20.049999999999997,9.060000000000002,-20.049999999999997,-3.930000000000007,0.39000000000000057,9.680000000000007,1.5600000000000023,-12.14,-21.620000000000005,-6.230000000000004,3.9099999999999966,-0.4899999999999949,-10.189999999999998,-20.489999999999995,-4.549999999999997,-1.519999999999996,-17.980000000000004,-10.530000000000001,-18.870000000000005,2.0799999999999983,-8.590000000000003,-15.239999999999995,-8.019999999999996,-12.5,-5.299999999999997,-15.510000000000005,-12.090000000000003,-1.9599999999999937,-4.3799999999999955,-17.430000000000007,-7.180000000000007,-14.430000000000007,3.4200000000000017,-6.340000000000003,-7.739999999999995,-16.439999999999998,-3.6599999999999966,-19.700000000000003,-12.069999999999993,-16.42,-10.730000000000004,-5.420000000000002,-11.790000000000006,-20.379999999999995,-5.859999999999999,-2.5799999999999983,-5.849999999999994,-17.22,-3.9200000000000017,-1.2000000000000028,-9.299999999999997,-15.200000000000003,-4.840000000000003,-5.810000000000002,-6.049999999999997,-6.780000000000001,-3.480000000000004,-16.42,-4.799999999999997,-8.75,-7.780000000000001,-4.900000000000006,-8.659999999999997,-5.409999999999997,-10.599999999999994,-14.530000000000001,-6.349999999999994,-6.090000000000003,-14.730000000000004,-3.700000000000003],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.0,58.05,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0368,0.0466,34.4,32.3,-21.03,-6.099999999999994],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,56.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0312,0.0379,38.5,36.2,-17.680000000000007,-5.969999999999999],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,54.08,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0298,0.0379,38.0,37.1,-21.370000000000005,-2.3700000000000045],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,50.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0322,0.041,42.0,39.8,-21.459999999999994,-5.239999999999995],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.18,50.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0293,0.0383,38.1,35.8,-23.5,-6.040000000000006],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,48.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0321,0.0408,42.9,39.7,-21.319999999999993,-7.459999999999994],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,47.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0326,0.0418,42.6,38.9,-22.010000000000005,-8.689999999999998],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.87,46.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0327,0.0417,41.5,39.5,-21.58,-4.819999999999993],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,45.65,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0325,0.0409,42.2,39.5,-20.540000000000006,-6.400000000000006],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.61,45.62,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.032,0.0408,42.4,39.7,-21.569999999999993,-6.3700000000000045],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0184,0.0213,55.0,52.6,-13.620000000000005,-4.359999999999999],[\"KnutJaegersberg\\u002fQwen-1_8B-Llamafied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.84,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0196,0.0205,51.9,52.7,-4.390000000000001,1.5400000000000063],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.26,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0328,0.0409,42.5,39.4,-19.799999999999997,-7.290000000000006],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,42.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0334,0.0411,42.6,39.0,-18.730000000000004,-8.450000000000003],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0329,0.0409,40.0,40.7,-19.560000000000002,1.75],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.02,41.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0185,0.021,53.2,52.3,-11.900000000000006,-1.6899999999999977],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0329,0.0408,40.5,41.9,-19.36,3.4599999999999937],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,40.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0314,0.0408,44.1,39.9,-23.040000000000006,-9.519999999999996],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",\"float16\",\"pytorch\",6.0,40.1,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0332,0.0406,31.0,31.2,-18.230000000000004,0.6500000000000057],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,40.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0385,0.0474,26.9,27.1,-18.78,0.7399999999999949],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,39.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0329,0.0409,43.4,39.6,-19.560000000000002,-8.760000000000005],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.9,39.3,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0315,0.0401,40.4,42.8,-21.450000000000003,5.939999999999998],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0277,0.0253,39.5,41.6,9.489999999999995,5.319999999999993],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0315,0.0401,40.4,40.9,-21.450000000000003,1.2399999999999949],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0367,0.0375,27.7,28.9,-2.1299999999999955,4.329999999999998],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0256,0.0254,41.3,42.6,0.7900000000000063,3.1500000000000057],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.026,0.0251,40.4,42.9,3.5900000000000034,6.189999999999998],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,36.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0181,0.0202,53.9,51.1,-10.400000000000006,-5.189999999999998],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0241,0.0264,43.9,46.4,-8.709999999999994,5.689999999999998],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0266,0.0255,40.1,41.9,4.310000000000002,4.489999999999995],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",3.0,36.07,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0202,0.0201,53.5,55.6,0.5,3.930000000000007],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.3,35.71,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0185,0.0206,54.8,52.8,-10.189999999999998,-3.6500000000000057],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0189,0.019,53.7,55.4,-0.5300000000000011,3.1700000000000017],[\"bn22\\u002ftinyllama_frankenmerge\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.54,34.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0266,0.0296,37.3,34.4,-10.14,-7.769999999999996],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0194,0.0191,53.5,54.8,1.5699999999999932,2.430000000000007],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0333,0.0418,39.6,40.7,-20.33,2.780000000000001],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.03,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0187,0.02,53.2,51.4,-6.5,-3.3799999999999955],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0288,0.0357,66.4,79.9,-19.33,20.33],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",\"float16\",\"pytorch\",5.08,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.033,0.0382,35.3,36.7,-13.61,3.969999999999999],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,33.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0178,0.0204,53.8,50.6,-12.75,-5.950000000000003],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0172,0.0182,59.7,60.9,-5.489999999999995,2.010000000000005],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0125,0.0131,78.2,79.7,-4.579999999999998,1.9200000000000017],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,32.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0316,0.0386,38.7,36.7,-18.129999999999995,-5.170000000000002],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,32.43,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.04,0.0474,26.7,27.1,-15.61,1.5],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.03,0.0301,36.2,37.8,-0.3299999999999983,4.420000000000002],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0284,0.0303,35.8,37.1,-6.269999999999996,3.6299999999999955],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,31.86,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.018,0.02,52.1,50.1,-10.0,-3.8400000000000034],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0181,0.0186,55.3,56.0,-2.6899999999999977,1.269999999999996],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0172,0.0205,79.2,79.7,-16.099999999999994,0.6299999999999955],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.46,30.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0188,0.0216,51.0,47.4,-12.959999999999994,-7.060000000000002],[\"facebook\\u002fxglm-564M\",\"XGLM\",\"float16\",\"pytorch\",0.56,29.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0158,0.0162,74.6,74.1,-2.469999999999999,-0.6700000000000017],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,29.53,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0163,0.0208,82.5,76.6,-21.629999999999995,-7.150000000000006],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00838,0.00928,121.0,115.0,-9.700000000000003,-4.959999999999994],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00784,0.00921,118.0,109.0,-14.879999999999995,-7.6299999999999955],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",0.16,29.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0089,0.00962,112.0,111.0,-7.480000000000004,-0.8900000000000006],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00925,0.00966,109.0,107.0,-4.239999999999995,-1.8299999999999983],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0172,0.0189,61.4,59.9,-8.989999999999995,-2.4399999999999977],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00789,0.00938,116.0,107.0,-15.879999999999995,-7.760000000000005],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00482,0.00499,207.0,198.0,-3.4099999999999966,-4.349999999999994],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0151,0.0156,67.3,71.4,-3.2099999999999937,6.090000000000003],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00918,0.00968,108.0,106.0,-5.170000000000002,-1.8499999999999943],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.1,28.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00495,0.00567,185.0,173.0,-12.700000000000003,-6.489999999999995],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0196,0.0191,50.4,55.0,2.6200000000000045,9.129999999999995],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00494,0.00493,206.0,201.0,0.20000000000000284,-2.430000000000007],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00468,0.00483,209.0,206.0,-3.1099999999999994,-1.4399999999999977],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.06,28.7,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00452,0.00523,196.0,186.0,-13.579999999999998,-5.099999999999994],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00472,0.00492,207.0,199.0,-4.069999999999993,-3.8599999999999994],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0047,0.00482,207.0,204.0,-2.489999999999995,-1.4500000000000028],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",1.12,28.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.011,0.0115,86.4,93.4,-4.349999999999994,8.099999999999994],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.0,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00458,0.00559,278.0,274.0,-18.069999999999993,-1.4399999999999977],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00476,0.00492,204.0,201.0,-3.25,-1.4699999999999989],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00584,0.0065,176.0,170.0,-10.150000000000006,-3.4099999999999966],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00332,0.00359,309.0,302.0,-7.519999999999996,-2.269999999999996],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00591,0.00631,178.0,169.0,-6.340000000000003,-5.060000000000002],[\"instructkr\\u002fko-wand-136M\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.14,28.29,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0077,0.00837,128.0,122.0,-8.0,-4.689999999999998],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00472,0.0049,207.0,202.0,-3.6700000000000017,-2.4200000000000017],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.08,28.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00459,0.00531,195.0,185.0,-13.560000000000002,-5.1299999999999955],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,27.95,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00752,0.00786,134.0,132.0,-4.329999999999998,-1.4899999999999949],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,27.73,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.011,0.0134,90.4,81.5,-17.909999999999997,-9.849999999999994]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\",\"legendgroup\":\"GPTQ.4bit+ExllamaV2\",\"marker\":{\"color\":\"#00FE35\"},\"name\":\"GPTQ.4bit+ExllamaV2\",\"notched\":false,\"offsetgroup\":\"GPTQ.4bit+ExllamaV2\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-J\",\"\u267e\ufe0f CodeGen\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"\u2b50 StarCoder\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-Neo\",\"\u24c2\ufe0f Mistral\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"\ud83c\udf38 Bloom\",\"\u24c2\ufe0f Mistral\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-21.03,-17.680000000000007,-21.370000000000005,-21.459999999999994,-23.5,-21.319999999999993,-22.010000000000005,-21.58,-20.540000000000006,-21.569999999999993,-13.620000000000005,-4.390000000000001,-19.799999999999997,-18.730000000000004,-19.560000000000002,-11.900000000000006,-19.36,-23.040000000000006,-18.230000000000004,-18.78,-19.560000000000002,-21.450000000000003,9.489999999999995,-21.450000000000003,-2.1299999999999955,0.7900000000000063,3.5900000000000034,-10.400000000000006,-8.709999999999994,4.310000000000002,0.5,-10.189999999999998,-0.5300000000000011,-10.14,1.5699999999999932,-20.33,-6.5,-19.33,-13.61,-12.75,-5.489999999999995,-4.579999999999998,-18.129999999999995,-15.61,-0.3299999999999983,-6.269999999999996,-10.0,-2.6899999999999977,-16.099999999999994,-12.959999999999994,-2.469999999999999,-21.629999999999995,-9.700000000000003,-14.879999999999995,-7.480000000000004,-4.239999999999995,-8.989999999999995,-15.879999999999995,-3.4099999999999966,-3.2099999999999937,-5.170000000000002,-12.700000000000003,2.6200000000000045,0.20000000000000284,-3.1099999999999994,-13.579999999999998,-4.069999999999993,-2.489999999999995,-4.349999999999994,-18.069999999999993,-3.25,-10.150000000000006,-7.519999999999996,-6.340000000000003,-8.0,-3.6700000000000017,-13.560000000000002,-4.329999999999998,-17.909999999999997],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,56.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0312,0.0627,38.5,34.3,-50.24,-10.909999999999997],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,54.08,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0298,0.0646,38.0,33.6,-53.87,-11.579999999999998],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,50.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0322,0.0721,42.0,36.5,-55.34,-13.099999999999994],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.18,50.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0293,0.063,38.1,34.0,-53.49,-10.760000000000005],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,48.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0321,0.071,42.9,36.0,-54.79,-16.08],[\"microsoft\\u002fphi-1_5\",\"phi\",\"float16\",\"pytorch\",0.0,47.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0273,0.0208,36.4,49.1,31.25,34.889999999999986],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,47.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0326,0.0714,42.6,36.6,-54.34,-14.079999999999998],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.87,46.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0327,0.0725,41.5,35.6,-54.9,-14.219999999999999],[\"stabilityai\\u002fstablelm-3b-4e1t\",\"\ud83d\udd34 StableLM-Epoch\",\"float16\",\"pytorch\",2.8,46.58,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0228,0.0371,42.4,38.2,-38.54,-9.909999999999997],[\"stabilityai\\u002fstablelm-base-alpha-7b-v2\",\"\ud83d\udd34 StableLM-Alpha\",\"float16\",\"pytorch\",6.89,46.18,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0303,0.0707,43.1,38.9,-57.14,-9.739999999999995],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,45.65,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0325,0.0728,42.2,36.3,-55.36,-13.980000000000004],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.61,45.62,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.032,0.0708,42.4,37.3,-54.8,-12.030000000000001],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0184,0.0351,55.0,49.0,-47.58,-10.909999999999997],[\"KnutJaegersberg\\u002fQwen-1_8B-Llamafied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.84,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0196,0.023,51.9,45.8,-14.780000000000001,-11.75],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.26,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0328,0.0714,42.5,37.1,-54.06,-12.709999999999994],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,42.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0334,0.0724,42.6,37.0,-53.87,-13.150000000000006],[\"Writer\\u002fpalmyra-large\",\"GPT-2\",\"float16\",\"pytorch\",0.0,42.09,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0758,0.0578,27.1,27.2,31.139999999999986,0.37000000000000455],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0329,0.0725,40.0,36.8,-54.62,-8.0],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.02,41.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0185,0.035,53.2,49.4,-47.14,-7.140000000000001],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0329,0.0718,40.5,37.4,-54.18,-7.650000000000006],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,40.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0314,0.071,44.1,36.6,-55.77,-17.010000000000005],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",\"float16\",\"pytorch\",6.0,40.1,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0332,0.0647,31.0,29.0,-48.69,-6.450000000000003],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,40.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0385,0.0771,26.9,26.0,-50.06,-3.3499999999999943],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,39.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0329,0.0712,43.4,37.0,-53.79,-14.75],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.9,39.3,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0315,0.0728,40.4,37.7,-56.73,-6.680000000000007],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0277,0.0401,39.5,37.6,-30.92,-4.810000000000002],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0315,0.071,40.4,37.8,-55.63,-6.439999999999998],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0367,0.0577,27.7,25.5,-36.4,-7.939999999999998],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0256,0.0404,41.3,37.9,-36.63,-8.230000000000004],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",\"float16\",\"pytorch\",1.0,37.07,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.017,0.0193,65.1,60.1,-11.920000000000002,-7.680000000000007],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.026,0.04,40.4,37.4,-35.0,-7.430000000000007],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,36.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0181,0.0195,53.9,48.4,-7.180000000000007,-10.200000000000003],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",\"float16\",\"pytorch\",6.7,36.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.029,0.0263,55.1,53.8,10.269999999999996,-2.3599999999999994],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0241,0.0402,43.9,41.2,-40.05,-6.150000000000006],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0266,0.0393,40.1,37.8,-32.31999999999999,-5.739999999999995],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",3.0,36.07,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0202,0.0357,53.5,49.7,-43.42,-7.099999999999994],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.3,35.71,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0185,0.0211,54.8,48.1,-12.319999999999993,-12.230000000000004],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",\"float16\",\"pytorch\",0.0,35.18,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0229,0.0201,71.4,70.8,13.930000000000007,-0.8400000000000034],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0189,0.0228,53.7,48.8,-17.11,-9.120000000000005],[\"bn22\\u002ftinyllama_frankenmerge\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.54,34.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0266,0.0284,37.3,33.2,-6.340000000000003,-10.989999999999995],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0194,0.0235,53.5,49.5,-17.450000000000003,-7.480000000000004],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0333,0.0758,39.6,37.0,-56.07,-6.569999999999993],[\"gpt2-xl\",\"GPT-2\",\"float16\",\"pytorch\",1.61,34.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0342,0.0319,30.9,31.2,7.209999999999994,0.9699999999999989],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.03,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0187,0.0206,53.2,45.9,-9.219999999999999,-13.719999999999999],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0288,0.0741,66.4,73.3,-61.13,10.39],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",\"float16\",\"pytorch\",1.44,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0147,0.014,70.4,72.6,5.0,3.1200000000000045],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",\"float16\",\"pytorch\",5.08,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.033,0.0594,35.3,30.0,-44.44,-15.010000000000005],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,33.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0178,0.0205,53.8,47.5,-13.170000000000002,-11.709999999999994],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0172,0.0239,59.7,52.8,-28.03,-11.560000000000002],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",\"float16\",\"pytorch\",2.7,33.25,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0196,0.0189,53.9,55.7,3.700000000000003,3.3400000000000034],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0125,0.0154,78.2,72.4,-18.83,-7.420000000000002],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,32.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0316,0.0641,38.7,34.0,-50.7,-12.14],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,32.43,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.04,0.0771,26.7,25.5,-48.12,-4.489999999999995],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.03,0.0482,36.2,33.6,-37.76,-7.180000000000007],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0284,0.0481,35.8,32.7,-40.96,-8.659999999999997],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,31.86,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.018,0.0196,52.1,47.8,-8.159999999999997,-8.25],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0181,0.0203,55.3,49.0,-10.840000000000003,-11.39],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0172,0.0365,79.2,73.5,-52.88,-7.200000000000003],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",\"float16\",\"pytorch\",1.3,31.3,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0151,0.0153,70.4,67.6,-1.3100000000000023,-3.980000000000004],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",\"float16\",\"pytorch\",0.47,30.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0133,0.014,71.0,69.7,-5.0,-1.8299999999999983],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",\"float16\",\"pytorch\",0.38,30.4,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0155,0.0161,62.8,61.9,-3.730000000000004,-1.4300000000000068],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.46,30.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0188,0.0213,51.0,44.0,-11.739999999999995,-13.730000000000004],[\"facebook\\u002fxglm-564M\",\"XGLM\",\"float16\",\"pytorch\",0.56,29.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0158,0.0179,74.6,58.4,-11.730000000000004,-21.72],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,29.53,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0163,0.0362,82.5,70.2,-54.97,-14.909999999999997],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00838,0.00962,121.0,103.0,-12.89,-14.879999999999995],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00784,0.0096,118.0,98.5,-18.33,-16.53],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",0.16,29.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0089,0.0108,112.0,92.1,-17.590000000000003,-17.769999999999996],[\"cerebras\\u002fCerebras-GPT-256M\",\"GPT-2\",\"float16\",\"pytorch\",0.26,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00825,0.0085,120.0,114.0,-2.9399999999999977,-5.0],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00925,0.0103,109.0,95.5,-10.189999999999998,-12.39],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0172,0.0233,61.4,51.9,-26.180000000000007,-15.469999999999999],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00789,0.00908,116.0,102.0,-13.11,-12.069999999999993],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00482,0.00521,207.0,182.0,-7.489999999999995,-12.079999999999998],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0151,0.0163,67.3,60.7,-7.359999999999999,-9.810000000000002],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00918,0.0105,108.0,92.4,-12.569999999999993,-14.439999999999998],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.1,28.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00495,0.00567,185.0,158.0,-12.700000000000003,-14.590000000000003],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0196,0.0201,50.4,49.8,-2.489999999999995,-1.1899999999999977],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00494,0.00513,206.0,185.0,-3.700000000000003,-10.189999999999998],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00468,0.00552,209.0,176.0,-15.219999999999999,-15.790000000000006],[\"Locutusque\\u002fTinyMistral-248M-v2\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,28.78,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0114,0.013,88.2,76.6,-12.310000000000002,-13.150000000000006],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.06,28.7,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00452,0.00528,196.0,175.0,-14.39,-10.709999999999994],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00472,0.00532,207.0,183.0,-11.280000000000001,-11.590000000000003],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0047,0.00545,207.0,182.0,-13.760000000000005,-12.079999999999998],[\"gpt2\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.53,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00807,0.00867,120.0,115.0,-6.920000000000002,-4.170000000000002],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",1.12,28.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.011,0.0151,86.4,79.2,-27.150000000000006,-8.329999999999998],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",\"float16\",\"pytorch\",0.19,28.45,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00693,0.0071,139.0,136.0,-2.3900000000000006,-2.1599999999999966],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.0,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00458,0.00972,278.0,252.0,-52.88,-9.349999999999994],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00476,0.00529,204.0,181.0,-10.019999999999996,-11.269999999999996],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00584,0.00674,176.0,151.0,-13.349999999999994,-14.200000000000003],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00332,0.00371,309.0,272.0,-10.510000000000005,-11.969999999999999],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.4,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00816,0.00847,120.0,120.0,-3.6599999999999966,0.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00591,0.00635,178.0,153.0,-6.930000000000007,-14.040000000000006],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00472,0.00513,207.0,183.0,-7.989999999999995,-11.590000000000003],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,28.19,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00582,0.00643,178.0,149.0,-9.489999999999995,-16.290000000000006],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.08,28.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00459,0.00528,195.0,172.0,-13.069999999999993,-11.790000000000006],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,27.95,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00752,0.00856,134.0,115.0,-12.150000000000006,-14.180000000000007],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",\"float16\",\"pytorch\",0.11,27.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00601,0.00615,162.0,158.0,-2.280000000000001,-2.469999999999999],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,27.73,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.011,0.0129,90.4,75.7,-14.730000000000004,-16.260000000000005],[\"ai-forever\\u002fmGPT\",\"GPT-2\",\"float16\",\"pytorch\",0.0,27.61,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0156,0.0165,64.4,63.0,-5.450000000000003,-2.1700000000000017],[\"team-lucid\\u002fmptk-1b\",\"\ud83e\uddf1 MPT\",\"float16\",\"pytorch\",1.31,20.84,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.012,0.0174,85.3,76.6,-31.03,-10.200000000000003]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\",\"legendgroup\":\"AWQ.4bit+GEMM\",\"marker\":{\"color\":\"#6A76FC\"},\"name\":\"AWQ.4bit+GEMM\",\"notched\":false,\"offsetgroup\":\"AWQ.4bit+GEMM\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"phi\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83d\udd34 StableLM-Epoch\",\"\ud83d\udd34 StableLM-Alpha\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-J\",\"\u267e\ufe0f CodeGen\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd85 Falcon\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-2\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"GPT-2\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"GPT-2\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"\u2b50 StarCoder\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-2\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\ud83c\udf38 Bloom\",\"GPT-2\",\"\u24c2\ufe0f Mistral\",\"GPT-2\",\"\ud83e\uddf1 MPT\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-50.24,-53.87,-55.34,-53.49,-54.79,31.25,-54.34,-54.9,-38.54,-57.14,-55.36,-54.8,-47.58,-14.780000000000001,-54.06,-53.87,31.139999999999986,-54.62,-47.14,-54.18,-55.77,-48.69,-50.06,-53.79,-56.73,-30.92,-55.63,-36.4,-36.63,-11.920000000000002,-35.0,-7.180000000000007,10.269999999999996,-40.05,-32.31999999999999,-43.42,-12.319999999999993,13.930000000000007,-17.11,-6.340000000000003,-17.450000000000003,-56.07,7.209999999999994,-9.219999999999999,-61.13,5.0,-44.44,-13.170000000000002,-28.03,3.700000000000003,-18.83,-50.7,-48.12,-37.76,-40.96,-8.159999999999997,-10.840000000000003,-52.88,-1.3100000000000023,-5.0,-3.730000000000004,-11.739999999999995,-11.730000000000004,-54.97,-12.89,-18.33,-17.590000000000003,-2.9399999999999977,-10.189999999999998,-26.180000000000007,-13.11,-7.489999999999995,-7.359999999999999,-12.569999999999993,-12.700000000000003,-2.489999999999995,-3.700000000000003,-15.219999999999999,-12.310000000000002,-14.39,-11.280000000000001,-13.760000000000005,-6.920000000000002,-27.150000000000006,-2.3900000000000006,-52.88,-10.019999999999996,-13.349999999999994,-10.510000000000005,-3.6599999999999966,-6.930000000000007,-7.989999999999995,-9.489999999999995,-13.069999999999993,-12.150000000000006,-2.280000000000001,-14.730000000000004,-5.450000000000003,-31.03],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.0,58.05,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0368,0.0687,34.4,34.0,-46.43,-1.1599999999999966],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,56.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0312,0.0548,38.5,37.8,-43.07,-1.8199999999999932],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,54.08,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0298,0.0553,38.0,38.8,-46.11,2.1099999999999994],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,50.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0322,0.0632,42.0,40.2,-49.05,-4.290000000000006],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.18,50.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0293,0.0548,38.1,37.4,-46.53,-1.8400000000000034],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,48.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0321,0.0629,42.9,41.3,-48.97,-3.730000000000004],[\"microsoft\\u002fphi-1_5\",\"phi\",\"float16\",\"pytorch\",0.0,47.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0273,0.0203,36.4,52.5,34.47999999999999,44.22999999999999],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,47.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0326,0.0619,42.6,38.8,-47.33,-8.920000000000002],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.87,46.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0327,0.0619,41.5,41.1,-47.17,-0.9599999999999937],[\"stabilityai\\u002fstablelm-base-alpha-7b-v2\",\"\ud83d\udd34 StableLM-Alpha\",\"float16\",\"pytorch\",6.89,46.18,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0303,0.0611,43.1,41.7,-50.41,-3.25],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,45.65,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0325,0.0617,42.2,40.2,-47.33,-4.739999999999995],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.61,45.62,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.032,0.0611,42.4,41.5,-47.63,-2.1200000000000045],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0184,0.0313,55.0,52.8,-41.21,-4.0],[\"KnutJaegersberg\\u002fQwen-1_8B-Llamafied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.84,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0196,0.0217,51.9,50.7,-9.680000000000007,-2.3100000000000023],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.26,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0328,0.0619,42.5,39.9,-47.01,-6.1200000000000045],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,42.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0334,0.0614,42.6,41.5,-45.6,-2.5799999999999983],[\"Writer\\u002fpalmyra-large\",\"GPT-2\",\"float16\",\"pytorch\",0.0,42.09,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0758,0.0582,27.1,27.2,30.24000000000001,0.37000000000000455],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0329,0.0639,40.0,39.2,-48.51,-2.0],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.02,41.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0185,0.0309,53.2,55.9,-40.13,5.079999999999998],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0329,0.0634,40.5,40.3,-48.11,-0.4899999999999949],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,40.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0314,0.0614,44.1,40.7,-48.86,-7.709999999999994],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",\"float16\",\"pytorch\",6.0,40.1,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0332,0.0571,31.0,31.2,-41.86,0.6500000000000057],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,40.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0385,0.0685,26.9,27.2,-43.8,1.1200000000000045],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,39.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0329,0.0612,43.4,40.1,-46.24,-7.599999999999994],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0277,0.0375,39.5,39.7,-26.129999999999995,0.5100000000000051],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0315,0.0632,40.4,39.7,-50.16,-1.730000000000004],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0367,0.0534,27.7,26.7,-31.269999999999996,-3.6099999999999994],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0256,0.0359,41.3,40.3,-28.689999999999998,-2.4200000000000017],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",\"float16\",\"pytorch\",1.0,37.07,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.017,0.0183,65.1,65.7,-7.099999999999994,0.9200000000000017],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.026,0.0359,40.4,39.8,-27.58,-1.4899999999999949],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,36.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0181,0.0202,53.9,50.1,-10.400000000000006,-7.049999999999997],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",\"float16\",\"pytorch\",6.7,36.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.029,0.0264,55.1,54.6,9.849999999999994,-0.9099999999999966],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0241,0.0366,43.9,44.9,-34.150000000000006,2.280000000000001],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0266,0.037,40.1,40.5,-28.11,1.0],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",3.0,36.07,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0202,0.0312,53.5,53.1,-35.260000000000005,-0.75],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.3,35.71,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0185,0.0201,54.8,55.0,-7.959999999999994,0.35999999999999943],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",\"float16\",\"pytorch\",0.0,35.18,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0229,0.0201,71.4,72.4,13.930000000000007,1.4000000000000057],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0189,0.0218,53.7,52.6,-13.299999999999997,-2.049999999999997],[\"bn22\\u002ftinyllama_frankenmerge\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.54,34.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0266,0.0286,37.3,36.2,-6.989999999999995,-2.950000000000003],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0194,0.0222,53.5,53.9,-12.61,0.75],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0333,0.0636,39.6,39.2,-47.64,-1.0100000000000051],[\"gpt2-xl\",\"GPT-2\",\"float16\",\"pytorch\",1.61,34.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0342,0.032,30.9,31.2,6.8799999999999955,0.9699999999999989],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.03,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0187,0.0199,53.2,52.6,-6.030000000000001,-1.1299999999999955],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0288,0.0591,66.4,75.2,-51.27,13.25],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",\"float16\",\"pytorch\",1.44,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0147,0.0143,70.4,71.2,2.799999999999997,1.1400000000000006],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",\"float16\",\"pytorch\",5.08,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.033,0.0567,35.3,34.4,-41.8,-2.549999999999997],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,33.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0178,0.0208,53.8,51.4,-14.420000000000002,-4.459999999999994],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0172,0.0227,59.7,60.6,-24.230000000000004,1.5100000000000051],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",\"float16\",\"pytorch\",2.7,33.25,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0196,0.019,53.9,55.0,3.1599999999999966,2.0400000000000063],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0125,0.0154,78.2,76.1,-18.83,-2.6899999999999977],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,32.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0316,0.0554,38.7,37.2,-42.96,-3.8799999999999955],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,32.43,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.04,0.0682,26.7,26.8,-41.35,0.37000000000000455],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.03,0.0447,36.2,35.7,-32.89,-1.3799999999999955],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0284,0.045,35.8,35.6,-36.89,-0.5600000000000023],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,31.86,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.018,0.0201,52.1,52.4,-10.450000000000003,0.5799999999999983],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0181,0.0202,55.3,52.8,-10.400000000000006,-4.519999999999996],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0172,0.0314,79.2,78.0,-45.22,-1.519999999999996],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",\"float16\",\"pytorch\",1.3,31.3,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0151,0.0142,70.4,72.0,6.340000000000003,2.269999999999996],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",\"float16\",\"pytorch\",0.47,30.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0133,0.0136,71.0,70.8,-2.2099999999999937,-0.28000000000000114],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",\"float16\",\"pytorch\",0.38,30.4,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0155,0.0162,62.8,61.4,-4.319999999999993,-2.230000000000004],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.46,30.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0188,0.0217,51.0,48.9,-13.36,-4.1200000000000045],[\"facebook\\u002fxglm-564M\",\"XGLM\",\"float16\",\"pytorch\",0.56,29.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0158,0.0177,74.6,68.7,-10.730000000000004,-7.909999999999997],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,29.53,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0163,0.0308,82.5,82.0,-47.08,-0.6099999999999994],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00838,0.00975,121.0,116.0,-14.049999999999997,-4.1299999999999955],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00784,0.00919,118.0,109.0,-14.689999999999998,-7.6299999999999955],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",0.16,29.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0089,0.0105,112.0,103.0,-15.239999999999995,-8.040000000000006],[\"cerebras\\u002fCerebras-GPT-256M\",\"GPT-2\",\"float16\",\"pytorch\",0.26,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00825,0.00836,120.0,115.0,-1.3199999999999932,-4.170000000000002],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00925,0.0106,109.0,102.0,-12.739999999999995,-6.420000000000002],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0172,0.0224,61.4,57.6,-23.209999999999994,-6.189999999999998],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00789,0.00926,116.0,112.0,-14.790000000000006,-3.450000000000003],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00482,0.00535,207.0,192.0,-9.909999999999997,-7.25],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0151,0.0164,67.3,66.9,-7.930000000000007,-0.5900000000000034],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00918,0.0103,108.0,103.0,-10.870000000000005,-4.6299999999999955],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.1,28.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00495,0.00559,185.0,177.0,-11.450000000000003,-4.319999999999993],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0196,0.02,50.4,53.8,-2.0,6.75],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00494,0.00515,206.0,196.0,-4.079999999999998,-4.849999999999994],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00468,0.0053,209.0,195.0,-11.700000000000003,-6.700000000000003],[\"Locutusque\\u002fTinyMistral-248M-v2\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,28.78,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0114,0.0128,88.2,85.9,-10.939999999999998,-2.6099999999999994],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.06,28.7,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00452,0.00526,196.0,189.0,-14.069999999999993,-3.569999999999993],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00472,0.00513,207.0,195.0,-7.989999999999995,-5.799999999999997],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0047,0.00516,207.0,193.0,-8.909999999999997,-6.760000000000005],[\"gpt2\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.53,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00807,0.00804,120.0,121.0,0.37000000000000455,0.8299999999999983],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",1.12,28.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.011,0.0144,86.4,87.9,-23.61,1.7399999999999949],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",\"float16\",\"pytorch\",0.19,28.45,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00693,0.00715,139.0,135.0,-3.0799999999999983,-2.8799999999999955],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.0,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00458,0.00827,278.0,275.0,-44.62,-1.0799999999999983],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00476,0.00545,204.0,185.0,-12.659999999999997,-9.310000000000002],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00584,0.00684,176.0,163.0,-14.620000000000005,-7.390000000000001],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00332,0.00375,309.0,293.0,-11.469999999999999,-5.180000000000007],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.4,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00816,0.00817,120.0,120.0,-0.12000000000000455,0.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00591,0.00651,178.0,169.0,-9.219999999999999,-5.060000000000002],[\"instructkr\\u002fko-wand-136M\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.14,28.29,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0077,0.00841,128.0,125.0,-8.439999999999998,-2.3400000000000034],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00472,0.00521,207.0,193.0,-9.400000000000006,-6.760000000000005],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,28.19,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00582,0.00634,178.0,169.0,-8.200000000000003,-5.060000000000002],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.08,28.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00459,0.00534,195.0,193.0,-14.040000000000006,-1.0300000000000011],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,27.95,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00752,0.00843,134.0,126.0,-10.790000000000006,-5.969999999999999],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",\"float16\",\"pytorch\",0.11,27.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00601,0.00625,162.0,158.0,-3.8400000000000034,-2.469999999999999],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,27.73,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.011,0.0129,90.4,84.4,-14.730000000000004,-6.640000000000001],[\"ai-forever\\u002fmGPT\",\"GPT-2\",\"float16\",\"pytorch\",0.0,27.61,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0156,0.0162,64.4,63.3,-3.700000000000003,-1.7099999999999937],[\"team-lucid\\u002fmptk-1b\",\"\ud83e\uddf1 MPT\",\"float16\",\"pytorch\",1.31,20.84,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.012,0.0161,85.3,84.2,-25.47,-1.2900000000000063]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\",\"legendgroup\":\"AWQ.4bit+GEMV\",\"marker\":{\"color\":\"#FED4C4\"},\"name\":\"AWQ.4bit+GEMV\",\"notched\":false,\"offsetgroup\":\"AWQ.4bit+GEMV\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"phi\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83d\udd34 StableLM-Alpha\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-J\",\"\u267e\ufe0f CodeGen\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd85 Falcon\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-2\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"GPT-2\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"GPT-2\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"\u2b50 StarCoder\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-2\",\"GPT-Neo\",\"\u24c2\ufe0f Mistral\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\ud83c\udf38 Bloom\",\"GPT-2\",\"\u24c2\ufe0f Mistral\",\"GPT-2\",\"\ud83e\uddf1 MPT\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-46.43,-43.07,-46.11,-49.05,-46.53,-48.97,34.47999999999999,-47.33,-47.17,-50.41,-47.33,-47.63,-41.21,-9.680000000000007,-47.01,-45.6,30.24000000000001,-48.51,-40.13,-48.11,-48.86,-41.86,-43.8,-46.24,-26.129999999999995,-50.16,-31.269999999999996,-28.689999999999998,-7.099999999999994,-27.58,-10.400000000000006,9.849999999999994,-34.150000000000006,-28.11,-35.260000000000005,-7.959999999999994,13.930000000000007,-13.299999999999997,-6.989999999999995,-12.61,-47.64,6.8799999999999955,-6.030000000000001,-51.27,2.799999999999997,-41.8,-14.420000000000002,-24.230000000000004,3.1599999999999966,-18.83,-42.96,-41.35,-32.89,-36.89,-10.450000000000003,-10.400000000000006,-45.22,6.340000000000003,-2.2099999999999937,-4.319999999999993,-13.36,-10.730000000000004,-47.08,-14.049999999999997,-14.689999999999998,-15.239999999999995,-1.3199999999999932,-12.739999999999995,-23.209999999999994,-14.790000000000006,-9.909999999999997,-7.930000000000007,-10.870000000000005,-11.450000000000003,-2.0,-4.079999999999998,-11.700000000000003,-10.939999999999998,-14.069999999999993,-7.989999999999995,-8.909999999999997,0.37000000000000455,-23.61,-3.0799999999999983,-44.62,-12.659999999999997,-14.620000000000005,-11.469999999999999,-0.12000000000000455,-9.219999999999999,-8.439999999999998,-9.400000000000006,-8.200000000000003,-14.040000000000006,-10.790000000000006,-3.8400000000000034,-14.730000000000004,-3.700000000000003,-25.47],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"}],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"LLM Architecture\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Prefill Speedup (%)\"}},\"legend\":{\"title\":{\"text\":\"Quantization Scheme\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"boxmode\":\"group\",\"title\":{\"text\":\"Prefill Latency Speedup per Architecture\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":600}}"
                },
                "show_label": false,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "plot",
                "elem_classes": [],
                "name": "plot",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "12c46a60185f5c11b6b6b6f9adbe826c",
            "api_info": {
                "properties": {
                    "type": {
                        "enum": [
                            "altair",
                            "bokeh",
                            "plotly",
                            "matplotlib"
                        ],
                        "title": "Type",
                        "type": "string"
                    },
                    "plot": {
                        "title": "Plot",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "plot"
                ],
                "title": "PlotData",
                "type": "object"
            },
            "example_inputs": null
        },
        {
            "id": 51,
            "type": "plot",
            "props": {
                "value": {
                    "type": "plotly",
                    "plot": "{\"data\":[{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.0,58.05,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0368,0.0464,34.4,32.1,-20.689999999999998,-6.689999999999998],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,56.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0312,0.0359,38.5,36.6,-13.090000000000003,-4.939999999999998],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,54.08,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0298,0.0368,38.0,36.8,-19.019999999999996,-3.1599999999999966],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,50.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0322,0.0398,42.0,40.2,-19.099999999999994,-4.290000000000006],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.18,50.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0293,0.036,38.1,36.4,-18.61,-4.459999999999994],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,48.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0321,0.0397,42.9,40.5,-19.14,-5.590000000000003],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,47.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0326,0.0405,42.6,39.2,-19.510000000000005,-7.980000000000004],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.87,46.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0327,0.0399,41.5,39.4,-18.049999999999997,-5.060000000000002],[\"stabilityai\\u002fstablelm-3b-4e1t\",\"\ud83d\udd34 StableLM-Epoch\",\"float16\",\"pytorch\",2.8,46.58,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0228,0.0276,42.4,40.5,-17.39,-4.480000000000004],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,45.65,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0325,0.0402,42.2,38.1,-19.150000000000006,-9.719999999999999],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.61,45.62,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.032,0.0399,42.4,39.2,-19.799999999999997,-7.549999999999997],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0184,0.0215,55.0,50.4,-14.420000000000002,-8.36],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.26,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0328,0.04,42.5,37.4,-18.0,-12.0],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,42.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0334,0.0397,42.6,40.0,-15.870000000000005,-6.099999999999994],[\"Writer\\u002fpalmyra-large\",\"GPT-2\",\"float16\",\"pytorch\",0.0,42.09,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0758,0.0951,27.1,35.2,-20.290000000000006,29.889999999999986],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0329,0.0395,40.0,42.5,-16.709999999999994,6.25],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.02,41.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0185,0.0206,53.2,52.3,-10.189999999999998,-1.6899999999999977],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0329,0.0406,40.5,42.1,-18.97,3.950000000000003],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,40.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0314,0.0407,44.1,39.8,-22.849999999999994,-9.75],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",\"float16\",\"pytorch\",6.0,40.1,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0332,0.0412,31.0,30.8,-19.42,-0.6500000000000057],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,40.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0385,0.0457,26.9,27.2,-15.75,1.1200000000000045],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,39.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0329,0.0405,43.4,39.1,-18.769999999999996,-9.909999999999997],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.9,39.3,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0315,0.0394,40.4,42.4,-20.049999999999997,4.950000000000003],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0277,0.0254,39.5,42.7,9.060000000000002,8.099999999999994],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0315,0.0394,40.4,40.0,-20.049999999999997,-0.9899999999999949],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0367,0.0382,27.7,27.7,-3.930000000000007,0.0],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0256,0.0255,41.3,42.5,0.39000000000000057,2.9099999999999966],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",\"float16\",\"pytorch\",1.0,37.07,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.017,0.0155,65.1,70.1,9.680000000000007,7.680000000000007],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.026,0.0256,40.4,42.2,1.5600000000000023,4.459999999999994],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,36.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0181,0.0206,53.9,50.8,-12.14,-5.75],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",\"float16\",\"pytorch\",6.7,36.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.029,0.037,55.1,55.6,-21.620000000000005,0.9099999999999966],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0241,0.0257,43.9,45.5,-6.230000000000004,3.6400000000000006],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0266,0.0256,40.1,42.4,3.9099999999999966,5.739999999999995],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",3.0,36.07,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0202,0.0203,53.5,55.0,-0.4899999999999949,2.799999999999997],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.3,35.71,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0185,0.0206,54.8,52.4,-10.189999999999998,-4.3799999999999955],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",\"float16\",\"pytorch\",0.0,35.18,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0229,0.0288,71.4,76.8,-20.489999999999995,7.560000000000002],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0189,0.0198,53.7,54.8,-4.549999999999997,2.049999999999997],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0194,0.0197,53.5,55.6,-1.519999999999996,3.930000000000007],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0333,0.0406,39.6,41.1,-17.980000000000004,3.7900000000000063],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.03,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0187,0.0209,53.2,50.4,-10.530000000000001,-5.260000000000005],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0288,0.0355,66.4,79.9,-18.870000000000005,20.33],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",\"float16\",\"pytorch\",1.44,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0147,0.0144,70.4,75.4,2.0799999999999983,7.099999999999994],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",\"float16\",\"pytorch\",5.08,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.033,0.0361,35.3,36.6,-8.590000000000003,3.680000000000007],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,33.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0178,0.021,53.8,48.1,-15.239999999999995,-10.590000000000003],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0172,0.0187,59.7,60.9,-8.019999999999996,2.010000000000005],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",\"float16\",\"pytorch\",2.7,33.25,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0196,0.0224,53.9,57.4,-12.5,6.489999999999995],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0125,0.0132,78.2,81.2,-5.299999999999997,3.8400000000000034],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,32.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0316,0.0374,38.7,37.2,-15.510000000000005,-3.8799999999999955],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,32.43,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.04,0.0455,26.7,26.4,-12.090000000000003,-1.1200000000000045],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.03,0.0306,36.2,37.0,-1.9599999999999937,2.2099999999999937],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0284,0.0297,35.8,37.2,-4.3799999999999955,3.9099999999999966],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,31.86,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.018,0.0218,52.1,48.9,-17.430000000000007,-6.140000000000001],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0181,0.0195,55.3,54.5,-7.180000000000007,-1.4500000000000028],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0172,0.0201,79.2,81.5,-14.430000000000007,2.9000000000000057],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",\"float16\",\"pytorch\",1.3,31.3,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0151,0.0146,70.4,77.0,3.4200000000000017,9.379999999999995],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",\"float16\",\"pytorch\",0.47,30.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0133,0.0142,71.0,75.4,-6.340000000000003,6.200000000000003],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",\"float16\",\"pytorch\",0.38,30.4,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0155,0.0168,62.8,64.9,-7.739999999999995,3.3400000000000034],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.46,30.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0188,0.0225,51.0,46.2,-16.439999999999998,-9.409999999999997],[\"facebook\\u002fxglm-564M\",\"XGLM\",\"float16\",\"pytorch\",0.56,29.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0158,0.0164,74.6,71.4,-3.6599999999999966,-4.290000000000006],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,29.53,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0163,0.0203,82.5,76.3,-19.700000000000003,-7.519999999999996],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00838,0.00953,121.0,114.0,-12.069999999999993,-5.790000000000006],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00784,0.00938,118.0,105.0,-16.42,-11.019999999999996],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",0.16,29.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0089,0.00997,112.0,109.0,-10.730000000000004,-2.680000000000007],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00925,0.00978,109.0,107.0,-5.420000000000002,-1.8299999999999983],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0172,0.0195,61.4,59.3,-11.790000000000006,-3.4200000000000017],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00789,0.00991,116.0,108.0,-20.379999999999995,-6.900000000000006],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00482,0.00512,207.0,204.0,-5.859999999999999,-1.4500000000000028],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0151,0.0155,67.3,70.4,-2.5799999999999983,4.609999999999999],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00918,0.00975,108.0,106.0,-5.849999999999994,-1.8499999999999943],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.1,28.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00495,0.00598,185.0,170.0,-17.22,-8.11],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0196,0.0204,50.4,54.5,-3.9200000000000017,8.129999999999995],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00494,0.005,206.0,199.0,-1.2000000000000028,-3.4000000000000057],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00468,0.00516,209.0,202.0,-9.299999999999997,-3.3499999999999943],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.06,28.7,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00452,0.00533,196.0,185.0,-15.200000000000003,-5.609999999999999],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00472,0.00496,207.0,201.0,-4.840000000000003,-2.9000000000000057],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0047,0.00499,207.0,198.0,-5.810000000000002,-4.349999999999994],[\"gpt2\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.53,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00807,0.00859,120.0,125.0,-6.049999999999997,4.170000000000002],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",1.12,28.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.011,0.0118,86.4,91.1,-6.780000000000001,5.439999999999998],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",\"float16\",\"pytorch\",0.19,28.45,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00693,0.00718,139.0,143.0,-3.480000000000004,2.8799999999999955],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.0,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00458,0.00548,278.0,265.0,-16.42,-4.680000000000007],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00476,0.005,204.0,198.0,-4.799999999999997,-2.9399999999999977],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00584,0.0064,176.0,170.0,-8.75,-3.4099999999999966],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00332,0.0036,309.0,301.0,-7.780000000000001,-2.5900000000000034],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.4,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00816,0.00858,120.0,122.0,-4.900000000000006,1.6700000000000017],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00591,0.00647,178.0,168.0,-8.659999999999997,-5.6200000000000045],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00472,0.00499,207.0,199.0,-5.409999999999997,-3.8599999999999994],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,28.19,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00582,0.00651,178.0,167.0,-10.599999999999994,-6.180000000000007],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.08,28.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00459,0.00537,195.0,185.0,-14.530000000000001,-5.1299999999999955],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,27.95,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00752,0.00803,134.0,129.0,-6.349999999999994,-3.730000000000004],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",\"float16\",\"pytorch\",0.11,27.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00601,0.0064,162.0,167.0,-6.090000000000003,3.0900000000000034],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,27.73,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.011,0.0129,90.4,85.9,-14.730000000000004,-4.980000000000004],[\"ai-forever\\u002fmGPT\",\"GPT-2\",\"float16\",\"pytorch\",0.0,27.61,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0156,0.0162,64.4,66.9,-3.700000000000003,3.8799999999999955]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\",\"legendgroup\":\"GPTQ.4bit+ExllamaV1\",\"marker\":{\"color\":\"#FD3216\"},\"name\":\"GPTQ.4bit+ExllamaV1\",\"notched\":false,\"offsetgroup\":\"GPTQ.4bit+ExllamaV1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83d\udd34 StableLM-Epoch\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-J\",\"\u267e\ufe0f CodeGen\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd85 Falcon\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-2\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"GPT-2\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"\u2b50 StarCoder\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-2\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\ud83c\udf38 Bloom\",\"GPT-2\",\"\u24c2\ufe0f Mistral\",\"GPT-2\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-6.689999999999998,-4.939999999999998,-3.1599999999999966,-4.290000000000006,-4.459999999999994,-5.590000000000003,-7.980000000000004,-5.060000000000002,-4.480000000000004,-9.719999999999999,-7.549999999999997,-8.36,-12.0,-6.099999999999994,29.889999999999986,6.25,-1.6899999999999977,3.950000000000003,-9.75,-0.6500000000000057,1.1200000000000045,-9.909999999999997,4.950000000000003,8.099999999999994,-0.9899999999999949,0.0,2.9099999999999966,7.680000000000007,4.459999999999994,-5.75,0.9099999999999966,3.6400000000000006,5.739999999999995,2.799999999999997,-4.3799999999999955,7.560000000000002,2.049999999999997,3.930000000000007,3.7900000000000063,-5.260000000000005,20.33,7.099999999999994,3.680000000000007,-10.590000000000003,2.010000000000005,6.489999999999995,3.8400000000000034,-3.8799999999999955,-1.1200000000000045,2.2099999999999937,3.9099999999999966,-6.140000000000001,-1.4500000000000028,2.9000000000000057,9.379999999999995,6.200000000000003,3.3400000000000034,-9.409999999999997,-4.290000000000006,-7.519999999999996,-5.790000000000006,-11.019999999999996,-2.680000000000007,-1.8299999999999983,-3.4200000000000017,-6.900000000000006,-1.4500000000000028,4.609999999999999,-1.8499999999999943,-8.11,8.129999999999995,-3.4000000000000057,-3.3499999999999943,-5.609999999999999,-2.9000000000000057,-4.349999999999994,4.170000000000002,5.439999999999998,2.8799999999999955,-4.680000000000007,-2.9399999999999977,-3.4099999999999966,-2.5900000000000034,1.6700000000000017,-5.6200000000000045,-3.8599999999999994,-6.180000000000007,-5.1299999999999955,-3.730000000000004,3.0900000000000034,-4.980000000000004,3.8799999999999955],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.0,58.05,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0368,0.0466,34.4,32.3,-21.03,-6.099999999999994],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,56.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0312,0.0379,38.5,36.2,-17.680000000000007,-5.969999999999999],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,54.08,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0298,0.0379,38.0,37.1,-21.370000000000005,-2.3700000000000045],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,50.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0322,0.041,42.0,39.8,-21.459999999999994,-5.239999999999995],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.18,50.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0293,0.0383,38.1,35.8,-23.5,-6.040000000000006],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,48.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0321,0.0408,42.9,39.7,-21.319999999999993,-7.459999999999994],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,47.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0326,0.0418,42.6,38.9,-22.010000000000005,-8.689999999999998],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.87,46.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0327,0.0417,41.5,39.5,-21.58,-4.819999999999993],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,45.65,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0325,0.0409,42.2,39.5,-20.540000000000006,-6.400000000000006],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.61,45.62,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.032,0.0408,42.4,39.7,-21.569999999999993,-6.3700000000000045],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0184,0.0213,55.0,52.6,-13.620000000000005,-4.359999999999999],[\"KnutJaegersberg\\u002fQwen-1_8B-Llamafied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.84,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0196,0.0205,51.9,52.7,-4.390000000000001,1.5400000000000063],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.26,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0328,0.0409,42.5,39.4,-19.799999999999997,-7.290000000000006],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,42.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0334,0.0411,42.6,39.0,-18.730000000000004,-8.450000000000003],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0329,0.0409,40.0,40.7,-19.560000000000002,1.75],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.02,41.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0185,0.021,53.2,52.3,-11.900000000000006,-1.6899999999999977],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0329,0.0408,40.5,41.9,-19.36,3.4599999999999937],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,40.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0314,0.0408,44.1,39.9,-23.040000000000006,-9.519999999999996],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",\"float16\",\"pytorch\",6.0,40.1,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0332,0.0406,31.0,31.2,-18.230000000000004,0.6500000000000057],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,40.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0385,0.0474,26.9,27.1,-18.78,0.7399999999999949],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,39.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0329,0.0409,43.4,39.6,-19.560000000000002,-8.760000000000005],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.9,39.3,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0315,0.0401,40.4,42.8,-21.450000000000003,5.939999999999998],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0277,0.0253,39.5,41.6,9.489999999999995,5.319999999999993],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0315,0.0401,40.4,40.9,-21.450000000000003,1.2399999999999949],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0367,0.0375,27.7,28.9,-2.1299999999999955,4.329999999999998],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0256,0.0254,41.3,42.6,0.7900000000000063,3.1500000000000057],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.026,0.0251,40.4,42.9,3.5900000000000034,6.189999999999998],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,36.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0181,0.0202,53.9,51.1,-10.400000000000006,-5.189999999999998],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0241,0.0264,43.9,46.4,-8.709999999999994,5.689999999999998],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0266,0.0255,40.1,41.9,4.310000000000002,4.489999999999995],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",3.0,36.07,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0202,0.0201,53.5,55.6,0.5,3.930000000000007],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.3,35.71,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0185,0.0206,54.8,52.8,-10.189999999999998,-3.6500000000000057],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0189,0.019,53.7,55.4,-0.5300000000000011,3.1700000000000017],[\"bn22\\u002ftinyllama_frankenmerge\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.54,34.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0266,0.0296,37.3,34.4,-10.14,-7.769999999999996],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0194,0.0191,53.5,54.8,1.5699999999999932,2.430000000000007],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0333,0.0418,39.6,40.7,-20.33,2.780000000000001],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.03,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0187,0.02,53.2,51.4,-6.5,-3.3799999999999955],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0288,0.0357,66.4,79.9,-19.33,20.33],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",\"float16\",\"pytorch\",5.08,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.033,0.0382,35.3,36.7,-13.61,3.969999999999999],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,33.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0178,0.0204,53.8,50.6,-12.75,-5.950000000000003],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0172,0.0182,59.7,60.9,-5.489999999999995,2.010000000000005],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0125,0.0131,78.2,79.7,-4.579999999999998,1.9200000000000017],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,32.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0316,0.0386,38.7,36.7,-18.129999999999995,-5.170000000000002],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,32.43,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.04,0.0474,26.7,27.1,-15.61,1.5],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.03,0.0301,36.2,37.8,-0.3299999999999983,4.420000000000002],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0284,0.0303,35.8,37.1,-6.269999999999996,3.6299999999999955],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,31.86,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.018,0.02,52.1,50.1,-10.0,-3.8400000000000034],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0181,0.0186,55.3,56.0,-2.6899999999999977,1.269999999999996],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0172,0.0205,79.2,79.7,-16.099999999999994,0.6299999999999955],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.46,30.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0188,0.0216,51.0,47.4,-12.959999999999994,-7.060000000000002],[\"facebook\\u002fxglm-564M\",\"XGLM\",\"float16\",\"pytorch\",0.56,29.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0158,0.0162,74.6,74.1,-2.469999999999999,-0.6700000000000017],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,29.53,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0163,0.0208,82.5,76.6,-21.629999999999995,-7.150000000000006],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00838,0.00928,121.0,115.0,-9.700000000000003,-4.959999999999994],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00784,0.00921,118.0,109.0,-14.879999999999995,-7.6299999999999955],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",0.16,29.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0089,0.00962,112.0,111.0,-7.480000000000004,-0.8900000000000006],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00925,0.00966,109.0,107.0,-4.239999999999995,-1.8299999999999983],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0172,0.0189,61.4,59.9,-8.989999999999995,-2.4399999999999977],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00789,0.00938,116.0,107.0,-15.879999999999995,-7.760000000000005],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00482,0.00499,207.0,198.0,-3.4099999999999966,-4.349999999999994],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0151,0.0156,67.3,71.4,-3.2099999999999937,6.090000000000003],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00918,0.00968,108.0,106.0,-5.170000000000002,-1.8499999999999943],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.1,28.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00495,0.00567,185.0,173.0,-12.700000000000003,-6.489999999999995],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0196,0.0191,50.4,55.0,2.6200000000000045,9.129999999999995],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00494,0.00493,206.0,201.0,0.20000000000000284,-2.430000000000007],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00468,0.00483,209.0,206.0,-3.1099999999999994,-1.4399999999999977],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.06,28.7,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00452,0.00523,196.0,186.0,-13.579999999999998,-5.099999999999994],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00472,0.00492,207.0,199.0,-4.069999999999993,-3.8599999999999994],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0047,0.00482,207.0,204.0,-2.489999999999995,-1.4500000000000028],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",1.12,28.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.011,0.0115,86.4,93.4,-4.349999999999994,8.099999999999994],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.0,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00458,0.00559,278.0,274.0,-18.069999999999993,-1.4399999999999977],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00476,0.00492,204.0,201.0,-3.25,-1.4699999999999989],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00584,0.0065,176.0,170.0,-10.150000000000006,-3.4099999999999966],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00332,0.00359,309.0,302.0,-7.519999999999996,-2.269999999999996],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00591,0.00631,178.0,169.0,-6.340000000000003,-5.060000000000002],[\"instructkr\\u002fko-wand-136M\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.14,28.29,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.0077,0.00837,128.0,122.0,-8.0,-4.689999999999998],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00472,0.0049,207.0,202.0,-3.6700000000000017,-2.4200000000000017],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.08,28.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00459,0.00531,195.0,185.0,-13.560000000000002,-5.1299999999999955],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,27.95,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.00752,0.00786,134.0,132.0,-4.329999999999998,-1.4899999999999949],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,27.73,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV2\",0.011,0.0134,90.4,81.5,-17.909999999999997,-9.849999999999994]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\",\"legendgroup\":\"GPTQ.4bit+ExllamaV2\",\"marker\":{\"color\":\"#00FE35\"},\"name\":\"GPTQ.4bit+ExllamaV2\",\"notched\":false,\"offsetgroup\":\"GPTQ.4bit+ExllamaV2\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-J\",\"\u267e\ufe0f CodeGen\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"\u2b50 StarCoder\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-Neo\",\"\u24c2\ufe0f Mistral\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"\ud83c\udf38 Bloom\",\"\u24c2\ufe0f Mistral\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-6.099999999999994,-5.969999999999999,-2.3700000000000045,-5.239999999999995,-6.040000000000006,-7.459999999999994,-8.689999999999998,-4.819999999999993,-6.400000000000006,-6.3700000000000045,-4.359999999999999,1.5400000000000063,-7.290000000000006,-8.450000000000003,1.75,-1.6899999999999977,3.4599999999999937,-9.519999999999996,0.6500000000000057,0.7399999999999949,-8.760000000000005,5.939999999999998,5.319999999999993,1.2399999999999949,4.329999999999998,3.1500000000000057,6.189999999999998,-5.189999999999998,5.689999999999998,4.489999999999995,3.930000000000007,-3.6500000000000057,3.1700000000000017,-7.769999999999996,2.430000000000007,2.780000000000001,-3.3799999999999955,20.33,3.969999999999999,-5.950000000000003,2.010000000000005,1.9200000000000017,-5.170000000000002,1.5,4.420000000000002,3.6299999999999955,-3.8400000000000034,1.269999999999996,0.6299999999999955,-7.060000000000002,-0.6700000000000017,-7.150000000000006,-4.959999999999994,-7.6299999999999955,-0.8900000000000006,-1.8299999999999983,-2.4399999999999977,-7.760000000000005,-4.349999999999994,6.090000000000003,-1.8499999999999943,-6.489999999999995,9.129999999999995,-2.430000000000007,-1.4399999999999977,-5.099999999999994,-3.8599999999999994,-1.4500000000000028,8.099999999999994,-1.4399999999999977,-1.4699999999999989,-3.4099999999999966,-2.269999999999996,-5.060000000000002,-4.689999999999998,-2.4200000000000017,-5.1299999999999955,-1.4899999999999949,-9.849999999999994],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,56.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0312,0.0627,38.5,34.3,-50.24,-10.909999999999997],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,54.08,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0298,0.0646,38.0,33.6,-53.87,-11.579999999999998],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,50.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0322,0.0721,42.0,36.5,-55.34,-13.099999999999994],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.18,50.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0293,0.063,38.1,34.0,-53.49,-10.760000000000005],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,48.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0321,0.071,42.9,36.0,-54.79,-16.08],[\"microsoft\\u002fphi-1_5\",\"phi\",\"float16\",\"pytorch\",0.0,47.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0273,0.0208,36.4,49.1,31.25,34.889999999999986],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,47.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0326,0.0714,42.6,36.6,-54.34,-14.079999999999998],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.87,46.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0327,0.0725,41.5,35.6,-54.9,-14.219999999999999],[\"stabilityai\\u002fstablelm-3b-4e1t\",\"\ud83d\udd34 StableLM-Epoch\",\"float16\",\"pytorch\",2.8,46.58,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0228,0.0371,42.4,38.2,-38.54,-9.909999999999997],[\"stabilityai\\u002fstablelm-base-alpha-7b-v2\",\"\ud83d\udd34 StableLM-Alpha\",\"float16\",\"pytorch\",6.89,46.18,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0303,0.0707,43.1,38.9,-57.14,-9.739999999999995],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,45.65,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0325,0.0728,42.2,36.3,-55.36,-13.980000000000004],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.61,45.62,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.032,0.0708,42.4,37.3,-54.8,-12.030000000000001],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0184,0.0351,55.0,49.0,-47.58,-10.909999999999997],[\"KnutJaegersberg\\u002fQwen-1_8B-Llamafied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.84,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0196,0.023,51.9,45.8,-14.780000000000001,-11.75],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.26,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0328,0.0714,42.5,37.1,-54.06,-12.709999999999994],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,42.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0334,0.0724,42.6,37.0,-53.87,-13.150000000000006],[\"Writer\\u002fpalmyra-large\",\"GPT-2\",\"float16\",\"pytorch\",0.0,42.09,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0758,0.0578,27.1,27.2,31.139999999999986,0.37000000000000455],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0329,0.0725,40.0,36.8,-54.62,-8.0],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.02,41.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0185,0.035,53.2,49.4,-47.14,-7.140000000000001],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0329,0.0718,40.5,37.4,-54.18,-7.650000000000006],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,40.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0314,0.071,44.1,36.6,-55.77,-17.010000000000005],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",\"float16\",\"pytorch\",6.0,40.1,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0332,0.0647,31.0,29.0,-48.69,-6.450000000000003],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,40.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0385,0.0771,26.9,26.0,-50.06,-3.3499999999999943],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,39.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0329,0.0712,43.4,37.0,-53.79,-14.75],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.9,39.3,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0315,0.0728,40.4,37.7,-56.73,-6.680000000000007],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0277,0.0401,39.5,37.6,-30.92,-4.810000000000002],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0315,0.071,40.4,37.8,-55.63,-6.439999999999998],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0367,0.0577,27.7,25.5,-36.4,-7.939999999999998],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0256,0.0404,41.3,37.9,-36.63,-8.230000000000004],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",\"float16\",\"pytorch\",1.0,37.07,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.017,0.0193,65.1,60.1,-11.920000000000002,-7.680000000000007],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.026,0.04,40.4,37.4,-35.0,-7.430000000000007],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,36.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0181,0.0195,53.9,48.4,-7.180000000000007,-10.200000000000003],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",\"float16\",\"pytorch\",6.7,36.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.029,0.0263,55.1,53.8,10.269999999999996,-2.3599999999999994],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0241,0.0402,43.9,41.2,-40.05,-6.150000000000006],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0266,0.0393,40.1,37.8,-32.31999999999999,-5.739999999999995],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",3.0,36.07,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0202,0.0357,53.5,49.7,-43.42,-7.099999999999994],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.3,35.71,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0185,0.0211,54.8,48.1,-12.319999999999993,-12.230000000000004],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",\"float16\",\"pytorch\",0.0,35.18,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0229,0.0201,71.4,70.8,13.930000000000007,-0.8400000000000034],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0189,0.0228,53.7,48.8,-17.11,-9.120000000000005],[\"bn22\\u002ftinyllama_frankenmerge\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.54,34.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0266,0.0284,37.3,33.2,-6.340000000000003,-10.989999999999995],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0194,0.0235,53.5,49.5,-17.450000000000003,-7.480000000000004],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0333,0.0758,39.6,37.0,-56.07,-6.569999999999993],[\"gpt2-xl\",\"GPT-2\",\"float16\",\"pytorch\",1.61,34.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0342,0.0319,30.9,31.2,7.209999999999994,0.9699999999999989],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.03,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0187,0.0206,53.2,45.9,-9.219999999999999,-13.719999999999999],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0288,0.0741,66.4,73.3,-61.13,10.39],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",\"float16\",\"pytorch\",1.44,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0147,0.014,70.4,72.6,5.0,3.1200000000000045],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",\"float16\",\"pytorch\",5.08,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.033,0.0594,35.3,30.0,-44.44,-15.010000000000005],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,33.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0178,0.0205,53.8,47.5,-13.170000000000002,-11.709999999999994],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0172,0.0239,59.7,52.8,-28.03,-11.560000000000002],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",\"float16\",\"pytorch\",2.7,33.25,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0196,0.0189,53.9,55.7,3.700000000000003,3.3400000000000034],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0125,0.0154,78.2,72.4,-18.83,-7.420000000000002],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,32.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0316,0.0641,38.7,34.0,-50.7,-12.14],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,32.43,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.04,0.0771,26.7,25.5,-48.12,-4.489999999999995],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.03,0.0482,36.2,33.6,-37.76,-7.180000000000007],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0284,0.0481,35.8,32.7,-40.96,-8.659999999999997],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,31.86,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.018,0.0196,52.1,47.8,-8.159999999999997,-8.25],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0181,0.0203,55.3,49.0,-10.840000000000003,-11.39],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0172,0.0365,79.2,73.5,-52.88,-7.200000000000003],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",\"float16\",\"pytorch\",1.3,31.3,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0151,0.0153,70.4,67.6,-1.3100000000000023,-3.980000000000004],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",\"float16\",\"pytorch\",0.47,30.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0133,0.014,71.0,69.7,-5.0,-1.8299999999999983],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",\"float16\",\"pytorch\",0.38,30.4,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0155,0.0161,62.8,61.9,-3.730000000000004,-1.4300000000000068],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.46,30.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0188,0.0213,51.0,44.0,-11.739999999999995,-13.730000000000004],[\"facebook\\u002fxglm-564M\",\"XGLM\",\"float16\",\"pytorch\",0.56,29.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0158,0.0179,74.6,58.4,-11.730000000000004,-21.72],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,29.53,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0163,0.0362,82.5,70.2,-54.97,-14.909999999999997],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00838,0.00962,121.0,103.0,-12.89,-14.879999999999995],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00784,0.0096,118.0,98.5,-18.33,-16.53],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",0.16,29.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0089,0.0108,112.0,92.1,-17.590000000000003,-17.769999999999996],[\"cerebras\\u002fCerebras-GPT-256M\",\"GPT-2\",\"float16\",\"pytorch\",0.26,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00825,0.0085,120.0,114.0,-2.9399999999999977,-5.0],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00925,0.0103,109.0,95.5,-10.189999999999998,-12.39],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0172,0.0233,61.4,51.9,-26.180000000000007,-15.469999999999999],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00789,0.00908,116.0,102.0,-13.11,-12.069999999999993],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00482,0.00521,207.0,182.0,-7.489999999999995,-12.079999999999998],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0151,0.0163,67.3,60.7,-7.359999999999999,-9.810000000000002],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00918,0.0105,108.0,92.4,-12.569999999999993,-14.439999999999998],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.1,28.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00495,0.00567,185.0,158.0,-12.700000000000003,-14.590000000000003],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0196,0.0201,50.4,49.8,-2.489999999999995,-1.1899999999999977],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00494,0.00513,206.0,185.0,-3.700000000000003,-10.189999999999998],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00468,0.00552,209.0,176.0,-15.219999999999999,-15.790000000000006],[\"Locutusque\\u002fTinyMistral-248M-v2\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,28.78,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0114,0.013,88.2,76.6,-12.310000000000002,-13.150000000000006],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.06,28.7,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00452,0.00528,196.0,175.0,-14.39,-10.709999999999994],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00472,0.00532,207.0,183.0,-11.280000000000001,-11.590000000000003],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0047,0.00545,207.0,182.0,-13.760000000000005,-12.079999999999998],[\"gpt2\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.53,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00807,0.00867,120.0,115.0,-6.920000000000002,-4.170000000000002],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",1.12,28.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.011,0.0151,86.4,79.2,-27.150000000000006,-8.329999999999998],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",\"float16\",\"pytorch\",0.19,28.45,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00693,0.0071,139.0,136.0,-2.3900000000000006,-2.1599999999999966],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.0,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00458,0.00972,278.0,252.0,-52.88,-9.349999999999994],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00476,0.00529,204.0,181.0,-10.019999999999996,-11.269999999999996],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00584,0.00674,176.0,151.0,-13.349999999999994,-14.200000000000003],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00332,0.00371,309.0,272.0,-10.510000000000005,-11.969999999999999],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.4,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00816,0.00847,120.0,120.0,-3.6599999999999966,0.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00591,0.00635,178.0,153.0,-6.930000000000007,-14.040000000000006],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00472,0.00513,207.0,183.0,-7.989999999999995,-11.590000000000003],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,28.19,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00582,0.00643,178.0,149.0,-9.489999999999995,-16.290000000000006],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.08,28.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00459,0.00528,195.0,172.0,-13.069999999999993,-11.790000000000006],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,27.95,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00752,0.00856,134.0,115.0,-12.150000000000006,-14.180000000000007],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",\"float16\",\"pytorch\",0.11,27.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00601,0.00615,162.0,158.0,-2.280000000000001,-2.469999999999999],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,27.73,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.011,0.0129,90.4,75.7,-14.730000000000004,-16.260000000000005],[\"ai-forever\\u002fmGPT\",\"GPT-2\",\"float16\",\"pytorch\",0.0,27.61,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0156,0.0165,64.4,63.0,-5.450000000000003,-2.1700000000000017],[\"team-lucid\\u002fmptk-1b\",\"\ud83e\uddf1 MPT\",\"float16\",\"pytorch\",1.31,20.84,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.012,0.0174,85.3,76.6,-31.03,-10.200000000000003]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\",\"legendgroup\":\"AWQ.4bit+GEMM\",\"marker\":{\"color\":\"#6A76FC\"},\"name\":\"AWQ.4bit+GEMM\",\"notched\":false,\"offsetgroup\":\"AWQ.4bit+GEMM\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"phi\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83d\udd34 StableLM-Epoch\",\"\ud83d\udd34 StableLM-Alpha\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-J\",\"\u267e\ufe0f CodeGen\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd85 Falcon\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-2\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"GPT-2\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"GPT-2\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"\u2b50 StarCoder\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-2\",\"GPT-Neo\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\ud83c\udf38 Bloom\",\"GPT-2\",\"\u24c2\ufe0f Mistral\",\"GPT-2\",\"\ud83e\uddf1 MPT\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-10.909999999999997,-11.579999999999998,-13.099999999999994,-10.760000000000005,-16.08,34.889999999999986,-14.079999999999998,-14.219999999999999,-9.909999999999997,-9.739999999999995,-13.980000000000004,-12.030000000000001,-10.909999999999997,-11.75,-12.709999999999994,-13.150000000000006,0.37000000000000455,-8.0,-7.140000000000001,-7.650000000000006,-17.010000000000005,-6.450000000000003,-3.3499999999999943,-14.75,-6.680000000000007,-4.810000000000002,-6.439999999999998,-7.939999999999998,-8.230000000000004,-7.680000000000007,-7.430000000000007,-10.200000000000003,-2.3599999999999994,-6.150000000000006,-5.739999999999995,-7.099999999999994,-12.230000000000004,-0.8400000000000034,-9.120000000000005,-10.989999999999995,-7.480000000000004,-6.569999999999993,0.9699999999999989,-13.719999999999999,10.39,3.1200000000000045,-15.010000000000005,-11.709999999999994,-11.560000000000002,3.3400000000000034,-7.420000000000002,-12.14,-4.489999999999995,-7.180000000000007,-8.659999999999997,-8.25,-11.39,-7.200000000000003,-3.980000000000004,-1.8299999999999983,-1.4300000000000068,-13.730000000000004,-21.72,-14.909999999999997,-14.879999999999995,-16.53,-17.769999999999996,-5.0,-12.39,-15.469999999999999,-12.069999999999993,-12.079999999999998,-9.810000000000002,-14.439999999999998,-14.590000000000003,-1.1899999999999977,-10.189999999999998,-15.790000000000006,-13.150000000000006,-10.709999999999994,-11.590000000000003,-12.079999999999998,-4.170000000000002,-8.329999999999998,-2.1599999999999966,-9.349999999999994,-11.269999999999996,-14.200000000000003,-11.969999999999999,0.0,-14.040000000000006,-11.590000000000003,-16.290000000000006,-11.790000000000006,-14.180000000000007,-2.469999999999999,-16.260000000000005,-2.1700000000000017,-10.200000000000003],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",7.0,58.05,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0368,0.0687,34.4,34.0,-46.43,-1.1599999999999966],[\"01-ai\\u002fYi-6B-200K\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,56.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0312,0.0548,38.5,37.8,-43.07,-1.8199999999999932],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.06,54.08,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0298,0.0553,38.0,38.8,-46.11,2.1099999999999994],[\"meta-llama\\u002fLlama-2-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,50.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0322,0.0632,42.0,40.2,-49.05,-4.290000000000006],[\"beomi\\u002fYi-Ko-6B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.18,50.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0293,0.0548,38.1,37.4,-46.53,-1.8400000000000034],[\"fblgit\\u002funa-llama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,48.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0321,0.0629,42.9,41.3,-48.97,-3.730000000000004],[\"microsoft\\u002fphi-1_5\",\"phi\",\"float16\",\"pytorch\",0.0,47.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0273,0.0203,36.4,52.5,34.47999999999999,44.22999999999999],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,47.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0326,0.0619,42.6,38.8,-47.33,-8.920000000000002],[\"sarvamai\\u002fOpenHathi-7B-Hi-v0.1-Base\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.87,46.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0327,0.0619,41.5,41.1,-47.17,-0.9599999999999937],[\"stabilityai\\u002fstablelm-base-alpha-7b-v2\",\"\ud83d\udd34 StableLM-Alpha\",\"float16\",\"pytorch\",6.89,46.18,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0303,0.0611,43.1,41.7,-50.41,-3.25],[\"huggingface\\u002fllama-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,45.65,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0325,0.0617,42.2,40.2,-47.33,-4.739999999999995],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.61,45.62,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.032,0.0611,42.4,41.5,-47.63,-2.1200000000000045],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0184,0.0313,55.0,52.8,-41.21,-4.0],[\"KnutJaegersberg\\u002fQwen-1_8B-Llamafied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.84,44.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0196,0.0217,51.9,50.7,-9.680000000000007,-2.3100000000000023],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,44.26,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0328,0.0619,42.5,39.9,-47.01,-6.1200000000000045],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,42.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0334,0.0614,42.6,41.5,-45.6,-2.5799999999999983],[\"Writer\\u002fpalmyra-large\",\"GPT-2\",\"float16\",\"pytorch\",0.0,42.09,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0758,0.0582,27.1,27.2,30.24000000000001,0.37000000000000455],[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,41.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0329,0.0639,40.0,39.2,-48.51,-2.0],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.02,41.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0185,0.0309,53.2,55.9,-40.13,5.079999999999998],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,41.25,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0329,0.0634,40.5,40.3,-48.11,-0.4899999999999949],[\"LLM360\\u002fAmber\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.0,40.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0314,0.0614,44.1,40.7,-48.86,-7.709999999999994],[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",\"float16\",\"pytorch\",6.0,40.1,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0332,0.0571,31.0,31.2,-41.86,0.6500000000000057],[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,40.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0385,0.0685,26.9,27.2,-43.8,1.1200000000000045],[\"codellama\\u002fCodeLlama-7b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",6.74,39.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0329,0.0612,43.4,40.1,-46.24,-7.599999999999994],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,38.54,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0277,0.0375,39.5,39.7,-26.129999999999995,0.5100000000000051],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",6.65,38.06,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0315,0.0632,40.4,39.7,-50.16,-1.730000000000004],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,37.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0367,0.0534,27.7,26.7,-31.269999999999996,-3.6099999999999994],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,37.09,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0256,0.0359,41.3,40.3,-28.689999999999998,-2.4200000000000017],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",\"float16\",\"pytorch\",1.0,37.07,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.017,0.0183,65.1,65.7,-7.099999999999994,0.9200000000000017],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",2.91,36.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.026,0.0359,40.4,39.8,-27.58,-1.4899999999999949],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,36.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0181,0.0202,53.9,50.1,-10.400000000000006,-7.049999999999997],[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",\"float16\",\"pytorch\",6.7,36.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.029,0.0264,55.1,54.6,9.849999999999994,-0.9099999999999966],[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",\"float16\",\"pytorch\",2.72,36.2,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0241,0.0366,43.9,44.9,-34.150000000000006,2.280000000000001],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",\"float16\",\"pytorch\",4.0,36.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0266,0.037,40.1,40.5,-28.11,1.0],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",3.0,36.07,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0202,0.0312,53.5,53.1,-35.260000000000005,-0.75],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.3,35.71,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0185,0.0201,54.8,55.0,-7.959999999999994,0.35999999999999943],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",\"float16\",\"pytorch\",0.0,35.18,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0229,0.0201,71.4,72.4,13.930000000000007,1.4000000000000057],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.4,35.0,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0189,0.0218,53.7,52.6,-13.299999999999997,-2.049999999999997],[\"bn22\\u002ftinyllama_frankenmerge\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.54,34.64,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0266,0.0286,37.3,36.2,-6.989999999999995,-2.950000000000003],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.31,34.46,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0194,0.0222,53.5,53.9,-12.61,0.75],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,34.42,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0333,0.0636,39.6,39.2,-47.64,-1.0100000000000051],[\"gpt2-xl\",\"GPT-2\",\"float16\",\"pytorch\",1.61,34.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0342,0.032,30.9,31.2,6.8799999999999955,0.9699999999999989],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.03,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0187,0.0199,53.2,52.6,-6.030000000000001,-1.1299999999999955],[\"stabilityai\\u002fstablelm-base-alpha-7b\",\"GPT-NeoX\",\"float16\",\"pytorch\",7.0,34.37,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0288,0.0591,66.4,75.2,-51.27,13.25],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",\"float16\",\"pytorch\",1.44,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0147,0.0143,70.4,71.2,2.799999999999997,1.1400000000000006],[\"facebook\\u002fxglm-4.5B\",\"XGLM\",\"float16\",\"pytorch\",5.08,34.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.033,0.0567,35.3,34.4,-41.8,-2.549999999999997],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,33.72,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0178,0.0208,53.8,51.4,-14.420000000000002,-4.459999999999994],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",\"float16\",\"pytorch\",1.37,33.58,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0172,0.0227,59.7,60.6,-24.230000000000004,1.5100000000000051],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",\"float16\",\"pytorch\",2.7,33.25,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0196,0.019,53.9,55.0,3.1599999999999966,2.0400000000000063],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",1.08,32.78,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0125,0.0154,78.2,76.1,-18.83,-2.6899999999999977],[\"Kunhao\\u002fpile-7b-250b-tokens\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",7.0,32.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0316,0.0554,38.7,37.2,-42.96,-3.8799999999999955],[\"Salesforce\\u002fcodegen-6B-multi\",\"\u267e\ufe0f CodeGen\",\"float16\",\"pytorch\",6.0,32.43,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.04,0.0682,26.7,26.8,-41.35,0.37000000000000455],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.03,0.0447,36.2,35.7,-32.89,-1.3799999999999955],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.95,32.14,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0284,0.045,35.8,35.6,-36.89,-0.5600000000000023],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.1,31.86,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.018,0.0201,52.1,52.4,-10.450000000000003,0.5799999999999983],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.51,31.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0181,0.0202,55.3,52.8,-10.400000000000006,-4.519999999999996],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",\"float16\",\"pytorch\",3.0,31.5,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0172,0.0314,79.2,78.0,-45.22,-1.519999999999996],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",\"float16\",\"pytorch\",1.3,31.3,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0151,0.0142,70.4,72.0,6.340000000000003,2.269999999999996],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",\"float16\",\"pytorch\",0.47,30.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0133,0.0136,71.0,70.8,-2.2099999999999937,-0.28000000000000114],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",\"float16\",\"pytorch\",0.38,30.4,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0155,0.0162,62.8,61.4,-4.319999999999993,-2.230000000000004],[\"ahxt\\u002fllama2_xs_460M_experimental\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.46,30.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0188,0.0217,51.0,48.9,-13.36,-4.1200000000000045],[\"facebook\\u002fxglm-564M\",\"XGLM\",\"float16\",\"pytorch\",0.56,29.55,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0158,0.0177,74.6,68.7,-10.730000000000004,-7.909999999999997],[\"winglian\\u002fLlama-2-3b-hf\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",3.0,29.53,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0163,0.0308,82.5,82.0,-47.08,-0.6099999999999994],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",\"float16\",\"pytorch\",0.15,29.47,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00838,0.00975,121.0,116.0,-14.049999999999997,-4.1299999999999955],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00784,0.00919,118.0,109.0,-14.689999999999998,-7.6299999999999955],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",0.16,29.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0089,0.0105,112.0,103.0,-15.239999999999995,-8.040000000000006],[\"cerebras\\u002fCerebras-GPT-256M\",\"GPT-2\",\"float16\",\"pytorch\",0.26,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00825,0.00836,120.0,115.0,-1.3199999999999932,-4.170000000000002],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.38,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00925,0.0106,109.0,102.0,-12.739999999999995,-6.420000000000002],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,29.28,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0172,0.0224,61.4,57.6,-23.209999999999994,-6.189999999999998],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.22,29.23,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00789,0.00926,116.0,112.0,-14.790000000000006,-3.450000000000003],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,29.15,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00482,0.00535,207.0,192.0,-9.909999999999997,-7.25],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0151,0.0164,67.3,66.9,-7.930000000000007,-0.5900000000000034],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.21,29.02,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00918,0.0103,108.0,103.0,-10.870000000000005,-4.6299999999999955],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.1,28.97,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00495,0.00559,185.0,177.0,-11.450000000000003,-4.319999999999993],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.0,28.88,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0196,0.02,50.4,53.8,-2.0,6.75],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.85,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00494,0.00515,206.0,196.0,-4.079999999999998,-4.849999999999994],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.81,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00468,0.0053,209.0,195.0,-11.700000000000003,-6.700000000000003],[\"Locutusque\\u002fTinyMistral-248M-v2\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,28.78,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0114,0.0128,88.2,85.9,-10.939999999999998,-2.6099999999999994],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.06,28.7,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00452,0.00526,196.0,189.0,-14.069999999999993,-3.569999999999993],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.61,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00472,0.00513,207.0,195.0,-7.989999999999995,-5.799999999999997],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.6,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0047,0.00516,207.0,193.0,-8.909999999999997,-6.760000000000005],[\"gpt2\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.53,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00807,0.00804,120.0,121.0,0.37000000000000455,0.8299999999999983],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",\"float16\",\"pytorch\",1.12,28.49,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.011,0.0144,86.4,87.9,-23.61,1.7399999999999949],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",\"float16\",\"pytorch\",0.19,28.45,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00693,0.00715,139.0,135.0,-3.0799999999999983,-2.8799999999999955],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",1.0,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00458,0.00827,278.0,275.0,-44.62,-1.0799999999999983],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.1,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00476,0.00545,204.0,185.0,-12.659999999999997,-9.310000000000002],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.44,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00584,0.00684,176.0,163.0,-14.620000000000005,-7.390000000000001],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.03,28.41,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00332,0.00375,309.0,293.0,-11.469999999999999,-5.180000000000007],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",\"float16\",\"pytorch\",0.14,28.4,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00816,0.00817,120.0,120.0,-0.12000000000000455,0.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.01,28.31,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00591,0.00651,178.0,169.0,-9.219999999999999,-5.060000000000002],[\"instructkr\\u002fko-wand-136M\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.14,28.29,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0077,0.00841,128.0,125.0,-8.439999999999998,-2.3400000000000034],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",\"float16\",\"pytorch\",0.03,28.27,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00472,0.00521,207.0,193.0,-9.400000000000006,-6.760000000000005],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",\"float16\",\"pytorch\",0.0,28.19,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00582,0.00634,178.0,169.0,-8.200000000000003,-5.060000000000002],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",\"float16\",\"pytorch\",0.08,28.17,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00459,0.00534,195.0,193.0,-14.040000000000006,-1.0300000000000011],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,27.95,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00752,0.00843,134.0,126.0,-10.790000000000006,-5.969999999999999],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",\"float16\",\"pytorch\",0.11,27.75,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.00601,0.00625,162.0,158.0,-3.8400000000000034,-2.469999999999999],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",\"float16\",\"pytorch\",0.25,27.73,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.011,0.0129,90.4,84.4,-14.730000000000004,-6.640000000000001],[\"ai-forever\\u002fmGPT\",\"GPT-2\",\"float16\",\"pytorch\",0.0,27.61,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.0156,0.0162,64.4,63.3,-3.700000000000003,-1.7099999999999937],[\"team-lucid\\u002fmptk-1b\",\"\ud83e\uddf1 MPT\",\"float16\",\"pytorch\",1.31,20.84,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMV\",0.012,0.0161,85.3,84.2,-25.47,-1.2900000000000063]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\",\"legendgroup\":\"AWQ.4bit+GEMV\",\"marker\":{\"color\":\"#FED4C4\"},\"name\":\"AWQ.4bit+GEMV\",\"notched\":false,\"offsetgroup\":\"AWQ.4bit+GEMV\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"phi\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83d\udd34 StableLM-Alpha\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-J\",\"\u267e\ufe0f CodeGen\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd85 Falcon\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-Neo\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"\ud83e\udd99 LLaMA\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-2\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"GPT-2\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"\u267e\ufe0f CodeGen\",\"GPT-NeoX\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"GPT-2\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"XGLM\",\"\ud83e\udd99 LLaMA\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\u2b50 StarCoder\",\"GPT-2\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"\ud83c\udf38 Bloom\",\"GPT-NeoX\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-NeoX\",\"\u24c2\ufe0f Mistral\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-NeoX\",\"GPT-2\",\"\u2b50 StarCoder\",\"GPT-2\",\"\ud83e\udd99 LLaMA\",\"GPT-NeoX\",\"GPT-Neo\",\"GPT-Neo\",\"GPT-2\",\"GPT-Neo\",\"\u24c2\ufe0f Mistral\",\"GPT-NeoX\",\"GPT-Neo\",\"\ud83e\udd99 LLaMA\",\"\ud83c\udf38 Bloom\",\"GPT-2\",\"\u24c2\ufe0f Mistral\",\"GPT-2\",\"\ud83e\uddf1 MPT\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-1.1599999999999966,-1.8199999999999932,2.1099999999999994,-4.290000000000006,-1.8400000000000034,-3.730000000000004,44.22999999999999,-8.920000000000002,-0.9599999999999937,-3.25,-4.739999999999995,-2.1200000000000045,-4.0,-2.3100000000000023,-6.1200000000000045,-2.5799999999999983,0.37000000000000455,-2.0,5.079999999999998,-0.4899999999999949,-7.709999999999994,0.6500000000000057,1.1200000000000045,-7.599999999999994,0.5100000000000051,-1.730000000000004,-3.6099999999999994,-2.4200000000000017,0.9200000000000017,-1.4899999999999949,-7.049999999999997,-0.9099999999999966,2.280000000000001,1.0,-0.75,0.35999999999999943,1.4000000000000057,-2.049999999999997,-2.950000000000003,0.75,-1.0100000000000051,0.9699999999999989,-1.1299999999999955,13.25,1.1400000000000006,-2.549999999999997,-4.459999999999994,1.5100000000000051,2.0400000000000063,-2.6899999999999977,-3.8799999999999955,0.37000000000000455,-1.3799999999999955,-0.5600000000000023,0.5799999999999983,-4.519999999999996,-1.519999999999996,2.269999999999996,-0.28000000000000114,-2.230000000000004,-4.1200000000000045,-7.909999999999997,-0.6099999999999994,-4.1299999999999955,-7.6299999999999955,-8.040000000000006,-4.170000000000002,-6.420000000000002,-6.189999999999998,-3.450000000000003,-7.25,-0.5900000000000034,-4.6299999999999955,-4.319999999999993,6.75,-4.849999999999994,-6.700000000000003,-2.6099999999999994,-3.569999999999993,-5.799999999999997,-6.760000000000005,0.8299999999999983,1.7399999999999949,-2.8799999999999955,-1.0799999999999983,-9.310000000000002,-7.390000000000001,-5.180000000000007,0.0,-5.060000000000002,-2.3400000000000034,-6.760000000000005,-5.060000000000002,-1.0300000000000011,-5.969999999999999,-2.469999999999999,-6.640000000000001,-1.7099999999999937,-1.2900000000000063],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"}],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"LLM Architecture\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Decode Speedup (%)\"}},\"legend\":{\"title\":{\"text\":\"Quantization Scheme\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"boxmode\":\"group\",\"title\":{\"text\":\"Decode Throughput Speedup per Architecture\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":600}}"
                },
                "show_label": false,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "plot",
                "elem_classes": [],
                "name": "plot",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "12c46a60185f5c11b6b6b6f9adbe826c",
            "api_info": {
                "properties": {
                    "type": {
                        "enum": [
                            "altair",
                            "bokeh",
                            "plotly",
                            "matplotlib"
                        ],
                        "title": "Type",
                        "type": "string"
                    },
                    "plot": {
                        "title": "Plot",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "plot"
                ],
                "title": "PlotData",
                "type": "object"
            },
            "example_inputs": null
        },
        {
            "id": 52,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 53,
            "type": "tabitem",
            "props": {
                "label": "RTX4090-24GB-450W \ud83d\udcbb",
                "id": 1,
                "name": "tabitem"
            },
            "skip_api": true,
            "component_class_id": "f72faf2b8abb615823b199272218625a"
        },
        {
            "id": 54,
            "type": "html",
            "props": {
                "value": "Use this control panel to filter the leaderboard.",
                "show_label": true,
                "visible": true,
                "elem_id": "text",
                "elem_classes": [],
                "name": "html",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "0ef1a4eade65d710fcd9fc56f6586b0e",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "<p>Hello</p>"
        },
        {
            "id": 55,
            "type": "textbox",
            "props": {
                "value": "hf-dgx-01",
                "lines": 1,
                "max_lines": 20,
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": false,
                "autofocus": false,
                "autoscroll": true,
                "elem_classes": [],
                "type": "text",
                "rtl": false,
                "show_copy_button": false,
                "name": "textbox",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "777333cda51df020195231508683831a",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "Hello!!"
        },
        {
            "id": 56,
            "type": "row",
            "props": {
                "variant": "default",
                "visible": true,
                "equal_height": true,
                "name": "row"
            },
            "skip_api": true,
            "component_class_id": "9a37ebd52f5e3ac2d2a1b25e6c1b3eba"
        },
        {
            "id": 57,
            "type": "column",
            "props": {
                "scale": 1,
                "min_width": 320,
                "variant": "default",
                "visible": true,
                "name": "column"
            },
            "skip_api": true,
            "component_class_id": "cc17d07a8a70ee1001b1e05bebd9e3a3"
        },
        {
            "id": 58,
            "type": "textbox",
            "props": {
                "value": "",
                "lines": 1,
                "max_lines": 20,
                "label": "Model \ud83e\udd17",
                "info": "\ud83d\udd0d Search for a model name",
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "search-bar",
                "autofocus": false,
                "autoscroll": true,
                "elem_classes": [],
                "type": "text",
                "rtl": false,
                "show_copy_button": false,
                "name": "textbox",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "777333cda51df020195231508683831a",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "Hello!!"
        },
        {
            "id": 59,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 60,
            "type": "row",
            "props": {
                "variant": "default",
                "visible": true,
                "equal_height": true,
                "name": "row"
            },
            "skip_api": true,
            "component_class_id": "9a37ebd52f5e3ac2d2a1b25e6c1b3eba"
        },
        {
            "id": 61,
            "type": "column",
            "props": {
                "scale": 1,
                "min_width": 320,
                "variant": "panel",
                "visible": true,
                "name": "column"
            },
            "skip_api": true,
            "component_class_id": "cc17d07a8a70ee1001b1e05bebd9e3a3"
        },
        {
            "id": 62,
            "type": "slider",
            "props": {
                "minimum": 0,
                "maximum": 100,
                "value": 0,
                "step": 1,
                "label": "Open LLM Score (%) \ud83d\udcc8",
                "info": "\ud83c\udf9a\ufe0f Slide to minimum Open LLM score",
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "threshold-slider",
                "elem_classes": [],
                "name": "slider",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "92d475377d8dbf22997635901e7c301a",
            "api_info": {
                "type": "number",
                "description": "numeric value between 0 and 100"
            },
            "example_inputs": 0
        },
        {
            "id": 63,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 64,
            "type": "column",
            "props": {
                "scale": 1,
                "min_width": 320,
                "variant": "panel",
                "visible": true,
                "name": "column"
            },
            "skip_api": true,
            "component_class_id": "cc17d07a8a70ee1001b1e05bebd9e3a3"
        },
        {
            "id": 65,
            "type": "slider",
            "props": {
                "minimum": 0,
                "maximum": 81920,
                "value": 81920,
                "step": 100,
                "label": "Peak Memory (MB) \ud83d\udcc8",
                "info": "\ud83c\udf9a\ufe0f Slide to maximum Peak Memory",
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "memory-slider",
                "elem_classes": [],
                "name": "slider",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "92d475377d8dbf22997635901e7c301a",
            "api_info": {
                "type": "number",
                "description": "numeric value between 0 and 81920"
            },
            "example_inputs": 0
        },
        {
            "id": 66,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 67,
            "type": "column",
            "props": {
                "scale": 1,
                "min_width": 320,
                "variant": "default",
                "visible": true,
                "name": "column"
            },
            "skip_api": true,
            "component_class_id": "cc17d07a8a70ee1001b1e05bebd9e3a3"
        },
        {
            "id": 68,
            "type": "checkboxgroup",
            "props": {
                "choices": [
                    [
                        "pytorch",
                        "pytorch"
                    ]
                ],
                "value": [
                    "pytorch"
                ],
                "type": "value",
                "label": "Backends \ud83c\udfed",
                "info": "\u2611\ufe0f Select the backends",
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "backend-checkboxes",
                "elem_classes": [],
                "name": "checkboxgroup",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "cf1be62e677c31d1fe76c5501de84c66",
            "api_info": {
                "items": {
                    "enum": [
                        "pytorch"
                    ],
                    "type": "string"
                },
                "title": "Checkbox Group",
                "type": "array"
            },
            "example_inputs": [
                "pytorch"
            ]
        },
        {
            "id": 69,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 70,
            "type": "row",
            "props": {
                "variant": "default",
                "visible": true,
                "equal_height": true,
                "name": "row"
            },
            "skip_api": true,
            "component_class_id": "9a37ebd52f5e3ac2d2a1b25e6c1b3eba"
        },
        {
            "id": 71,
            "type": "column",
            "props": {
                "scale": 1,
                "min_width": 320,
                "variant": "panel",
                "visible": true,
                "name": "column"
            },
            "skip_api": true,
            "component_class_id": "cc17d07a8a70ee1001b1e05bebd9e3a3"
        },
        {
            "id": 72,
            "type": "checkboxgroup",
            "props": {
                "choices": [
                    [
                        "float32",
                        "float32"
                    ],
                    [
                        "float16",
                        "float16"
                    ],
                    [
                        "bfloat16",
                        "bfloat16"
                    ]
                ],
                "value": [
                    "float32",
                    "float16",
                    "bfloat16"
                ],
                "type": "value",
                "label": "Load DTypes \ud83d\udce5",
                "info": "\u2611\ufe0f Select the load data types",
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "dtype-checkboxes",
                "elem_classes": [],
                "name": "checkboxgroup",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "cf1be62e677c31d1fe76c5501de84c66",
            "api_info": {
                "items": {
                    "enum": [
                        "float32",
                        "float16",
                        "bfloat16"
                    ],
                    "type": "string"
                },
                "title": "Checkbox Group",
                "type": "array"
            },
            "example_inputs": [
                "float32"
            ]
        },
        {
            "id": 73,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 74,
            "type": "column",
            "props": {
                "scale": 1,
                "min_width": 320,
                "variant": "panel",
                "visible": true,
                "name": "column"
            },
            "skip_api": true,
            "component_class_id": "cc17d07a8a70ee1001b1e05bebd9e3a3"
        },
        {
            "id": 75,
            "type": "checkboxgroup",
            "props": {
                "choices": [
                    [
                        "None",
                        "None"
                    ],
                    [
                        "BetterTransformer",
                        "BetterTransformer"
                    ],
                    [
                        "FlashAttentionV2",
                        "FlashAttentionV2"
                    ]
                ],
                "value": [
                    "None",
                    "BetterTransformer",
                    "FlashAttentionV2"
                ],
                "type": "value",
                "label": "Optimizations \ud83d\udee0\ufe0f",
                "info": "\u2611\ufe0f Select the optimization",
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "optimization-checkboxes",
                "elem_classes": [],
                "name": "checkboxgroup",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "cf1be62e677c31d1fe76c5501de84c66",
            "api_info": {
                "items": {
                    "enum": [
                        "None",
                        "BetterTransformer",
                        "FlashAttentionV2"
                    ],
                    "type": "string"
                },
                "title": "Checkbox Group",
                "type": "array"
            },
            "example_inputs": [
                "None"
            ]
        },
        {
            "id": 76,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 77,
            "type": "column",
            "props": {
                "scale": 2,
                "min_width": 320,
                "variant": "default",
                "visible": true,
                "name": "column"
            },
            "skip_api": true,
            "component_class_id": "cc17d07a8a70ee1001b1e05bebd9e3a3"
        },
        {
            "id": 78,
            "type": "checkboxgroup",
            "props": {
                "choices": [
                    [
                        "None",
                        "None"
                    ],
                    [
                        "BnB.4bit",
                        "BnB.4bit"
                    ],
                    [
                        "BnB.8bit",
                        "BnB.8bit"
                    ],
                    [
                        "GPTQ.4bit",
                        "GPTQ.4bit"
                    ],
                    [
                        "GPTQ.4bit+ExllamaV1",
                        "GPTQ.4bit+ExllamaV1"
                    ],
                    [
                        "GPTQ.4bit+ExllamaV2",
                        "GPTQ.4bit+ExllamaV2"
                    ],
                    [
                        "AWQ.4bit+GEMM",
                        "AWQ.4bit+GEMM"
                    ],
                    [
                        "AWQ.4bit+GEMV",
                        "AWQ.4bit+GEMV"
                    ]
                ],
                "value": [
                    "None",
                    "BnB.4bit",
                    "BnB.8bit",
                    "GPTQ.4bit",
                    "GPTQ.4bit+ExllamaV1",
                    "GPTQ.4bit+ExllamaV2",
                    "AWQ.4bit+GEMM",
                    "AWQ.4bit+GEMV"
                ],
                "type": "value",
                "label": "Quantizations \ud83d\udddc\ufe0f",
                "info": "\u2611\ufe0f Select the quantization schemes",
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "quantization-checkboxes",
                "elem_classes": [],
                "name": "checkboxgroup",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "cf1be62e677c31d1fe76c5501de84c66",
            "api_info": {
                "items": {
                    "enum": [
                        "None",
                        "BnB.4bit",
                        "BnB.8bit",
                        "GPTQ.4bit",
                        "GPTQ.4bit+ExllamaV1",
                        "GPTQ.4bit+ExllamaV2",
                        "AWQ.4bit+GEMM",
                        "AWQ.4bit+GEMV"
                    ],
                    "type": "string"
                },
                "title": "Checkbox Group",
                "type": "array"
            },
            "example_inputs": [
                "None"
            ]
        },
        {
            "id": 79,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 80,
            "type": "row",
            "props": {
                "variant": "default",
                "visible": true,
                "equal_height": true,
                "name": "row"
            },
            "skip_api": true,
            "component_class_id": "9a37ebd52f5e3ac2d2a1b25e6c1b3eba"
        },
        {
            "id": 81,
            "type": "button",
            "props": {
                "value": "Filter \ud83d\ude80",
                "variant": "secondary",
                "visible": true,
                "interactive": true,
                "elem_id": "filter-button",
                "elem_classes": [],
                "name": "button",
                "_selectable": false
            },
            "skip_api": true,
            "component_class_id": "e6d4e61f49a0e5522730f312b1fb6380"
        },
        {
            "id": 82,
            "type": "tabs",
            "props": {
                "visible": true,
                "elem_classes": [
                    "subtabs"
                ],
                "name": "tabs"
            },
            "skip_api": true,
            "component_class_id": "9e076167ca1265948472576573f64ef5"
        },
        {
            "id": 83,
            "type": "tabitem",
            "props": {
                "label": "Leaderboard \ud83c\udfc5",
                "id": 0,
                "name": "tabitem"
            },
            "skip_api": true,
            "component_class_id": "f72faf2b8abb615823b199272218625a"
        },
        {
            "id": 84,
            "type": "html",
            "props": {
                "value": "\ud83d\udc49 Scroll to the right \ud83d\udc49 for additional columns.",
                "show_label": true,
                "visible": true,
                "elem_id": "text",
                "elem_classes": [],
                "name": "html",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "0ef1a4eade65d710fcd9fc56f6586b0e",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "<p>Hello</p>"
        },
        {
            "id": 85,
            "type": "dataframe",
            "props": {
                "value": {
                    "headers": [
                        "Model \ud83e\udd17",
                        "Arch \ud83c\udfdb\ufe0f",
                        "Params (B)",
                        "Open LLM Score (%)",
                        "Backend \ud83c\udfed",
                        "DType \ud83d\udce5",
                        "Optimization \ud83d\udee0\ufe0f",
                        "Quantization \ud83d\udddc\ufe0f",
                        "Prefill Latency (s)",
                        "Decode Throughput (tokens/s)",
                        "Allocated Memory (MB)",
                        "Energy (tokens/kWh)",
                        "E2E Latency (s)",
                        "E2E Throughput (tokens/s)",
                        "Reserved Memory (MB)",
                        "Used Memory (MB)"
                    ],
                    "data": [
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rishiraj/CatPPT-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rishiraj/CatPPT-base</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "72.25 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0314,
                            49.5,
                            15171,
                            657894.0,
                            5.18,
                            49.4,
                            15374,
                            16232
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/upstage/SOLAR-10.7B-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">upstage/SOLAR-10.7B-v1.0</a>",
                            "\ud83e\udd99 LLaMA",
                            10.73,
                            "66.04 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0452,
                            33.4,
                            21781,
                            454545.0,
                            7.69,
                            33.3,
                            22047,
                            22906
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/seungduk/KoSOLAR-10.7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">seungduk/KoSOLAR-10.7B-v0.1</a>",
                            "\ud83e\udd99 LLaMA",
                            10.86,
                            "66.04 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0453,
                            33.3,
                            22045,
                            438596.0,
                            7.71,
                            33.2,
                            22353,
                            23212
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Deci/DeciLM-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Deci/DeciLM-7B</a>",
                            "\ud83d\udd35 deci",
                            7.04,
                            "61.55 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0305,
                            51.3,
                            14290,
                            709219.0,
                            5.0,
                            51.2,
                            14357,
                            15215
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-2</a>",
                            "phi",
                            2.78,
                            "61.33 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0436,
                            69.9,
                            11951,
                            990099.0,
                            3.69,
                            69.4,
                            11978,
                            12831
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-v0.1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "60.97 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0315,
                            49.2,
                            15171,
                            689655.0,
                            5.21,
                            49.1,
                            15374,
                            16232
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/scb10x/typhoon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">scb10x/typhoon-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.0,
                            "58.05 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0318,
                            49.3,
                            15225,
                            680272.0,
                            5.2,
                            49.2,
                            15430,
                            16289
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B</a>",
                            "\ud83e\udd99 LLaMA",
                            6.06,
                            "54.08 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0866,
                            33.8,
                            24537,
                            456621.0,
                            7.64,
                            33.5,
                            24672,
                            25525
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/japanese-stablelm-base-gamma-7b</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "52.59 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0315,
                            49.3,
                            14701,
                            671140.0,
                            5.2,
                            49.2,
                            14900,
                            15758
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/augmxnt/shisa-base-7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">augmxnt/shisa-base-7b-v1</a>",
                            "\u24c2\ufe0f Mistral",
                            7.96,
                            "51.64 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0328,
                            47.4,
                            16692,
                            636942.0,
                            5.41,
                            47.3,
                            16972,
                            17830
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/vishesht27/22-Neuro_Model\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vishesht27/22-Neuro_Model</a>",
                            "\u24c2\ufe0f Mistral",
                            7.24,
                            "50.23 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0314,
                            49.2,
                            15171,
                            680272.0,
                            5.21,
                            49.1,
                            15374,
                            16232
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-icl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-icl</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.93 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0282,
                            52.8,
                            14056,
                            704225.0,
                            4.86,
                            52.7,
                            14092,
                            14951
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-1_5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-1_5</a>",
                            "phi",
                            0.0,
                            "47.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.00986,
                            116.0,
                            1304,
                            2915451.0,
                            2.2,
                            116.0,
                            1402,
                            2263
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-1_5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-1_5</a>",
                            "phi",
                            0.0,
                            "47.69 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0102,
                            111.0,
                            3057,
                            2247191.0,
                            2.3,
                            111.0,
                            3229,
                            4088
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-1_5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-1_5</a>",
                            "phi",
                            0.0,
                            "47.69*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.0141,
                            105.0,
                            1269,
                            1766784.0,
                            2.45,
                            104.0,
                            1354,
                            2213
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-1_5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-1_5</a>",
                            "phi",
                            0.0,
                            "47.69 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0227,
                            123.0,
                            6098,
                            1754385.0,
                            2.1,
                            122.0,
                            6142,
                            6994
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-1_5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-1_5</a>",
                            "phi",
                            0.0,
                            "47.69*",
                            "pytorch",
                            "float16",
                            "FlashAttentionV2",
                            "BnB.8bit",
                            0.0311,
                            32.6,
                            1848,
                            1081081.0,
                            7.86,
                            32.6,
                            1944,
                            2811
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/golaxy/gowizardlm\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">golaxy/gowizardlm</a>",
                            "\ud83e\udd99 LLaMA",
                            0.0,
                            "47.64 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0287,
                            51.9,
                            14653,
                            704225.0,
                            4.94,
                            51.8,
                            14682,
                            15540
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/itsliupeng/openllama-7b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">itsliupeng/openllama-7b-base</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "47.09 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0282,
                            52.7,
                            14056,
                            724637.0,
                            4.87,
                            52.6,
                            14092,
                            14951
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-3b-4e1t\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-3b-4e1t</a>",
                            "\ud83d\udd34 StableLM-Epoch",
                            2.8,
                            "46.58 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0413,
                            68.4,
                            12068,
                            917431.0,
                            3.77,
                            67.9,
                            12117,
                            12969
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/DevaMalla/llama-base-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DevaMalla/llama-base-7b</a>",
                            "\ud83e\udd99 LLaMA",
                            6.61,
                            "45.62 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0279,
                            52.9,
                            14056,
                            724637.0,
                            4.85,
                            52.8,
                            14092,
                            14951
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-2-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "44.75 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0159,
                            102.0,
                            6401,
                            1538461.0,
                            2.52,
                            102.0,
                            6452,
                            7311
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-2-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "44.75 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0428,
                            60.6,
                            12793,
                            854700.0,
                            4.25,
                            60.2,
                            12893,
                            13745
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "44.26 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0283,
                            52.7,
                            14056,
                            719424.0,
                            4.87,
                            52.6,
                            14092,
                            14951
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-7b</a>",
                            "\ud83e\udd85 Falcon",
                            7.0,
                            "44.17 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0343,
                            43.2,
                            13945,
                            636942.0,
                            5.93,
                            43.2,
                            14061,
                            14920
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/calm2-7b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/calm2-7b-chat</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "43.27 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0287,
                            51.9,
                            15101,
                            699300.0,
                            4.94,
                            51.8,
                            15126,
                            15985
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_7b</a>",
                            "\ud83e\udd99 LLaMA",
                            7.0,
                            "42.31 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0282,
                            52.9,
                            14056,
                            729927.0,
                            4.85,
                            52.8,
                            14092,
                            14951
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>",
                            "GPT-NeoX",
                            7.0,
                            "41.49 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0294,
                            50.9,
                            14429,
                            680272.0,
                            5.04,
                            50.8,
                            14470,
                            15329
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.02,
                            "41.44 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0159,
                            102.0,
                            6401,
                            1536098.0,
                            2.52,
                            102.0,
                            6452,
                            7311
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/GeneZC/MiniMA-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GeneZC/MiniMA-3B</a>",
                            "\ud83e\udd99 LLaMA",
                            3.02,
                            "41.44 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0427,
                            60.6,
                            12793,
                            833333.0,
                            4.25,
                            60.2,
                            12893,
                            13745
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-7B-v0.1</a>",
                            "GPT-NeoX",
                            6.65,
                            "41.25 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0293,
                            51.0,
                            14429,
                            675675.0,
                            5.03,
                            50.9,
                            14470,
                            15329
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "40.28 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0185,
                            90.1,
                            7280,
                            1297016.0,
                            2.85,
                            89.8,
                            7440,
                            8299
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b_v2</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "40.28 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.054,
                            52.7,
                            14582,
                            735294.0,
                            4.89,
                            52.4,
                            14677,
                            15530
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-j-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-j-6b</a>",
                            "GPT-J",
                            6.0,
                            "40.10 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0263,
                            53.8,
                            12721,
                            757575.0,
                            4.77,
                            53.7,
                            12769,
                            13628
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/codegen-6B-nl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/codegen-6B-nl</a>",
                            "\u267e\ufe0f CodeGen",
                            6.0,
                            "40.00 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0318,
                            45.8,
                            14855,
                            645161.0,
                            5.6,
                            45.7,
                            14902,
                            15761
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b-deduped</a>",
                            "GPT-NeoX",
                            12.0,
                            "39.70 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0475,
                            32.6,
                            24619,
                            411522.0,
                            7.87,
                            32.5,
                            24647,
                            25506
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.9b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.9b-deduped</a>",
                            "GPT-NeoX",
                            6.9,
                            "39.30 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0295,
                            50.9,
                            14403,
                            680272.0,
                            5.04,
                            50.8,
                            14445,
                            15303
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-7b1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-7b1</a>",
                            "\ud83c\udf38 Bloom",
                            7.07,
                            "39.18 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0288,
                            56.0,
                            14651,
                            675675.0,
                            4.58,
                            55.9,
                            14690,
                            15549
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b</a>",
                            "GPT-NeoX",
                            12.0,
                            "38.82 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0483,
                            32.5,
                            24619,
                            403225.0,
                            7.9,
                            32.4,
                            24647,
                            25506
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0174,
                            88.2,
                            6068,
                            1364256.0,
                            2.91,
                            88.0,
                            6144,
                            7003
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>",
                            "GPT-NeoX",
                            3.0,
                            "38.54 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0415,
                            60.7,
                            12028,
                            884955.0,
                            4.24,
                            60.4,
                            12069,
                            12921
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "38.26 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0185,
                            90.4,
                            7280,
                            1291989.0,
                            2.84,
                            90.1,
                            7440,
                            8299
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/openlm-research/open_llama_3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openlm-research/open_llama_3b</a>",
                            "\ud83e\udd99 LLaMA",
                            3.0,
                            "38.26 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.054,
                            52.7,
                            14582,
                            729927.0,
                            4.89,
                            52.4,
                            14677,
                            15530
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.7b</a>",
                            "GPT-NeoX",
                            6.65,
                            "38.06 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0287,
                            51.2,
                            14403,
                            684931.0,
                            5.01,
                            51.1,
                            14445,
                            15303
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0</a>",
                            "GPT-NeoX",
                            4.0,
                            "37.31 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0599,
                            42.1,
                            17177,
                            636942.0,
                            6.11,
                            41.9,
                            17207,
                            18059
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.017,
                            88.5,
                            6052,
                            1386962.0,
                            2.9,
                            88.3,
                            6123,
                            6982
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.7b</a>",
                            "GPT-NeoX",
                            2.91,
                            "37.09 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0418,
                            61.2,
                            11995,
                            909090.0,
                            4.21,
                            60.8,
                            12043,
                            12896
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-rw-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-rw-1b</a>",
                            "\ud83e\udd85 Falcon",
                            1.0,
                            "37.07 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00885,
                            153.0,
                            2834,
                            2610966.0,
                            1.68,
                            152.0,
                            2858,
                            3717
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-rw-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-rw-1b</a>",
                            "\ud83e\udd85 Falcon",
                            1.0,
                            "37.07 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0204,
                            132.0,
                            5659,
                            1821493.0,
                            1.95,
                            131.0,
                            5700,
                            6552
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0167,
                            90.1,
                            6052,
                            1400560.0,
                            2.85,
                            89.8,
                            6123,
                            6982
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b-deduped</a>",
                            "GPT-NeoX",
                            2.91,
                            "36.72 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0419,
                            61.2,
                            11995,
                            900900.0,
                            4.21,
                            60.8,
                            12043,
                            12896
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "36.42 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00798,
                            121.0,
                            2279,
                            2444987.0,
                            2.11,
                            121.0,
                            2403,
                            3262
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "36.42 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0182,
                            128.0,
                            4492,
                            1968503.0,
                            2.02,
                            127.0,
                            4548,
                            5401
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-6.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-6.7B</a>",
                            "GPT-2",
                            6.7,
                            "36.27 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0268,
                            53.6,
                            13997,
                            729927.0,
                            4.79,
                            53.4,
                            14040,
                            14899
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>",
                            "GPT-Neo",
                            2.72,
                            "36.20 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0418,
                            63.4,
                            11555,
                            943396.0,
                            4.06,
                            63.1,
                            11593,
                            12445
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dampish/StellarX-4B-V0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampish/StellarX-4B-V0.2</a>",
                            "GPT-NeoX",
                            4.0,
                            "36.15 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0413,
                            60.9,
                            12028,
                            917431.0,
                            4.23,
                            60.5,
                            12069,
                            12921
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>",
                            "\ud83c\udf38 Bloom",
                            3.0,
                            "36.07 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0163,
                            108.0,
                            6365,
                            1443001.0,
                            2.39,
                            107.0,
                            6434,
                            7292
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>",
                            "\ud83c\udf38 Bloom",
                            3.0,
                            "36.07 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0441,
                            65.7,
                            12734,
                            884955.0,
                            3.92,
                            65.3,
                            12859,
                            13712
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</a>",
                            "\ud83e\udd99 LLaMA",
                            1.3,
                            "35.71 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00882,
                            131.0,
                            2991,
                            2386634.0,
                            1.95,
                            131.0,
                            3114,
                            3972
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</a>",
                            "\ud83e\udd99 LLaMA",
                            1.3,
                            "35.71 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0225,
                            105.0,
                            5969,
                            1709401.0,
                            2.46,
                            104.0,
                            5991,
                            6843
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-base</a>",
                            "GPT-2",
                            0.0,
                            "35.18 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0201,
                            70.6,
                            10607,
                            961538.0,
                            3.63,
                            70.5,
                            10649,
                            11508
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Writer/palmyra-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Writer/palmyra-base</a>",
                            "GPT-2",
                            0.0,
                            "35.18 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0553,
                            39.7,
                            21105,
                            440528.0,
                            6.48,
                            39.5,
                            21214,
                            22067
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00996,
                            120.0,
                            3147,
                            2217294.0,
                            2.13,
                            120.0,
                            3202,
                            4061
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.4b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.4b-deduped</a>",
                            "GPT-NeoX",
                            1.4,
                            "35.00 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0221,
                            104.0,
                            6186,
                            1718213.0,
                            2.48,
                            103.0,
                            6232,
                            7085
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bn22/tinyllama_frankenmerge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bn22/tinyllama_frankenmerge</a>",
                            "\ud83e\udd99 LLaMA",
                            1.54,
                            "34.64 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0112,
                            87.3,
                            3168,
                            1730103.0,
                            2.93,
                            87.4,
                            3342,
                            4201
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bn22/tinyllama_frankenmerge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bn22/tinyllama_frankenmerge</a>",
                            "\ud83e\udd99 LLaMA",
                            1.54,
                            "34.64 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0268,
                            89.5,
                            6279,
                            1438848.0,
                            2.88,
                            88.9,
                            6341,
                            7194
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0099,
                            121.0,
                            3147,
                            2242152.0,
                            2.12,
                            121.0,
                            3202,
                            4061
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1.3b</a>",
                            "GPT-NeoX",
                            1.31,
                            "34.46 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0218,
                            105.0,
                            6186,
                            1700680.0,
                            2.44,
                            105.0,
                            6232,
                            7085
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-GPTrio\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-GPTrio</a>",
                            "GPT-NeoX",
                            0.0,
                            "34.42 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0306,
                            48.9,
                            16060,
                            649350.0,
                            5.24,
                            48.9,
                            16089,
                            16948
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2-xl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2-xl</a>",
                            "GPT-2",
                            1.61,
                            "34.38 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0293,
                            69.5,
                            7061,
                            1197604.0,
                            3.7,
                            69.2,
                            7121,
                            7974
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-480k-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.03,
                            "34.37 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00796,
                            122.0,
                            2279,
                            2525252.0,
                            2.1,
                            122.0,
                            2403,
                            3262
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-480k-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            1.03,
                            "34.37 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0187,
                            126.0,
                            4492,
                            2061855.0,
                            2.05,
                            125.0,
                            4548,
                            5401
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-1.3b</a>",
                            "GPT-2",
                            1.44,
                            "34.31 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00882,
                            152.0,
                            2999,
                            2710027.0,
                            1.69,
                            151.0,
                            3045,
                            3903
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-1.3b</a>",
                            "GPT-2",
                            1.44,
                            "34.31 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0191,
                            103.0,
                            5888,
                            1474926.0,
                            2.49,
                            103.0,
                            5928,
                            6781
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/facebook/xglm-4.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/xglm-4.5B</a>",
                            "XGLM",
                            5.08,
                            "34.31 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.063,
                            40.3,
                            19042,
                            568181.0,
                            6.39,
                            40.1,
                            19081,
                            19934
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-240k-503b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "33.72 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00818,
                            118.0,
                            2279,
                            2506265.0,
                            2.18,
                            117.0,
                            2403,
                            3262
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-intermediate-step-240k-503b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "33.72 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0187,
                            126.0,
                            4492,
                            2057613.0,
                            2.04,
                            125.0,
                            4548,
                            5401
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>",
                            "GPT-Neo",
                            1.37,
                            "33.58 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0223,
                            110.0,
                            5775,
                            1689189.0,
                            2.33,
                            110.0,
                            5817,
                            6669
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-2.7B</a>",
                            "GPT-2",
                            2.7,
                            "33.25 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0158,
                            97.0,
                            5799,
                            1552795.0,
                            2.65,
                            96.6,
                            5869,
                            6728
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-2.7B</a>",
                            "GPT-2",
                            2.7,
                            "33.25 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0382,
                            61.9,
                            11488,
                            781250.0,
                            4.16,
                            61.5,
                            11525,
                            12378
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00678,
                            175.0,
                            2243,
                            3300330.0,
                            1.47,
                            174.0,
                            2292,
                            3150
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-1b-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-1b-deduped</a>",
                            "GPT-NeoX",
                            1.08,
                            "32.78 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0155,
                            152.0,
                            4411,
                            2421307.0,
                            1.7,
                            151.0,
                            4445,
                            5298
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b-8k</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.23 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0553,
                            47.1,
                            16325,
                            704225.0,
                            5.47,
                            46.8,
                            16601,
                            17453
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/rinna/bilingual-gpt-neox-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rinna/bilingual-gpt-neox-4b</a>",
                            "GPT-NeoX",
                            3.95,
                            "32.14 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0552,
                            47.4,
                            16325,
                            689655.0,
                            5.44,
                            47.1,
                            16601,
                            17453
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-step-50K-105b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "31.86 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00823,
                            118.0,
                            2279,
                            2551020.0,
                            2.18,
                            117.0,
                            2403,
                            3262
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PY007/TinyLlama-1.1B-step-50K-105b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.1,
                            "31.86 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0185,
                            127.0,
                            4492,
                            2024291.0,
                            2.03,
                            126.0,
                            4548,
                            5401
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00899,
                            116.0,
                            1028,
                            3623188.0,
                            2.21,
                            116.0,
                            1121,
                            1980
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.55 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00953,
                            123.0,
                            1948,
                            2785515.0,
                            2.08,
                            123.0,
                            2006,
                            2859
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0148,
                            98.8,
                            7829,
                            1312335.0,
                            2.59,
                            98.8,
                            7853,
                            8712
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-base-alpha-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-base-alpha-3b</a>",
                            "GPT-NeoX",
                            3.0,
                            "31.50 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0457,
                            55.6,
                            15380,
                            704225.0,
                            4.64,
                            55.2,
                            15466,
                            16318
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-1.3B</a>",
                            "GPT-2",
                            1.3,
                            "31.30 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00891,
                            152.0,
                            2943,
                            2724795.0,
                            1.69,
                            151.0,
                            2984,
                            3842
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-1.3B</a>",
                            "GPT-2",
                            1.3,
                            "31.30 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0185,
                            106.0,
                            5776,
                            1468428.0,
                            2.43,
                            105.0,
                            5802,
                            6655
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m-deduped</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.29 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00859,
                            120.0,
                            1028,
                            3610108.0,
                            2.13,
                            120.0,
                            1121,
                            1980
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m-deduped</a>",
                            "GPT-NeoX",
                            0.51,
                            "31.29 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00944,
                            120.0,
                            1948,
                            2865329.0,
                            2.14,
                            120.0,
                            2006,
                            2859
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-430m-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-430m-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            0.43,
                            "30.45 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0106,
                            101.0,
                            1784,
                            2624671.0,
                            2.54,
                            101.0,
                            1811,
                            2664
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-356m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-356m</a>",
                            "GPT-2",
                            0.47,
                            "30.41 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00851,
                            156.0,
                            1792,
                            3389830.0,
                            1.64,
                            156.0,
                            1828,
                            2681
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/robowaifudev/megatron-gpt2-345m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">robowaifudev/megatron-gpt2-345m</a>",
                            "GPT-2",
                            0.38,
                            "30.40 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00741,
                            137.0,
                            844,
                            4149377.0,
                            1.87,
                            137.0,
                            935,
                            1794
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/robowaifudev/megatron-gpt2-345m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">robowaifudev/megatron-gpt2-345m</a>",
                            "GPT-2",
                            0.38,
                            "30.40 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00882,
                            142.0,
                            1656,
                            3174603.0,
                            1.8,
                            142.0,
                            1696,
                            2549
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ahxt/LiteLlama-460M-1T\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ahxt/LiteLlama-460M-1T</a>",
                            "\ud83e\udd99 LLaMA",
                            0.46,
                            "30.16 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.01,
                            128.0,
                            1936,
                            2865329.0,
                            2.01,
                            127.0,
                            1986,
                            2838
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>",
                            "GPT-Neo",
                            0.15,
                            "29.47 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00429,
                            270.0,
                            658,
                            7246376.0,
                            0.948,
                            270.0,
                            746,
                            1599
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.44 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00388,
                            245.0,
                            500,
                            7352941.0,
                            1.04,
                            246.0,
                            549,
                            1408
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.44 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00439,
                            292.0,
                            932,
                            6578947.0,
                            0.878,
                            292.0,
                            968,
                            1821
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/tiny_starcoder_py\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/tiny_starcoder_py</a>",
                            "\u2b50 StarCoder",
                            0.16,
                            "29.41 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0044,
                            243.0,
                            446,
                            7751937.0,
                            1.05,
                            244.0,
                            469,
                            1328
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/tiny_starcoder_py\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/tiny_starcoder_py</a>",
                            "\u2b50 StarCoder",
                            0.16,
                            "29.41 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00443,
                            252.0,
                            804,
                            6134969.0,
                            1.01,
                            253.0,
                            870,
                            1722
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-256M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-256M</a>",
                            "GPT-2",
                            0.26,
                            "29.38 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00395,
                            266.0,
                            665,
                            7194244.0,
                            0.963,
                            266.0,
                            748,
                            1607
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00453,
                            230.0,
                            458,
                            7194244.0,
                            1.11,
                            231.0,
                            509,
                            1368
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m-deduped</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.38 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00474,
                            230.0,
                            828,
                            6024096.0,
                            1.11,
                            231.0,
                            903,
                            1756
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-256M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-256M</a>",
                            "GPT-2",
                            0.26,
                            "29.38 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00598,
                            250.0,
                            1257,
                            5494505.0,
                            1.03,
                            249.0,
                            1321,
                            2173
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/SmolLlamix-8x101M-take2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/SmolLlamix-8x101M-take2</a>",
                            "\u24c2\ufe0f Mixtral",
                            0.4,
                            "29.35 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0119,
                            172.0,
                            894,
                            5813953.0,
                            1.49,
                            172.0,
                            945,
                            1804
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/SmolLlamix-8x101M-take2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/SmolLlamix-8x101M-take2</a>",
                            "\u24c2\ufe0f Mixtral",
                            0.4,
                            "29.35 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0127,
                            181.0,
                            1697,
                            5208333.0,
                            1.42,
                            180.0,
                            1895,
                            2748
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0102,
                            126.0,
                            3048,
                            2331002.0,
                            2.04,
                            125.0,
                            3131,
                            3989
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bit-dny/MindLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bit-dny/MindLLM</a>",
                            "GPT-Neo",
                            0.0,
                            "29.28 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.023,
                            107.0,
                            5980,
                            1742160.0,
                            2.4,
                            107.0,
                            6025,
                            6877
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.23 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00391,
                            245.0,
                            500,
                            7407407.0,
                            1.04,
                            246.0,
                            549,
                            1408
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</a>",
                            "\ud83e\udd99 LLaMA",
                            0.22,
                            "29.23 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00439,
                            294.0,
                            932,
                            6578947.0,
                            0.871,
                            294.0,
                            968,
                            1821
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00226,
                            437.0,
                            221,
                            15082956.0,
                            0.586,
                            437.0,
                            236,
                            1089
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-KI_v1-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "29.15 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00237,
                            424.0,
                            126,
                            15822784.0,
                            0.603,
                            425.0,
                            142,
                            1001
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-1M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-1M</a>",
                            "GPT-Neo",
                            0.0,
                            "29.14 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00233,
                            409.0,
                            112,
                            14245014.0,
                            0.625,
                            410.0,
                            140,
                            992
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-1M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-1M</a>",
                            "GPT-Neo",
                            0.0,
                            "29.14 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00255,
                            384.0,
                            77,
                            13869625.0,
                            0.667,
                            384.0,
                            96,
                            955
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00741,
                            142.0,
                            1939,
                            3058103.0,
                            1.8,
                            142.0,
                            2084,
                            2943
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11*",
                            "pytorch",
                            "float16",
                            "None",
                            "AWQ.4bit+GEMM",
                            0.00998,
                            136.0,
                            942,
                            3389830.0,
                            1.89,
                            135.0,
                            1073,
                            1932
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit+ExllamaV1",
                            0.01,
                            151.0,
                            962,
                            3787878.0,
                            1.7,
                            151.0,
                            1090,
                            3897
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11*",
                            "pytorch",
                            "float16",
                            "None",
                            "GPTQ.4bit",
                            0.0153,
                            119.0,
                            943,
                            2985074.0,
                            2.16,
                            119.0,
                            1084,
                            1942
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0161,
                            147.0,
                            3893,
                            2421307.0,
                            1.75,
                            146.0,
                            3942,
                            4795
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-large</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "29.11*",
                            "pytorch",
                            "float16",
                            "None",
                            "BnB.4bit",
                            0.0192,
                            68.5,
                            941,
                            2118644.0,
                            3.74,
                            68.4,
                            1113,
                            1972
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00468,
                            226.0,
                            828,
                            5882352.0,
                            1.13,
                            227.0,
                            903,
                            1756
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>",
                            "GPT-NeoX",
                            0.21,
                            "29.02 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00468,
                            222.0,
                            458,
                            7299270.0,
                            1.15,
                            223.0,
                            509,
                            1368
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/SmolLlamix-8x101M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/SmolLlamix-8x101M</a>",
                            "\u24c2\ufe0f Mixtral",
                            0.4,
                            "28.98 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0118,
                            168.0,
                            895,
                            5494505.0,
                            1.53,
                            167.0,
                            945,
                            1804
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/SmolLlamix-8x101M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/SmolLlamix-8x101M</a>",
                            "\u24c2\ufe0f Mixtral",
                            0.4,
                            "28.98 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0126,
                            176.0,
                            1697,
                            4975124.0,
                            1.46,
                            175.0,
                            1895,
                            2750
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.1,
                            "28.97 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00232,
                            401.0,
                            269,
                            12804097.0,
                            0.638,
                            401.0,
                            293,
                            1152
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>",
                            "\ud83e\udd99 LLaMA",
                            0.1,
                            "28.97 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00249,
                            462.0,
                            456,
                            11261261.0,
                            0.554,
                            462.0,
                            513,
                            1366
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00898,
                            118.0,
                            1976,
                            2754820.0,
                            2.18,
                            117.0,
                            2113,
                            2972
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/open-calm-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/open-calm-large</a>",
                            "GPT-NeoX",
                            0.0,
                            "28.88 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0152,
                            119.0,
                            3836,
                            2262443.0,
                            2.17,
                            118.0,
                            3915,
                            4767
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00229,
                            449.0,
                            221,
                            15503875.0,
                            0.57,
                            449.0,
                            236,
                            1089
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.85 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0023,
                            423.0,
                            126,
                            15527950.0,
                            0.605,
                            423.0,
                            142,
                            1001
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00236,
                            431.0,
                            221,
                            14947683.0,
                            0.593,
                            432.0,
                            236,
                            1089
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ethzanalytics/pythia-31m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ethzanalytics/pythia-31m</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.81 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00241,
                            416.0,
                            126,
                            15037593.0,
                            0.615,
                            416.0,
                            142,
                            1001
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248M-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248M-v2</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "28.78 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00516,
                            204.0,
                            608,
                            5780346.0,
                            1.26,
                            203.0,
                            656,
                            1515
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248M-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248M-v2</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "28.78 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00552,
                            228.0,
                            1153,
                            5128205.0,
                            1.13,
                            227.0,
                            1199,
                            2051
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/verysmol_llama-v11-KIx2</a>",
                            "\ud83e\udd99 LLaMA",
                            0.06,
                            "28.70 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00203,
                            494.0,
                            288,
                            13054830.0,
                            0.518,
                            494.0,
                            327,
                            1179
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/verysmol_llama-v11-KIx2</a>",
                            "\ud83e\udd99 LLaMA",
                            0.06,
                            "28.70 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00214,
                            416.0,
                            180,
                            14749262.0,
                            0.615,
                            416.0,
                            209,
                            1068
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-4-169m-pile\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-4-169m-pile</a>",
                            "\ud83d\udc26\u200d\u2b1b RWKV",
                            0.17,
                            "28.64 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00568,
                            195.0,
                            740,
                            5319148.0,
                            1.32,
                            194.0,
                            815,
                            1668
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00229,
                            455.0,
                            221,
                            15797788.0,
                            0.562,
                            456.0,
                            236,
                            1089
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-scratch-bf16</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.61 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00233,
                            441.0,
                            126,
                            16103059.0,
                            0.58,
                            441.0,
                            142,
                            1001
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00231,
                            440.0,
                            126,
                            16025641.0,
                            0.582,
                            440.0,
                            142,
                            1001
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.60 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00234,
                            450.0,
                            221,
                            15220700.0,
                            0.569,
                            450.0,
                            236,
                            1089
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/gpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2</a>",
                            "GPT-2",
                            0.14,
                            "28.53 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00418,
                            252.0,
                            621,
                            6578947.0,
                            1.01,
                            253.0,
                            717,
                            1569
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/gpt_bigcode-santacoder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/gpt_bigcode-santacoder</a>",
                            "\u2b50 StarCoder",
                            1.12,
                            "28.49 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0176,
                            166.0,
                            4628,
                            2267573.0,
                            1.56,
                            164.0,
                            4708,
                            5560
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-126m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-126m</a>",
                            "GPT-2",
                            0.19,
                            "28.45 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00387,
                            287.0,
                            721,
                            6711409.0,
                            0.894,
                            286.0,
                            803,
                            1655
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/Mixtral-GQA-400m-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/Mixtral-GQA-400m-v2</a>",
                            "\u24c2\ufe0f Mixtral",
                            2.01,
                            "28.45 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.0143,
                            159.0,
                            4083,
                            3816793.0,
                            1.61,
                            159.0,
                            4112,
                            4971
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/Mixtral-GQA-400m-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/Mixtral-GQA-400m-v2</a>",
                            "\u24c2\ufe0f Mixtral",
                            2.01,
                            "28.45 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0219,
                            143.0,
                            8095,
                            2801120.0,
                            1.8,
                            142.0,
                            8143,
                            8995
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00246,
                            421.0,
                            216,
                            15151515.0,
                            0.608,
                            421.0,
                            236,
                            1095
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-70m-deduped\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-70m-deduped</a>",
                            "GPT-NeoX",
                            0.1,
                            "28.44 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00257,
                            431.0,
                            397,
                            12836970.0,
                            0.594,
                            431.0,
                            427,
                            1280
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-28M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-28M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.44 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00277,
                            405.0,
                            320,
                            12690355.0,
                            0.632,
                            405.0,
                            356,
                            1208
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/budecosystem/boomer-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">budecosystem/boomer-1b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.0,
                            "28.44 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.004,
                            355.0,
                            2229,
                            5128205.0,
                            0.723,
                            354.0,
                            2250,
                            3108
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/budecosystem/boomer-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">budecosystem/boomer-1b</a>",
                            "\ud83e\udd99 LLaMA",
                            1.0,
                            "28.44 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0136,
                            206.0,
                            4446,
                            2680965.0,
                            1.25,
                            205.0,
                            4510,
                            5363
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-33M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-33M</a>",
                            "GPT-Neo",
                            0.03,
                            "28.41 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00194,
                            651.0,
                            366,
                            15797788.0,
                            0.394,
                            650.0,
                            419,
                            1271
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/SaylorTwift/gpt2_test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SaylorTwift/gpt2_test</a>",
                            "GPT-2",
                            0.14,
                            "28.40 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00413,
                            257.0,
                            621,
                            6622516.0,
                            0.998,
                            257.0,
                            717,
                            1569
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-8M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-8M</a>",
                            "GPT-Neo",
                            0.01,
                            "28.31 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00264,
                            397.0,
                            183,
                            13245033.0,
                            0.646,
                            396.0,
                            216,
                            1068
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/instructkr/ko-wand-136M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">instructkr/ko-wand-136M</a>",
                            "\u24c2\ufe0f Mistral",
                            0.14,
                            "28.29 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00327,
                            303.0,
                            347,
                            8196721.0,
                            0.844,
                            303.0,
                            383,
                            1242
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/instructkr/ko-wand-136M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">instructkr/ko-wand-136M</a>",
                            "\u24c2\ufe0f Mistral",
                            0.14,
                            "28.29 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00355,
                            311.0,
                            613,
                            7936507.0,
                            0.824,
                            311.0,
                            668,
                            1521
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00229,
                            436.0,
                            126,
                            15174506.0,
                            0.587,
                            436.0,
                            142,
                            1001
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/pythia-31m-simplewiki-2048</a>",
                            "GPT-NeoX",
                            0.03,
                            "28.27 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00232,
                            450.0,
                            221,
                            15037593.0,
                            0.569,
                            450.0,
                            236,
                            1089
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/roneneldan/TinyStories-3M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">roneneldan/TinyStories-3M</a>",
                            "GPT-Neo",
                            0.0,
                            "28.19 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00238,
                            415.0,
                            132,
                            13698630.0,
                            0.617,
                            415.0,
                            161,
                            1013
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-81M-tied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-81M-tied</a>",
                            "\ud83e\udd99 LLaMA",
                            0.08,
                            "28.17 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00224,
                            409.0,
                            232,
                            13642564.0,
                            0.625,
                            410.0,
                            276,
                            1135
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-81M-tied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-81M-tied</a>",
                            "\ud83e\udd99 LLaMA",
                            0.08,
                            "28.17 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00255,
                            479.0,
                            387,
                            11273957.0,
                            0.535,
                            479.0,
                            438,
                            1290
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-small</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "27.95 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00383,
                            276.0,
                            480,
                            8264462.0,
                            0.927,
                            276.0,
                            553,
                            1412
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/TurkuNLP/gpt3-finnish-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TurkuNLP/gpt3-finnish-small</a>",
                            "\ud83c\udf38 Bloom",
                            0.0,
                            "27.95 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00447,
                            277.0,
                            934,
                            6369426.0,
                            0.924,
                            277.0,
                            1035,
                            1888
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-111M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-111M</a>",
                            "GPT-2",
                            0.11,
                            "27.75 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00294,
                            352.0,
                            332,
                            11061946.0,
                            0.727,
                            352.0,
                            381,
                            1240
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/cerebras/Cerebras-GPT-111M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cerebras/Cerebras-GPT-111M</a>",
                            "GPT-2",
                            0.11,
                            "27.75 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00321,
                            357.0,
                            592,
                            8333333.0,
                            0.717,
                            357.0,
                            675,
                            1527
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248m</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "27.73 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00517,
                            201.0,
                            608,
                            5780346.0,
                            1.28,
                            200.0,
                            656,
                            1515
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248m</a>",
                            "\u24c2\ufe0f Mistral",
                            0.25,
                            "27.73 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.00559,
                            222.0,
                            1153,
                            5263157.0,
                            1.16,
                            221.0,
                            1199,
                            2051
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ai-forever/mGPT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ai-forever/mGPT</a>",
                            "GPT-2",
                            0.0,
                            "27.61 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00973,
                            133.0,
                            3147,
                            2469135.0,
                            1.93,
                            133.0,
                            3191,
                            4050
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/ai-forever/mGPT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ai-forever/mGPT</a>",
                            "GPT-2",
                            0.0,
                            "27.61 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0208,
                            99.2,
                            6184,
                            1345895.0,
                            2.59,
                            98.8,
                            6218,
                            7070
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/team-lucid/mptk-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">team-lucid/mptk-1b</a>",
                            "\ud83e\uddf1 MPT",
                            1.31,
                            "20.84 ",
                            "pytorch",
                            "float16",
                            "None",
                            "None",
                            0.00773,
                            192.0,
                            2834,
                            3144654.0,
                            1.34,
                            191.0,
                            2856,
                            3715
                        ],
                        [
                            "<a target=\"_blank\" href=\"https://huggingface.co/team-lucid/mptk-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">team-lucid/mptk-1b</a>",
                            "\ud83e\uddf1 MPT",
                            1.31,
                            "20.84 ",
                            "pytorch",
                            "float32",
                            "None",
                            "None",
                            0.0201,
                            142.0,
                            5657,
                            1968503.0,
                            1.82,
                            141.0,
                            5697,
                            6550
                        ]
                    ],
                    "metadata": null
                },
                "headers": [
                    "Model \ud83e\udd17",
                    "Arch \ud83c\udfdb\ufe0f",
                    "Params (B)",
                    "Open LLM Score (%)",
                    "DType \ud83d\udce5",
                    "Backend \ud83c\udfed",
                    "Optimization \ud83d\udee0\ufe0f",
                    "Quantization \ud83d\udddc\ufe0f",
                    "Prefill Latency (s)",
                    "Decode Throughput (tokens/s)",
                    "Allocated Memory (MB)",
                    "Energy (tokens/kWh)",
                    "E2E Latency (s)",
                    "E2E Throughput (tokens/s)",
                    "Reserved Memory (MB)",
                    "Used Memory (MB)"
                ],
                "row_count": [
                    1,
                    "dynamic"
                ],
                "col_count": [
                    16,
                    "dynamic"
                ],
                "datatype": [
                    "markdown",
                    "markdown",
                    "number",
                    "number",
                    "str",
                    "str",
                    "str",
                    "str",
                    "number",
                    "number",
                    "number",
                    "number",
                    "number",
                    "number",
                    "number",
                    "number"
                ],
                "type": "pandas",
                "latex_delimiters": [
                    {
                        "left": "$$",
                        "right": "$$",
                        "display": true
                    }
                ],
                "show_label": true,
                "height": 500,
                "min_width": 160,
                "visible": true,
                "elem_id": "table",
                "elem_classes": [],
                "wrap": false,
                "line_breaks": true,
                "column_widths": [],
                "name": "dataframe",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "dd8782b182950d6602be53e99ace6aa6",
            "api_info": {
                "properties": {
                    "headers": {
                        "items": {
                            "type": "string"
                        },
                        "title": "Headers",
                        "type": "array"
                    },
                    "data": {
                        "items": {
                            "items": {},
                            "type": "array"
                        },
                        "title": "Data",
                        "type": "array"
                    },
                    "metadata": {
                        "anyOf": [
                            {
                                "additionalProperties": {
                                    "anyOf": [
                                        {
                                            "items": {},
                                            "type": "array"
                                        },
                                        {
                                            "type": "null"
                                        }
                                    ]
                                },
                                "type": "object"
                            },
                            {
                                "type": "null"
                            }
                        ],
                        "default": null,
                        "title": "Metadata"
                    }
                },
                "required": [
                    "headers",
                    "data"
                ],
                "title": "DataframeData",
                "type": "object"
            },
            "example_inputs": {
                "headers": [
                    "a",
                    "b"
                ],
                "data": [
                    [
                        "foo",
                        "bar"
                    ]
                ]
            }
        },
        {
            "id": 86,
            "type": "html",
            "props": {
                "value": "\ud83d\udc46 Hover over the points \ud83d\udc46 for additional information. ",
                "show_label": true,
                "visible": true,
                "elem_id": "text",
                "elem_classes": [],
                "name": "html",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "0ef1a4eade65d710fcd9fc56f6586b0e",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "<p>Hello</p>"
        },
        {
            "id": 87,
            "type": "plot",
            "props": {
                "value": {
                    "type": "plotly",
                    "plot": "{\"data\":[{\"customdata\":[[\"rishiraj\\u002fCatPPT-base\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"None\",72.25,0.0314,49.5,15171,5.18,49.4],[\"mistralai\\u002fMistral-7B-v0.1\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"None\",60.97,0.0315,49.2,15171,5.21,49.1],[\"scb10x\\u002ftyphoon-7b\",\"\u24c2\ufe0f Mistral\",7.0,\"float16\",\"pytorch\",\"None\",\"None\",58.05,0.0318,49.3,15225,5.2,49.2],[\"stabilityai\\u002fjapanese-stablelm-base-gamma-7b\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"None\",52.59,0.0315,49.3,14701,5.2,49.2],[\"augmxnt\\u002fshisa-base-7b-v1\",\"\u24c2\ufe0f Mistral\",7.96,\"float16\",\"pytorch\",\"None\",\"None\",51.64,0.0328,47.4,16692,5.41,47.3],[\"vishesht27\\u002f22-Neuro_Model\",\"\u24c2\ufe0f Mistral\",7.24,\"float16\",\"pytorch\",\"None\",\"None\",50.23,0.0314,49.2,15171,5.21,49.1],[\"Locutusque\\u002fTinyMistral-248M-v2\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"None\",\"None\",28.78,0.00516,204.0,608,1.26,203.0],[\"Locutusque\\u002fTinyMistral-248M-v2\",\"\u24c2\ufe0f Mistral\",0.25,\"float32\",\"pytorch\",\"None\",\"None\",28.78,0.00552,228.0,1153,1.13,227.0],[\"instructkr\\u002fko-wand-136M\",\"\u24c2\ufe0f Mistral\",0.14,\"float16\",\"pytorch\",\"None\",\"None\",28.29,0.00327,303.0,347,0.844,303.0],[\"instructkr\\u002fko-wand-136M\",\"\u24c2\ufe0f Mistral\",0.14,\"float32\",\"pytorch\",\"None\",\"None\",28.29,0.00355,311.0,613,0.824,311.0],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",0.25,\"float16\",\"pytorch\",\"None\",\"None\",27.73,0.00517,201.0,608,1.28,200.0],[\"Locutusque\\u002fTinyMistral-248m\",\"\u24c2\ufe0f Mistral\",0.25,\"float32\",\"pytorch\",\"None\",\"None\",27.73,0.00559,222.0,1153,1.16,221.0]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\u24c2\ufe0f Mistral\",\"marker\":{\"color\":\"#FD3216\",\"size\":[15171,15171,15225,14701,16692,15171,608,1153,347,613,608,1153],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\u24c2\ufe0f Mistral\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[5.18,5.21,5.2,5.2,5.41,5.21,1.26,1.13,0.844,0.824,1.28,1.16],\"xaxis\":\"x\",\"y\":[72.25,60.97,58.05,52.59,51.64,50.23,28.78,28.78,28.29,28.29,27.73,27.73],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"upstage\\u002fSOLAR-10.7B-v1.0\",\"\ud83e\udd99 LLaMA\",10.73,\"float16\",\"pytorch\",\"None\",\"None\",66.04,0.0452,33.4,21781,7.69,33.3],[\"seungduk\\u002fKoSOLAR-10.7B-v0.1\",\"\ud83e\udd99 LLaMA\",10.86,\"float16\",\"pytorch\",\"None\",\"None\",66.04,0.0453,33.3,22045,7.71,33.2],[\"01-ai\\u002fYi-6B\",\"\ud83e\udd99 LLaMA\",6.06,\"float32\",\"pytorch\",\"None\",\"None\",54.08,0.0866,33.8,24537,7.64,33.5],[\"itsliupeng\\u002fopenllama-7b-icl\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"None\",47.93,0.0282,52.8,14056,4.86,52.7],[\"golaxy\\u002fgowizardlm\",\"\ud83e\udd99 LLaMA\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",47.64,0.0287,51.9,14653,4.94,51.8],[\"itsliupeng\\u002fopenllama-7b-base\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"None\",47.09,0.0282,52.7,14056,4.87,52.6],[\"DevaMalla\\u002fllama-base-7b\",\"\ud83e\udd99 LLaMA\",6.61,\"float16\",\"pytorch\",\"None\",\"None\",45.62,0.0279,52.9,14056,4.85,52.8],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"None\",44.75,0.0159,102.0,6401,2.52,102.0],[\"GeneZC\\u002fMiniMA-2-3B\",\"\ud83e\udd99 LLaMA\",3.0,\"float32\",\"pytorch\",\"None\",\"None\",44.75,0.0428,60.6,12793,4.25,60.2],[\"openlm-research\\u002fopen_llama_7b_v2\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"None\",44.26,0.0283,52.7,14056,4.87,52.6],[\"cyberagent\\u002fcalm2-7b-chat\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"None\",43.27,0.0287,51.9,15101,4.94,51.8],[\"openlm-research\\u002fopen_llama_7b\",\"\ud83e\udd99 LLaMA\",7.0,\"float16\",\"pytorch\",\"None\",\"None\",42.31,0.0282,52.9,14056,4.85,52.8],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",3.02,\"float16\",\"pytorch\",\"None\",\"None\",41.44,0.0159,102.0,6401,2.52,102.0],[\"GeneZC\\u002fMiniMA-3B\",\"\ud83e\udd99 LLaMA\",3.02,\"float32\",\"pytorch\",\"None\",\"None\",41.44,0.0427,60.6,12793,4.25,60.2],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"None\",40.28,0.0185,90.1,7280,2.85,89.8],[\"openlm-research\\u002fopen_llama_3b_v2\",\"\ud83e\udd99 LLaMA\",3.0,\"float32\",\"pytorch\",\"None\",\"None\",40.28,0.054,52.7,14582,4.89,52.4],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",3.0,\"float16\",\"pytorch\",\"None\",\"None\",38.26,0.0185,90.4,7280,2.84,90.1],[\"openlm-research\\u002fopen_llama_3b\",\"\ud83e\udd99 LLaMA\",3.0,\"float32\",\"pytorch\",\"None\",\"None\",38.26,0.054,52.7,14582,4.89,52.4],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"None\",36.42,0.00798,121.0,2279,2.11,121.0],[\"TinyLlama\\u002fTinyLlama-1.1B-intermediate-step-1431k-3T\",\"\ud83e\udd99 LLaMA\",1.1,\"float32\",\"pytorch\",\"None\",\"None\",36.42,0.0182,128.0,4492,2.02,127.0],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",1.3,\"float16\",\"pytorch\",\"None\",\"None\",35.71,0.00882,131.0,2991,1.95,131.0],[\"Dans-DiscountModels\\u002fShearedLlama-1.3b-FFT-Test1\",\"\ud83e\udd99 LLaMA\",1.3,\"float32\",\"pytorch\",\"None\",\"None\",35.71,0.0225,105.0,5969,2.46,104.0],[\"bn22\\u002ftinyllama_frankenmerge\",\"\ud83e\udd99 LLaMA\",1.54,\"float16\",\"pytorch\",\"None\",\"None\",34.64,0.0112,87.3,3168,2.93,87.4],[\"bn22\\u002ftinyllama_frankenmerge\",\"\ud83e\udd99 LLaMA\",1.54,\"float32\",\"pytorch\",\"None\",\"None\",34.64,0.0268,89.5,6279,2.88,88.9],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",1.03,\"float16\",\"pytorch\",\"None\",\"None\",34.37,0.00796,122.0,2279,2.1,122.0],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-480k-1T\",\"\ud83e\udd99 LLaMA\",1.03,\"float32\",\"pytorch\",\"None\",\"None\",34.37,0.0187,126.0,4492,2.05,125.0],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"None\",33.72,0.00818,118.0,2279,2.18,117.0],[\"PY007\\u002fTinyLlama-1.1B-intermediate-step-240k-503b\",\"\ud83e\udd99 LLaMA\",1.1,\"float32\",\"pytorch\",\"None\",\"None\",33.72,0.0187,126.0,4492,2.04,125.0],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",1.1,\"float16\",\"pytorch\",\"None\",\"None\",31.86,0.00823,118.0,2279,2.18,117.0],[\"PY007\\u002fTinyLlama-1.1B-step-50K-105b\",\"\ud83e\udd99 LLaMA\",1.1,\"float32\",\"pytorch\",\"None\",\"None\",31.86,0.0185,127.0,4492,2.03,126.0],[\"ahxt\\u002fLiteLlama-460M-1T\",\"\ud83e\udd99 LLaMA\",0.46,\"float32\",\"pytorch\",\"None\",\"None\",30.16,0.01,128.0,1936,2.01,127.0],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"None\",29.44,0.00388,245.0,500,1.04,246.0],[\"BEE-spoke-data\\u002fsmol_llama-220M-GQA\",\"\ud83e\udd99 LLaMA\",0.22,\"float32\",\"pytorch\",\"None\",\"None\",29.44,0.00439,292.0,932,0.878,292.0],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",0.22,\"float16\",\"pytorch\",\"None\",\"None\",29.23,0.00391,245.0,500,1.04,246.0],[\"BEE-spoke-data\\u002fNanoLlama-GQA-L10-A32_KV8-v13-KI\",\"\ud83e\udd99 LLaMA\",0.22,\"float32\",\"pytorch\",\"None\",\"None\",29.23,0.00439,294.0,932,0.871,294.0],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",0.1,\"float16\",\"pytorch\",\"None\",\"None\",28.97,0.00232,401.0,269,0.638,401.0],[\"BEE-spoke-data\\u002fsmol_llama-101M-GQA\",\"\ud83e\udd99 LLaMA\",0.1,\"float32\",\"pytorch\",\"None\",\"None\",28.97,0.00249,462.0,456,0.554,462.0],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",0.06,\"float32\",\"pytorch\",\"None\",\"None\",28.7,0.00203,494.0,288,0.518,494.0],[\"BEE-spoke-data\\u002fverysmol_llama-v11-KIx2\",\"\ud83e\udd99 LLaMA\",0.06,\"float16\",\"pytorch\",\"None\",\"None\",28.7,0.00214,416.0,180,0.615,416.0],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",1.0,\"float16\",\"pytorch\",\"None\",\"None\",28.44,0.004,355.0,2229,0.723,354.0],[\"budecosystem\\u002fboomer-1b\",\"\ud83e\udd99 LLaMA\",1.0,\"float32\",\"pytorch\",\"None\",\"None\",28.44,0.0136,206.0,4446,1.25,205.0],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",0.08,\"float16\",\"pytorch\",\"None\",\"None\",28.17,0.00224,409.0,232,0.625,410.0],[\"BEE-spoke-data\\u002fsmol_llama-81M-tied\",\"\ud83e\udd99 LLaMA\",0.08,\"float32\",\"pytorch\",\"None\",\"None\",28.17,0.00255,479.0,387,0.535,479.0]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83e\udd99 LLaMA\",\"marker\":{\"color\":\"#00FE35\",\"size\":[21781,22045,24537,14056,14653,14056,14056,6401,12793,14056,15101,14056,6401,12793,7280,14582,7280,14582,2279,4492,2991,5969,3168,6279,2279,4492,2279,4492,2279,4492,1936,500,932,500,932,269,456,288,180,2229,4446,232,387],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83e\udd99 LLaMA\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[7.69,7.71,7.64,4.86,4.94,4.87,4.85,2.52,4.25,4.87,4.94,4.85,2.52,4.25,2.85,4.89,2.84,4.89,2.11,2.02,1.95,2.46,2.93,2.88,2.1,2.05,2.18,2.04,2.18,2.03,2.01,1.04,0.878,1.04,0.871,0.638,0.554,0.518,0.615,0.723,1.25,0.625,0.535],\"xaxis\":\"x\",\"y\":[66.04,66.04,54.08,47.93,47.64,47.09,45.62,44.75,44.75,44.26,43.27,42.31,41.44,41.44,40.28,40.28,38.26,38.26,36.42,36.42,35.71,35.71,34.64,34.64,34.37,34.37,33.72,33.72,31.86,31.86,30.16,29.44,29.44,29.23,29.23,28.97,28.97,28.7,28.7,28.44,28.44,28.17,28.17],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"Deci\\u002fDeciLM-7B\",\"\ud83d\udd35 deci\",7.04,\"float16\",\"pytorch\",\"None\",\"None\",61.55,0.0305,51.3,14290,5.0,51.2]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83d\udd35 deci\",\"marker\":{\"color\":\"#6A76FC\",\"size\":[14290],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83d\udd35 deci\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[5.0],\"xaxis\":\"x\",\"y\":[61.55],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"microsoft\\u002fphi-2\",\"phi\",2.78,\"float32\",\"pytorch\",\"None\",\"None\",61.33,0.0436,69.9,11951,3.69,69.4],[\"microsoft\\u002fphi-1_5\",\"phi\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",47.69,0.00986,116.0,1304,2.2,116.0],[\"microsoft\\u002fphi-1_5\",\"phi\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",47.69,0.0102,111.0,3057,2.3,111.0],[\"microsoft\\u002fphi-1_5\",\"phi\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",47.69,0.0141,105.0,1269,2.45,104.0],[\"microsoft\\u002fphi-1_5\",\"phi\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",47.69,0.0227,123.0,6098,2.1,122.0],[\"microsoft\\u002fphi-1_5\",\"phi\",0.0,\"float16\",\"pytorch\",\"FlashAttentionV2\",\"BnB.8bit\",47.69,0.0311,32.6,1848,7.86,32.6]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"phi\",\"marker\":{\"color\":\"#FED4C4\",\"size\":[11951,1304,3057,1269,6098,1848],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"phi\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[3.69,2.2,2.3,2.45,2.1,7.86],\"xaxis\":\"x\",\"y\":[61.33,47.69,47.69,47.69,47.69,47.69],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"stabilityai\\u002fstablelm-3b-4e1t\",\"\ud83d\udd34 StableLM-Epoch\",2.8,\"float32\",\"pytorch\",\"None\",\"None\",46.58,0.0413,68.4,12068,3.77,67.9]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83d\udd34 StableLM-Epoch\",\"marker\":{\"color\":\"#FE00CE\",\"size\":[12068],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83d\udd34 StableLM-Epoch\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[3.77],\"xaxis\":\"x\",\"y\":[46.58],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"tiiuae\\u002ffalcon-7b\",\"\ud83e\udd85 Falcon\",7.0,\"float16\",\"pytorch\",\"None\",\"None\",44.17,0.0343,43.2,13945,5.93,43.2],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",1.0,\"float16\",\"pytorch\",\"None\",\"None\",37.07,0.00885,153.0,2834,1.68,152.0],[\"tiiuae\\u002ffalcon-rw-1b\",\"\ud83e\udd85 Falcon\",1.0,\"float32\",\"pytorch\",\"None\",\"None\",37.07,0.0204,132.0,5659,1.95,131.0]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83e\udd85 Falcon\",\"marker\":{\"color\":\"#0DF9FF\",\"size\":[13945,2834,5659],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83e\udd85 Falcon\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[5.93,1.68,1.95],\"xaxis\":\"x\",\"y\":[44.17,37.07,37.07],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"togethercomputer\\u002fRedPajama-INCITE-7B-Base\",\"GPT-NeoX\",7.0,\"float16\",\"pytorch\",\"None\",\"None\",41.49,0.0294,50.9,14429,5.04,50.8],[\"togethercomputer\\u002fRedPajama-INCITE-Base-7B-v0.1\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"None\",41.25,0.0293,51.0,14429,5.03,50.9],[\"EleutherAI\\u002fpythia-12b-deduped\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"None\",\"None\",39.7,0.0475,32.6,24619,7.87,32.5],[\"EleutherAI\\u002fpythia-6.9b-deduped\",\"GPT-NeoX\",6.9,\"float16\",\"pytorch\",\"None\",\"None\",39.3,0.0295,50.9,14403,5.04,50.8],[\"EleutherAI\\u002fpythia-12b\",\"GPT-NeoX\",12.0,\"float16\",\"pytorch\",\"None\",\"None\",38.82,0.0483,32.5,24619,7.9,32.4],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"None\",38.54,0.0174,88.2,6068,2.91,88.0],[\"togethercomputer\\u002fRedPajama-INCITE-Base-3B-v1\",\"GPT-NeoX\",3.0,\"float32\",\"pytorch\",\"None\",\"None\",38.54,0.0415,60.7,12028,4.24,60.4],[\"EleutherAI\\u002fpythia-6.7b\",\"GPT-NeoX\",6.65,\"float16\",\"pytorch\",\"None\",\"None\",38.06,0.0287,51.2,14403,5.01,51.1],[\"Dampish\\u002fStellarX-4B-V0\",\"GPT-NeoX\",4.0,\"float32\",\"pytorch\",\"None\",\"None\",37.31,0.0599,42.1,17177,6.11,41.9],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"None\",37.09,0.017,88.5,6052,2.9,88.3],[\"EleutherAI\\u002fpythia-2.7b\",\"GPT-NeoX\",2.91,\"float32\",\"pytorch\",\"None\",\"None\",37.09,0.0418,61.2,11995,4.21,60.8],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"float16\",\"pytorch\",\"None\",\"None\",36.72,0.0167,90.1,6052,2.85,89.8],[\"EleutherAI\\u002fpythia-2.8b-deduped\",\"GPT-NeoX\",2.91,\"float32\",\"pytorch\",\"None\",\"None\",36.72,0.0419,61.2,11995,4.21,60.8],[\"Dampish\\u002fStellarX-4B-V0.2\",\"GPT-NeoX\",4.0,\"float32\",\"pytorch\",\"None\",\"None\",36.15,0.0413,60.9,12028,4.23,60.5],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"float16\",\"pytorch\",\"None\",\"None\",35.0,0.00996,120.0,3147,2.13,120.0],[\"EleutherAI\\u002fpythia-1.4b-deduped\",\"GPT-NeoX\",1.4,\"float32\",\"pytorch\",\"None\",\"None\",35.0,0.0221,104.0,6186,2.48,103.0],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"float16\",\"pytorch\",\"None\",\"None\",34.46,0.0099,121.0,3147,2.12,121.0],[\"EleutherAI\\u002fpythia-1.3b\",\"GPT-NeoX\",1.31,\"float32\",\"pytorch\",\"None\",\"None\",34.46,0.0218,105.0,6186,2.44,105.0],[\"NYTK\\u002fPULI-GPTrio\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",34.42,0.0306,48.9,16060,5.24,48.9],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"float16\",\"pytorch\",\"None\",\"None\",32.78,0.00678,175.0,2243,1.47,174.0],[\"EleutherAI\\u002fpythia-1b-deduped\",\"GPT-NeoX\",1.08,\"float32\",\"pytorch\",\"None\",\"None\",32.78,0.0155,152.0,4411,1.7,151.0],[\"rinna\\u002fbilingual-gpt-neox-4b-8k\",\"GPT-NeoX\",3.95,\"float32\",\"pytorch\",\"None\",\"None\",32.23,0.0553,47.1,16325,5.47,46.8],[\"rinna\\u002fbilingual-gpt-neox-4b\",\"GPT-NeoX\",3.95,\"float32\",\"pytorch\",\"None\",\"None\",32.14,0.0552,47.4,16325,5.44,47.1],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"None\",31.55,0.00899,116.0,1028,2.21,116.0],[\"EleutherAI\\u002fpythia-410m\",\"GPT-NeoX\",0.51,\"float32\",\"pytorch\",\"None\",\"None\",31.55,0.00953,123.0,1948,2.08,123.0],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"float16\",\"pytorch\",\"None\",\"None\",31.5,0.0148,98.8,7829,2.59,98.8],[\"stabilityai\\u002fstablelm-base-alpha-3b\",\"GPT-NeoX\",3.0,\"float32\",\"pytorch\",\"None\",\"None\",31.5,0.0457,55.6,15380,4.64,55.2],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",0.51,\"float16\",\"pytorch\",\"None\",\"None\",31.29,0.00859,120.0,1028,2.13,120.0],[\"EleutherAI\\u002fpythia-410m-deduped\",\"GPT-NeoX\",0.51,\"float32\",\"pytorch\",\"None\",\"None\",31.29,0.00944,120.0,1948,2.14,120.0],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"None\",29.38,0.00453,230.0,458,1.11,231.0],[\"EleutherAI\\u002fpythia-160m-deduped\",\"GPT-NeoX\",0.21,\"float32\",\"pytorch\",\"None\",\"None\",29.38,0.00474,230.0,828,1.11,231.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",29.15,0.00226,437.0,221,0.586,437.0],[\"pszemraj\\u002fpythia-31m-KI_v1-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"None\",29.15,0.00237,424.0,126,0.603,425.0],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"float32\",\"pytorch\",\"None\",\"None\",29.02,0.00468,226.0,828,1.13,227.0],[\"EleutherAI\\u002fpythia-160m\",\"GPT-NeoX\",0.21,\"float16\",\"pytorch\",\"None\",\"None\",29.02,0.00468,222.0,458,1.15,223.0],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",28.88,0.00898,118.0,1976,2.18,117.0],[\"cyberagent\\u002fopen-calm-large\",\"GPT-NeoX\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",28.88,0.0152,119.0,3836,2.17,118.0],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",28.85,0.00229,449.0,221,0.57,449.0],[\"pszemraj\\u002fpythia-31m-goodwiki-deduped-2048-scratch\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"None\",28.85,0.0023,423.0,126,0.605,423.0],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",28.81,0.00236,431.0,221,0.593,432.0],[\"ethzanalytics\\u002fpythia-31m\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"None\",28.81,0.00241,416.0,126,0.615,416.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",28.61,0.00229,455.0,221,0.562,456.0],[\"pszemraj\\u002fpythia-31m-simplewiki-scratch-bf16\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"None\",28.61,0.00233,441.0,126,0.58,441.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"None\",28.6,0.00231,440.0,126,0.582,440.0],[\"pszemraj\\u002fpythia-31m-simplepile-lite-2048-scratch-2e\",\"GPT-NeoX\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",28.6,0.00234,450.0,221,0.569,450.0],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"float16\",\"pytorch\",\"None\",\"None\",28.44,0.00246,421.0,216,0.608,421.0],[\"EleutherAI\\u002fpythia-70m-deduped\",\"GPT-NeoX\",0.1,\"float32\",\"pytorch\",\"None\",\"None\",28.44,0.00257,431.0,397,0.594,431.0],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"float16\",\"pytorch\",\"None\",\"None\",28.27,0.00229,436.0,126,0.587,436.0],[\"pszemraj\\u002fpythia-31m-simplewiki-2048\",\"GPT-NeoX\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",28.27,0.00232,450.0,221,0.569,450.0]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"GPT-NeoX\",\"marker\":{\"color\":\"#F6F926\",\"size\":[14429,14429,24619,14403,24619,6068,12028,14403,17177,6052,11995,6052,11995,12028,3147,6186,3147,6186,16060,2243,4411,16325,16325,1028,1948,7829,15380,1028,1948,458,828,221,126,828,458,1976,3836,221,126,221,126,221,126,126,221,216,397,126,221],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"GPT-NeoX\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[5.04,5.03,7.87,5.04,7.9,2.91,4.24,5.01,6.11,2.9,4.21,2.85,4.21,4.23,2.13,2.48,2.12,2.44,5.24,1.47,1.7,5.47,5.44,2.21,2.08,2.59,4.64,2.13,2.14,1.11,1.11,0.586,0.603,1.13,1.15,2.18,2.17,0.57,0.605,0.593,0.615,0.562,0.58,0.582,0.569,0.608,0.594,0.587,0.569],\"xaxis\":\"x\",\"y\":[41.49,41.25,39.7,39.3,38.82,38.54,38.54,38.06,37.31,37.09,37.09,36.72,36.72,36.15,35.0,35.0,34.46,34.46,34.42,32.78,32.78,32.23,32.14,31.55,31.55,31.5,31.5,31.29,31.29,29.38,29.38,29.15,29.15,29.02,29.02,28.88,28.88,28.85,28.85,28.81,28.81,28.61,28.61,28.6,28.6,28.44,28.44,28.27,28.27],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"EleutherAI\\u002fgpt-j-6b\",\"GPT-J\",6.0,\"float16\",\"pytorch\",\"None\",\"None\",40.1,0.0263,53.8,12721,4.77,53.7]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"GPT-J\",\"marker\":{\"color\":\"#FF9616\",\"size\":[12721],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"GPT-J\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[4.77],\"xaxis\":\"x\",\"y\":[40.1],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"Salesforce\\u002fcodegen-6B-nl\",\"\u267e\ufe0f CodeGen\",6.0,\"float16\",\"pytorch\",\"None\",\"None\",40.0,0.0318,45.8,14855,5.6,45.7]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\u267e\ufe0f CodeGen\",\"marker\":{\"color\":\"#479B55\",\"size\":[14855],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\u267e\ufe0f CodeGen\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[5.6],\"xaxis\":\"x\",\"y\":[40.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bigscience\\u002fbloom-7b1\",\"\ud83c\udf38 Bloom\",7.07,\"float16\",\"pytorch\",\"None\",\"None\",39.18,0.0288,56.0,14651,4.58,55.9],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",3.0,\"float16\",\"pytorch\",\"None\",\"None\",36.07,0.0163,108.0,6365,2.39,107.0],[\"bigscience\\u002fbloom-3b\",\"\ud83c\udf38 Bloom\",3.0,\"float32\",\"pytorch\",\"None\",\"None\",36.07,0.0441,65.7,12734,3.92,65.3],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",29.11,0.00741,142.0,1939,1.8,142.0],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"AWQ.4bit+GEMM\",29.11,0.00998,136.0,942,1.89,135.0],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit+ExllamaV1\",29.11,0.01,151.0,962,1.7,151.0],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"GPTQ.4bit\",29.11,0.0153,119.0,943,2.16,119.0],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",29.11,0.0161,147.0,3893,1.75,146.0],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"BnB.4bit\",29.11,0.0192,68.5,941,3.74,68.4],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",27.95,0.00383,276.0,480,0.927,276.0],[\"TurkuNLP\\u002fgpt3-finnish-small\",\"\ud83c\udf38 Bloom\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",27.95,0.00447,277.0,934,0.924,277.0]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83c\udf38 Bloom\",\"marker\":{\"color\":\"#EEA6FB\",\"size\":[14651,6365,12734,1939,942,962,943,3893,941,480,934],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83c\udf38 Bloom\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[4.58,2.39,3.92,1.8,1.89,1.7,2.16,1.75,3.74,0.927,0.924],\"xaxis\":\"x\",\"y\":[39.18,36.07,36.07,29.11,29.11,29.11,29.11,29.11,29.11,27.95,27.95],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"cerebras\\u002fCerebras-GPT-6.7B\",\"GPT-2\",6.7,\"float16\",\"pytorch\",\"None\",\"None\",36.27,0.0268,53.6,13997,4.79,53.4],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",35.18,0.0201,70.6,10607,3.63,70.5],[\"Writer\\u002fpalmyra-base\",\"GPT-2\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",35.18,0.0553,39.7,21105,6.48,39.5],[\"gpt2-xl\",\"GPT-2\",1.61,\"float32\",\"pytorch\",\"None\",\"None\",34.38,0.0293,69.5,7061,3.7,69.2],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",1.44,\"float16\",\"pytorch\",\"None\",\"None\",34.31,0.00882,152.0,2999,1.69,151.0],[\"AI-Sweden-Models\\u002fgpt-sw3-1.3b\",\"GPT-2\",1.44,\"float32\",\"pytorch\",\"None\",\"None\",34.31,0.0191,103.0,5888,2.49,103.0],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",2.7,\"float16\",\"pytorch\",\"None\",\"None\",33.25,0.0158,97.0,5799,2.65,96.6],[\"cerebras\\u002fCerebras-GPT-2.7B\",\"GPT-2\",2.7,\"float32\",\"pytorch\",\"None\",\"None\",33.25,0.0382,61.9,11488,4.16,61.5],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",1.3,\"float16\",\"pytorch\",\"None\",\"None\",31.3,0.00891,152.0,2943,1.69,151.0],[\"cerebras\\u002fCerebras-GPT-1.3B\",\"GPT-2\",1.3,\"float32\",\"pytorch\",\"None\",\"None\",31.3,0.0185,106.0,5776,2.43,105.0],[\"AI-Sweden-Models\\u002fgpt-sw3-356m\",\"GPT-2\",0.47,\"float32\",\"pytorch\",\"None\",\"None\",30.41,0.00851,156.0,1792,1.64,156.0],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",0.38,\"float16\",\"pytorch\",\"None\",\"None\",30.4,0.00741,137.0,844,1.87,137.0],[\"robowaifudev\\u002fmegatron-gpt2-345m\",\"GPT-2\",0.38,\"float32\",\"pytorch\",\"None\",\"None\",30.4,0.00882,142.0,1656,1.8,142.0],[\"cerebras\\u002fCerebras-GPT-256M\",\"GPT-2\",0.26,\"float16\",\"pytorch\",\"None\",\"None\",29.38,0.00395,266.0,665,0.963,266.0],[\"cerebras\\u002fCerebras-GPT-256M\",\"GPT-2\",0.26,\"float32\",\"pytorch\",\"None\",\"None\",29.38,0.00598,250.0,1257,1.03,249.0],[\"gpt2\",\"GPT-2\",0.14,\"float32\",\"pytorch\",\"None\",\"None\",28.53,0.00418,252.0,621,1.01,253.0],[\"AI-Sweden-Models\\u002fgpt-sw3-126m\",\"GPT-2\",0.19,\"float32\",\"pytorch\",\"None\",\"None\",28.45,0.00387,287.0,721,0.894,286.0],[\"SaylorTwift\\u002fgpt2_test\",\"GPT-2\",0.14,\"float32\",\"pytorch\",\"None\",\"None\",28.4,0.00413,257.0,621,0.998,257.0],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",0.11,\"float16\",\"pytorch\",\"None\",\"None\",27.75,0.00294,352.0,332,0.727,352.0],[\"cerebras\\u002fCerebras-GPT-111M\",\"GPT-2\",0.11,\"float32\",\"pytorch\",\"None\",\"None\",27.75,0.00321,357.0,592,0.717,357.0],[\"ai-forever\\u002fmGPT\",\"GPT-2\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",27.61,0.00973,133.0,3147,1.93,133.0],[\"ai-forever\\u002fmGPT\",\"GPT-2\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",27.61,0.0208,99.2,6184,2.59,98.8]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"GPT-2\",\"marker\":{\"color\":\"#DC587D\",\"size\":[13997,10607,21105,7061,2999,5888,5799,11488,2943,5776,1792,844,1656,665,1257,621,721,621,332,592,3147,6184],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"GPT-2\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[4.79,3.63,6.48,3.7,1.69,2.49,2.65,4.16,1.69,2.43,1.64,1.87,1.8,0.963,1.03,1.01,0.894,0.998,0.727,0.717,1.93,2.59],\"xaxis\":\"x\",\"y\":[36.27,35.18,35.18,34.38,34.31,34.31,33.25,33.25,31.3,31.3,30.41,30.4,30.4,29.38,29.38,28.53,28.45,28.4,27.75,27.75,27.61,27.61],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"EleutherAI\\u002fgpt-neo-2.7B\",\"GPT-Neo\",2.72,\"float32\",\"pytorch\",\"None\",\"None\",36.2,0.0418,63.4,11555,4.06,63.1],[\"EleutherAI\\u002fgpt-neo-1.3B\",\"GPT-Neo\",1.37,\"float32\",\"pytorch\",\"None\",\"None\",33.58,0.0223,110.0,5775,2.33,110.0],[\"EleutherAI\\u002fgpt-neo-125m\",\"GPT-Neo\",0.15,\"float32\",\"pytorch\",\"None\",\"None\",29.47,0.00429,270.0,658,0.948,270.0],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",29.28,0.0102,126.0,3048,2.04,125.0],[\"bit-dny\\u002fMindLLM\",\"GPT-Neo\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",29.28,0.023,107.0,5980,2.4,107.0],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",29.14,0.00233,409.0,112,0.625,410.0],[\"roneneldan\\u002fTinyStories-1M\",\"GPT-Neo\",0.0,\"float16\",\"pytorch\",\"None\",\"None\",29.14,0.00255,384.0,77,0.667,384.0],[\"roneneldan\\u002fTinyStories-28M\",\"GPT-Neo\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",28.44,0.00277,405.0,320,0.632,405.0],[\"roneneldan\\u002fTinyStories-33M\",\"GPT-Neo\",0.03,\"float32\",\"pytorch\",\"None\",\"None\",28.41,0.00194,651.0,366,0.394,650.0],[\"roneneldan\\u002fTinyStories-8M\",\"GPT-Neo\",0.01,\"float32\",\"pytorch\",\"None\",\"None\",28.31,0.00264,397.0,183,0.646,396.0],[\"roneneldan\\u002fTinyStories-3M\",\"GPT-Neo\",0.0,\"float32\",\"pytorch\",\"None\",\"None\",28.19,0.00238,415.0,132,0.617,415.0]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"GPT-Neo\",\"marker\":{\"color\":\"#D626FF\",\"size\":[11555,5775,658,3048,5980,112,77,320,366,183,132],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"GPT-Neo\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[4.06,2.33,0.948,2.04,2.4,0.625,0.667,0.632,0.394,0.646,0.617],\"xaxis\":\"x\",\"y\":[36.2,33.58,29.47,29.28,29.28,29.14,29.14,28.44,28.41,28.31,28.19],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"facebook\\u002fxglm-4.5B\",\"XGLM\",5.08,\"float32\",\"pytorch\",\"None\",\"None\",34.31,0.063,40.3,19042,6.39,40.1]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"XGLM\",\"marker\":{\"color\":\"#6E899C\",\"size\":[19042],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"XGLM\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[6.39],\"xaxis\":\"x\",\"y\":[34.31],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"RWKV\\u002frwkv-4-430m-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",0.43,\"float32\",\"pytorch\",\"None\",\"None\",30.45,0.0106,101.0,1784,2.54,101.0],[\"RWKV\\u002frwkv-4-169m-pile\",\"\ud83d\udc26\u200d\u2b1b RWKV\",0.17,\"float32\",\"pytorch\",\"None\",\"None\",28.64,0.00568,195.0,740,1.32,194.0]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83d\udc26\u200d\u2b1b RWKV\",\"marker\":{\"color\":\"#00B5F7\",\"size\":[1784,740],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83d\udc26\u200d\u2b1b RWKV\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[2.54,1.32],\"xaxis\":\"x\",\"y\":[30.45,28.64],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",0.16,\"float16\",\"pytorch\",\"None\",\"None\",29.41,0.0044,243.0,446,1.05,244.0],[\"bigcode\\u002ftiny_starcoder_py\",\"\u2b50 StarCoder\",0.16,\"float32\",\"pytorch\",\"None\",\"None\",29.41,0.00443,252.0,804,1.01,253.0],[\"bigcode\\u002fgpt_bigcode-santacoder\",\"\u2b50 StarCoder\",1.12,\"float32\",\"pytorch\",\"None\",\"None\",28.49,0.0176,166.0,4628,1.56,164.0]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\u2b50 StarCoder\",\"marker\":{\"color\":\"#B68E00\",\"size\":[446,804,4628],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\u2b50 StarCoder\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.05,1.01,1.56],\"xaxis\":\"x\",\"y\":[29.41,29.41,28.49],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"chargoddard\\u002fSmolLlamix-8x101M-take2\",\"\u24c2\ufe0f Mixtral\",0.4,\"float16\",\"pytorch\",\"None\",\"None\",29.35,0.0119,172.0,894,1.49,172.0],[\"chargoddard\\u002fSmolLlamix-8x101M-take2\",\"\u24c2\ufe0f Mixtral\",0.4,\"float32\",\"pytorch\",\"None\",\"None\",29.35,0.0127,181.0,1697,1.42,180.0],[\"chargoddard\\u002fSmolLlamix-8x101M\",\"\u24c2\ufe0f Mixtral\",0.4,\"float16\",\"pytorch\",\"None\",\"None\",28.98,0.0118,168.0,895,1.53,167.0],[\"chargoddard\\u002fSmolLlamix-8x101M\",\"\u24c2\ufe0f Mixtral\",0.4,\"float32\",\"pytorch\",\"None\",\"None\",28.98,0.0126,176.0,1697,1.46,175.0],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",2.01,\"float16\",\"pytorch\",\"None\",\"None\",28.45,0.0143,159.0,4083,1.61,159.0],[\"BEE-spoke-data\\u002fMixtral-GQA-400m-v2\",\"\u24c2\ufe0f Mixtral\",2.01,\"float32\",\"pytorch\",\"None\",\"None\",28.45,0.0219,143.0,8095,1.8,142.0]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\u24c2\ufe0f Mixtral\",\"marker\":{\"color\":\"#C9FBE5\",\"size\":[894,1697,895,1697,4083,8095],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\u24c2\ufe0f Mixtral\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.49,1.42,1.53,1.46,1.61,1.8],\"xaxis\":\"x\",\"y\":[29.35,29.35,28.98,28.98,28.45,28.45],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"team-lucid\\u002fmptk-1b\",\"\ud83e\uddf1 MPT\",1.31,\"float16\",\"pytorch\",\"None\",\"None\",20.84,0.00773,192.0,2834,1.34,191.0],[\"team-lucid\\u002fmptk-1b\",\"\ud83e\uddf1 MPT\",1.31,\"float32\",\"pytorch\",\"None\",\"None\",20.84,0.0201,142.0,5657,1.82,141.0]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eAllocated Memory (MB):\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eE2E Latency (s):\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003eE2E Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[12]}\",\"legendgroup\":\"\ud83e\uddf1 MPT\",\"marker\":{\"color\":\"#FF0092\",\"size\":[2834,5657],\"sizemode\":\"area\",\"sizeref\":61.5475,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\ud83e\uddf1 MPT\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.34,1.82],\"xaxis\":\"x\",\"y\":[20.84,20.84],\"yaxis\":\"y\",\"type\":\"scatter\"}],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Time To Generate 256 Tokens (s)\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Open LLM Score (%)\"}},\"legend\":{\"title\":{\"text\":\"LLM Architecture\"},\"tracegroupgap\":0,\"itemsizing\":\"constant\"},\"margin\":{\"t\":60},\"title\":{\"text\":\"Latency vs. Score vs. Memory\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":600}}"
                },
                "show_label": false,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "plot",
                "elem_classes": [],
                "name": "plot",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "12c46a60185f5c11b6b6b6f9adbe826c",
            "api_info": {
                "properties": {
                    "type": {
                        "enum": [
                            "altair",
                            "bokeh",
                            "plotly",
                            "matplotlib"
                        ],
                        "title": "Type",
                        "type": "string"
                    },
                    "plot": {
                        "title": "Plot",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "plot"
                ],
                "title": "PlotData",
                "type": "object"
            },
            "example_inputs": null
        },
        {
            "id": 88,
            "type": "tabitem",
            "props": {
                "label": "BetterTransformer \ud83d\udcc8",
                "id": 2,
                "name": "tabitem"
            },
            "skip_api": true,
            "component_class_id": "f72faf2b8abb615823b199272218625a"
        },
        {
            "id": 89,
            "type": "html",
            "props": {
                "value": "\ud83d\udc46 Hover over the points \ud83d\udc46 for additional information.",
                "show_label": true,
                "visible": true,
                "elem_id": "text",
                "elem_classes": [],
                "name": "html",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "0ef1a4eade65d710fcd9fc56f6586b0e",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "<p>Hello</p>"
        },
        {
            "id": 90,
            "type": "plot",
            "props": {
                "value": {
                    "type": "plotly",
                    "plot": "{\"data\":[],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"LLM Architecture\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Prefill Speedup (%)\"}},\"legend\":{\"tracegroupgap\":0,\"title\":{\"text\":\"Quantization Scheme\"}},\"margin\":{\"t\":60},\"boxmode\":\"group\",\"title\":{\"text\":\"Prefill Latency Speedup per Architecture, Compared To Non-Optimized Model\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":600}}"
                },
                "show_label": false,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "plot",
                "elem_classes": [],
                "name": "plot",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "12c46a60185f5c11b6b6b6f9adbe826c",
            "api_info": {
                "properties": {
                    "type": {
                        "enum": [
                            "altair",
                            "bokeh",
                            "plotly",
                            "matplotlib"
                        ],
                        "title": "Type",
                        "type": "string"
                    },
                    "plot": {
                        "title": "Plot",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "plot"
                ],
                "title": "PlotData",
                "type": "object"
            },
            "example_inputs": null
        },
        {
            "id": 91,
            "type": "plot",
            "props": {
                "value": {
                    "type": "plotly",
                    "plot": "{\"data\":[],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"LLM Architecture\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Decode Speedup (%)\"}},\"legend\":{\"tracegroupgap\":0,\"title\":{\"text\":\"Quantization Scheme\"}},\"margin\":{\"t\":60},\"boxmode\":\"group\",\"title\":{\"text\":\"Decode Throughput Speedup per Architecture, Compared To Non-Optimized Model\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":600}}"
                },
                "show_label": false,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "plot",
                "elem_classes": [],
                "name": "plot",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "12c46a60185f5c11b6b6b6f9adbe826c",
            "api_info": {
                "properties": {
                    "type": {
                        "enum": [
                            "altair",
                            "bokeh",
                            "plotly",
                            "matplotlib"
                        ],
                        "title": "Type",
                        "type": "string"
                    },
                    "plot": {
                        "title": "Plot",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "plot"
                ],
                "title": "PlotData",
                "type": "object"
            },
            "example_inputs": null
        },
        {
            "id": 92,
            "type": "tabitem",
            "props": {
                "label": "FlashAttentionV2 \ud83d\udcc8",
                "id": 3,
                "name": "tabitem"
            },
            "skip_api": true,
            "component_class_id": "f72faf2b8abb615823b199272218625a"
        },
        {
            "id": 93,
            "type": "html",
            "props": {
                "value": "\ud83d\udc46 Hover over the points \ud83d\udc46 for additional information.",
                "show_label": true,
                "visible": true,
                "elem_id": "text",
                "elem_classes": [],
                "name": "html",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "0ef1a4eade65d710fcd9fc56f6586b0e",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "<p>Hello</p>"
        },
        {
            "id": 94,
            "type": "plot",
            "props": {
                "value": {
                    "type": "plotly",
                    "plot": "{\"data\":[],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"LLM Architecture\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Prefill Speedup (%)\"}},\"legend\":{\"tracegroupgap\":0,\"title\":{\"text\":\"Quantization Scheme\"}},\"margin\":{\"t\":60},\"boxmode\":\"group\",\"title\":{\"text\":\"Prefill Latency Speedup per Architecture, Compared To Non-Optimized Model\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":600}}"
                },
                "show_label": false,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "plot",
                "elem_classes": [],
                "name": "plot",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "12c46a60185f5c11b6b6b6f9adbe826c",
            "api_info": {
                "properties": {
                    "type": {
                        "enum": [
                            "altair",
                            "bokeh",
                            "plotly",
                            "matplotlib"
                        ],
                        "title": "Type",
                        "type": "string"
                    },
                    "plot": {
                        "title": "Plot",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "plot"
                ],
                "title": "PlotData",
                "type": "object"
            },
            "example_inputs": null
        },
        {
            "id": 95,
            "type": "plot",
            "props": {
                "value": {
                    "type": "plotly",
                    "plot": "{\"data\":[],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"LLM Architecture\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Decode Speedup (%)\"}},\"legend\":{\"tracegroupgap\":0,\"title\":{\"text\":\"Quantization Scheme\"}},\"margin\":{\"t\":60},\"boxmode\":\"group\",\"title\":{\"text\":\"Decode Throughput Speedup per Architecture, Compared To Non-Optimized Model\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":600}}"
                },
                "show_label": false,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "plot",
                "elem_classes": [],
                "name": "plot",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "12c46a60185f5c11b6b6b6f9adbe826c",
            "api_info": {
                "properties": {
                    "type": {
                        "enum": [
                            "altair",
                            "bokeh",
                            "plotly",
                            "matplotlib"
                        ],
                        "title": "Type",
                        "type": "string"
                    },
                    "plot": {
                        "title": "Plot",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "plot"
                ],
                "title": "PlotData",
                "type": "object"
            },
            "example_inputs": null
        },
        {
            "id": 96,
            "type": "tabitem",
            "props": {
                "label": "Custom Quantization Kernels \ud83d\udcc8",
                "id": 4,
                "name": "tabitem"
            },
            "skip_api": true,
            "component_class_id": "f72faf2b8abb615823b199272218625a"
        },
        {
            "id": 97,
            "type": "html",
            "props": {
                "value": "\ud83d\udc46 Hover over the points \ud83d\udc46 for additional information.",
                "show_label": true,
                "visible": true,
                "elem_id": "text",
                "elem_classes": [],
                "name": "html",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "0ef1a4eade65d710fcd9fc56f6586b0e",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "<p>Hello</p>"
        },
        {
            "id": 98,
            "type": "plot",
            "props": {
                "value": {
                    "type": "plotly",
                    "plot": "{\"data\":[{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"microsoft\\u002fphi-1_5\",\"phi\",\"float16\",\"pytorch\",0.0,47.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0102,0.00986,111.0,116.0,3.450000000000003,4.5],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00741,0.01,142.0,151.0,-25.900000000000006,6.340000000000003]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\",\"legendgroup\":\"GPTQ.4bit+ExllamaV1\",\"marker\":{\"color\":\"#FD3216\"},\"name\":\"GPTQ.4bit+ExllamaV1\",\"notched\":false,\"offsetgroup\":\"GPTQ.4bit+ExllamaV1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"phi\",\"\ud83c\udf38 Bloom\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[3.450000000000003,-25.900000000000006],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"microsoft\\u002fphi-1_5\",\"phi\",\"float16\",\"pytorch\",0.0,47.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0102,0.0141,111.0,105.0,-27.659999999999997,-5.409999999999997],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00741,0.00998,142.0,136.0,-25.75,-4.230000000000004]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\",\"legendgroup\":\"AWQ.4bit+GEMM\",\"marker\":{\"color\":\"#00FE35\"},\"name\":\"AWQ.4bit+GEMM\",\"notched\":false,\"offsetgroup\":\"AWQ.4bit+GEMM\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"phi\",\"\ud83c\udf38 Bloom\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-27.659999999999997,-25.75],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"}],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"LLM Architecture\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Prefill Speedup (%)\"}},\"legend\":{\"title\":{\"text\":\"Quantization Scheme\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"boxmode\":\"group\",\"title\":{\"text\":\"Prefill Latency Speedup per Architecture\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":600}}"
                },
                "show_label": false,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "plot",
                "elem_classes": [],
                "name": "plot",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "12c46a60185f5c11b6b6b6f9adbe826c",
            "api_info": {
                "properties": {
                    "type": {
                        "enum": [
                            "altair",
                            "bokeh",
                            "plotly",
                            "matplotlib"
                        ],
                        "title": "Type",
                        "type": "string"
                    },
                    "plot": {
                        "title": "Plot",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "plot"
                ],
                "title": "PlotData",
                "type": "object"
            },
            "example_inputs": null
        },
        {
            "id": 99,
            "type": "plot",
            "props": {
                "value": {
                    "type": "plotly",
                    "plot": "{\"data\":[{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"microsoft\\u002fphi-1_5\",\"phi\",\"float16\",\"pytorch\",0.0,47.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.0102,0.00986,111.0,116.0,3.450000000000003,4.5],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"GPTQ.4bit+ExllamaV1\",0.00741,0.01,142.0,151.0,-25.900000000000006,6.340000000000003]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\",\"legendgroup\":\"GPTQ.4bit+ExllamaV1\",\"marker\":{\"color\":\"#FD3216\"},\"name\":\"GPTQ.4bit+ExllamaV1\",\"notched\":false,\"offsetgroup\":\"GPTQ.4bit+ExllamaV1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"phi\",\"\ud83c\udf38 Bloom\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[4.5,6.340000000000003],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"customdata\":[[\"microsoft\\u002fphi-1_5\",\"phi\",\"float16\",\"pytorch\",0.0,47.69,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.0102,0.0141,111.0,105.0,-27.659999999999997,-5.409999999999997],[\"TurkuNLP\\u002fgpt3-finnish-large\",\"\ud83c\udf38 Bloom\",\"float16\",\"pytorch\",0.0,29.11,\"float16\",\"pytorch\",\"None\",\"None\",\"None\",\"AWQ.4bit+GEMM\",0.00741,0.00998,142.0,136.0,-25.75,-4.230000000000004]],\"hovertemplate\":\"\\u003cb\\u003eModel \ud83e\udd17:\\u003c\\u002fb\\u003e %{customdata[0]}\\u003cbr\\u003e\\u003cb\\u003eArch \ud83c\udfdb\ufe0f:\\u003c\\u002fb\\u003e %{customdata[1]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[2]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[3]}\\u003cbr\\u003e\\u003cb\\u003eParams (B):\\u003c\\u002fb\\u003e %{customdata[4]}\\u003cbr\\u003e\\u003cb\\u003eOpen LLM Score (%):\\u003c\\u002fb\\u003e %{customdata[5]}\\u003cbr\\u003e\\u003cb\\u003eDType \ud83d\udce5:\\u003c\\u002fb\\u003e %{customdata[6]}\\u003cbr\\u003e\\u003cb\\u003eBackend \ud83c\udfed:\\u003c\\u002fb\\u003e %{customdata[7]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f:\\u003c\\u002fb\\u003e %{customdata[8]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f:\\u003c\\u002fb\\u003e %{customdata[9]}\\u003cbr\\u003e\\u003cb\\u003eOptimization \ud83d\udee0\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[10]}\\u003cbr\\u003e\\u003cb\\u003eQuantization \ud83d\udddc\ufe0f Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[11]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s):\\u003c\\u002fb\\u003e %{customdata[12]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency (s) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[13]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs):\\u003c\\u002fb\\u003e %{customdata[14]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput (tokens\\u002fs) Custom Kernel:\\u003c\\u002fb\\u003e %{customdata[15]}\\u003cbr\\u003e\\u003cb\\u003ePrefill Latency Speedup (%):\\u003c\\u002fb\\u003e %{customdata[16]}\\u003cbr\\u003e\\u003cb\\u003eDecode Throughput Speedup (%):\\u003c\\u002fb\\u003e %{customdata[17]}\",\"legendgroup\":\"AWQ.4bit+GEMM\",\"marker\":{\"color\":\"#00FE35\"},\"name\":\"AWQ.4bit+GEMM\",\"notched\":false,\"offsetgroup\":\"AWQ.4bit+GEMM\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"phi\",\"\ud83c\udf38 Bloom\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[-5.409999999999997,-4.230000000000004],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"}],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"LLM Architecture\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Decode Speedup (%)\"}},\"legend\":{\"title\":{\"text\":\"Quantization Scheme\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"boxmode\":\"group\",\"title\":{\"text\":\"Decode Throughput Speedup per Architecture\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":600}}"
                },
                "show_label": false,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "plot",
                "elem_classes": [],
                "name": "plot",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "12c46a60185f5c11b6b6b6f9adbe826c",
            "api_info": {
                "properties": {
                    "type": {
                        "enum": [
                            "altair",
                            "bokeh",
                            "plotly",
                            "matplotlib"
                        ],
                        "title": "Type",
                        "type": "string"
                    },
                    "plot": {
                        "title": "Plot",
                        "type": "string"
                    }
                },
                "required": [
                    "type",
                    "plot"
                ],
                "title": "PlotData",
                "type": "object"
            },
            "example_inputs": null
        },
        {
            "id": 100,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        },
        {
            "id": 101,
            "type": "tabitem",
            "props": {
                "label": "About \ud83d\udcd6",
                "id": 3,
                "name": "tabitem"
            },
            "skip_api": true,
            "component_class_id": "f72faf2b8abb615823b199272218625a"
        },
        {
            "id": 102,
            "type": "html",
            "props": {
                "value": "<h3>About the \ud83e\udd17 LLM-Perf Leaderboard \ud83c\udfcb\ufe0f</h3>\n<ul>\n    <li>To avoid communication-dependent results, only one GPU is used.</li>\n    <li>Score is the average evaluation score obtained from the <a href=\"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\">\ud83e\udd17 Open LLM Leaderboard</a>.</li>\n    <li>LLMs are running on a singleton batch with a prompt size of 256 and generating a 256 tokens.</li>\n    <li>Energy consumption is measured in kWh using CodeCarbon and taking into consideration the GPU, CPU, RAM and location of the machine.</li>\n    <li>We measure three types of memory: Max Allocated Memory, Max Reserved Memory and Max Used Memory. The first two being reported by PyTorch and the last one being observed using PyNVML.</li>\n</ul>\n",
                "show_label": true,
                "visible": true,
                "elem_classes": [
                    "descriptive-text"
                ],
                "name": "html",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "0ef1a4eade65d710fcd9fc56f6586b0e",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "<p>Hello</p>"
        },
        {
            "id": 103,
            "type": "markdown",
            "props": {
                "value": "Here's an example of the configuration file used to benchmark the models with Optimum-Benchmark:\n```yaml\ndefaults:\n  - backend: pytorch\n  - _base_ # inheriting from base config\n  - _self_ # for hydra 1.1 compatibility\n\nexperiment_name: pytorch+cuda+float16+gptq-4bit+exllama-v1\ndevice: cuda\n\nbackend:\n  no_weights: true\n  torch_dtype: float16\n  quantization_scheme: gptq\n  quantization_config:\n    bits: 4\n    use_cuda_fp16: false\n    use_exllama: true\n    exllama_config:\n      version: 1\n```\n\nWhere the base config is:\n```yaml\ndefaults:\n  - benchmark: inference # default benchmark\n  - launcher: process # isolated process launcher\n  - experiment # inheriting from experiment config\n  - _self_ # for hydra 1.1 compatibility\n  - override hydra/job_logging: colorlog # colorful logging\n  - override hydra/hydra_logging: colorlog # colorful logging\n\nhydra:\n  run:\n    dir: dataset/${oc.env:HOSTNAME}/${experiment_name}/${model}\n  job:\n    chdir: true\n    env_set:\n      COUNTRY_ISO_CODE: FRA\n      OVERRIDE_BENCHMARKS: 0\n      CUDA_VISIBLE_DEVICES: 0\n      CUDA_DEVICE_ORDER: PCI_BUS_ID\n\nbackend:\n  continuous_isolation: true\n\nbenchmark:\n  duration: 10\n  memory: true\n  energy: true\n\n  input_shapes:\n    batch_size: 1\n    sequence_length: 256\n\n  new_tokens: 256\n\nhub_kwargs:\n  trust_remote_code: true\n```",
                "show_label": true,
                "rtl": false,
                "latex_delimiters": [
                    {
                        "left": "$$",
                        "right": "$$",
                        "display": true
                    }
                ],
                "visible": true,
                "elem_classes": [
                    "descriptive-text"
                ],
                "sanitize_html": true,
                "line_breaks": false,
                "name": "markdown",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "810c5f8552642d85df14958a33f75123",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "# Hello!"
        },
        {
            "id": 104,
            "type": "row",
            "props": {
                "variant": "default",
                "visible": true,
                "equal_height": true,
                "name": "row"
            },
            "skip_api": true,
            "component_class_id": "9a37ebd52f5e3ac2d2a1b25e6c1b3eba"
        },
        {
            "id": 105,
            "type": "accordion",
            "props": {
                "label": "\ud83d\udcd9 Citation",
                "open": false,
                "visible": true,
                "name": "accordion"
            },
            "skip_api": true,
            "component_class_id": "ec423bca25f9f6f1dd31b9fe6730e770"
        },
        {
            "id": 106,
            "type": "textbox",
            "props": {
                "value": "@misc{llm-perf-leaderboard,\n  author = {Ilyas Moutawwakil, R\u00e9gis Pierrard},\n  title = {LLM-Perf Leaderboard},\n  year = {2023},\n  publisher = {Hugging Face},\n  howpublished = \"\\url{https://huggingface.co/spaces/optimum/llm-perf-leaderboard}\",\n@software{optimum-benchmark,\n  author = {Ilyas Moutawwakil, R\u00e9gis Pierrard},\n  publisher = {Hugging Face},\n  title = {Optimum-Benchmark: A framework for benchmarking the performance of Transformers models with different hardwares, backends and optimizations.},\n}\n",
                "lines": 1,
                "max_lines": 20,
                "label": "Copy the following snippet to cite these results.",
                "show_label": true,
                "container": true,
                "min_width": 160,
                "visible": true,
                "elem_id": "citation-button",
                "autofocus": false,
                "autoscroll": true,
                "elem_classes": [],
                "type": "text",
                "rtl": false,
                "show_copy_button": true,
                "name": "textbox",
                "_selectable": false
            },
            "skip_api": false,
            "component_class_id": "777333cda51df020195231508683831a",
            "api_info": {
                "type": "string"
            },
            "example_inputs": "Hello!!"
        },
        {
            "id": 107,
            "type": "form",
            "props": {
                "scale": 0,
                "min_width": 0,
                "name": "form"
            },
            "skip_api": true,
            "component_class_id": "8a922308b8adf2794816d4dee399dbae"
        }
    ],
    "css": "\n.logo {\n    width: 300px;\n    height: auto;\n    margin: 0 auto;\n    max-width: 100%\n    object-fit: contain;\n}\n.text {\n    font-size: 16px !important;\n}\n\n.tabs button {\n    font-size: 20px;\n}\n.subtabs button {\n    font-size: 20px;\n}\n\n#citation-button span {\n    font-size: 16px !important;\n}\n\n#citation-button textarea {\n    font-size: 16px !important;\n}\n\n#citation-button > label > button {\n    margin: 6px;\n    transform: scale(1.3);\n}\n",
    "js": null,
    "head": null,
    "title": "Gradio",
    "space_id": "optimum/llm-perf-leaderboard",
    "enable_queue": true,
    "show_error": false,
    "show_api": true,
    "is_colab": false,
    "stylesheets": [
        "https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@400;600&display=swap",
        "https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600&display=swap"
    ],
    "theme": "default",
    "protocol": "sse",
    "layout": {
        "id": 0,
        "children": [
            {
                "id": 1
            },
            {
                "id": 2
            },
            {
                "id": 3
            },
            {
                "id": 4,
                "children": [
                    {
                        "id": 5,
                        "children": [
                            {
                                "id": 6
                            },
                            {
                                "id": 52,
                                "children": [
                                    {
                                        "id": 7
                                    }
                                ]
                            },
                            {
                                "id": 8,
                                "children": [
                                    {
                                        "id": 9,
                                        "children": [
                                            {
                                                "id": 11,
                                                "children": [
                                                    {
                                                        "id": 10
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": 12,
                                "children": [
                                    {
                                        "id": 13,
                                        "children": [
                                            {
                                                "id": 15,
                                                "children": [
                                                    {
                                                        "id": 14
                                                    }
                                                ]
                                            }
                                        ]
                                    },
                                    {
                                        "id": 16,
                                        "children": [
                                            {
                                                "id": 18,
                                                "children": [
                                                    {
                                                        "id": 17
                                                    }
                                                ]
                                            }
                                        ]
                                    },
                                    {
                                        "id": 19,
                                        "children": [
                                            {
                                                "id": 21,
                                                "children": [
                                                    {
                                                        "id": 20
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": 22,
                                "children": [
                                    {
                                        "id": 23,
                                        "children": [
                                            {
                                                "id": 25,
                                                "children": [
                                                    {
                                                        "id": 24
                                                    }
                                                ]
                                            }
                                        ]
                                    },
                                    {
                                        "id": 26,
                                        "children": [
                                            {
                                                "id": 28,
                                                "children": [
                                                    {
                                                        "id": 27
                                                    }
                                                ]
                                            }
                                        ]
                                    },
                                    {
                                        "id": 29,
                                        "children": [
                                            {
                                                "id": 31,
                                                "children": [
                                                    {
                                                        "id": 30
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": 32,
                                "children": [
                                    {
                                        "id": 33
                                    }
                                ]
                            },
                            {
                                "id": 34,
                                "children": [
                                    {
                                        "id": 35,
                                        "children": [
                                            {
                                                "id": 36
                                            },
                                            {
                                                "id": 37
                                            },
                                            {
                                                "id": 38
                                            },
                                            {
                                                "id": 39
                                            }
                                        ]
                                    },
                                    {
                                        "id": 40,
                                        "children": [
                                            {
                                                "id": 41
                                            },
                                            {
                                                "id": 42
                                            },
                                            {
                                                "id": 43
                                            }
                                        ]
                                    },
                                    {
                                        "id": 44,
                                        "children": [
                                            {
                                                "id": 45
                                            },
                                            {
                                                "id": 46
                                            },
                                            {
                                                "id": 47
                                            }
                                        ]
                                    },
                                    {
                                        "id": 48,
                                        "children": [
                                            {
                                                "id": 49
                                            },
                                            {
                                                "id": 50
                                            },
                                            {
                                                "id": 51
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": 53,
                        "children": [
                            {
                                "id": 54
                            },
                            {
                                "id": 100,
                                "children": [
                                    {
                                        "id": 55
                                    }
                                ]
                            },
                            {
                                "id": 56,
                                "children": [
                                    {
                                        "id": 57,
                                        "children": [
                                            {
                                                "id": 59,
                                                "children": [
                                                    {
                                                        "id": 58
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": 60,
                                "children": [
                                    {
                                        "id": 61,
                                        "children": [
                                            {
                                                "id": 63,
                                                "children": [
                                                    {
                                                        "id": 62
                                                    }
                                                ]
                                            }
                                        ]
                                    },
                                    {
                                        "id": 64,
                                        "children": [
                                            {
                                                "id": 66,
                                                "children": [
                                                    {
                                                        "id": 65
                                                    }
                                                ]
                                            }
                                        ]
                                    },
                                    {
                                        "id": 67,
                                        "children": [
                                            {
                                                "id": 69,
                                                "children": [
                                                    {
                                                        "id": 68
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": 70,
                                "children": [
                                    {
                                        "id": 71,
                                        "children": [
                                            {
                                                "id": 73,
                                                "children": [
                                                    {
                                                        "id": 72
                                                    }
                                                ]
                                            }
                                        ]
                                    },
                                    {
                                        "id": 74,
                                        "children": [
                                            {
                                                "id": 76,
                                                "children": [
                                                    {
                                                        "id": 75
                                                    }
                                                ]
                                            }
                                        ]
                                    },
                                    {
                                        "id": 77,
                                        "children": [
                                            {
                                                "id": 79,
                                                "children": [
                                                    {
                                                        "id": 78
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "id": 80,
                                "children": [
                                    {
                                        "id": 81
                                    }
                                ]
                            },
                            {
                                "id": 82,
                                "children": [
                                    {
                                        "id": 83,
                                        "children": [
                                            {
                                                "id": 84
                                            },
                                            {
                                                "id": 85
                                            },
                                            {
                                                "id": 86
                                            },
                                            {
                                                "id": 87
                                            }
                                        ]
                                    },
                                    {
                                        "id": 88,
                                        "children": [
                                            {
                                                "id": 89
                                            },
                                            {
                                                "id": 90
                                            },
                                            {
                                                "id": 91
                                            }
                                        ]
                                    },
                                    {
                                        "id": 92,
                                        "children": [
                                            {
                                                "id": 93
                                            },
                                            {
                                                "id": 94
                                            },
                                            {
                                                "id": 95
                                            }
                                        ]
                                    },
                                    {
                                        "id": 96,
                                        "children": [
                                            {
                                                "id": 97
                                            },
                                            {
                                                "id": 98
                                            },
                                            {
                                                "id": 99
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "id": 101,
                        "children": [
                            {
                                "id": 102
                            },
                            {
                                "id": 103
                            }
                        ]
                    }
                ]
            },
            {
                "id": 104,
                "children": [
                    {
                        "id": 105,
                        "children": [
                            {
                                "id": 107,
                                "children": [
                                    {
                                        "id": 106
                                    }
                                ]
                            }
                        ]
                    }
                ]
            }
        ]
    },
    "dependencies": [
        {
            "targets": [
                [
                    33,
                    "click"
                ]
            ],
            "inputs": [
                7,
                10,
                20,
                24,
                27,
                30,
                14,
                17
            ],
            "outputs": [
                37,
                39,
                42,
                43,
                46,
                47,
                50,
                51
            ],
            "backend_fn": true,
            "js": null,
            "queue": null,
            "api_name": "filter_fn",
            "scroll_to_output": false,
            "show_progress": "full",
            "every": null,
            "batch": false,
            "max_batch_size": 4,
            "cancels": [],
            "types": {
                "continuous": false,
                "generator": false
            },
            "collects_event_data": false,
            "trigger_after": null,
            "trigger_only_on_success": false,
            "trigger_mode": "once"
        },
        {
            "targets": [
                [
                    81,
                    "click"
                ]
            ],
            "inputs": [
                55,
                58,
                68,
                72,
                75,
                78,
                62,
                65
            ],
            "outputs": [
                85,
                87,
                90,
                91,
                94,
                95,
                98,
                99
            ],
            "backend_fn": true,
            "js": null,
            "queue": null,
            "api_name": "filter_fn_1",
            "scroll_to_output": false,
            "show_progress": "full",
            "every": null,
            "batch": false,
            "max_batch_size": 4,
            "cancels": [],
            "types": {
                "continuous": false,
                "generator": false
            },
            "collects_event_data": false,
            "trigger_after": null,
            "trigger_only_on_success": false,
            "trigger_mode": "once"
        }
    ],
    "root": "https://optimum-llm-perf-leaderboard.hf.space/--replicas/5k6lu"
}