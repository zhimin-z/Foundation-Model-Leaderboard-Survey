[
    {
        "table_id":2549,
        "row_id":64306,
        "rank":1,
        "method":"BEiT-3",
        "mlmodel":{

        },
        "method_short":"BEiT-3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-22",
        "metrics":{
            "overall":"84.03",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":84.03,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1062207,
            "title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
            "url":"\/paper\/image-as-a-foreign-language-beit-pretraining",
            "published":"2022-08-22T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":103625,
        "rank":2,
        "method":"mPLUG-Huge",
        "mlmodel":{

        },
        "method_short":"mPLUG-Huge",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-24",
        "metrics":{
            "overall":"83.62",
            "yes\/no":"94.83",
            "number":"69.82",
            "other":"77.02"
        },
        "raw_metrics":{
            "overall":83.62,
            "yes\/no":94.83,
            "number":69.82,
            "other":77.02
        },
        "uses_additional_data":false,
        "paper":{
            "id":1015224,
            "title":"mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections",
            "url":"\/paper\/mplug-effective-and-efficient-vision-language",
            "published":"2022-05-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-effective-and-efficient-vision-language\/review\/?hl=103625"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":103006,
        "rank":3,
        "method":"ONE-PEACE",
        "mlmodel":{

        },
        "method_short":"ONE-PEACE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-18",
        "metrics":{
            "overall":"82.52",
            "yes\/no":"94.85",
            "number":"72.24",
            "other":"74.15"
        },
        "raw_metrics":{
            "overall":82.52,
            "yes\/no":94.85,
            "number":72.24,
            "other":74.15
        },
        "uses_additional_data":false,
        "paper":{
            "id":1211430,
            "title":"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
            "url":"\/paper\/one-peace-exploring-one-general",
            "published":"2023-05-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-peace-exploring-one-general\/review\/?hl=103006"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":80504,
        "rank":4,
        "method":"X2-VLM (large)",
        "mlmodel":{

        },
        "method_short":"X2-VLM ",
        "method_details":"large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-22",
        "metrics":{
            "overall":"81.8",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":81.8,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1116038,
            "title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
            "url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for",
            "published":"2022-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for\/review\/?hl=80504"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":43952,
        "rank":5,
        "method":"VLMo",
        "mlmodel":{

        },
        "method_short":"VLMo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-03",
        "metrics":{
            "overall":"81.30",
            "yes\/no":"94.68",
            "number":"67.26",
            "other":"72.87"
        },
        "raw_metrics":{
            "overall":81.3,
            "yes\/no":94.68,
            "number":67.26,
            "other":72.87
        },
        "uses_additional_data":false,
        "paper":{
            "id":900016,
            "title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts",
            "url":"\/paper\/vlmo-unified-vision-language-pre-training",
            "published":"2021-11-03T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":43163,
        "rank":6,
        "method":"Florence",
        "mlmodel":{

        },
        "method_short":"Florence",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-22",
        "metrics":{
            "overall":"80.36",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":80.36,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":914419,
            "title":"Florence: A New Foundation Model for Computer Vision",
            "url":"\/paper\/florence-a-new-foundation-model-for-computer",
            "published":"2021-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/florence-a-new-foundation-model-for-computer\/review\/?hl=43163"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":43423,
        "rank":7,
        "method":"SimVLM",
        "mlmodel":{

        },
        "method_short":"SimVLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-08-24",
        "metrics":{
            "overall":"80.34",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":80.34,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":856712,
            "title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
            "url":"\/paper\/simvlm-simple-visual-language-model",
            "published":"2021-08-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/simvlm-simple-visual-language-model\/review\/?hl=43423"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":80503,
        "rank":8,
        "method":"X2-VLM (base)",
        "mlmodel":{

        },
        "method_short":"X2-VLM ",
        "method_details":"base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-22",
        "metrics":{
            "overall":"80.2",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":80.2,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1116038,
            "title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
            "url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for",
            "published":"2022-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for\/review\/?hl=80503"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":104615,
        "rank":9,
        "method":"VAST",
        "mlmodel":{

        },
        "method_short":"VAST",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "overall":"80.19",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":80.19,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":101428,
        "rank":10,
        "method":"VALOR",
        "mlmodel":{

        },
        "method_short":"VALOR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-17",
        "metrics":{
            "overall":"78.62",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":78.62,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1191887,
            "title":"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset",
            "url":"\/paper\/valor-vision-audio-language-omni-perception",
            "published":"2023-04-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/valor-vision-audio-language-omni-perception\/review\/?hl=101428"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":62150,
        "rank":11,
        "method":"Prompt Tuning",
        "mlmodel":{

        },
        "method_short":"Prompt Tuning",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-04",
        "metrics":{
            "overall":"78.53",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":78.53,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1054811,
            "title":"Prompt Tuning for Generative Multimodal Pretrained Models",
            "url":"\/paper\/prompt-tuning-for-generative-multimodal",
            "published":"2022-08-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/prompt-tuning-for-generative-multimodal\/review\/?hl=62150"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":98777,
        "rank":12,
        "method":"Prismer",
        "mlmodel":{

        },
        "method_short":"Prismer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-04",
        "metrics":{
            "overall":"78.49",
            "yes\/no":"93.09",
            "number":"61.39",
            "other":"69.70"
        },
        "raw_metrics":{
            "overall":78.49,
            "yes\/no":93.09,
            "number":61.39,
            "other":69.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1168347,
            "title":"Prismer: A Vision-Language Model with An Ensemble of Experts",
            "url":"\/paper\/prismer-a-vision-language-model-with-an",
            "published":"2023-03-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/prismer-a-vision-language-model-with-an\/review\/?hl=98777"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":24577,
        "rank":13,
        "method":"MSR + MS Cog. Svcs., X10 models",
        "mlmodel":{

        },
        "method_short":"MSR + MS Cog. Svcs., X10 models",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-07",
        "metrics":{
            "overall":"77.45",
            "yes\/no":"92.38",
            "number":"62.55",
            "other":"67.87"
        },
        "raw_metrics":{
            "overall":77.45,
            "yes\/no":92.38,
            "number":62.55,
            "other":67.87
        },
        "uses_additional_data":false,
        "paper":{
            "id":733129,
            "title":"VinVL: Revisiting Visual Representations in Vision-Language Models",
            "url":"\/paper\/vinvl-making-visual-representations-matter-in",
            "published":"2021-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vinvl-making-visual-representations-matter-in\/review\/?hl=24577"
        },
        "external_source_url":"https:\/\/eval.ai\/web\/challenges\/challenge-page\/514\/leaderboard\/1386#leaderboardrank-1",
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":24533,
        "rank":14,
        "method":"MSR + MS Cog. Svcs.",
        "mlmodel":{

        },
        "method_short":"MSR + MS Cog. Svcs.",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-12",
        "metrics":{
            "overall":"76.63",
            "yes\/no":"92.04",
            "number":"61.5",
            "other":"66.68"
        },
        "raw_metrics":{
            "overall":76.63,
            "yes\/no":92.04,
            "number":61.5,
            "other":66.68
        },
        "uses_additional_data":false,
        "paper":{
            "id":733129,
            "title":"VinVL: Revisiting Visual Representations in Vision-Language Models",
            "url":"\/paper\/vinvl-making-visual-representations-matter-in",
            "published":"2021-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vinvl-making-visual-representations-matter-in\/review\/?hl=24533"
        },
        "external_source_url":"https:\/\/eval.ai\/web\/challenges\/challenge-page\/514\/leaderboard\/1386#leaderboardrank-3",
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":44599,
        "rank":15,
        "method":"ALBEF (14M)",
        "mlmodel":{

        },
        "method_short":"ALBEF ",
        "method_details":"14M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-16",
        "metrics":{
            "overall":"76.04",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":76.04,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":836928,
            "title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
            "url":"\/paper\/align-before-fuse-vision-and-language",
            "published":"2021-07-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/align-before-fuse-vision-and-language\/review\/?hl=44599"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":23091,
        "rank":16,
        "method":"BGN, ensemble",
        "mlmodel":{

        },
        "method_short":"BGN, ensemble",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-15",
        "metrics":{
            "overall":"75.92",
            "yes\/no":"90.89",
            "number":"61.13",
            "other":"66.28"
        },
        "raw_metrics":{
            "overall":75.92,
            "yes\/no":90.89,
            "number":61.13,
            "other":66.28
        },
        "uses_additional_data":false,
        "paper":{
            "id":146578,
            "title":"Bilinear Graph Networks for Visual Question Answering",
            "url":"\/paper\/graph-reasoning-networks-for-visual-question",
            "published":"2019-07-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/graph-reasoning-networks-for-visual-question\/review\/?hl=23091"
        },
        "external_source_url":"https:\/\/eval.ai\/web\/challenges\/challenge-page\/514\/leaderboard\/1386#leaderboardrank-6",
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":24384,
        "rank":17,
        "method":"ERNIE-ViL-single model",
        "mlmodel":{

        },
        "method_short":"ERNIE-ViL-single model",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-01",
        "metrics":{
            "overall":"74.93",
            "yes\/no":"90.83",
            "number":"56.79",
            "other":"65.24"
        },
        "raw_metrics":{
            "overall":74.93,
            "yes\/no":90.83,
            "number":56.79,
            "other":65.24
        },
        "uses_additional_data":false,
        "paper":{
            "id":206405,
            "title":"ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph",
            "url":"\/paper\/ernie-vil-knowledge-enhanced-vision-language",
            "published":"2020-06-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/ernie-vil-knowledge-enhanced-vision-language\/review\/?hl=24384"
        },
        "external_source_url":"https:\/\/eval.ai\/web\/challenges\/challenge-page\/514\/leaderboard\/1386#leaderboardrank-11",
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":23100,
        "rank":18,
        "method":"Single, w\/o VLP",
        "mlmodel":{

        },
        "method_short":"Single, w\/o VLP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-14",
        "metrics":{
            "overall":"74.16",
            "yes\/no":"89.18",
            "number":"58.01",
            "other":"64.77"
        },
        "raw_metrics":{
            "overall":74.16,
            "yes\/no":89.18,
            "number":58.01,
            "other":64.77
        },
        "uses_additional_data":false,
        "paper":{
            "id":179526,
            "title":"In Defense of Grid Features for Visual Question Answering",
            "url":"\/paper\/in-defense-of-grid-features-for-visual",
            "published":"2020-01-10T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":"https:\/\/eval.ai\/web\/challenges\/challenge-page\/514\/leaderboard\/1386#leaderboardrank-15",
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":23102,
        "rank":19,
        "method":"Single, w\/o VLP",
        "mlmodel":{

        },
        "method_short":"Single, w\/o VLP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-10",
        "metrics":{
            "overall":"73.86",
            "yes\/no":"89.46",
            "number":"58.62",
            "other":"63.78"
        },
        "raw_metrics":{
            "overall":73.86,
            "yes\/no":89.46,
            "number":58.62,
            "other":63.78
        },
        "uses_additional_data":false,
        "paper":{
            "id":192462,
            "title":"Deep Multimodal Neural Architecture Search",
            "url":"\/paper\/deep-multimodal-neural-architecture-search",
            "published":"2020-04-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deep-multimodal-neural-architecture-search\/review\/?hl=23102"
        },
        "external_source_url":"https:\/\/eval.ai\/web\/challenges\/challenge-page\/514\/leaderboard\/1386#leaderboardrank-18",
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14776,
        "rank":20,
        "method":"UNITER (Large)",
        "mlmodel":{

        },
        "method_short":"UNITER ",
        "method_details":"Large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-25",
        "metrics":{
            "overall":"73.4",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":73.4,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":156206,
            "title":"UNITER: UNiversal Image-TExt Representation Learning",
            "url":"\/paper\/uniter-learning-universal-image-text-1",
            "published":"2019-09-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/uniter-learning-universal-image-text-1\/review\/?hl=14776"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":16079,
        "rank":21,
        "method":"X-101 grid features + MCAN",
        "mlmodel":{

        },
        "method_short":"X-101 grid features + MCAN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-08",
        "metrics":{
            "overall":"72.71",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":72.71,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":179526,
            "title":"In Defense of Grid Features for Visual Question Answering",
            "url":"\/paper\/in-defense-of-grid-features-for-visual",
            "published":"2020-01-10T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14777,
        "rank":22,
        "method":"LXMERT",
        "mlmodel":{

        },
        "method_short":"LXMERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-20",
        "metrics":{
            "overall":"72.5",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":72.5,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":150646,
            "title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
            "url":"\/paper\/lxmert-learning-cross-modality-encoder",
            "published":"2019-08-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lxmert-learning-cross-modality-encoder\/review\/?hl=14777"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14778,
        "rank":23,
        "method":"VL-BERTLARGE",
        "mlmodel":{

        },
        "method_short":"VL-BERTLARGE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-22",
        "metrics":{
            "overall":"72.2",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":72.2,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":150926,
            "title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations",
            "url":"\/paper\/vl-bert-pre-training-of-generic-visual",
            "published":"2019-08-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vl-bert-pre-training-of-generic-visual\/review\/?hl=14778"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":18116,
        "rank":24,
        "method":"MCAN+VC",
        "mlmodel":{

        },
        "method_short":"MCAN+VC",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-02-27",
        "metrics":{
            "overall":"71.49",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":71.49,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":185038,
            "title":"Visual Commonsense R-CNN",
            "url":"\/paper\/visual-commonsense-r-cnn",
            "published":"2020-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-commonsense-r-cnn\/review\/?hl=18116"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14779,
        "rank":25,
        "method":"VisualBERT",
        "mlmodel":{

        },
        "method_short":"VisualBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-09",
        "metrics":{
            "overall":"71",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":71.0,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":149599,
            "title":"VisualBERT: A Simple and Performant Baseline for Vision and Language",
            "url":"\/paper\/visualbert-a-simple-and-performant-baseline",
            "published":"2019-08-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visualbert-a-simple-and-performant-baseline\/review\/?hl=14779"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14780,
        "rank":26,
        "method":"MCANed-6",
        "mlmodel":{

        },
        "method_short":"MCANed-6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-25",
        "metrics":{
            "overall":"70.9",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":70.9,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":143988,
            "title":"Deep Modular Co-Attention Networks for Visual Question Answering",
            "url":"\/paper\/deep-modular-co-attention-networks-for-visual-1",
            "published":"2019-06-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deep-modular-co-attention-networks-for-visual-1\/review\/?hl=14780"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14781,
        "rank":27,
        "method":"Unified VLP",
        "mlmodel":{

        },
        "method_short":"Unified VLP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-24",
        "metrics":{
            "overall":"70.7",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":70.7,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":154802,
            "title":"Unified Vision-Language Pre-Training for Image Captioning and VQA",
            "url":"\/paper\/unified-vision-language-pre-training-for",
            "published":"2019-09-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unified-vision-language-pre-training-for\/review\/?hl=14781"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14782,
        "rank":28,
        "method":"BAN+Glove+Counter",
        "mlmodel":{

        },
        "method_short":"BAN+Glove+Counter",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-05-21",
        "metrics":{
            "overall":"70.4",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":70.4,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":3201,
            "title":"Bilinear Attention Networks",
            "url":"\/paper\/bilinear-attention-networks",
            "published":"2018-05-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bilinear-attention-networks\/review\/?hl=14782"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14783,
        "rank":29,
        "method":"Up-Down",
        "mlmodel":{

        },
        "method_short":"Up-Down",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-07-25",
        "metrics":{
            "overall":"70.34",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":70.34,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":8320,
            "title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
            "url":"\/paper\/bottom-up-and-top-down-attention-for-image",
            "published":"2017-07-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bottom-up-and-top-down-attention-for-image\/review\/?hl=14783"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14784,
        "rank":30,
        "method":"Image features from bottom-up attention (adaptive K, ensemble)",
        "mlmodel":{

        },
        "method_short":"Image features from bottom-up attention ",
        "method_details":"adaptive K, ensemble",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-08-09",
        "metrics":{
            "overall":"70.3",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":70.3,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":19063,
            "title":"Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge",
            "url":"\/paper\/tips-and-tricks-for-visual-question-answering",
            "published":"2017-08-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tips-and-tricks-for-visual-question-answering\/review\/?hl=14784"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14785,
        "rank":31,
        "method":"Caption VQA",
        "mlmodel":{

        },
        "method_short":"Caption VQA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-03",
        "metrics":{
            "overall":"69.7",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":69.7,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":118480,
            "title":"Generating Question Relevant Captions to Aid Visual Question Answering",
            "url":"\/paper\/190600513",
            "published":"2019-06-03T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/190600513\/review\/?hl=14785"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14786,
        "rank":32,
        "method":"MuRel",
        "mlmodel":{

        },
        "method_short":"MuRel",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-25",
        "metrics":{
            "overall":"68.4",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":68.4,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":106702,
            "title":"MUREL: Multimodal Relational Reasoning for Visual Question Answering",
            "url":"\/paper\/murel-multimodal-relational-reasoning-for",
            "published":"2019-02-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/murel-multimodal-relational-reasoning-for\/review\/?hl=14786"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14787,
        "rank":33,
        "method":"DMN",
        "mlmodel":{

        },
        "method_short":"DMN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-02-15",
        "metrics":{
            "overall":"68.4",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":68.4,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":10147,
            "title":"Learning to Count Objects in Natural Images for Visual Question Answering",
            "url":"\/paper\/learning-to-count-objects-in-natural-images",
            "published":"2018-02-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-to-count-objects-in-natural-images\/review\/?hl=14787"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14788,
        "rank":34,
        "method":"BLOCK",
        "mlmodel":{

        },
        "method_short":"BLOCK",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-01-31",
        "metrics":{
            "overall":"67.9",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":67.9,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":93173,
            "title":"BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection",
            "url":"\/paper\/block-bilinear-superdiagonal-fusion-for",
            "published":"2019-01-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/block-bilinear-superdiagonal-fusion-for\/review\/?hl=14788"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14789,
        "rank":35,
        "method":"MUTAN",
        "mlmodel":{

        },
        "method_short":"MUTAN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-05-18",
        "metrics":{
            "overall":"67.4",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":67.4,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":22787,
            "title":"MUTAN: Multimodal Tucker Fusion for Visual Question Answering",
            "url":"\/paper\/mutan-multimodal-tucker-fusion-for-visual",
            "published":"2017-05-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mutan-multimodal-tucker-fusion-for-visual\/review\/?hl=14789"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":17292,
        "rank":36,
        "method":"2D continuous softmax",
        "mlmodel":{

        },
        "method_short":"2D continuous softmax",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-12",
        "metrics":{
            "overall":"66.27",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":66.27,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":201993,
            "title":"Sparse and Continuous Attention Mechanisms",
            "url":"\/paper\/sparse-and-continuous-attention-mechanisms",
            "published":"2020-06-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sparse-and-continuous-attention-mechanisms\/review\/?hl=17292"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14790,
        "rank":37,
        "method":"MCB [11, 12]",
        "mlmodel":{

        },
        "method_short":"MCB [11, 12]",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-12-02",
        "metrics":{
            "overall":"62.27",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":62.27,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":22922,
            "title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
            "url":"\/paper\/making-the-v-in-vqa-matter-elevating-the-role",
            "published":"2016-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/making-the-v-in-vqa-matter-elevating-the-role\/review\/?hl=14790"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14791,
        "rank":38,
        "method":"Language-only",
        "mlmodel":{

        },
        "method_short":"Language-only",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-12-02",
        "metrics":{
            "overall":"44.26",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":44.26,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":22922,
            "title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
            "url":"\/paper\/making-the-v-in-vqa-matter-elevating-the-role",
            "published":"2016-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/making-the-v-in-vqa-matter-elevating-the-role\/review\/?hl=14791"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2549,
        "row_id":14792,
        "rank":39,
        "method":"Prior",
        "mlmodel":{

        },
        "method_short":"Prior",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-12-02",
        "metrics":{
            "overall":"25.98",
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "raw_metrics":{
            "overall":25.98,
            "yes\/no":null,
            "number":null,
            "other":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":22922,
            "title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
            "url":"\/paper\/making-the-v-in-vqa-matter-elevating-the-role",
            "published":"2016-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/making-the-v-in-vqa-matter-elevating-the-role\/review\/?hl=14792"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]