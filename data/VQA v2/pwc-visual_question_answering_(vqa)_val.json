[
    {
        "table_id":18034,
        "row_id":96140,
        "rank":1,
        "method":"BLIP-2 ViT-G FlanT5 XXL (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G FlanT5 XXL ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"65.2"
        },
        "raw_metrics":{
            "Accuracy":65.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96140"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18034,
        "row_id":73804,
        "rank":2,
        "method":"PNP-VQA",
        "mlmodel":{

        },
        "method_short":"PNP-VQA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-17",
        "metrics":{
            "Accuracy":"63.3"
        },
        "raw_metrics":{
            "Accuracy":63.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1094335,
            "title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training",
            "url":"\/paper\/plug-and-play-vqa-zero-shot-vqa-by-conjoining",
            "published":"2022-10-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/plug-and-play-vqa-zero-shot-vqa-by-conjoining\/review\/?hl=73804"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":18034,
        "row_id":96139,
        "rank":3,
        "method":"BLIP-2 ViT-G FlanT5 XL (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G FlanT5 XL ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"63.1"
        },
        "raw_metrics":{
            "Accuracy":63.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96139"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18034,
        "row_id":96138,
        "rank":4,
        "method":"BLIP-2 ViT-L FlanT5 XL (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-L FlanT5 XL ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"62.6"
        },
        "raw_metrics":{
            "Accuracy":62.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96138"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18034,
        "row_id":96137,
        "rank":5,
        "method":"BLIP-2 ViT-G OPT 6.7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G OPT 6.7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"54.3"
        },
        "raw_metrics":{
            "Accuracy":54.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96137"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18034,
        "row_id":96136,
        "rank":6,
        "method":"BLIP-2 ViT-G OPT 2.7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G OPT 2.7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"53.5"
        },
        "raw_metrics":{
            "Accuracy":53.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96136"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18034,
        "row_id":96135,
        "rank":7,
        "method":"BLIP-2 ViT-L OPT 2.7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-L OPT 2.7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"50.1"
        },
        "raw_metrics":{
            "Accuracy":50.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96135"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18034,
        "row_id":96141,
        "rank":8,
        "method":"Few VLM (zero-shot)",
        "mlmodel":{

        },
        "method_short":"Few VLM ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-16",
        "metrics":{
            "Accuracy":"47.7"
        },
        "raw_metrics":{
            "Accuracy":47.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":890167,
            "title":"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models",
            "url":"\/paper\/a-good-prompt-is-worth-millions-of-parameters",
            "published":"2021-10-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-good-prompt-is-worth-millions-of-parameters\/review\/?hl=96141"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":18034,
        "row_id":56658,
        "rank":9,
        "method":"MetaLM",
        "mlmodel":{

        },
        "method_short":"MetaLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-13",
        "metrics":{
            "Accuracy":"41.1"
        },
        "raw_metrics":{
            "Accuracy":41.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1026079,
            "title":"Language Models are General-Purpose Interfaces",
            "url":"\/paper\/language-models-are-general-purpose",
            "published":"2022-06-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-general-purpose\/review\/?hl=56658"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":18034,
        "row_id":56659,
        "rank":10,
        "method":"VLKD(ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"VLKD",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-16",
        "metrics":{
            "Accuracy":"38.6"
        },
        "raw_metrics":{
            "Accuracy":38.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":920364,
            "title":"Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation",
            "url":"\/paper\/enabling-multimodal-generation-on-clip-via",
            "published":"2021-11-16T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":18034,
        "row_id":56660,
        "rank":11,
        "method":"Frozen",
        "mlmodel":{

        },
        "method_short":"Frozen",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-25",
        "metrics":{
            "Accuracy":"29.5"
        },
        "raw_metrics":{
            "Accuracy":29.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":825770,
            "title":"Multimodal Few-Shot Learning with Frozen Language Models",
            "url":"\/paper\/multimodal-few-shot-learning-with-frozen",
            "published":"2021-06-25T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/multimodal-few-shot-learning-with-frozen\/review\/?hl=56660"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    }
]