[
    {
        "table_id":24390,
        "row_id":107144,
        "rank":1,
        "method":"BLIP-2 ViT-G OPT 6.7B (fine-tuned)",
        "mlmodel":{

        },
        "Model":"BLIP-2 ViT-G OPT 6.7B ",
        "method_details":"fine-tuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"82.30"
        },
        "raw_metrics":{
            "Accuracy":82.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=107144"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24390,
        "row_id":113020,
        "rank":2,
        "method":"CoCa",
        "mlmodel":{

        },
        "Model":"CoCa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Accuracy":"82.3"
        },
        "raw_metrics":{
            "Accuracy":82.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=113020"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24390,
        "row_id":107345,
        "rank":3,
        "method":"OFA",
        "mlmodel":{

        },
        "Model":"OFA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-07",
        "metrics":{
            "Accuracy":"82.0"
        },
        "raw_metrics":{
            "Accuracy":82.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":956719,
            "title":"OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework",
            "url":"\/paper\/unifying-architectures-tasks-and-modalities",
            "published":"2022-02-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unifying-architectures-tasks-and-modalities\/review\/?hl=107345"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24390,
        "row_id":107140,
        "rank":4,
        "method":"BLIP-2 ViT-G OPT 2.7B (fine-tuned)",
        "mlmodel":{

        },
        "Model":"BLIP-2 ViT-G OPT 2.7B ",
        "method_details":"fine-tuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"81.74"
        },
        "raw_metrics":{
            "Accuracy":81.74
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=107140"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24390,
        "row_id":107141,
        "rank":5,
        "method":"BLIP-2 ViT-G FlanT5 XL (fine-tuned)",
        "mlmodel":{

        },
        "Model":"BLIP-2 ViT-G FlanT5 XL ",
        "method_details":"fine-tuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"81.66"
        },
        "raw_metrics":{
            "Accuracy":81.66
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=107141"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24390,
        "row_id":111274,
        "rank":6,
        "method":"Aurora (ours, r=64)",
        "mlmodel":{

        },
        "Model":"Aurora ",
        "method_details":"ours, r=64",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Accuracy":"77.69"
        },
        "raw_metrics":{
            "Accuracy":77.69
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24390,
        "row_id":110631,
        "rank":7,
        "method":"VK-OOD",
        "mlmodel":{

        },
        "Model":"VK-OOD",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-11",
        "metrics":{
            "Accuracy":"76.8"
        },
        "raw_metrics":{
            "Accuracy":76.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1156677,
            "title":"Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis",
            "url":"\/paper\/differentiable-outlier-detection-enable",
            "published":"2023-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/differentiable-outlier-detection-enable\/review\/?hl=110631"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24390,
        "row_id":111251,
        "rank":8,
        "method":"LXMERT (low-magnitude pruning)",
        "mlmodel":{

        },
        "Model":"LXMERT ",
        "method_details":"low-magnitude pruning",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-23",
        "metrics":{
            "Accuracy":"70.72"
        },
        "raw_metrics":{
            "Accuracy":70.72
        },
        "uses_additional_data":false,
        "paper":{
            "id":1307342,
            "title":"LXMERT Model Compression for Visual Question Answering",
            "url":"\/paper\/lxmert-model-compression-for-visual-question",
            "published":"2023-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lxmert-model-compression-for-visual-question\/review\/?hl=111251"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]