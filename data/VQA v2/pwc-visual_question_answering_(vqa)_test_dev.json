[
    {
        "table_id":2548,
        "row_id":66432,
        "rank":1,
        "method":"PaLI",
        "mlmodel":{

        },
        "Model":"PaLI",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-14",
        "metrics":{
            "Accuracy":"84.3"
        },
        "raw_metrics":{
            "Accuracy":84.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1074922,
            "title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model",
            "url":"\/paper\/pali-a-jointly-scaled-multilingual-language",
            "published":"2022-09-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pali-a-jointly-scaled-multilingual-language\/review\/?hl=66432"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":64305,
        "rank":2,
        "method":"BEiT-3",
        "mlmodel":{

        },
        "Model":"BEiT-3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-22",
        "metrics":{
            "Accuracy":"84.19"
        },
        "raw_metrics":{
            "Accuracy":84.19
        },
        "uses_additional_data":false,
        "paper":{
            "id":1062207,
            "title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
            "url":"\/paper\/image-as-a-foreign-language-beit-pretraining",
            "published":"2022-08-22T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":48331,
        "rank":3,
        "method":"VLMo",
        "mlmodel":{

        },
        "Model":"VLMo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-03",
        "metrics":{
            "Accuracy":"82.78"
        },
        "raw_metrics":{
            "Accuracy":82.78
        },
        "uses_additional_data":false,
        "paper":{
            "id":900016,
            "title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts",
            "url":"\/paper\/vlmo-unified-vision-language-pre-training",
            "published":"2021-11-03T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":103005,
        "rank":4,
        "method":"ONE-PEACE",
        "mlmodel":{

        },
        "Model":"ONE-PEACE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-18",
        "metrics":{
            "Accuracy":"82.6"
        },
        "raw_metrics":{
            "Accuracy":82.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1211430,
            "title":"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
            "url":"\/paper\/one-peace-exploring-one-general",
            "published":"2023-05-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-peace-exploring-one-general\/review\/?hl=103005"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":63475,
        "rank":5,
        "method":"mPLUG (Huge)",
        "mlmodel":{

        },
        "Model":"mPLUG ",
        "method_details":"Huge",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-24",
        "metrics":{
            "Accuracy":"82.43"
        },
        "raw_metrics":{
            "Accuracy":82.43
        },
        "uses_additional_data":false,
        "paper":{
            "id":1015224,
            "title":"mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections",
            "url":"\/paper\/mplug-effective-and-efficient-vision-language",
            "published":"2022-05-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-effective-and-efficient-vision-language\/review\/?hl=63475"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":80502,
        "rank":6,
        "method":"X2-VLM (large)",
        "mlmodel":{

        },
        "Model":"X2-VLM ",
        "method_details":"large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-22",
        "metrics":{
            "Accuracy":"81.9"
        },
        "raw_metrics":{
            "Accuracy":81.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":1116038,
            "title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
            "url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for",
            "published":"2022-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for\/review\/?hl=80502"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":63476,
        "rank":7,
        "method":"MMU",
        "mlmodel":{

        },
        "Model":"MMU",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-30",
        "metrics":{
            "Accuracy":"81.26"
        },
        "raw_metrics":{
            "Accuracy":81.26
        },
        "uses_additional_data":false,
        "paper":{
            "id":911739,
            "title":"Achieving Human Parity on Visual Question Answering",
            "url":"\/paper\/achieving-human-parity-on-visual-question",
            "published":"2021-11-17T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/achieving-human-parity-on-visual-question\/review\/?hl=63476"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":96426,
        "rank":8,
        "method":"mPLUG-2",
        "mlmodel":{

        },
        "Model":"mPLUG-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-01",
        "metrics":{
            "Accuracy":"81.11"
        },
        "raw_metrics":{
            "Accuracy":81.11
        },
        "uses_additional_data":false,
        "paper":{
            "id":1151002,
            "title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",
            "url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation",
            "published":"2023-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation\/review\/?hl=96426"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":80501,
        "rank":9,
        "method":"X2-VLM (base)",
        "mlmodel":{

        },
        "Model":"X2-VLM ",
        "method_details":"base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-22",
        "metrics":{
            "Accuracy":"80.4"
        },
        "raw_metrics":{
            "Accuracy":80.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1116038,
            "title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
            "url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for",
            "published":"2022-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for\/review\/?hl=80501"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":103927,
        "rank":10,
        "method":"XFM (base)",
        "mlmodel":{

        },
        "Model":"XFM ",
        "method_details":"base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-12",
        "metrics":{
            "Accuracy":"80.4"
        },
        "raw_metrics":{
            "Accuracy":80.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1141541,
            "title":"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",
            "url":"\/paper\/toward-building-general-foundation-models-for",
            "published":"2023-01-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/toward-building-general-foundation-models-for\/review\/?hl=103927"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":104614,
        "rank":11,
        "method":"VAST",
        "mlmodel":{

        },
        "Model":"VAST",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Accuracy":"80.23"
        },
        "raw_metrics":{
            "Accuracy":80.23
        },
        "uses_additional_data":true,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":43165,
        "rank":12,
        "method":"Florence",
        "mlmodel":{

        },
        "Model":"Florence",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-22",
        "metrics":{
            "Accuracy":"80.16"
        },
        "raw_metrics":{
            "Accuracy":80.16
        },
        "uses_additional_data":false,
        "paper":{
            "id":914419,
            "title":"Florence: A New Foundation Model for Computer Vision",
            "url":"\/paper\/florence-a-new-foundation-model-for-computer",
            "published":"2021-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/florence-a-new-foundation-model-for-computer\/review\/?hl=43165"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":43424,
        "rank":13,
        "method":"SimVLM",
        "mlmodel":{

        },
        "Model":"SimVLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-08-24",
        "metrics":{
            "Accuracy":"80.03"
        },
        "raw_metrics":{
            "Accuracy":80.03
        },
        "uses_additional_data":false,
        "paper":{
            "id":856712,
            "title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
            "url":"\/paper\/simvlm-simple-visual-language-model",
            "published":"2021-08-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/simvlm-simple-visual-language-model\/review\/?hl=43424"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":101427,
        "rank":14,
        "method":"VALOR",
        "mlmodel":{

        },
        "Model":"VALOR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-17",
        "metrics":{
            "Accuracy":"78.46"
        },
        "raw_metrics":{
            "Accuracy":78.46
        },
        "uses_additional_data":true,
        "paper":{
            "id":1191887,
            "title":"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset",
            "url":"\/paper\/valor-vision-audio-language-omni-perception",
            "published":"2023-04-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/valor-vision-audio-language-omni-perception\/review\/?hl=101427"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":99015,
        "rank":15,
        "method":"Prismer",
        "mlmodel":{

        },
        "Model":"Prismer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-04",
        "metrics":{
            "Accuracy":"78.43"
        },
        "raw_metrics":{
            "Accuracy":78.43
        },
        "uses_additional_data":false,
        "paper":{
            "id":1168347,
            "title":"Prismer: A Vision-Language Model with An Ensemble of Experts",
            "url":"\/paper\/prismer-a-vision-language-model-with-an",
            "published":"2023-03-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/prismer-a-vision-language-model-with-an\/review\/?hl=99015"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":47856,
        "rank":16,
        "method":"X-VLM (base)",
        "mlmodel":{

        },
        "Model":"X-VLM ",
        "method_details":"base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-16",
        "metrics":{
            "Accuracy":"78.22"
        },
        "raw_metrics":{
            "Accuracy":78.22
        },
        "uses_additional_data":false,
        "paper":{
            "id":911012,
            "title":"Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts",
            "url":"\/paper\/multi-grained-vision-language-pre-training",
            "published":"2021-11-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multi-grained-vision-language-pre-training\/review\/?hl=47856"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":44598,
        "rank":17,
        "method":"ALBEF (14M)",
        "mlmodel":{

        },
        "Model":"ALBEF ",
        "method_details":"14M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-16",
        "metrics":{
            "Accuracy":"75.84"
        },
        "raw_metrics":{
            "Accuracy":75.84
        },
        "uses_additional_data":false,
        "paper":{
            "id":836928,
            "title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
            "url":"\/paper\/align-before-fuse-vision-and-language",
            "published":"2021-07-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/align-before-fuse-vision-and-language\/review\/?hl=44598"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":18113,
        "rank":18,
        "method":"Oscar",
        "mlmodel":{

        },
        "Model":"Oscar",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-13",
        "metrics":{
            "Accuracy":"73.82"
        },
        "raw_metrics":{
            "Accuracy":73.82
        },
        "uses_additional_data":false,
        "paper":{
            "id":190875,
            "title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
            "url":"\/paper\/oscar-object-semantics-aligned-pre-training",
            "published":"2020-04-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/oscar-object-semantics-aligned-pre-training\/review\/?hl=18113"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14759,
        "rank":19,
        "method":"UNITER (Large)",
        "mlmodel":{

        },
        "Model":"UNITER ",
        "method_details":"Large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-25",
        "metrics":{
            "Accuracy":"73.24"
        },
        "raw_metrics":{
            "Accuracy":73.24
        },
        "uses_additional_data":false,
        "paper":{
            "id":156206,
            "title":"UNITER: UNiversal Image-TExt Representation Learning",
            "url":"\/paper\/uniter-learning-universal-image-text-1",
            "published":"2019-09-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/uniter-learning-universal-image-text-1\/review\/?hl=14759"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":16080,
        "rank":20,
        "method":"X-101 grid features + MCAN",
        "mlmodel":{

        },
        "Model":"X-101 grid features + MCAN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-08",
        "metrics":{
            "Accuracy":"72.59"
        },
        "raw_metrics":{
            "Accuracy":72.59
        },
        "uses_additional_data":false,
        "paper":{
            "id":179526,
            "title":"In Defense of Grid Features for Visual Question Answering",
            "url":"\/paper\/in-defense-of-grid-features-for-visual",
            "published":"2020-01-10T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":40528,
        "rank":21,
        "method":"CFR",
        "mlmodel":{

        },
        "Model":"CFR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-06",
        "metrics":{
            "Accuracy":"72.5"
        },
        "raw_metrics":{
            "Accuracy":72.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":880243,
            "title":"Coarse-to-Fine Reasoning for Visual Question Answering",
            "url":"\/paper\/coarse-to-fine-reasoning-for-visual-question",
            "published":"2021-10-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coarse-to-fine-reasoning-for-visual-question\/review\/?hl=40528"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14760,
        "rank":22,
        "method":"VL-BERTLARGE",
        "mlmodel":{

        },
        "Model":"VL-BERTLARGE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-22",
        "metrics":{
            "Accuracy":"71.79"
        },
        "raw_metrics":{
            "Accuracy":71.79
        },
        "uses_additional_data":false,
        "paper":{
            "id":150926,
            "title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations",
            "url":"\/paper\/vl-bert-pre-training-of-generic-visual",
            "published":"2019-08-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vl-bert-pre-training-of-generic-visual\/review\/?hl=14760"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":35249,
        "rank":23,
        "method":"ViLT-B\/32",
        "mlmodel":{

        },
        "Model":"ViLT-B\/32",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-05",
        "metrics":{
            "Accuracy":"71.26"
        },
        "raw_metrics":{
            "Accuracy":71.26
        },
        "uses_additional_data":false,
        "paper":{
            "id":742872,
            "title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
            "url":"\/paper\/vilt-vision-and-language-transformer-without",
            "published":"2021-02-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vilt-vision-and-language-transformer-without\/review\/?hl=35249"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":18115,
        "rank":24,
        "method":"MCAN+VC",
        "mlmodel":{

        },
        "Model":"MCAN+VC",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-02-27",
        "metrics":{
            "Accuracy":"71.21"
        },
        "raw_metrics":{
            "Accuracy":71.21
        },
        "uses_additional_data":false,
        "paper":{
            "id":185038,
            "title":"Visual Commonsense R-CNN",
            "url":"\/paper\/visual-commonsense-r-cnn",
            "published":"2020-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-commonsense-r-cnn\/review\/?hl=18115"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14761,
        "rank":25,
        "method":"VL-BERTBASE",
        "mlmodel":{

        },
        "Model":"VL-BERTBASE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-22",
        "metrics":{
            "Accuracy":"71.16"
        },
        "raw_metrics":{
            "Accuracy":71.16
        },
        "uses_additional_data":false,
        "paper":{
            "id":150926,
            "title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations",
            "url":"\/paper\/vl-bert-pre-training-of-generic-visual",
            "published":"2019-08-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vl-bert-pre-training-of-generic-visual\/review\/?hl=14761"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14762,
        "rank":26,
        "method":"VisualBERT",
        "mlmodel":{

        },
        "Model":"VisualBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-09",
        "metrics":{
            "Accuracy":"70.8"
        },
        "raw_metrics":{
            "Accuracy":70.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":149599,
            "title":"VisualBERT: A Simple and Performant Baseline for Vision and Language",
            "url":"\/paper\/visualbert-a-simple-and-performant-baseline",
            "published":"2019-08-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visualbert-a-simple-and-performant-baseline\/review\/?hl=14762"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14763,
        "rank":27,
        "method":"MCANed-6",
        "mlmodel":{

        },
        "Model":"MCANed-6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-25",
        "metrics":{
            "Accuracy":"70.63"
        },
        "raw_metrics":{
            "Accuracy":70.63
        },
        "uses_additional_data":false,
        "paper":{
            "id":143988,
            "title":"Deep Modular Co-Attention Networks for Visual Question Answering",
            "url":"\/paper\/deep-modular-co-attention-networks-for-visual-1",
            "published":"2019-06-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deep-modular-co-attention-networks-for-visual-1\/review\/?hl=14763"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":50754,
        "rank":28,
        "method":"ViLBERT",
        "mlmodel":{

        },
        "Model":"ViLBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-06",
        "metrics":{
            "Accuracy":"70.55"
        },
        "raw_metrics":{
            "Accuracy":70.55
        },
        "uses_additional_data":false,
        "paper":{
            "id":149243,
            "title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
            "url":"\/paper\/vilbert-pretraining-task-agnostic",
            "published":"2019-08-06T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14764,
        "rank":29,
        "method":"BAN+Glove+Counter",
        "mlmodel":{

        },
        "Model":"BAN+Glove+Counter",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-05-21",
        "metrics":{
            "Accuracy":"70.04"
        },
        "raw_metrics":{
            "Accuracy":70.04
        },
        "uses_additional_data":false,
        "paper":{
            "id":3201,
            "title":"Bilinear Attention Networks",
            "url":"\/paper\/bilinear-attention-networks",
            "published":"2018-05-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bilinear-attention-networks\/review\/?hl=14764"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14765,
        "rank":30,
        "method":"LXMERT (Pre-train + scratch)",
        "mlmodel":{

        },
        "Model":"LXMERT ",
        "method_details":"Pre-train + scratch",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-20",
        "metrics":{
            "Accuracy":"69.9"
        },
        "raw_metrics":{
            "Accuracy":69.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":150646,
            "title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
            "url":"\/paper\/lxmert-learning-cross-modality-encoder",
            "published":"2019-08-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lxmert-learning-cross-modality-encoder\/review\/?hl=14765"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14766,
        "rank":31,
        "method":"Image features from bottom-up attention (adaptive K, ensemble)",
        "mlmodel":{

        },
        "Model":"Image features from bottom-up attention ",
        "method_details":"adaptive K, ensemble",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-08-09",
        "metrics":{
            "Accuracy":"69.87"
        },
        "raw_metrics":{
            "Accuracy":69.87
        },
        "uses_additional_data":false,
        "paper":{
            "id":19063,
            "title":"Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge",
            "url":"\/paper\/tips-and-tricks-for-visual-question-answering",
            "published":"2017-08-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tips-and-tricks-for-visual-question-answering\/review\/?hl=14766"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14767,
        "rank":32,
        "method":"Pythia v0.3 + LoRRA",
        "mlmodel":{

        },
        "Model":"Pythia v0.3 + LoRRA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-18",
        "metrics":{
            "Accuracy":"69.21"
        },
        "raw_metrics":{
            "Accuracy":69.21
        },
        "uses_additional_data":false,
        "paper":{
            "id":112032,
            "title":"Towards VQA Models That Can Read",
            "url":"\/paper\/towards-vqa-models-that-can-read",
            "published":"2019-04-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/towards-vqa-models-that-can-read\/review\/?hl=14767"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14768,
        "rank":33,
        "method":"DMN",
        "mlmodel":{

        },
        "Model":"DMN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-02-15",
        "metrics":{
            "Accuracy":"68.09"
        },
        "raw_metrics":{
            "Accuracy":68.09
        },
        "uses_additional_data":false,
        "paper":{
            "id":10147,
            "title":"Learning to Count Objects in Natural Images for Visual Question Answering",
            "url":"\/paper\/learning-to-count-objects-in-natural-images",
            "published":"2018-02-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-to-count-objects-in-natural-images\/review\/?hl=14768"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":99570,
        "rank":34,
        "method":"LaKo",
        "mlmodel":{

        },
        "Model":"LaKo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-26",
        "metrics":{
            "Accuracy":"68.07"
        },
        "raw_metrics":{
            "Accuracy":68.07
        },
        "uses_additional_data":false,
        "paper":{
            "id":1049899,
            "title":"LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection",
            "url":"\/paper\/lako-knowledge-driven-visual-question",
            "published":"2022-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lako-knowledge-driven-visual-question\/review\/?hl=99570"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14769,
        "rank":35,
        "method":"MuRel",
        "mlmodel":{

        },
        "Model":"MuRel",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-25",
        "metrics":{
            "Accuracy":"68.03"
        },
        "raw_metrics":{
            "Accuracy":68.03
        },
        "uses_additional_data":false,
        "paper":{
            "id":106702,
            "title":"MUREL: Multimodal Relational Reasoning for Visual Question Answering",
            "url":"\/paper\/murel-multimodal-relational-reasoning-for",
            "published":"2019-02-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/murel-multimodal-relational-reasoning-for\/review\/?hl=14769"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14770,
        "rank":36,
        "method":"BLOCK",
        "mlmodel":{

        },
        "Model":"BLOCK",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-01-31",
        "metrics":{
            "Accuracy":"67.58"
        },
        "raw_metrics":{
            "Accuracy":67.58
        },
        "uses_additional_data":false,
        "paper":{
            "id":93173,
            "title":"BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection",
            "url":"\/paper\/block-bilinear-superdiagonal-fusion-for",
            "published":"2019-01-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/block-bilinear-superdiagonal-fusion-for\/review\/?hl=14770"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14771,
        "rank":37,
        "method":"MUTAN",
        "mlmodel":{

        },
        "Model":"MUTAN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-05-18",
        "metrics":{
            "Accuracy":"67.42"
        },
        "raw_metrics":{
            "Accuracy":67.42
        },
        "uses_additional_data":false,
        "paper":{
            "id":22787,
            "title":"MUTAN: Multimodal Tucker Fusion for Visual Question Answering",
            "url":"\/paper\/mutan-multimodal-tucker-fusion-for-visual",
            "published":"2017-05-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mutan-multimodal-tucker-fusion-for-visual\/review\/?hl=14771"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14772,
        "rank":38,
        "method":"BAN2-CTI",
        "mlmodel":{

        },
        "Model":"BAN2-CTI",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-26",
        "metrics":{
            "Accuracy":"67.4"
        },
        "raw_metrics":{
            "Accuracy":67.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":156168,
            "title":"Compact Trilinear Interaction for Visual Question Answering",
            "url":"\/paper\/compact-trilinear-interaction-for-visual",
            "published":"2019-09-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/compact-trilinear-interaction-for-visual\/review\/?hl=14772"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":17291,
        "rank":39,
        "method":"2D continuous softmax",
        "mlmodel":{

        },
        "Model":"2D continuous softmax",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-12",
        "metrics":{
            "Accuracy":"65.96"
        },
        "raw_metrics":{
            "Accuracy":65.96
        },
        "uses_additional_data":false,
        "paper":{
            "id":201993,
            "title":"Sparse and Continuous Attention Mechanisms",
            "url":"\/paper\/sparse-and-continuous-attention-mechanisms",
            "published":"2020-06-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sparse-and-continuous-attention-mechanisms\/review\/?hl=17291"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":96129,
        "rank":40,
        "method":"BLIP-2 ViT-G FlanT5 XXL (zero-shot)",
        "mlmodel":{

        },
        "Model":"BLIP-2 ViT-G FlanT5 XXL ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"65"
        },
        "raw_metrics":{
            "Accuracy":65.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96129"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14773,
        "rank":41,
        "method":"N2NMN (ResNet-152, policy search)",
        "mlmodel":{

        },
        "Model":"N2NMN ",
        "method_details":"ResNet-152, policy search",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-04-18",
        "metrics":{
            "Accuracy":"64.9"
        },
        "raw_metrics":{
            "Accuracy":64.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":17679,
            "title":"Learning to Reason: End-to-End Module Networks for Visual Question Answering",
            "url":"\/paper\/learning-to-reason-end-to-end-module-networks",
            "published":"2017-04-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-to-reason-end-to-end-module-networks\/review\/?hl=14773"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":73805,
        "rank":42,
        "method":"PNP-VQA",
        "mlmodel":{

        },
        "Model":"PNP-VQA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-17",
        "metrics":{
            "Accuracy":"64.8"
        },
        "raw_metrics":{
            "Accuracy":64.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1094335,
            "title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training",
            "url":"\/paper\/plug-and-play-vqa-zero-shot-vqa-by-conjoining",
            "published":"2022-10-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/plug-and-play-vqa-zero-shot-vqa-by-conjoining\/review\/?hl=73805"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14774,
        "rank":43,
        "method":"MCB",
        "mlmodel":{

        },
        "Model":"MCB",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-06-06",
        "metrics":{
            "Accuracy":"64.7"
        },
        "raw_metrics":{
            "Accuracy":64.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":30320,
            "title":"Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding",
            "url":"\/paper\/multimodal-compact-bilinear-pooling-for",
            "published":"2016-06-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multimodal-compact-bilinear-pooling-for\/review\/?hl=14774"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":14775,
        "rank":44,
        "method":"RUBi",
        "mlmodel":{

        },
        "Model":"RUBi",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-24",
        "metrics":{
            "Accuracy":"63.18"
        },
        "raw_metrics":{
            "Accuracy":63.18
        },
        "uses_additional_data":false,
        "paper":{
            "id":143892,
            "title":"RUBi: Reducing Unimodal Biases in Visual Question Answering",
            "url":"\/paper\/rubi-reducing-unimodal-biases-in-visual",
            "published":"2019-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rubi-reducing-unimodal-biases-in-visual\/review\/?hl=14775"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":96128,
        "rank":45,
        "method":"BLIP-2 ViT-G FlanT5 XL (zero-shot)",
        "mlmodel":{

        },
        "Model":"BLIP-2 ViT-G FlanT5 XL ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"63"
        },
        "raw_metrics":{
            "Accuracy":63.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96128"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":96130,
        "rank":46,
        "method":"BLIP-2 ViT-L FlanT5 XL (zero-shot)",
        "mlmodel":{

        },
        "Model":"BLIP-2 ViT-L FlanT5 XL ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"62.3"
        },
        "raw_metrics":{
            "Accuracy":62.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96130"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":96134,
        "rank":47,
        "method":"Flamingo 80B",
        "mlmodel":{

        },
        "Model":"Flamingo 80B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-29",
        "metrics":{
            "Accuracy":"56.3"
        },
        "raw_metrics":{
            "Accuracy":56.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1001838,
            "title":"Flamingo: a Visual Language Model for Few-Shot Learning",
            "url":"\/paper\/flamingo-a-visual-language-model-for-few-shot-1",
            "published":"2022-04-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":96126,
        "rank":48,
        "method":"BLIP-2 ViT-G OPT 6.7B (zero-shot)",
        "mlmodel":{

        },
        "Model":"BLIP-2 ViT-G OPT 6.7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"52.6"
        },
        "raw_metrics":{
            "Accuracy":52.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96126"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":96127,
        "rank":49,
        "method":"BLIP-2 ViT-G OPT 2.7B (zero-shot)",
        "mlmodel":{

        },
        "Model":"BLIP-2 ViT-G OPT 2.7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"52.3"
        },
        "raw_metrics":{
            "Accuracy":52.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96127"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":96133,
        "rank":50,
        "method":"Flamingo 9B",
        "mlmodel":{

        },
        "Model":"Flamingo 9B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-29",
        "metrics":{
            "Accuracy":"51.8"
        },
        "raw_metrics":{
            "Accuracy":51.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1001838,
            "title":"Flamingo: a Visual Language Model for Few-Shot Learning",
            "url":"\/paper\/flamingo-a-visual-language-model-for-few-shot-1",
            "published":"2022-04-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":98420,
        "rank":51,
        "method":"KOSMOS-1 1.6B (zero-shot)",
        "mlmodel":{

        },
        "Model":"KOSMOS-1 1.6B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Accuracy":"51.0"
        },
        "raw_metrics":{
            "Accuracy":51.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":96125,
        "rank":52,
        "method":"BLIP-2 ViT-L OPT 2.7B (zero-shot)",
        "mlmodel":{

        },
        "Model":"BLIP-2 ViT-L OPT 2.7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"49.7"
        },
        "raw_metrics":{
            "Accuracy":49.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96125"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":96132,
        "rank":53,
        "method":"Flamingo 3B",
        "mlmodel":{

        },
        "Model":"Flamingo 3B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-29",
        "metrics":{
            "Accuracy":"49.2"
        },
        "raw_metrics":{
            "Accuracy":49.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1001838,
            "title":"Flamingo: a Visual Language Model for Few-Shot Learning",
            "url":"\/paper\/flamingo-a-visual-language-model-for-few-shot-1",
            "published":"2022-04-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2548,
        "row_id":96131,
        "rank":54,
        "method":"VLKD",
        "mlmodel":{

        },
        "Model":"VLKD",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-16",
        "metrics":{
            "Accuracy":"44.5"
        },
        "raw_metrics":{
            "Accuracy":44.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":920364,
            "title":"Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation",
            "url":"\/paper\/enabling-multimodal-generation-on-clip-via",
            "published":"2021-11-16T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]