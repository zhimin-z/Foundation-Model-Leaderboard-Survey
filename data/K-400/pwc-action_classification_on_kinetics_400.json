[
    {
        "table_id":1176,
        "row_id":86791,
        "rank":1,
        "method":"InternVideo",
        "mlmodel":{

        },
        "method_short":"InternVideo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Acc@1":"91.1",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":91.1,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124231,
            "title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
            "url":"\/paper\/internvideo-general-video-foundation-models",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internvideo-general-video-foundation-models\/review\/?hl=86791"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":113023,
        "rank":2,
        "method":"OmniVec",
        "mlmodel":{

        },
        "method_short":"OmniVec",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-07",
        "metrics":{
            "Acc@1":"91.1",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":91.1,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1319233,
            "title":"OmniVec: Learning robust representations with cross modal sharing",
            "url":"\/paper\/omnivec-learning-robust-representations-with",
            "published":"2023-11-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivec-learning-robust-representations-with\/review\/?hl=113023"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":86974,
        "rank":3,
        "method":"TubeViT-H (ImageNet-1k)",
        "mlmodel":{

        },
        "method_short":"TubeViT-H ",
        "method_details":"ImageNet-1k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Acc@1":"90.9",
            "Acc@5":"98.9",
            "FLOPs (G) x views":"176400x4x3",
            "Parameters (M)":"632"
        },
        "raw_metrics":{
            "Acc@1":90.9,
            "Acc@5":98.9,
            "FLOPs (G) x views":176400.0,
            "Parameters (M)":632.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124229,
            "title":"Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning",
            "url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for\/review\/?hl=86974"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":110507,
        "rank":4,
        "method":"Unmasked Teacher (ViT-L)",
        "mlmodel":{

        },
        "method_short":"Unmasked Teacher ",
        "method_details":"ViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "Acc@1":"90.6",
            "Acc@5":"98.7",
            "FLOPs (G) x views":"1434\u00d73\u00d74",
            "Parameters (M)":"304"
        },
        "raw_metrics":{
            "Acc@1":90.6,
            "Acc@5":98.7,
            "FLOPs (G) x views":1434.0,
            "Parameters (M)":304.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1181934,
            "title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
            "url":"\/paper\/unmasked-teacher-towards-training-efficient",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":100380,
        "rank":5,
        "method":"UMT-L (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"UMT-L ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "Acc@1":"90.6",
            "Acc@5":"98.7",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":90.6,
            "Acc@5":98.7,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1181934,
            "title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
            "url":"\/paper\/unmasked-teacher-towards-training-efficient",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":145,
                "name":"ViT",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":86973,
        "rank":6,
        "method":"TubeVit-L (ImageNet-1k)",
        "mlmodel":{

        },
        "method_short":"TubeVit-L ",
        "method_details":"ImageNet-1k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Acc@1":"90.2",
            "Acc@5":"98.6",
            "FLOPs (G) x views":"95300x4x3",
            "Parameters (M)":"307"
        },
        "raw_metrics":{
            "Acc@1":90.2,
            "Acc@5":98.6,
            "FLOPs (G) x views":95300.0,
            "Parameters (M)":307.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124229,
            "title":"Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning",
            "url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for\/review\/?hl=86973"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":77318,
        "rank":7,
        "method":"UniFormerV2-L (ViT-L, 336)",
        "mlmodel":{

        },
        "method_short":"UniFormerV2-L ",
        "method_details":"ViT-L, 336",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-22",
        "metrics":{
            "Acc@1":"90.0",
            "Acc@5":"98.4",
            "FLOPs (G) x views":"75300x3x2",
            "Parameters (M)":"354"
        },
        "raw_metrics":{
            "Acc@1":90.0,
            "Acc@5":98.4,
            "FLOPs (G) x views":75300.0,
            "Parameters (M)":354.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1087805,
            "title":"UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer",
            "url":"\/paper\/uniformerv2-spatiotemporal-learning-by-arming",
            "published":"2022-09-22T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":458,
                "name":"K710 Pre-training",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":100523,
        "rank":8,
        "method":"VideoMAE V2-g (64x266x266)",
        "mlmodel":{

        },
        "method_short":"VideoMAE V2-g ",
        "method_details":"64x266x266",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-29",
        "metrics":{
            "Acc@1":"90.0",
            "Acc@5":"98.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":90.0,
            "Acc@5":98.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1182705,
            "title":"VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
            "url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders",
            "published":"2023-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders\/review\/?hl=100523"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":56060,
        "rank":9,
        "method":"MTV-H (WTS 60M)",
        "mlmodel":{

        },
        "method_short":"MTV-H ",
        "method_details":"WTS 60M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-12",
        "metrics":{
            "Acc@1":"89.9",
            "Acc@5":"98.3",
            "FLOPs (G) x views":"735700x4x3",
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":89.9,
            "Acc@5":98.3,
            "FLOPs (G) x views":735700.0,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":944453,
            "title":"Multiview Transformers for Video Recognition",
            "url":"\/paper\/multiview-transformers-for-video-recognition",
            "published":"2022-01-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiview-transformers-for-video-recognition\/review\/?hl=56060"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":172,
                "name":"MTV",
                "color":"#27b6d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":107493,
        "rank":10,
        "method":"TAdaFormer-L\/14",
        "mlmodel":{

        },
        "method_short":"TAdaFormer-L\/14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-10",
        "metrics":{
            "Acc@1":"89.9",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":89.9,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1262390,
            "title":"Temporally-Adaptive Models for Efficient Video Understanding",
            "url":"\/paper\/temporally-adaptive-models-for-efficient",
            "published":"2023-08-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporally-adaptive-models-for-efficient\/review\/?hl=107493"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":78929,
        "rank":11,
        "method":"EVA",
        "mlmodel":{

        },
        "method_short":"EVA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-14",
        "metrics":{
            "Acc@1":"89.7",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":89.7,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1110592,
            "title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
            "url":"\/paper\/eva-exploring-the-limits-of-masked-visual",
            "published":"2022-11-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/eva-exploring-the-limits-of-masked-visual\/review\/?hl=78929"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":106198,
        "rank":12,
        "method":"ATM",
        "mlmodel":{

        },
        "method_short":"ATM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Acc@1":"89.4",
            "Acc@5":"98.3",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":89.4,
            "Acc@5":98.3,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248104,
            "title":"What Can Simple Arithmetic Operations Do for Temporal Modeling?",
            "url":"\/paper\/what-can-simple-arithmetic-operations-do-for",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/what-can-simple-arithmetic-operations-do-for\/review\/?hl=106198"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":54001,
        "rank":13,
        "method":"CoCa (finetuned)",
        "mlmodel":{

        },
        "method_short":"CoCa ",
        "method_details":"finetuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Acc@1":"88.9",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":88.9,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=54001"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":88695,
        "rank":14,
        "method":"BIKE (CLIP ViT-L\/14)",
        "mlmodel":{

        },
        "method_short":"BIKE ",
        "method_details":"CLIP ViT-L\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-31",
        "metrics":{
            "Acc@1":"88.7",
            "Acc@5":"98.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":88.7,
            "Acc@5":98.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136846,
            "title":"Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained Vision-Language Models",
            "url":"\/paper\/bidirectional-cross-modal-knowledge",
            "published":"2022-12-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bidirectional-cross-modal-knowledge\/review\/?hl=88695"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":110509,
        "rank":15,
        "method":"ILA (ViT-L\/14)",
        "mlmodel":{

        },
        "method_short":"ILA ",
        "method_details":"ViT-L\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-20",
        "metrics":{
            "Acc@1":"88.7",
            "Acc@5":"97.8",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":88.7,
            "Acc@5":97.8,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1194756,
            "title":"Implicit Temporal Modeling with Learnable Alignment for Video Recognition",
            "url":"\/paper\/implicit-temporal-modeling-with-learnable",
            "published":"2023-04-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/implicit-temporal-modeling-with-learnable\/review\/?hl=110509"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":112688,
        "rank":16,
        "method":"Side4Video (EVA, ViT-E\/14)",
        "mlmodel":{

        },
        "method_short":"Side4Video ",
        "method_details":"EVA, ViT-E\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Acc@1":"88.6",
            "Acc@5":"98.2",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":88.6,
            "Acc@5":98.2,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327957,
            "title":"Side4Video: Spatial-Temporal Side Network for Memory-Efficient Image-to-Video Transfer Learning",
            "url":"\/paper\/side4video-spatial-temporal-side-network-for",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/side4video-spatial-temporal-side-network-for\/review\/?hl=112688"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":86972,
        "rank":17,
        "method":"TubeVit-B (ImageNet-1k)",
        "mlmodel":{

        },
        "method_short":"TubeVit-B ",
        "method_details":"ImageNet-1k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Acc@1":"88.6",
            "Acc@5":"97.6",
            "FLOPs (G) x views":"8700x3x4",
            "Parameters (M)":"86"
        },
        "raw_metrics":{
            "Acc@1":88.6,
            "Acc@5":97.6,
            "FLOPs (G) x views":8700.0,
            "Parameters (M)":86.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124229,
            "title":"Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning",
            "url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-video-vits-sparse-video-tubes-for\/review\/?hl=86972"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":100524,
        "rank":18,
        "method":"VideoMAE V2-g",
        "mlmodel":{

        },
        "method_short":"VideoMAE V2-g",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-29",
        "metrics":{
            "Acc@1":"88.5",
            "Acc@5":"98.1",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":88.5,
            "Acc@5":98.1,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1182705,
            "title":"VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
            "url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders",
            "published":"2023-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders\/review\/?hl=100524"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":102988,
        "rank":19,
        "method":"ONE-PEACE",
        "mlmodel":{

        },
        "method_short":"ONE-PEACE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-18",
        "metrics":{
            "Acc@1":"88.1",
            "Acc@5":"97.8",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":88.1,
            "Acc@5":97.8,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1211430,
            "title":"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
            "url":"\/paper\/one-peace-exploring-one-general",
            "published":"2023-05-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-peace-exploring-one-general\/review\/?hl=102988"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":53997,
        "rank":20,
        "method":"CoCa (frozen)",
        "mlmodel":{

        },
        "method_short":"CoCa ",
        "method_details":"frozen",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Acc@1":"88.0",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":88.0,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=53997"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":96961,
        "rank":21,
        "method":"ViT-22B",
        "mlmodel":{

        },
        "method_short":"ViT-22B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-10",
        "metrics":{
            "Acc@1":"88.0",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":88.0,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1155994,
            "title":"Scaling Vision Transformers to 22 Billion Parameters",
            "url":"\/paper\/scaling-vision-transformers-to-22-billion",
            "published":"2023-02-10T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58630,
        "rank":22,
        "method":"Text4Vis (CLIP ViT-L\/14)",
        "mlmodel":{

        },
        "method_short":"Text4Vis ",
        "method_details":"CLIP ViT-L\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-04",
        "metrics":{
            "Acc@1":"87.8",
            "Acc@5":"97.6",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":87.8,
            "Acc@5":97.6,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1037200,
            "title":"Revisiting Classifier: Transferring Vision-Language Models for Video Recognition",
            "url":"\/paper\/transferring-textual-knowledge-for-visual",
            "published":"2022-07-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transferring-textual-knowledge-for-visual\/review\/?hl=58630"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":104383,
        "rank":23,
        "method":"Hiera-H (no extra data)",
        "mlmodel":{

        },
        "method_short":"Hiera-H ",
        "method_details":"no extra data",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-01",
        "metrics":{
            "Acc@1":"87.8",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":87.8,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1221231,
            "title":"Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
            "url":"\/paper\/hiera-a-hierarchical-vision-transformer",
            "published":"2023-06-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hiera-a-hierarchical-vision-transformer\/review\/?hl=104383"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":65049,
        "rank":24,
        "method":"EVL (CLIP ViT-L\/14@336px, frozen, 32 frames)",
        "mlmodel":{

        },
        "method_short":"EVL ",
        "method_details":"CLIP ViT-L\/14@336px, frozen, 32 frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-06",
        "metrics":{
            "Acc@1":"87.7",
            "Acc@5":"97.8",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":87.7,
            "Acc@5":97.8,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1055934,
            "title":"Frozen CLIP Models are Efficient Video Learners",
            "url":"\/paper\/frozen-clip-models-are-efficient-video",
            "published":"2022-08-06T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":104592,
        "rank":25,
        "method":"DualPath w\/ ViT-L\/14",
        "mlmodel":{

        },
        "method_short":"DualPath w\/ ViT-L\/14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-17",
        "metrics":{
            "Acc@1":"87.7",
            "Acc@5":"97.8",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":87.7,
            "Acc@5":97.8,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1175926,
            "title":"Dual-path Adaptation from Image to Video Transformers",
            "url":"\/paper\/dual-path-adaptation-from-image-to-video",
            "published":"2023-03-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dual-path-adaptation-from-image-to-video\/review\/?hl=104592"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":61368,
        "rank":26,
        "method":"X-CLIP(ViT-L\/14, CLIP)",
        "mlmodel":{

        },
        "method_short":"X-CLIP",
        "method_details":"ViT-L\/14, CLIP",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-04",
        "metrics":{
            "Acc@1":"87.7",
            "Acc@5":"97.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":87.7,
            "Acc@5":97.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1054742,
            "title":"Expanding Language-Image Pretrained Models for General Video Recognition",
            "url":"\/paper\/expanding-language-image-pretrained-models",
            "published":"2022-08-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/expanding-language-image-pretrained-models\/review\/?hl=61368"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":156,
                "name":"Vision Language",
                "color":"#86ba17"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":101812,
        "rank":27,
        "method":"AIM (CLIP ViT-L\/14, 32x224)",
        "mlmodel":{

        },
        "method_short":"AIM ",
        "method_details":"CLIP ViT-L\/14, 32x224",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-06",
        "metrics":{
            "Acc@1":"87.5",
            "Acc@5":"97.7",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":87.5,
            "Acc@5":97.7,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1153006,
            "title":"AIM: Adapting Image Models for Efficient Video Action Recognition",
            "url":"\/paper\/aim-adapting-image-models-for-efficient-video",
            "published":"2023-02-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/aim-adapting-image-models-for-efficient-video\/review\/?hl=101812"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":72352,
        "rank":28,
        "method":"VideoMAE (no extra data, ViT-H, 32x320x320)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"no extra data, ViT-H, 32x320x320",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "Acc@1":"87.4",
            "Acc@5":"97.6",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":87.4,
            "Acc@5":97.6,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=72352"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58243,
        "rank":29,
        "method":"ST-Adapter (ViT-L, CLIP)",
        "mlmodel":{

        },
        "method_short":"ST-Adapter ",
        "method_details":"ViT-L, CLIP",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-27",
        "metrics":{
            "Acc@1":"87.2",
            "Acc@5":"97.6",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":87.2,
            "Acc@5":97.6,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1034453,
            "title":"ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning",
            "url":"\/paper\/parameter-efficient-image-to-video-transfer",
            "published":"2022-06-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/parameter-efficient-image-to-video-transfer\/review\/?hl=58243"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":145,
                "name":"ViT",
                "color":"#2771D3"
            },
            {
                "id":256,
                "name":"Efficient Adaptation",
                "color":"#a527d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":109641,
        "rank":30,
        "method":"ZeroI2V ViT-L\/14",
        "mlmodel":{

        },
        "method_short":"ZeroI2V ViT-L\/14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-02",
        "metrics":{
            "Acc@1":"87.2",
            "Acc@5":"97.6",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":87.2,
            "Acc@5":97.6,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1292264,
            "title":"ZeroI2V: Zero-Cost Adaptation of Pre-trained Transformers from Image to Video",
            "url":"\/paper\/zeroi2v-zero-cost-adaptation-of-pre-trained",
            "published":"2023-10-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/zeroi2v-zero-cost-adaptation-of-pre-trained\/review\/?hl=109641"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":44407,
        "rank":31,
        "method":"CoVeR (JFT-3B)",
        "mlmodel":{

        },
        "method_short":"CoVeR ",
        "method_details":"JFT-3B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-16",
        "metrics":{
            "Acc@1":"87.2",
            "Acc@5":"97.5",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":87.2,
            "Acc@5":97.5,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":931113,
            "title":"Co-training Transformer with Videos and Images Improves Action Recognition",
            "url":"\/paper\/co-training-transformer-with-videos-and",
            "published":"2021-12-14T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":175,
                "name":"JointTraining",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":98825,
        "rank":32,
        "method":"MVD (K400 pretrain, ViT-H, 16x224x224)",
        "mlmodel":{

        },
        "method_short":"MVD ",
        "method_details":"K400 pretrain, ViT-H, 16x224x224",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "Acc@1":"87.2",
            "Acc@5":"97.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":87.2,
            "Acc@5":97.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1125592,
            "title":"Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning",
            "url":"\/paper\/masked-video-distillation-rethinking-masked",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-video-distillation-rethinking-masked\/review\/?hl=98825"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":96433,
        "rank":33,
        "method":"mPLUG-2",
        "mlmodel":{

        },
        "method_short":"mPLUG-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-01",
        "metrics":{
            "Acc@1":"87.1",
            "Acc@5":"97.7",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":87.1,
            "Acc@5":97.7,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1151002,
            "title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",
            "url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation",
            "published":"2023-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation\/review\/?hl=96433"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":44627,
        "rank":34,
        "method":"MaskFeat (K600, MViT-L)",
        "mlmodel":{

        },
        "method_short":"MaskFeat ",
        "method_details":"K600, MViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-16",
        "metrics":{
            "Acc@1":"87.0",
            "Acc@5":"97.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":87.0,
            "Acc@5":97.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":932132,
            "title":"Masked Feature Prediction for Self-Supervised Visual Pre-Training",
            "url":"\/paper\/masked-feature-prediction-for-self-supervised",
            "published":"2021-12-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-feature-prediction-for-self-supervised\/review\/?hl=44627"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":154,
                "name":"MViT",
                "color":"#d327c5"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":101659,
        "rank":35,
        "method":"VicTR (ViT-L\/14)",
        "mlmodel":{

        },
        "method_short":"VicTR ",
        "method_details":"ViT-L\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-05",
        "metrics":{
            "Acc@1":"87.0",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":87.0,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1186800,
            "title":"VicTR: Video-conditioned Text Representations for Activity Recognition",
            "url":"\/paper\/victr-video-conditioned-text-representations",
            "published":"2023-04-05T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/victr-video-conditioned-text-representations\/review\/?hl=101659"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":42927,
        "rank":36,
        "method":"Video-SwinV2-G (ImageNet-22k and external 70M pretrain)",
        "mlmodel":{

        },
        "method_short":"Video-SwinV2-G ",
        "method_details":"ImageNet-22k and external 70M pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-18",
        "metrics":{
            "Acc@1":"86.8",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":86.8,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":912369,
            "title":"Swin Transformer V2: Scaling Up Capacity and Resolution",
            "url":"\/paper\/swin-transformer-v2-scaling-up-capacity-and",
            "published":"2021-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/swin-transformer-v2-scaling-up-capacity-and\/review\/?hl=42927"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":44628,
        "rank":37,
        "method":"MaskFeat (no extra data, MViT-L)",
        "mlmodel":{

        },
        "method_short":"MaskFeat ",
        "method_details":"no extra data, MViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-16",
        "metrics":{
            "Acc@1":"86.7",
            "Acc@5":"97.3",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":86.7,
            "Acc@5":97.3,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932132,
            "title":"Masked Feature Prediction for Self-Supervised Visual Pre-Training",
            "url":"\/paper\/masked-feature-prediction-for-self-supervised",
            "published":"2021-12-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-feature-prediction-for-self-supervised\/review\/?hl=44628"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":154,
                "name":"MViT",
                "color":"#d327c5"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":72353,
        "rank":38,
        "method":"VideoMAE (no extra data, ViT-H)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"no extra data, ViT-H",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "Acc@1":"86.6",
            "Acc@5":"97.1",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":86.6,
            "Acc@5":97.1,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=72353"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":98826,
        "rank":39,
        "method":"MVD (K400 pretrain, ViT-L, 16x224x224)",
        "mlmodel":{

        },
        "method_short":"MVD ",
        "method_details":"K400 pretrain, ViT-L, 16x224x224",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "Acc@1":"86.4",
            "Acc@5":"97.0",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":86.4,
            "Acc@5":97.0,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1125592,
            "title":"Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning",
            "url":"\/paper\/masked-video-distillation-rethinking-masked",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-video-distillation-rethinking-masked\/review\/?hl=98826"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":107494,
        "rank":40,
        "method":"TAdaConvNeXtV2-B",
        "mlmodel":{

        },
        "method_short":"TAdaConvNeXtV2-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-10",
        "metrics":{
            "Acc@1":"86.4",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":86.4,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1262390,
            "title":"Temporally-Adaptive Models for Efficient Video Understanding",
            "url":"\/paper\/temporally-adaptive-models-for-efficient",
            "published":"2023-08-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporally-adaptive-models-for-efficient\/review\/?hl=107494"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":44412,
        "rank":41,
        "method":"CoVeR (JFT-300M)",
        "mlmodel":{

        },
        "method_short":"CoVeR ",
        "method_details":"JFT-300M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-14",
        "metrics":{
            "Acc@1":"86.3",
            "Acc@5":"97.2",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":86.3,
            "Acc@5":97.2,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":931113,
            "title":"Co-training Transformer with Videos and Images Improves Action Recognition",
            "url":"\/paper\/co-training-transformer-with-videos-and",
            "published":"2021-12-14T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":175,
                "name":"JointTraining",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":49996,
        "rank":42,
        "method":"VideoMAE (no extra data, ViT-L, 32x320x320)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"no extra data, ViT-L, 32x320x320",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "Acc@1":"86.1",
            "Acc@5":"97.3",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":86.1,
            "Acc@5":97.3,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=49996"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":53599,
        "rank":43,
        "method":"MViTv2-L (ImageNet-21k pretrain)",
        "mlmodel":{

        },
        "method_short":"MViTv2-L ",
        "method_details":"ImageNet-21k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Acc@1":"86.1",
            "Acc@5":"97.0",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":86.1,
            "Acc@5":97.0,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=53599"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":110510,
        "rank":44,
        "method":"ILA (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"ILA ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-20",
        "metrics":{
            "Acc@1":"85.7",
            "Acc@5":"97.2",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":85.7,
            "Acc@5":97.2,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1194756,
            "title":"Implicit Temporal Modeling with Learnable Alignment for Video Recognition",
            "url":"\/paper\/implicit-temporal-modeling-with-learnable",
            "published":"2023-04-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/implicit-temporal-modeling-with-learnable\/review\/?hl=110510"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":104591,
        "rank":45,
        "method":"DualPath w\/ ViT-B\/16",
        "mlmodel":{

        },
        "method_short":"DualPath w\/ ViT-B\/16",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-17",
        "metrics":{
            "Acc@1":"85.4",
            "Acc@5":"97.1",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":85.4,
            "Acc@5":97.1,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1175926,
            "title":"Dual-path Adaptation from Image to Video Transformers",
            "url":"\/paper\/dual-path-adaptation-from-image-to-video",
            "published":"2023-03-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dual-path-adaptation-from-image-to-video\/review\/?hl=104591"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":40305,
        "rank":46,
        "method":"TokenLearner 16at18 (L\/10)",
        "mlmodel":{

        },
        "method_short":"TokenLearner 16at18 ",
        "method_details":"L\/10",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-21",
        "metrics":{
            "Acc@1":"85.4",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":85.4,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":821783,
            "title":"TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?",
            "url":"\/paper\/tokenlearner-what-can-8-learned-tokens-do-for",
            "published":"2021-06-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tokenlearner-what-can-8-learned-tokens-do-for\/review\/?hl=40305"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":60450,
        "rank":47,
        "method":"MAR (50% mask, ViT-L, 16x4)",
        "mlmodel":{

        },
        "method_short":"MAR ",
        "method_details":"50% mask, ViT-L, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-24",
        "metrics":{
            "Acc@1":"85.3",
            "Acc@5":"96.3",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":85.3,
            "Acc@5":96.3,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1048887,
            "title":"MAR: Masked Autoencoders for Efficient Action Recognition",
            "url":"\/paper\/mar-masked-autoencoders-for-efficient-action",
            "published":"2022-07-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mar-masked-autoencoders-for-efficient-action\/review\/?hl=60450"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":49997,
        "rank":48,
        "method":"VideoMAE (no extra data, ViT-L, 16x4)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"no extra data, ViT-L, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "Acc@1":"85.2",
            "Acc@5":"96.8",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":85.2,
            "Acc@5":96.8,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=49997"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":112985,
        "rank":49,
        "method":"ViC-MAE (ViT-L)",
        "mlmodel":{

        },
        "method_short":"ViC-MAE ",
        "method_details":"ViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-21",
        "metrics":{
            "Acc@1":"85.1",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":85.1,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1177739,
            "title":"ViC-MAE: Self-Supervised Representation Learning from Images and Video with Contrastive Masked Autoencoders",
            "url":"\/paper\/visual-representation-learning-from-unlabeled",
            "published":"2023-03-21T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/visual-representation-learning-from-unlabeled\/review\/?hl=112985"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":35522,
        "rank":50,
        "method":"Swin-L (384x384, ImageNet-21k pretrain)",
        "mlmodel":{

        },
        "method_short":"Swin-L ",
        "method_details":"384x384, ImageNet-21k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Acc@1":"84.9",
            "Acc@5":"96.7",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":84.9,
            "Acc@5":96.7,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":824241,
            "title":"Video Swin Transformer",
            "url":"\/paper\/video-swin-transformer",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-swin-transformer\/review\/?hl=35522"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":30114,
        "rank":51,
        "method":"ViViT-L\/16x2 (JFT)",
        "mlmodel":{

        },
        "method_short":"ViViT-L\/16x2 ",
        "method_details":"JFT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Acc@1":"84.9",
            "Acc@5":"95.8",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":84.9,
            "Acc@5":95.8,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":758440,
            "title":"ViViT: A Video Vision Transformer",
            "url":"\/paper\/2103-15691",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-15691\/review\/?hl=30114"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":45668,
        "rank":52,
        "method":"OMNIVORE (Swin-L)",
        "mlmodel":{

        },
        "method_short":"OMNIVORE ",
        "method_details":"Swin-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-20",
        "metrics":{
            "Acc@1":"84.1",
            "Acc@5":"96.1",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":84.1,
            "Acc@5":96.1,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":948292,
            "title":"Omnivore: A Single Model for Many Visual Modalities",
            "url":"\/paper\/omnivore-a-single-model-for-many-visual",
            "published":"2022-01-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/omnivore-a-single-model-for-many-visual\/review\/?hl=45668"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":45669,
        "rank":53,
        "method":"OMNIVORE (Swin-B)",
        "mlmodel":{

        },
        "method_short":"OMNIVORE ",
        "method_details":"Swin-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-20",
        "metrics":{
            "Acc@1":"84.0",
            "Acc@5":"96.2",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":84.0,
            "Acc@5":96.2,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":948292,
            "title":"Omnivore: A Single Model for Many Visual Modalities",
            "url":"\/paper\/omnivore-a-single-model-for-many-visual",
            "published":"2022-01-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/omnivore-a-single-model-for-many-visual\/review\/?hl=45669"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":60451,
        "rank":54,
        "method":"MAR (75% mask, ViT-L, 16x4)",
        "mlmodel":{

        },
        "method_short":"MAR ",
        "method_details":"75% mask, ViT-L, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-24",
        "metrics":{
            "Acc@1":"83.9",
            "Acc@5":"96.0",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":83.9,
            "Acc@5":96.0,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1048887,
            "title":"MAR: Masked Autoencoders for Efficient Action Recognition",
            "url":"\/paper\/mar-masked-autoencoders-for-efficient-action",
            "published":"2022-07-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mar-masked-autoencoders-for-efficient-action\/review\/?hl=60451"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":43074,
        "rank":55,
        "method":"ActionCLIP (CLIP-pretrained)",
        "mlmodel":{

        },
        "method_short":"ActionCLIP ",
        "method_details":"CLIP-pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-17",
        "metrics":{
            "Acc@1":"83.8",
            "Acc@5":"97.1",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":83.8,
            "Acc@5":97.1,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":869790,
            "title":"ActionCLIP: A New Paradigm for Video Action Recognition",
            "url":"\/paper\/actionclip-a-new-paradigm-for-video-action",
            "published":"2021-09-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/actionclip-a-new-paradigm-for-video-action\/review\/?hl=43074"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":10502,
        "rank":56,
        "method":"OmniSource irCSN-152 (IG-Kinetics-65M pretrain)",
        "mlmodel":{

        },
        "method_short":"OmniSource irCSN-152 ",
        "method_details":"IG-Kinetics-65M pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-29",
        "metrics":{
            "Acc@1":"83.6",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":83.6,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":188764,
            "title":"Omni-sourced Webly-supervised Learning for Video Recognition",
            "url":"\/paper\/omni-sourced-webly-supervised-learning-for",
            "published":"2020-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/omni-sourced-webly-supervised-learning-for\/review\/?hl=10502"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":175,
                "name":"JointTraining",
                "color":"#2771D3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":98827,
        "rank":57,
        "method":"MVD (K400 pretrain, ViT-B, 16x224x224)",
        "mlmodel":{

        },
        "method_short":"MVD ",
        "method_details":"K400 pretrain, ViT-B, 16x224x224",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "Acc@1":"83.4",
            "Acc@5":"95.8",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":83.4,
            "Acc@5":95.8,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1125592,
            "title":"Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning",
            "url":"\/paper\/masked-video-distillation-rethinking-masked",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-video-distillation-rethinking-masked\/review\/?hl=98827"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":35523,
        "rank":58,
        "method":"Swin-L (ImageNet-21k pretrain)",
        "mlmodel":{

        },
        "method_short":"Swin-L ",
        "method_details":"ImageNet-21k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Acc@1":"83.1",
            "Acc@5":"95.9",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":83.1,
            "Acc@5":95.9,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":824241,
            "title":"Video Swin Transformer",
            "url":"\/paper\/video-swin-transformer",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-swin-transformer\/review\/?hl=35523"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":79066,
        "rank":59,
        "method":"SIFA",
        "mlmodel":{

        },
        "method_short":"SIFA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-14",
        "metrics":{
            "Acc@1":"83.1",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":83.1,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1026933,
            "title":"Stand-Alone Inter-Frame Attention in Video Models",
            "url":"\/paper\/stand-alone-inter-frame-attention-in-video-1",
            "published":"2022-06-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/stand-alone-inter-frame-attention-in-video-1\/review\/?hl=79066"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":43445,
        "rank":60,
        "method":"UniFormer-B (ImageNet-1K)",
        "mlmodel":{

        },
        "method_short":"UniFormer-B ",
        "method_details":"ImageNet-1K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Acc@1":"82.9",
            "Acc@5":"94.5",
            "FLOPs (G) x views":"259x4",
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":82.9,
            "Acc@5":94.5,
            "FLOPs (G) x views":259.0,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":883979,
            "title":"UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning",
            "url":"\/paper\/uniformer-unified-transformer-for-efficient",
            "published":"2021-09-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":173,
                "name":"UniFormer",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":10503,
        "rank":61,
        "method":"irCSN-152 (IG-Kinetics-65M pretrain)",
        "mlmodel":{

        },
        "method_short":"irCSN-152 ",
        "method_details":"IG-Kinetics-65M pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-02",
        "metrics":{
            "Acc@1":"82.8",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":82.8,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":113447,
            "title":"Large-scale weakly-supervised pre-training for video action recognition",
            "url":"\/paper\/large-scale-weakly-supervised-pre-training",
            "published":"2019-05-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/large-scale-weakly-supervised-pre-training\/review\/?hl=10503"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":50634,
        "rank":62,
        "method":"DirecFormer",
        "mlmodel":{

        },
        "method_short":"DirecFormer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-19",
        "metrics":{
            "Acc@1":"82.75",
            "Acc@5":"94.86",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":82.75,
            "Acc@5":94.86,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":980094,
            "title":"DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition",
            "url":"\/paper\/direcformer-a-directed-attention-in",
            "published":"2022-03-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/direcformer-a-directed-attention-in\/review\/?hl=50634"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":35524,
        "rank":63,
        "method":"Swin-B (ImageNet-21k pretrain)",
        "mlmodel":{

        },
        "method_short":"Swin-B ",
        "method_details":"ImageNet-21k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Acc@1":"82.7",
            "Acc@5":"95.5",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":82.7,
            "Acc@5":95.5,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":824241,
            "title":"Video Swin Transformer",
            "url":"\/paper\/video-swin-transformer",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-swin-transformer\/review\/?hl=35524"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27841,
        "rank":64,
        "method":"ir-CSN-152 (IG-65M pretraining)",
        "mlmodel":{

        },
        "method_short":"ir-CSN-152 ",
        "method_details":"IG-65M pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-04",
        "metrics":{
            "Acc@1":"82.6",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":82.6,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":110722,
            "title":"Video Classification with Channel-Separated Convolutional Networks",
            "url":"\/paper\/video-classification-with-channel-separated",
            "published":"2019-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-classification-with-channel-separated\/review\/?hl=27841"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27842,
        "rank":65,
        "method":"ip-CSN-152 (IG-65M pretraining)",
        "mlmodel":{

        },
        "method_short":"ip-CSN-152 ",
        "method_details":"IG-65M pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-04",
        "metrics":{
            "Acc@1":"82.5",
            "Acc@5":"95.3",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":82.5,
            "Acc@5":95.3,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":110722,
            "title":"Video Classification with Channel-Separated Convolutional Networks",
            "url":"\/paper\/video-classification-with-channel-separated",
            "published":"2019-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-classification-with-channel-separated\/review\/?hl=27842"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":79065,
        "rank":66,
        "method":"TPS",
        "mlmodel":{

        },
        "method_short":"TPS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-27",
        "metrics":{
            "Acc@1":"82.5",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":82.5,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1050620,
            "title":"Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition",
            "url":"\/paper\/spatiotemporal-self-attention-modeling-with",
            "published":"2022-07-27T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":110511,
        "rank":67,
        "method":"ILA (ViT-B\/32)",
        "mlmodel":{

        },
        "method_short":"ILA ",
        "method_details":"ViT-B\/32",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-20",
        "metrics":{
            "Acc@1":"82.4",
            "Acc@5":"95.8",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":82.4,
            "Acc@5":95.8,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1194756,
            "title":"Implicit Temporal Modeling with Learnable Alignment for Video Recognition",
            "url":"\/paper\/implicit-temporal-modeling-with-learnable",
            "published":"2023-04-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/implicit-temporal-modeling-with-learnable\/review\/?hl=110511"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":30515,
        "rank":68,
        "method":"VATT-Large",
        "mlmodel":{

        },
        "method_short":"VATT-Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Acc@1":"82.1",
            "Acc@5":"95.5",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":82.1,
            "Acc@5":95.5,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":787028,
            "title":"VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",
            "url":"\/paper\/vatt-transformers-for-multimodal-self",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vatt-transformers-for-multimodal-self\/review\/?hl=30515"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":79359,
        "rank":69,
        "method":"AdaMAE",
        "mlmodel":{

        },
        "method_short":"AdaMAE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-16",
        "metrics":{
            "Acc@1":"81.7",
            "Acc@5":"95.2",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":81.7,
            "Acc@5":95.2,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1112538,
            "title":"AdaMAE: Adaptive Masking for Efficient Spatiotemporal Learning with Masked Autoencoders",
            "url":"\/paper\/adamae-adaptive-masking-for-efficient",
            "published":"2022-11-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/adamae-adaptive-masking-for-efficient\/review\/?hl=79359"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":49998,
        "rank":70,
        "method":"VideoMAE (no extra data, ViT-B, 16x4)",
        "mlmodel":{

        },
        "method_short":"VideoMAE ",
        "method_details":"no extra data, ViT-B, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-23",
        "metrics":{
            "Acc@1":"81.5",
            "Acc@5":"95.1",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":81.5,
            "Acc@5":95.1,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":982160,
            "title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url":"\/paper\/videomae-masked-autoencoders-are-data-1",
            "published":"2022-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-masked-autoencoders-are-data-1\/review\/?hl=49998"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":28460,
        "rank":71,
        "method":"MoViNet-A6",
        "mlmodel":{

        },
        "method_short":"MoViNet-A6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Acc@1":"81.5",
            "Acc@5":null,
            "FLOPs (G) x views":"386x1",
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":81.5,
            "Acc@5":null,
            "FLOPs (G) x views":386.0,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28460"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":79068,
        "rank":72,
        "method":"MLP-3D",
        "mlmodel":{

        },
        "method_short":"MLP-3D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-13",
        "metrics":{
            "Acc@1":"81.4",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":81.4,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1025799,
            "title":"MLP-3D: A MLP-like 3D Architecture with Grouped Time Mixing",
            "url":"\/paper\/mlp-3d-a-mlp-like-3d-architecture-with-1",
            "published":"2022-06-13T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/mlp-3d-a-mlp-like-3d-architecture-with-1\/review\/?hl=79068"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27840,
        "rank":73,
        "method":"R[2+1]D-152 (IG-65M pretraining)",
        "mlmodel":{

        },
        "method_short":"R[2+1]D-152 ",
        "method_details":"IG-65M pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-04",
        "metrics":{
            "Acc@1":"81.3",
            "Acc@5":"95.1",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":81.3,
            "Acc@5":95.1,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":110722,
            "title":"Video Classification with Channel-Separated Convolutional Networks",
            "url":"\/paper\/video-classification-with-channel-separated",
            "published":"2019-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-classification-with-channel-separated\/review\/?hl=27840"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":19,
                "name":"R(2+1)D",
                "color":"#65a701"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":12894,
        "rank":74,
        "method":"LGD-3D Two-stream (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"LGD-3D Two-stream ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-13",
        "metrics":{
            "Acc@1":"81.2",
            "Acc@5":"95.2",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":81.2,
            "Acc@5":95.2,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142601,
            "title":"Learning Spatio-Temporal Representation with Local and Global Diffusion",
            "url":"\/paper\/learning-spatio-temporal-representation-with-3",
            "published":"2019-06-13T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/learning-spatio-temporal-representation-with-3\/review\/?hl=12894"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":30542,
        "rank":75,
        "method":"MViT-B, 64x3",
        "mlmodel":{

        },
        "method_short":"MViT-B, 64x3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Acc@1":"81.2",
            "Acc@5":"95.1",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":81.2,
            "Acc@5":95.1,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30542"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":35364,
        "rank":76,
        "method":"Motionformer-HR",
        "mlmodel":{

        },
        "method_short":"Motionformer-HR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-09",
        "metrics":{
            "Acc@1":"81.1",
            "Acc@5":"95.2",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":81.1,
            "Acc@5":95.2,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":815328,
            "title":"Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers",
            "url":"\/paper\/keeping-your-eye-on-the-ball-trajectory",
            "published":"2021-06-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/keeping-your-eye-on-the-ball-trajectory\/review\/?hl=35364"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":98828,
        "rank":77,
        "method":"MVD (K400 pretrain, ViT-S, 16x224x224)",
        "mlmodel":{

        },
        "method_short":"MVD ",
        "method_details":"K400 pretrain, ViT-S, 16x224x224",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "Acc@1":"81.0",
            "Acc@5":"94.8",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":81.0,
            "Acc@5":94.8,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1125592,
            "title":"Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning",
            "url":"\/paper\/masked-video-distillation-rethinking-masked",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-video-distillation-rethinking-masked\/review\/?hl=98828"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":60454,
        "rank":78,
        "method":"MAR (50% mask, ViT-B, 16x4)",
        "mlmodel":{

        },
        "method_short":"MAR ",
        "method_details":"50% mask, ViT-B, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-24",
        "metrics":{
            "Acc@1":"81.0",
            "Acc@5":"94.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":81.0,
            "Acc@5":94.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1048887,
            "title":"MAR: Masked Autoencoders for Efficient Action Recognition",
            "url":"\/paper\/mar-masked-autoencoders-for-efficient-action",
            "published":"2022-07-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mar-masked-autoencoders-for-efficient-action\/review\/?hl=60454"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":28461,
        "rank":79,
        "method":"MoViNet-A5",
        "mlmodel":{

        },
        "method_short":"MoViNet-A5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Acc@1":"80.9",
            "Acc@5":"94.9",
            "FLOPs (G) x views":"281x1",
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":80.9,
            "Acc@5":94.9,
            "FLOPs (G) x views":281.0,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28461"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":37549,
        "rank":80,
        "method":"MBT (AV)",
        "mlmodel":{

        },
        "method_short":"MBT ",
        "method_details":"AV",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-30",
        "metrics":{
            "Acc@1":"80.8",
            "Acc@5":"94.6",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":80.8,
            "Acc@5":94.6,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":828708,
            "title":"Attention Bottlenecks for Multimodal Fusion",
            "url":"\/paper\/attention-bottlenecks-for-multimodal-fusion",
            "published":"2021-06-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/attention-bottlenecks-for-multimodal-fusion\/review\/?hl=37549"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25416,
        "rank":81,
        "method":"TimeSformer-L (ImageNet-21k pretrain)",
        "mlmodel":{

        },
        "method_short":"TimeSformer-L ",
        "method_details":"ImageNet-21k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-09",
        "metrics":{
            "Acc@1":"80.7",
            "Acc@5":"94.7",
            "FLOPs (G) x views":"7140x3",
            "Parameters (M)":"121.4"
        },
        "raw_metrics":{
            "Acc@1":80.7,
            "Acc@5":94.7,
            "FLOPs (G) x views":7140.0,
            "Parameters (M)":121.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":744187,
            "title":"Is Space-Time Attention All You Need for Video Understanding?",
            "url":"\/paper\/is-space-time-attention-all-you-need-for",
            "published":"2021-02-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/is-space-time-attention-all-you-need-for\/review\/?hl=25416"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":35521,
        "rank":82,
        "method":"Swin-B (ImageNet-1k pretrain)",
        "mlmodel":{

        },
        "method_short":"Swin-B ",
        "method_details":"ImageNet-1k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Acc@1":"80.6",
            "Acc@5":"94.6",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":80.6,
            "Acc@5":94.6,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":824241,
            "title":"Video Swin Transformer",
            "url":"\/paper\/video-swin-transformer",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-swin-transformer\/review\/?hl=35521"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":35525,
        "rank":83,
        "method":"Swin-S (ImageNet-1k pretrain)",
        "mlmodel":{

        },
        "method_short":"Swin-S ",
        "method_details":"ImageNet-1k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Acc@1":"80.6",
            "Acc@5":"94.5",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":80.6,
            "Acc@5":94.5,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":824241,
            "title":"Video Swin Transformer",
            "url":"\/paper\/video-swin-transformer",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-swin-transformer\/review\/?hl=35525"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":31458,
        "rank":84,
        "method":"En-VidTr-L",
        "mlmodel":{

        },
        "method_short":"En-VidTr-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-23",
        "metrics":{
            "Acc@1":"80.5",
            "Acc@5":"94.6",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":80.5,
            "Acc@5":94.6,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787687,
            "title":"VidTr: Video Transformer Without Convolutions",
            "url":"\/paper\/vidtr-video-transformer-without-convolutions",
            "published":"2021-04-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/vidtr-video-transformer-without-convolutions\/review\/?hl=31458"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":28462,
        "rank":85,
        "method":"MoViNet-A4",
        "mlmodel":{

        },
        "method_short":"MoViNet-A4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Acc@1":"80.5",
            "Acc@5":"94.5",
            "FLOPs (G) x views":"105x1",
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":80.5,
            "Acc@5":94.5,
            "FLOPs (G) x views":105.0,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28462"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":41336,
        "rank":86,
        "method":"OmniSource SlowOnly R101 8x8(ImageNet pretrain)",
        "mlmodel":{

        },
        "method_short":"OmniSource SlowOnly R101 8x8",
        "method_details":"ImageNet pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-29",
        "metrics":{
            "Acc@1":"80.5",
            "Acc@5":"94.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":80.5,
            "Acc@5":94.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188764,
            "title":"Omni-sourced Webly-supervised Learning for Video Recognition",
            "url":"\/paper\/omni-sourced-webly-supervised-learning-for",
            "published":"2020-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/omni-sourced-webly-supervised-learning-for\/review\/?hl=41336"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":28821,
        "rank":87,
        "method":"STAM (64 Frames)",
        "mlmodel":{

        },
        "method_short":"STAM ",
        "method_details":"64 Frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-25",
        "metrics":{
            "Acc@1":"80.5",
            "Acc@5":null,
            "FLOPs (G) x views":"1040x1",
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":80.5,
            "Acc@5":null,
            "FLOPs (G) x views":1040.0,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":757279,
            "title":"An Image is Worth 16x16 Words, What is a Video Worth?",
            "url":"\/paper\/an-image-is-worth-16x16-words-what-is-a-video",
            "published":"2021-03-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-image-is-worth-16x16-words-what-is-a-video\/review\/?hl=28821"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25400,
        "rank":88,
        "method":"X3D-XXL",
        "mlmodel":{

        },
        "method_short":"X3D-XXL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-09",
        "metrics":{
            "Acc@1":"80.4",
            "Acc@5":"94.6",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":80.4,
            "Acc@5":94.6,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":190207,
            "title":"X3D: Expanding Architectures for Efficient Video Recognition",
            "url":"\/paper\/x3d-expanding-architectures-for-efficient",
            "published":"2020-04-09T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":39072,
        "rank":89,
        "method":"R3D-RS-200",
        "mlmodel":{

        },
        "method_short":"R3D-RS-200",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-03",
        "metrics":{
            "Acc@1":"80.4",
            "Acc@5":"94.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":80.4,
            "Acc@5":94.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":862026,
            "title":"Revisiting 3D ResNets for Video Recognition",
            "url":"\/paper\/revisiting-3d-resnets-for-video-recognition",
            "published":"2021-09-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revisiting-3d-resnets-for-video-recognition\/review\/?hl=39072"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":41337,
        "rank":90,
        "method":"OmniSource SlowOnly R101 8x8 (Scratch)",
        "mlmodel":{

        },
        "method_short":"OmniSource SlowOnly R101 8x8 ",
        "method_details":"Scratch",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-29",
        "metrics":{
            "Acc@1":"80.4",
            "Acc@5":"94.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":80.4,
            "Acc@5":94.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188764,
            "title":"Omni-sourced Webly-supervised Learning for Video Recognition",
            "url":"\/paper\/omni-sourced-webly-supervised-learning-for",
            "published":"2020-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/omni-sourced-webly-supervised-learning-for\/review\/?hl=41337"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":30541,
        "rank":91,
        "method":"MViT-B, 32x3",
        "mlmodel":{

        },
        "method_short":"MViT-B, 32x3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Acc@1":"80.2",
            "Acc@5":"94.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":80.2,
            "Acc@5":94.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30541"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27798,
        "rank":92,
        "method":"SlowFast 16x8 (ResNet-101 + NL)",
        "mlmodel":{

        },
        "method_short":"SlowFast 16x8 ",
        "method_details":"ResNet-101 + NL",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "Acc@1":"79.8",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.8,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=27798"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":34211,
        "rank":93,
        "method":"CT-Net Ensemble",
        "mlmodel":{

        },
        "method_short":"CT-Net Ensemble",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-03",
        "metrics":{
            "Acc@1":"79.8",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.8,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":810992,
            "title":"CT-Net: Channel Tensorization Network for Video Classification",
            "url":"\/paper\/ct-net-channel-tensorization-network-for-1",
            "published":"2021-06-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ct-net-channel-tensorization-network-for-1\/review\/?hl=34211"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":37568,
        "rank":94,
        "method":"ViT-B-VTN+ ImageNet-21K (84.0 [10])",
        "mlmodel":{

        },
        "method_short":"ViT-B-VTN+ ImageNet-21K ",
        "method_details":"84.0 [10]",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-01",
        "metrics":{
            "Acc@1":"79.8",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.8,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":741425,
            "title":"Video Transformer Network",
            "url":"\/paper\/video-transformer-network",
            "published":"2021-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-transformer-network\/review\/?hl=37568"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25415,
        "rank":95,
        "method":"TimeSformer-HR",
        "mlmodel":{

        },
        "method_short":"TimeSformer-HR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-09",
        "metrics":{
            "Acc@1":"79.7",
            "Acc@5":"94.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.7,
            "Acc@5":94.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":744187,
            "title":"Is Space-Time Attention All You Need for Video Understanding?",
            "url":"\/paper\/is-space-time-attention-all-you-need-for",
            "published":"2021-02-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/is-space-time-attention-all-you-need-for\/review\/?hl=25415"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":31457,
        "rank":96,
        "method":"En-VidTr-M",
        "mlmodel":{

        },
        "method_short":"En-VidTr-M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-23",
        "metrics":{
            "Acc@1":"79.7",
            "Acc@5":"94.2",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.7,
            "Acc@5":94.2,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787687,
            "title":"VidTr: Video Transformer Without Convolutions",
            "url":"\/paper\/vidtr-video-transformer-without-convolutions",
            "published":"2021-04-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/vidtr-video-transformer-without-convolutions\/review\/?hl=31457"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27865,
        "rank":97,
        "method":"LGD-3D RGB (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"LGD-3D RGB ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-13",
        "metrics":{
            "Acc@1":"79.4",
            "Acc@5":"94.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.4,
            "Acc@5":94.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142601,
            "title":"Learning Spatio-Temporal Representation with Local and Global Diffusion",
            "url":"\/paper\/learning-spatio-temporal-representation-with-3",
            "published":"2019-06-13T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/learning-spatio-temporal-representation-with-3\/review\/?hl=27865"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":30759,
        "rank":98,
        "method":"TDN-ResNet101 (ensemble, ImageNet pretrained, RGB only)",
        "mlmodel":{

        },
        "method_short":"TDN-ResNet101 ",
        "method_details":"ensemble, ImageNet pretrained, RGB only",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-18",
        "metrics":{
            "Acc@1":"79.4",
            "Acc@5":"94.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.4,
            "Acc@5":94.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":729685,
            "title":"TDN: Temporal Difference Networks for Efficient Action Recognition",
            "url":"\/paper\/tdn-temporal-difference-networks-for",
            "published":"2020-12-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tdn-temporal-difference-networks-for\/review\/?hl=30759"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":31456,
        "rank":99,
        "method":"En-VidTr-S",
        "mlmodel":{

        },
        "method_short":"En-VidTr-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-23",
        "metrics":{
            "Acc@1":"79.4",
            "Acc@5":"94",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.4,
            "Acc@5":94.0,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787687,
            "title":"VidTr: Video Transformer Without Convolutions",
            "url":"\/paper\/vidtr-video-transformer-without-convolutions",
            "published":"2021-04-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/vidtr-video-transformer-without-convolutions\/review\/?hl=31456"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":60455,
        "rank":100,
        "method":"MAR (75% mask, ViT-B, 16x4)",
        "mlmodel":{

        },
        "method_short":"MAR ",
        "method_details":"75% mask, ViT-B, 16x4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-24",
        "metrics":{
            "Acc@1":"79.4",
            "Acc@5":"93.7",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.4,
            "Acc@5":93.7,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1048887,
            "title":"MAR: Masked Autoencoders for Efficient Action Recognition",
            "url":"\/paper\/mar-masked-autoencoders-for-efficient-action",
            "published":"2022-07-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mar-masked-autoencoders-for-efficient-action\/review\/?hl=60455"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":163,
                "name":"No Extra Data",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":28820,
        "rank":101,
        "method":"STAM (16 Frames)",
        "mlmodel":{

        },
        "method_short":"STAM ",
        "method_details":"16 Frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-25",
        "metrics":{
            "Acc@1":"79.3",
            "Acc@5":null,
            "FLOPs (G) x views":"270x1",
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.3,
            "Acc@5":null,
            "FLOPs (G) x views":270.0,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":757279,
            "title":"An Image is Worth 16x16 Words, What is a Video Worth?",
            "url":"\/paper\/an-image-is-worth-16x16-words-what-is-a-video",
            "published":"2021-03-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-image-is-worth-16x16-words-what-is-a-video\/review\/?hl=28820"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27839,
        "rank":102,
        "method":"ip-CSN-152 (Sports-1M pretraining)",
        "mlmodel":{

        },
        "method_short":"ip-CSN-152 ",
        "method_details":"Sports-1M pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-04",
        "metrics":{
            "Acc@1":"79.2",
            "Acc@5":"93.8",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.2,
            "Acc@5":93.8,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":110722,
            "title":"Video Classification with Channel-Separated Convolutional Networks",
            "url":"\/paper\/video-classification-with-channel-separated",
            "published":"2019-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-classification-with-channel-separated\/review\/?hl=27839"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25401,
        "rank":103,
        "method":"CorrNet",
        "mlmodel":{

        },
        "method_short":"CorrNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-07",
        "metrics":{
            "Acc@1":"79.2",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.2,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142180,
            "title":"Video Modeling with Correlation Networks",
            "url":"\/paper\/video-modeling-with-correlation-networks",
            "published":"2019-06-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/video-modeling-with-correlation-networks\/review\/?hl=25401"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":69047,
        "rank":104,
        "method":"OmniVL",
        "mlmodel":{

        },
        "method_short":"OmniVL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-15",
        "metrics":{
            "Acc@1":"79.1",
            "Acc@5":"94.5",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.1,
            "Acc@5":94.5,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1075042,
            "title":"OmniVL:One Foundation Model for Image-Language and Video-Language Tasks",
            "url":"\/paper\/omnivl-one-foundation-model-for-image",
            "published":"2022-09-15T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivl-one-foundation-model-for-image\/review\/?hl=69047"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":28238,
        "rank":105,
        "method":"X3D-XL",
        "mlmodel":{

        },
        "method_short":"X3D-XL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-09",
        "metrics":{
            "Acc@1":"79.1",
            "Acc@5":"93.9",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.1,
            "Acc@5":93.9,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":190207,
            "title":"X3D: Expanding Architectures for Efficient Video Recognition",
            "url":"\/paper\/x3d-expanding-architectures-for-efficient",
            "published":"2020-04-09T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":34624,
        "rank":106,
        "method":"MVFNet-ResNet101 (ensemble, ImageNet pretrained, RGB only)",
        "mlmodel":{

        },
        "method_short":"MVFNet-ResNet101 ",
        "method_details":"ensemble, ImageNet pretrained, RGB only",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-13",
        "metrics":{
            "Acc@1":"79.1",
            "Acc@5":"93.8",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.1,
            "Acc@5":93.8,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":727533,
            "title":"MVFNet: Multi-View Fusion Network for Efficient Video Recognition",
            "url":"\/paper\/mvfnet-multi-view-fusion-network-for",
            "published":"2020-12-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mvfnet-multi-view-fusion-network-for\/review\/?hl=34624"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":51067,
        "rank":107,
        "method":"TAdaConvNeXt-T",
        "mlmodel":{

        },
        "method_short":"TAdaConvNeXt-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-12",
        "metrics":{
            "Acc@1":"79.1",
            "Acc@5":"93.7",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":79.1,
            "Acc@5":93.7,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":887048,
            "title":"TAda! Temporally-Adaptive Convolutions for Video Understanding",
            "url":"\/paper\/tada-temporally-adaptive-convolutions-for-1",
            "published":"2021-10-12T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":212,
                "name":"ConvNeXt",
                "color":"#77bb41"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27797,
        "rank":108,
        "method":"SlowFast 16x8 (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"SlowFast 16x8 ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "Acc@1":"78.9",
            "Acc@5":"93.5",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":78.9,
            "Acc@5":93.5,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=27797"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":6646,
        "rank":109,
        "method":"G-Blend (Sports-1M pretrain)",
        "mlmodel":{

        },
        "method_short":"G-Blend ",
        "method_details":"Sports-1M pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-29",
        "metrics":{
            "Acc@1":"78.9",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":78.9,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":118004,
            "title":"What Makes Training Multi-Modal Classification Networks Hard?",
            "url":"\/paper\/what-makes-training-multi-modal-networks-hard",
            "published":"2019-05-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/what-makes-training-multi-modal-networks-hard\/review\/?hl=6646"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":35519,
        "rank":110,
        "method":"Swin-T (ImageNet-1k pretrain)",
        "mlmodel":{

        },
        "method_short":"Swin-T ",
        "method_details":"ImageNet-1k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Acc@1":"78.8",
            "Acc@5":"93.6",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":78.8,
            "Acc@5":93.6,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":824241,
            "title":"Video Swin Transformer",
            "url":"\/paper\/video-swin-transformer",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-swin-transformer\/review\/?hl=35519"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":6680,
        "rank":111,
        "method":"GB + DF + LB (ResNet 152, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"GB + DF + LB ",
        "method_details":"ResNet 152, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-20",
        "metrics":{
            "Acc@1":"78.8",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":78.8,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":150843,
            "title":"Action recognition with spatial-temporal discriminative filter banks",
            "url":"\/paper\/190807625",
            "published":"2019-08-20T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/190807625\/review\/?hl=6680"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25034,
        "rank":112,
        "method":"ViT-B-VTN (3 layers, ImageNet pretrain)",
        "mlmodel":{

        },
        "method_short":"ViT-B-VTN ",
        "method_details":"3 layers, ImageNet pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-01",
        "metrics":{
            "Acc@1":"78.6",
            "Acc@5":"93.7",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":78.6,
            "Acc@5":93.7,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":741425,
            "title":"Video Transformer Network",
            "url":"\/paper\/video-transformer-network",
            "published":"2021-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-transformer-network\/review\/?hl=25034"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":30540,
        "rank":113,
        "method":"MViT-B, 16x4",
        "mlmodel":{

        },
        "method_short":"MViT-B, 16x4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Acc@1":"78.4",
            "Acc@5":"93.5",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":78.4,
            "Acc@5":93.5,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30540"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":28463,
        "rank":114,
        "method":"MoViNet-A3",
        "mlmodel":{

        },
        "method_short":"MoViNet-A3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Acc@1":"78.2",
            "Acc@5":"93.8",
            "FLOPs (G) x views":"56.9x1",
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":78.2,
            "Acc@5":93.8,
            "FLOPs (G) x views":56.9,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28463"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":45708,
        "rank":115,
        "method":"TAda2D-En (ResNet-50, 8+16 frames)",
        "mlmodel":{

        },
        "method_short":"TAda2D-En ",
        "method_details":"ResNet-50, 8+16 frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-12",
        "metrics":{
            "Acc@1":"78.2",
            "Acc@5":"93.5",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":78.2,
            "Acc@5":93.5,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":887048,
            "title":"TAda! Temporally-Adaptive Convolutions for Video Understanding",
            "url":"\/paper\/tada-temporally-adaptive-convolutions-for-1",
            "published":"2021-10-12T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":89418,
        "rank":116,
        "method":"SVT (finetune)",
        "mlmodel":{

        },
        "method_short":"SVT ",
        "method_details":"finetune",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Acc@1":"78.1",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":78.1,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":924698,
            "title":"Self-supervised Video Transformer",
            "url":"\/paper\/self-supervised-video-transformer",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-supervised-video-transformer\/review\/?hl=89418"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25414,
        "rank":117,
        "method":"TimeSformer",
        "mlmodel":{

        },
        "method_short":"TimeSformer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-09",
        "metrics":{
            "Acc@1":"78",
            "Acc@5":"93.7",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":78.0,
            "Acc@5":93.7,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":744187,
            "title":"Is Space-Time Attention All You Need for Video Understanding?",
            "url":"\/paper\/is-space-time-attention-all-you-need-for",
            "published":"2021-02-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/is-space-time-attention-all-you-need-for\/review\/?hl=25414"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27796,
        "rank":118,
        "method":"SlowFast 8x8 (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"SlowFast 8x8 ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "Acc@1":"77.9",
            "Acc@5":"93.2",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":77.9,
            "Acc@5":93.2,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=27796"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27781,
        "rank":119,
        "method":"RepFlow-50 ([2+1]D CNN, FcF, Non-local block)",
        "mlmodel":{

        },
        "method_short":"RepFlow-50 ",
        "method_details":"[2+1]D CNN, FcF, Non-local block",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-10-02",
        "metrics":{
            "Acc@1":"77.9",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":77.9,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":58715,
            "title":"Representation Flow for Action Recognition",
            "url":"\/paper\/representation-flow-for-action-recognition",
            "published":"2018-10-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/representation-flow-for-action-recognition\/review\/?hl=27781"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":6682,
        "rank":120,
        "method":"ip-CSN-152",
        "mlmodel":{

        },
        "method_short":"ip-CSN-152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-04",
        "metrics":{
            "Acc@1":"77.8",
            "Acc@5":"92.8",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":77.8,
            "Acc@5":92.8,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":110722,
            "title":"Video Classification with Channel-Separated Convolutional Networks",
            "url":"\/paper\/video-classification-with-channel-separated",
            "published":"2019-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-classification-with-channel-separated\/review\/?hl=6682"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25411,
        "rank":121,
        "method":"I3D + NL",
        "mlmodel":{

        },
        "method_short":"I3D + NL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-21",
        "metrics":{
            "Acc@1":"77.7",
            "Acc@5":"93.3",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":77.7,
            "Acc@5":93.3,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":6101,
            "title":"Non-local Neural Networks",
            "url":"\/paper\/non-local-neural-networks",
            "published":"2017-11-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/non-local-neural-networks\/review\/?hl=25411"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":6645,
        "rank":122,
        "method":"G-Blend",
        "mlmodel":{

        },
        "method_short":"G-Blend",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-29",
        "metrics":{
            "Acc@1":"77.7",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":77.7,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":118004,
            "title":"What Makes Training Multi-Modal Classification Networks Hard?",
            "url":"\/paper\/what-makes-training-multi-modal-networks-hard",
            "published":"2019-05-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/what-makes-training-multi-modal-networks-hard\/review\/?hl=6645"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":12893,
        "rank":123,
        "method":"HATNet (32 frames)",
        "mlmodel":{

        },
        "method_short":"HATNet ",
        "method_details":"32 frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-25",
        "metrics":{
            "Acc@1":"77.6",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":77.6,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":112715,
            "title":"Large Scale Holistic Video Understanding",
            "url":"\/paper\/holistic-large-scale-video-understanding",
            "published":"2019-04-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/holistic-large-scale-video-understanding\/review\/?hl=12893"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":28239,
        "rank":124,
        "method":"X3D-L",
        "mlmodel":{

        },
        "method_short":"X3D-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-09",
        "metrics":{
            "Acc@1":"77.5",
            "Acc@5":"92.9",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":77.5,
            "Acc@5":92.9,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":190207,
            "title":"X3D: Expanding Architectures for Efficient Video Recognition",
            "url":"\/paper\/x3d-expanding-architectures-for-efficient",
            "published":"2020-04-09T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":6668,
        "rank":125,
        "method":"CoST ResNet-101  (ImageNet pretrain)",
        "mlmodel":{

        },
        "method_short":"CoST ResNet-101  ",
        "method_details":"ImageNet pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-01",
        "metrics":{
            "Acc@1":"77.5",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":77.5,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":116001,
            "title":"Collaborative Spatiotemporal Feature Learning for Video Action Recognition",
            "url":"\/paper\/collaborative-spatiotemporal-feature-learning",
            "published":"2019-06-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":45709,
        "rank":126,
        "method":"TAda2D (ResNet-50, 16 frames)",
        "mlmodel":{

        },
        "method_short":"TAda2D ",
        "method_details":"ResNet-50, 16 frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-12",
        "metrics":{
            "Acc@1":"77.4",
            "Acc@5":"93.1",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":77.4,
            "Acc@5":93.1,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":887048,
            "title":"TAda! Temporally-Adaptive Convolutions for Video Understanding",
            "url":"\/paper\/tada-temporally-adaptive-convolutions-for-1",
            "published":"2021-10-12T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":6315,
        "rank":127,
        "method":"EvaNet",
        "mlmodel":{

        },
        "method_short":"EvaNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-11-26",
        "metrics":{
            "Acc@1":"77.4",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":77.4,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":63551,
            "title":"Evolving Space-Time Neural Architectures for Videos",
            "url":"\/paper\/evolving-space-time-neural-architectures-for",
            "published":"2018-11-26T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/evolving-space-time-neural-architectures-for\/review\/?hl=6315"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":21621,
        "rank":128,
        "method":"RNL+TSM Ensemble(ResNet50, 8 + 16 frames)",
        "mlmodel":{

        },
        "method_short":"RNL+TSM Ensemble",
        "method_details":"ResNet50, 8 + 16 frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-17",
        "metrics":{
            "Acc@1":"77.4",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":77.4,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":209203,
            "title":"Region-based Non-local Operation for Video Classification",
            "url":"\/paper\/region-based-non-local-operation-for-video",
            "published":"2020-07-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/region-based-non-local-operation-for-video\/review\/?hl=21621"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":35422,
        "rank":129,
        "method":"VIMPAC",
        "mlmodel":{

        },
        "method_short":"VIMPAC",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-21",
        "metrics":{
            "Acc@1":"77.4",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":77.4,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":821705,
            "title":"VIMPAC: Video Pre-Training via Masked Token Prediction and Contrastive Learning",
            "url":"\/paper\/vimpac-video-pre-training-via-masked-token",
            "published":"2021-06-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vimpac-video-pre-training-via-masked-token\/review\/?hl=35422"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":40703,
        "rank":130,
        "method":"BQN (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"BQN ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Acc@1":"77.3",
            "Acc@5":"93.2",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":77.3,
            "Acc@5":93.2,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758504,
            "title":"Busy-Quiet Video Disentangling for Video Classification",
            "url":"\/paper\/video-classification-with-finecoarse-networks",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-classification-with-finecoarse-networks\/review\/?hl=40703"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27771,
        "rank":131,
        "method":"S3D-G (RGB+Flow, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"S3D-G ",
        "method_details":"RGB+Flow, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-12-13",
        "metrics":{
            "Acc@1":"77.2",
            "Acc@5":"93",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":77.2,
            "Acc@5":93.0,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":13041,
            "title":"Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
            "url":"\/paper\/rethinking-spatiotemporal-feature-learning",
            "published":"2017-12-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-spatiotemporal-feature-learning\/review\/?hl=27771"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27795,
        "rank":132,
        "method":"SlowFast 8x8 (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"SlowFast 8x8 ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "Acc@1":"77",
            "Acc@5":"92.6",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":77.0,
            "Acc@5":92.6,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=27795"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":45710,
        "rank":133,
        "method":"TAda2D (ResNet-50, 8 frames)",
        "mlmodel":{

        },
        "method_short":"TAda2D ",
        "method_details":"ResNet-50, 8 frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-12",
        "metrics":{
            "Acc@1":"76.7",
            "Acc@5":"92.6",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":76.7,
            "Acc@5":92.6,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":887048,
            "title":"TAda! Temporally-Adaptive Convolutions for Video Understanding",
            "url":"\/paper\/tada-temporally-adaptive-convolutions-for-1",
            "published":"2021-10-12T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27813,
        "rank":134,
        "method":"D3D+S3D-G (RGB + RGB)",
        "mlmodel":{

        },
        "method_short":"D3D+S3D-G ",
        "method_details":"RGB + RGB",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-19",
        "metrics":{
            "Acc@1":"76.5",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":76.5,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":65765,
            "title":"D3D: Distilled 3D Networks for Video Action Recognition",
            "url":"\/paper\/d3d-distilled-3d-networks-for-video-action",
            "published":"2018-12-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/d3d-distilled-3d-networks-for-video-action\/review\/?hl=27813"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":20733,
        "rank":135,
        "method":"MSNet-R50 (16 frames, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"MSNet-R50 ",
        "method_details":"16 frames, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-20",
        "metrics":{
            "Acc@1":"76.4",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":76.4,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":209431,
            "title":"MotionSqueeze: Neural Motion Feature Learning for Video Understanding",
            "url":"\/paper\/motionsqueeze-neural-motion-feature-learning",
            "published":"2020-07-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/motionsqueeze-neural-motion-feature-learning\/review\/?hl=20733"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25402,
        "rank":136,
        "method":"GloRe",
        "mlmodel":{

        },
        "method_short":"GloRe",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-03",
        "metrics":{
            "Acc@1":"76.1",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":76.1,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":118554,
            "title":"Global Textual Relation Embedding for Relational Understanding",
            "url":"\/paper\/190600550",
            "published":"2019-06-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/190600550\/review\/?hl=25402"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":28240,
        "rank":137,
        "method":"X3D-M",
        "mlmodel":{

        },
        "method_short":"X3D-M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-09",
        "metrics":{
            "Acc@1":"76",
            "Acc@5":"92.3",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":76.0,
            "Acc@5":92.3,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":190207,
            "title":"X3D: Expanding Architectures for Efficient Video Recognition",
            "url":"\/paper\/x3d-expanding-architectures-for-efficient",
            "published":"2020-04-09T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":30539,
        "rank":138,
        "method":"MViT-S",
        "mlmodel":{

        },
        "method_short":"MViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Acc@1":"76",
            "Acc@5":"92.1",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":76.0,
            "Acc@5":92.1,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=30539"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":6669,
        "rank":139,
        "method":"CMA iter1 (16 frames)",
        "mlmodel":{

        },
        "method_short":"CMA iter1 ",
        "method_details":"16 frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-01",
        "metrics":{
            "Acc@1":"75.98",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":75.98,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":148881,
            "title":"Two-Stream Video Classification with Cross-Modality Attention",
            "url":"\/paper\/two-stream-video-classification-with-cross",
            "published":"2019-08-01T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/two-stream-video-classification-with-cross\/review\/?hl=6669"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27812,
        "rank":140,
        "method":"D3D (RGB)",
        "mlmodel":{

        },
        "method_short":"D3D ",
        "method_details":"RGB",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-19",
        "metrics":{
            "Acc@1":"75.9",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":75.9,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":65765,
            "title":"D3D: Distilled 3D Networks for Video Action Recognition",
            "url":"\/paper\/d3d-distilled-3d-networks-for-video-action",
            "published":"2018-12-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/d3d-distilled-3d-networks-for-video-action\/review\/?hl=27812"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25404,
        "rank":141,
        "method":"Oct-I3D + NL",
        "mlmodel":{

        },
        "method_short":"Oct-I3D + NL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-10",
        "metrics":{
            "Acc@1":"75.7",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":75.7,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":111242,
            "title":"Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution",
            "url":"\/paper\/drop-an-octave-reducing-spatial-redundancy-in",
            "published":"2019-04-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/drop-an-octave-reducing-spatial-redundancy-in\/review\/?hl=25404"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27794,
        "rank":142,
        "method":"SlowFast 4x16 (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"SlowFast 4x16 ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "Acc@1":"75.6",
            "Acc@5":"92.1",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":75.6,
            "Acc@5":92.1,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=27794"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27734,
        "rank":143,
        "method":"R[2+1]D-Flow (Sports-1M pretrain)",
        "mlmodel":{

        },
        "method_short":"R[2+1]D-Flow ",
        "method_details":"Sports-1M pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-30",
        "metrics":{
            "Acc@1":"75.4",
            "Acc@5":"91.9",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":75.4,
            "Acc@5":91.9,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":6204,
            "title":"A Closer Look at Spatiotemporal Convolutions for Action Recognition",
            "url":"\/paper\/a-closer-look-at-spatiotemporal-convolutions",
            "published":"2017-11-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":19,
                "name":"R(2+1)D",
                "color":"#65a701"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":6637,
        "rank":144,
        "method":"FASTER32",
        "mlmodel":{

        },
        "method_short":"FASTER32",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-10",
        "metrics":{
            "Acc@1":"75.1",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":75.1,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142365,
            "title":"FASTER Recurrent Networks for Efficient Video Classification",
            "url":"\/paper\/faster-recurrent-networks-for-video",
            "published":"2019-06-10T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/faster-recurrent-networks-for-video\/review\/?hl=6637"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":28464,
        "rank":145,
        "method":"MoViNet-A2",
        "mlmodel":{

        },
        "method_short":"MoViNet-A2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Acc@1":"75.0",
            "Acc@5":"92.3",
            "FLOPs (G) x views":"10.3x1",
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":75.0,
            "Acc@5":92.3,
            "FLOPs (G) x views":10.3,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28464"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":6675,
        "rank":146,
        "method":"MARS+RGB+Flow (64 frames)",
        "mlmodel":{

        },
        "method_short":"MARS+RGB+Flow ",
        "method_details":"64 frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-01",
        "metrics":{
            "Acc@1":"74.9",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":74.9,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":116002,
            "title":"MARS: Motion-Augmented RGB Stream for Action Recognition",
            "url":"\/paper\/mars-motion-augmented-rgb-stream-for-action",
            "published":"2019-06-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27770,
        "rank":147,
        "method":"S3D-G (RGB, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"S3D-G ",
        "method_details":"RGB, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-12-13",
        "metrics":{
            "Acc@1":"74.7",
            "Acc@5":"93.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":74.7,
            "Acc@5":93.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":13041,
            "title":"Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
            "url":"\/paper\/rethinking-spatiotemporal-feature-learning",
            "published":"2017-12-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-spatiotemporal-feature-learning\/review\/?hl=27770"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25406,
        "rank":148,
        "method":"TSM",
        "mlmodel":{

        },
        "method_short":"TSM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-11-20",
        "metrics":{
            "Acc@1":"74.7",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":74.7,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":62848,
            "title":"TSM: Temporal Shift Module for Efficient Video Understanding",
            "url":"\/paper\/temporal-shift-module-for-efficient-video",
            "published":"2018-11-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporal-shift-module-for-efficient-video\/review\/?hl=25406"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25407,
        "rank":149,
        "method":"A2 Net",
        "mlmodel":{

        },
        "method_short":"A2 Net",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-10-27",
        "metrics":{
            "Acc@1":"74.6",
            "Acc@5":"91.5",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":74.6,
            "Acc@5":91.5,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":60633,
            "title":"$A^2$-Nets: Double Attention Networks",
            "url":"\/paper\/a2-nets-double-attention-networks",
            "published":"2018-10-27T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/a2-nets-double-attention-networks\/review\/?hl=25407"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27733,
        "rank":150,
        "method":"R[2+1]D-RGB (Sports-1M pretrain)",
        "mlmodel":{

        },
        "method_short":"R[2+1]D-RGB ",
        "method_details":"Sports-1M pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-30",
        "metrics":{
            "Acc@1":"74.3",
            "Acc@5":"91.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":74.3,
            "Acc@5":91.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":6204,
            "title":"A Closer Look at Spatiotemporal Convolutions for Action Recognition",
            "url":"\/paper\/a-closer-look-at-spatiotemporal-convolutions",
            "published":"2017-11-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":19,
                "name":"R(2+1)D",
                "color":"#65a701"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27774,
        "rank":151,
        "method":"TSN",
        "mlmodel":{

        },
        "method_short":"TSN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-08-02",
        "metrics":{
            "Acc@1":"73.9",
            "Acc@5":"91.1",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":73.9,
            "Acc@5":91.1,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":31692,
            "title":"Temporal Segment Networks: Towards Good Practices for Deep Action Recognition",
            "url":"\/paper\/temporal-segment-networks-towards-good",
            "published":"2016-08-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporal-segment-networks-towards-good\/review\/?hl=27774"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27732,
        "rank":152,
        "method":"R[2+1]D-Two-Stream",
        "mlmodel":{

        },
        "method_short":"R[2+1]D-Two-Stream",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-30",
        "metrics":{
            "Acc@1":"73.9",
            "Acc@5":"90.9",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":73.9,
            "Acc@5":90.9,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":6204,
            "title":"A Closer Look at Spatiotemporal Convolutions for Action Recognition",
            "url":"\/paper\/a-closer-look-at-spatiotemporal-convolutions",
            "published":"2017-11-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":19,
                "name":"R(2+1)D",
                "color":"#65a701"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27823,
        "rank":153,
        "method":"TSN",
        "mlmodel":{

        },
        "method_short":"TSN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-08-16",
        "metrics":{
            "Acc@1":"73.9",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":73.9,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":18716,
            "title":"ConvNet Architecture Search for Spatiotemporal Feature Learning",
            "url":"\/paper\/convnet-architecture-search-for",
            "published":"2017-08-16T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":6649,
        "rank":154,
        "method":"STM (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"STM ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-07",
        "metrics":{
            "Acc@1":"73.7",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":73.7,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":149392,
            "title":"STM: SpatioTemporal and Motion Encoding for Action Recognition",
            "url":"\/paper\/stm-spatiotemporal-and-motion-encoding-for",
            "published":"2019-08-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/stm-spatiotemporal-and-motion-encoding-for\/review\/?hl=6649"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25419,
        "rank":155,
        "method":"bLVNet Fan et al. (2019)",
        "mlmodel":{

        },
        "method_short":"bLVNet Fan et al. ",
        "method_details":"2019",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-02",
        "metrics":{
            "Acc@1":"73.5",
            "Acc@5":"91.2",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":73.5,
            "Acc@5":91.2,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":175169,
            "title":"More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation",
            "url":"\/paper\/more-is-less-learning-efficient-video-1",
            "published":"2019-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/more-is-less-learning-efficient-video-1\/review\/?hl=25419"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58618,
        "rank":156,
        "method":"Co Slow_64",
        "mlmodel":{

        },
        "method_short":"Co Slow_64",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"73.05",
            "Acc@5":null,
            "FLOPs (G) x views":"6.90x1",
            "Parameters (M)":"32.45"
        },
        "raw_metrics":{
            "Acc@1":73.05,
            "Acc@5":null,
            "FLOPs (G) x views":6.9,
            "Parameters (M)":32.45
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58618"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25418,
        "rank":157,
        "method":"Inception-ResNet",
        "mlmodel":{

        },
        "method_short":"Inception-ResNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-08-12",
        "metrics":{
            "Acc@1":"73.0",
            "Acc@5":"90.9",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":73.0,
            "Acc@5":90.9,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":18898,
            "title":"Revisiting the Effectiveness of Off-the-shelf Temporal Modeling Approaches for Large-scale Video Classification",
            "url":"\/paper\/revisiting-the-effectiveness-of-off-the-shelf",
            "published":"2017-08-12T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/revisiting-the-effectiveness-of-off-the-shelf\/review\/?hl=25418"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25408,
        "rank":158,
        "method":"MFNet",
        "mlmodel":{

        },
        "method_short":"MFNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-07-30",
        "metrics":{
            "Acc@1":"72.8",
            "Acc@5":"90.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":72.8,
            "Acc@5":90.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":53819,
            "title":"Multi-Fiber Networks for Video Recognition",
            "url":"\/paper\/multi-fiber-networks-for-video-recognition",
            "published":"2018-07-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/multi-fiber-networks-for-video-recognition\/review\/?hl=25408"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":28465,
        "rank":159,
        "method":"MoViNet-A1",
        "mlmodel":{

        },
        "method_short":"MoViNet-A1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Acc@1":"72.7",
            "Acc@5":"91.2",
            "FLOPs (G) x views":"6.0x1",
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":72.7,
            "Acc@5":91.2,
            "FLOPs (G) x views":6.0,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28465"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27775,
        "rank":160,
        "method":"ARTNet",
        "mlmodel":{

        },
        "method_short":"ARTNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-24",
        "metrics":{
            "Acc@1":"72.4",
            "Acc@5":"90.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":72.4,
            "Acc@5":90.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":4408,
            "title":"Appearance-and-Relation Networks for Video Classification",
            "url":"\/paper\/appearance-and-relation-networks-for-video",
            "published":"2017-11-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/appearance-and-relation-networks-for-video\/review\/?hl=27775"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27866,
        "rank":161,
        "method":"LGD-3D Flow (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"LGD-3D Flow ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-13",
        "metrics":{
            "Acc@1":"72.3",
            "Acc@5":"90.9",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":72.3,
            "Acc@5":90.9,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142601,
            "title":"Learning Spatio-Temporal Representation with Local and Global Diffusion",
            "url":"\/paper\/learning-spatio-temporal-representation-with-3",
            "published":"2019-06-13T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/learning-spatio-temporal-representation-with-3\/review\/?hl=27866"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25412,
        "rank":162,
        "method":"R[2+1]D",
        "mlmodel":{

        },
        "method_short":"R[2+1]D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-30",
        "metrics":{
            "Acc@1":"72",
            "Acc@5":"90",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":72.0,
            "Acc@5":90.0,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":6204,
            "title":"A Closer Look at Spatiotemporal Convolutions for Action Recognition",
            "url":"\/paper\/a-closer-look-at-spatiotemporal-convolutions",
            "published":"2017-11-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":19,
                "name":"R(2+1)D",
                "color":"#65a701"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27730,
        "rank":163,
        "method":"R[2+1]D-RGB",
        "mlmodel":{

        },
        "method_short":"R[2+1]D-RGB",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-30",
        "metrics":{
            "Acc@1":"72",
            "Acc@5":"90",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":72.0,
            "Acc@5":90.0,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":6204,
            "title":"A Closer Look at Spatiotemporal Convolutions for Action Recognition",
            "url":"\/paper\/a-closer-look-at-spatiotemporal-convolutions",
            "published":"2017-11-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":19,
                "name":"R(2+1)D",
                "color":"#65a701"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":6640,
        "rank":164,
        "method":"FASTER16 w\/o sp",
        "mlmodel":{

        },
        "method_short":"FASTER16 w\/o sp",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-10",
        "metrics":{
            "Acc@1":"71.7",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":71.7,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142365,
            "title":"FASTER Recurrent Networks for Efficient Video Classification",
            "url":"\/paper\/faster-recurrent-networks-for-video",
            "published":"2019-06-10T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/faster-recurrent-networks-for-video\/review\/?hl=6640"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58620,
        "rank":165,
        "method":"Co X3D-L_64",
        "mlmodel":{

        },
        "method_short":"Co X3D-L_64",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"71.61",
            "Acc@5":null,
            "FLOPs (G) x views":"1.25x1",
            "Parameters (M)":"6.15"
        },
        "raw_metrics":{
            "Acc@1":71.61,
            "Acc@5":null,
            "FLOPs (G) x views":1.25,
            "Parameters (M)":6.15
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58620"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":25409,
        "rank":166,
        "method":"I3D",
        "mlmodel":{

        },
        "method_short":"I3D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-05-22",
        "metrics":{
            "Acc@1":"71.1",
            "Acc@5":"89.3",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":71.1,
            "Acc@5":89.3,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":10430,
            "title":"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
            "url":"\/paper\/quo-vadis-action-recognition-a-new-model-and",
            "published":"2017-05-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/quo-vadis-action-recognition-a-new-model-and\/review\/?hl=25409"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58622,
        "rank":167,
        "method":"Co X3D-M_64",
        "mlmodel":{

        },
        "method_short":"Co X3D-M_64",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"71.03",
            "Acc@5":null,
            "FLOPs (G) x views":"0.33x1",
            "Parameters (M)":"3.79"
        },
        "raw_metrics":{
            "Acc@1":71.03,
            "Acc@5":null,
            "FLOPs (G) x views":0.33,
            "Parameters (M)":3.79
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58622"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58610,
        "rank":168,
        "method":"X3D-L",
        "mlmodel":{

        },
        "method_short":"X3D-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"69.29",
            "Acc@5":null,
            "FLOPs (G) x views":"19.17x1",
            "Parameters (M)":"6.15"
        },
        "raw_metrics":{
            "Acc@1":69.29,
            "Acc@5":null,
            "FLOPs (G) x views":19.17,
            "Parameters (M)":6.15
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58610"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":6676,
        "rank":169,
        "method":"MARS+RGB+Flow (16 frames)",
        "mlmodel":{

        },
        "method_short":"MARS+RGB+Flow ",
        "method_details":"16 frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-01",
        "metrics":{
            "Acc@1":"68.9",
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":68.9,
            "Acc@5":null,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":116002,
            "title":"MARS: Motion-Augmented RGB Stream for Action Recognition",
            "url":"\/paper\/mars-motion-augmented-rgb-stream-for-action",
            "published":"2019-06-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58608,
        "rank":170,
        "method":"SlowFast-8\u00d78-R50",
        "mlmodel":{

        },
        "method_short":"SlowFast-8\u00d78-R50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"68.45",
            "Acc@5":null,
            "FLOPs (G) x views":"66.25x1",
            "Parameters (M)":"66.25"
        },
        "raw_metrics":{
            "Acc@1":68.45,
            "Acc@5":null,
            "FLOPs (G) x views":66.25,
            "Parameters (M)":66.25
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58608"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27769,
        "rank":171,
        "method":"S3D-G (Flow, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"S3D-G ",
        "method_details":"Flow, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-12-13",
        "metrics":{
            "Acc@1":"68",
            "Acc@5":"87.6",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":68.0,
            "Acc@5":87.6,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":13041,
            "title":"Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
            "url":"\/paper\/rethinking-spatiotemporal-feature-learning",
            "published":"2017-12-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-spatiotemporal-feature-learning\/review\/?hl=27769"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27731,
        "rank":172,
        "method":"R[2+1]D-Flow",
        "mlmodel":{

        },
        "method_short":"R[2+1]D-Flow",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-30",
        "metrics":{
            "Acc@1":"67.5",
            "Acc@5":"87.2",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":67.5,
            "Acc@5":87.2,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":6204,
            "title":"A Closer Look at Spatiotemporal Convolutions for Action Recognition",
            "url":"\/paper\/a-closer-look-at-spatiotemporal-convolutions",
            "published":"2017-11-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58607,
        "rank":173,
        "method":"Slow-8x8-R50",
        "mlmodel":{

        },
        "method_short":"Slow-8x8-R50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"67.42",
            "Acc@5":null,
            "FLOPs (G) x views":"54.87x1",
            "Parameters (M)":"32.45"
        },
        "raw_metrics":{
            "Acc@1":67.42,
            "Acc@5":null,
            "FLOPs (G) x views":54.87,
            "Parameters (M)":32.45
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58607"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58624,
        "rank":174,
        "method":"Co X3D-S_64",
        "mlmodel":{

        },
        "method_short":"Co X3D-S_64",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"67.33",
            "Acc@5":null,
            "FLOPs (G) x views":"0.17x1",
            "Parameters (M)":"3.79"
        },
        "raw_metrics":{
            "Acc@1":67.33,
            "Acc@5":null,
            "FLOPs (G) x views":0.17,
            "Parameters (M)":3.79
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58624"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58611,
        "rank":175,
        "method":"X3D-M",
        "mlmodel":{

        },
        "method_short":"X3D-M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"67.24",
            "Acc@5":null,
            "FLOPs (G) x views":"4.97x1",
            "Parameters (M)":"3.79"
        },
        "raw_metrics":{
            "Acc@1":67.24,
            "Acc@5":null,
            "FLOPs (G) x views":4.97,
            "Parameters (M)":3.79
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58611"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58609,
        "rank":176,
        "method":"SlowFast-4\u00d716-R50",
        "mlmodel":{

        },
        "method_short":"SlowFast-4\u00d716-R50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"67.06",
            "Acc@5":null,
            "FLOPs (G) x views":"36.46x1",
            "Parameters (M)":"34.48"
        },
        "raw_metrics":{
            "Acc@1":67.06,
            "Acc@5":null,
            "FLOPs (G) x views":36.46,
            "Parameters (M)":34.48
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58609"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58617,
        "rank":177,
        "method":"Co Slow_8",
        "mlmodel":{

        },
        "method_short":"Co Slow_8",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"65.90",
            "Acc@5":null,
            "FLOPs (G) x views":"6.90x1",
            "Parameters (M)":"32.45"
        },
        "raw_metrics":{
            "Acc@1":65.9,
            "Acc@5":null,
            "FLOPs (G) x views":6.9,
            "Parameters (M)":32.45
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58617"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":28466,
        "rank":178,
        "method":"MoViNet-A0",
        "mlmodel":{

        },
        "method_short":"MoViNet-A0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Acc@1":"65.8",
            "Acc@5":"87.4",
            "FLOPs (G) x views":"2.7x1",
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":65.8,
            "Acc@5":87.4,
            "FLOPs (G) x views":2.7,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28466"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58612,
        "rank":179,
        "method":"X3D-S",
        "mlmodel":{

        },
        "method_short":"X3D-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"64.71",
            "Acc@5":null,
            "FLOPs (G) x views":"2.06x1",
            "Parameters (M)":"3.79"
        },
        "raw_metrics":{
            "Acc@1":64.71,
            "Acc@5":null,
            "FLOPs (G) x views":2.06,
            "Parameters (M)":3.79
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58612"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58604,
        "rank":180,
        "method":"I3D-R50",
        "mlmodel":{

        },
        "method_short":"I3D-R50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"63.98",
            "Acc@5":null,
            "FLOPs (G) x views":"28.61x1",
            "Parameters (M)":"28.04"
        },
        "raw_metrics":{
            "Acc@1":63.98,
            "Acc@5":null,
            "FLOPs (G) x views":28.61,
            "Parameters (M)":28.04
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58604"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58619,
        "rank":181,
        "method":"Co X3D-L_16",
        "mlmodel":{

        },
        "method_short":"Co X3D-L_16",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"63.03",
            "Acc@5":null,
            "FLOPs (G) x views":"1.25x1",
            "Parameters (M)":"6.15"
        },
        "raw_metrics":{
            "Acc@1":63.03,
            "Acc@5":null,
            "FLOPs (G) x views":1.25,
            "Parameters (M)":6.15
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58619"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58621,
        "rank":182,
        "method":"Co X3D-M_16",
        "mlmodel":{

        },
        "method_short":"Co X3D-M_16",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"62.80",
            "Acc@5":null,
            "FLOPs (G) x views":"0.33x1",
            "Parameters (M)":"3.79"
        },
        "raw_metrics":{
            "Acc@1":62.8,
            "Acc@5":null,
            "FLOPs (G) x views":0.33,
            "Parameters (M)":3.79
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58621"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58623,
        "rank":183,
        "method":"Co X3D-S_13",
        "mlmodel":{

        },
        "method_short":"Co X3D-S_13",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"60.18",
            "Acc@5":null,
            "FLOPs (G) x views":"0.17x1",
            "Parameters (M)":"3.79"
        },
        "raw_metrics":{
            "Acc@1":60.18,
            "Acc@5":null,
            "FLOPs (G) x views":0.17,
            "Parameters (M)":3.79
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58623"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58615,
        "rank":184,
        "method":"Co I3D_8",
        "mlmodel":{

        },
        "method_short":"Co I3D_8",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"59.58",
            "Acc@5":null,
            "FLOPs (G) x views":"5.68x1",
            "Parameters (M)":"28.04"
        },
        "raw_metrics":{
            "Acc@1":59.58,
            "Acc@5":null,
            "FLOPs (G) x views":5.68,
            "Parameters (M)":28.04
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58615"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58606,
        "rank":185,
        "method":"R(2+1)D-18_16",
        "mlmodel":{

        },
        "method_short":"R",
        "method_details":"2+1",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"59.52",
            "Acc@5":null,
            "FLOPs (G) x views":"40.71x1",
            "Parameters (M)":"31.51"
        },
        "raw_metrics":{
            "Acc@1":59.52,
            "Acc@5":null,
            "FLOPs (G) x views":40.71,
            "Parameters (M)":31.51
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58606"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58613,
        "rank":186,
        "method":"X3D-XS",
        "mlmodel":{

        },
        "method_short":"X3D-XS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"59.37",
            "Acc@5":null,
            "FLOPs (G) x views":"0.64x1",
            "Parameters (M)":"3.79"
        },
        "raw_metrics":{
            "Acc@1":59.37,
            "Acc@5":null,
            "FLOPs (G) x views":0.64,
            "Parameters (M)":3.79
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58613"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58616,
        "rank":187,
        "method":"Co I3D_64",
        "mlmodel":{

        },
        "method_short":"Co I3D_64",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"56.86",
            "Acc@5":null,
            "FLOPs (G) x views":"5.68x1",
            "Parameters (M)":"28.04"
        },
        "raw_metrics":{
            "Acc@1":56.86,
            "Acc@5":null,
            "FLOPs (G) x views":5.68,
            "Parameters (M)":28.04
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58616"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58605,
        "rank":188,
        "method":"R(2+1)D-18_8",
        "mlmodel":{

        },
        "method_short":"R",
        "method_details":"2+1",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"53.52",
            "Acc@5":null,
            "FLOPs (G) x views":"20.35x1",
            "Parameters (M)":"31.51"
        },
        "raw_metrics":{
            "Acc@1":53.52,
            "Acc@5":null,
            "FLOPs (G) x views":20.35,
            "Parameters (M)":31.51
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58605"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":58614,
        "rank":189,
        "method":"RCU_8",
        "mlmodel":{

        },
        "method_short":"RCU_8",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Acc@1":"53.40",
            "Acc@5":null,
            "FLOPs (G) x views":"4.71x1",
            "Parameters (M)":"12.80"
        },
        "raw_metrics":{
            "Acc@1":53.4,
            "Acc@5":null,
            "FLOPs (G) x views":4.71,
            "Parameters (M)":12.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":809466,
            "title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos",
            "url":"\/paper\/continual-3d-convolutional-neural-networks",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/continual-3d-convolutional-neural-networks\/review\/?hl=58614"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":30113,
        "rank":190,
        "method":"ViViT-L\/16x2 320",
        "mlmodel":{

        },
        "method_short":"ViViT-L\/16x2 320",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Acc@1":null,
            "Acc@5":"94.7",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":null,
            "Acc@5":94.7,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":758440,
            "title":"ViViT: A Video Vision Transformer",
            "url":"\/paper\/2103-15691",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-15691\/review\/?hl=30113"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":37569,
        "rank":191,
        "method":"ViT-B-VTN+ ImageNet-21K (84.0 [10])",
        "mlmodel":{

        },
        "method_short":"ViT-B-VTN+ ImageNet-21K ",
        "method_details":"84.0 [10]",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-01",
        "metrics":{
            "Acc@1":null,
            "Acc@5":"94.2",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":null,
            "Acc@5":94.2,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":741425,
            "title":"Video Transformer Network",
            "url":"\/paper\/video-transformer-network",
            "published":"2021-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-transformer-network\/review\/?hl=37569"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":27799,
        "rank":192,
        "method":"SlowFast 16x8 (ResNet-101 + NL)",
        "mlmodel":{

        },
        "method_short":"SlowFast 16x8 ",
        "method_details":"ResNet-101 + NL",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-10",
        "metrics":{
            "Acc@1":null,
            "Acc@5":"93.9",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":null,
            "Acc@5":93.9,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":64768,
            "title":"SlowFast Networks for Video Recognition",
            "url":"\/paper\/slowfast-networks-for-video-recognition",
            "published":"2018-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slowfast-networks-for-video-recognition\/review\/?hl=27799"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":28236,
        "rank":193,
        "method":"ViT-B-VTN (1 layer, ImageNet pretrain)",
        "mlmodel":{

        },
        "method_short":"ViT-B-VTN ",
        "method_details":"1 layer, ImageNet pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-01",
        "metrics":{
            "Acc@1":null,
            "Acc@5":"93.4",
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":null,
            "Acc@5":93.4,
            "FLOPs (G) x views":null,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":741425,
            "title":"Video Transformer Network",
            "url":"\/paper\/video-transformer-network",
            "published":"2021-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-transformer-network\/review\/?hl=28236"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1176,
        "row_id":44010,
        "rank":194,
        "method":"MViT-B (train from scratch)",
        "mlmodel":{

        },
        "method_short":"MViT-B ",
        "method_details":"train from scratch",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Acc@1":null,
            "Acc@5":null,
            "FLOPs (G) x views":"225x5",
            "Parameters (M)":null
        },
        "raw_metrics":{
            "Acc@1":null,
            "Acc@5":null,
            "FLOPs (G) x views":225.0,
            "Parameters (M)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=44010"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":154,
                "name":"MViT",
                "color":"#d327c5"
            }
        ],
        "reports":[

        ]
    }
]