[
    {
        "Model":"mlabonne\/AlphaMonarch-7B",
        "Average":62.74,
        "AGIEval":45.37,
        "GPT4All":77.01,
        "TruthfulQA":78.39,
        "Bigbench":50.2,
        "URL":"https:\/\/huggingface.co\/mlabonne\/AlphaMonarch-7B",
        "Likes":12
    },
    {
        "Model":"mlabonne\/NeuralMonarch-7B",
        "Average":62.73,
        "AGIEval":45.31,
        "GPT4All":76.99,
        "TruthfulQA":78.35,
        "Bigbench":50.28,
        "URL":"https:\/\/huggingface.co\/mlabonne\/NeuralMonarch-7B",
        "Likes":5
    },
    {
        "Model":"mlabonne\/Monarch-7B",
        "Average":62.68,
        "AGIEval":45.48,
        "GPT4All":77.07,
        "TruthfulQA":78.04,
        "Bigbench":50.14,
        "URL":"https:\/\/huggingface.co\/mlabonne\/Monarch-7B",
        "Likes":4
    },
    {
        "Model":"mlabonne\/Monarch-7B-slerp",
        "Average":62.6,
        "AGIEval":45.13,
        "GPT4All":77.09,
        "TruthfulQA":78.63,
        "Bigbench":49.56,
        "URL":"https:\/\/huggingface.co\/mlabonne\/Monarch-7B-slerp",
        "Likes":0
    },
    {
        "Model":"bardsai\/jaskier-7b-dpo-v3.3",
        "Average":62.58,
        "AGIEval":44.57,
        "GPT4All":76.53,
        "TruthfulQA":80.0,
        "Bigbench":49.22,
        "URL":"https:\/\/huggingface.co\/bardsai\/jaskier-7b-dpo-v3.3",
        "Likes":1
    },
    {
        "Model":"mlabonne\/Monarch-7B-dare",
        "Average":62.58,
        "AGIEval":45.16,
        "GPT4All":77.22,
        "TruthfulQA":77.98,
        "Bigbench":49.95,
        "URL":"https:\/\/huggingface.co\/mlabonne\/Monarch-7B-dare",
        "Likes":0
    },
    {
        "Model":"CultriX\/NeuralTrix-bf16",
        "Average":62.57,
        "AGIEval":44.43,
        "GPT4All":76.43,
        "TruthfulQA":80.18,
        "Bigbench":49.23,
        "URL":"https:\/\/huggingface.co\/CultriX\/NeuralTrix-bf16",
        "Likes":1
    },
    {
        "Model":"CultriX\/NeuralTrix-7B-dpo",
        "Average":62.5,
        "AGIEval":44.61,
        "GPT4All":76.33,
        "TruthfulQA":79.8,
        "Bigbench":49.24,
        "URL":"https:\/\/huggingface.co\/CultriX\/NeuralTrix-7B-dpo",
        "Likes":7
    },
    {
        "Model":"mlabonne\/NeuBeagle-7B",
        "Average":62.39,
        "AGIEval":44.43,
        "GPT4All":76.62,
        "TruthfulQA":79.13,
        "Bigbench":49.38,
        "URL":"https:\/\/huggingface.co\/mlabonne\/NeuBeagle-7B",
        "Likes":0
    },
    {
        "Model":"mlabonne\/OmniTruthyBeagle-7B-v0",
        "Average":62.39,
        "AGIEval":45.72,
        "GPT4All":77.49,
        "TruthfulQA":76.16,
        "Bigbench":50.18,
        "URL":"https:\/\/huggingface.co\/mlabonne\/OmniTruthyBeagle-7B-v0",
        "Likes":0
    },
    {
        "Model":"mlabonne\/NeuralOmniBeagle-7B",
        "Average":62.3,
        "AGIEval":45.85,
        "GPT4All":77.26,
        "TruthfulQA":76.06,
        "Bigbench":50.03,
        "URL":"https:\/\/huggingface.co\/mlabonne\/NeuralOmniBeagle-7B",
        "Likes":5
    },
    {
        "Model":"shadowml\/MBeagleX-7B",
        "Average":62.28,
        "AGIEval":45.02,
        "GPT4All":76.87,
        "TruthfulQA":78.04,
        "Bigbench":49.18,
        "URL":"https:\/\/huggingface.co\/shadowml\/MBeagleX-7B",
        "Likes":0
    },
    {
        "Model":"mlabonne\/OmniTruthyBeagle-7B",
        "Average":62.21,
        "AGIEval":45.65,
        "GPT4All":77.22,
        "TruthfulQA":75.77,
        "Bigbench":50.21,
        "URL":"https:\/\/huggingface.co\/mlabonne\/OmniTruthyBeagle-7B",
        "Likes":0
    },
    {
        "Model":"mlabonne\/NeuralOmniBeagle-7B-v2",
        "Average":62.15,
        "AGIEval":45.86,
        "GPT4All":77.31,
        "TruthfulQA":75.34,
        "Bigbench":50.09,
        "URL":"https:\/\/huggingface.co\/mlabonne\/NeuralOmniBeagle-7B-v2",
        "Likes":0
    },
    {
        "Model":"shadowml\/MBTrix-7B",
        "Average":62.12,
        "AGIEval":44.92,
        "GPT4All":77.14,
        "TruthfulQA":77.26,
        "Bigbench":49.18,
        "URL":"https:\/\/huggingface.co\/shadowml\/MBTrix-7B",
        "Likes":0
    },
    {
        "Model":"mlabonne\/OmniBeagle-7B",
        "Average":62.05,
        "AGIEval":45.64,
        "GPT4All":77.48,
        "TruthfulQA":75.03,
        "Bigbench":50.03,
        "URL":"https:\/\/huggingface.co\/mlabonne\/OmniBeagle-7B",
        "Likes":17
    },
    {
        "Model":"mlabonne\/NeuralOmni-7B",
        "Average":61.9,
        "AGIEval":45.8,
        "GPT4All":77.5,
        "TruthfulQA":74.51,
        "Bigbench":49.8,
        "URL":"https:\/\/huggingface.co\/mlabonne\/NeuralOmni-7B",
        "Likes":1
    },
    {
        "Model":"shadowml\/OmnixBeagle-7B",
        "Average":61.84,
        "AGIEval":45.3,
        "GPT4All":77.64,
        "TruthfulQA":75.24,
        "Bigbench":49.2,
        "URL":"https:\/\/huggingface.co\/shadowml\/OmnixBeagle-7B",
        "Likes":0
    },
    {
        "Model":"mlabonne\/Omnarch-7B",
        "Average":61.75,
        "AGIEval":45.88,
        "GPT4All":77.28,
        "TruthfulQA":74.07,
        "Bigbench":49.76,
        "URL":"https:\/\/huggingface.co\/mlabonne\/Omnarch-7B",
        "Likes":4
    },
    {
        "Model":"mlabonne\/Beagle4",
        "Average":61.61,
        "AGIEval":45.5,
        "GPT4All":77.38,
        "TruthfulQA":73.84,
        "Bigbench":49.7,
        "URL":"https:\/\/huggingface.co\/mlabonne\/Beagle4",
        "Likes":1
    },
    {
        "Model":"mlabonne\/BeagleB-7B",
        "Average":61.5,
        "AGIEval":45.19,
        "GPT4All":77.75,
        "TruthfulQA":73.19,
        "Bigbench":49.88,
        "URL":"https:\/\/huggingface.co\/mlabonne\/BeagleB-7B",
        "Likes":1
    },
    {
        "Model":"mlabonne\/ArchBeagle-7B",
        "Average":61.4,
        "AGIEval":45.56,
        "GPT4All":77.32,
        "TruthfulQA":73.36,
        "Bigbench":49.36,
        "URL":"https:\/\/huggingface.co\/mlabonne\/ArchBeagle-7B",
        "Likes":1
    },
    {
        "Model":"shadowml\/BeagleSempra-7B",
        "Average":61.38,
        "AGIEval":45.56,
        "GPT4All":77.44,
        "TruthfulQA":73.35,
        "Bigbench":49.15,
        "URL":"https:\/\/huggingface.co\/shadowml\/BeagleSempra-7B",
        "Likes":1
    },
    {
        "Model":"shadowml\/BeagSake-7B",
        "Average":61.35,
        "AGIEval":45.9,
        "GPT4All":77.36,
        "TruthfulQA":72.82,
        "Bigbench":49.32,
        "URL":"https:\/\/huggingface.co\/shadowml\/BeagSake-7B",
        "Likes":0
    },
    {
        "Model":"shadowml\/WestBeagle-7B",
        "Average":61.21,
        "AGIEval":46.19,
        "GPT4All":77.23,
        "TruthfulQA":72.25,
        "Bigbench":49.15,
        "URL":"https:\/\/huggingface.co\/shadowml\/WestBeagle-7B",
        "Likes":1
    },
    {
        "Model":"shadowml\/WestBeagle-7B-gen3",
        "Average":61.13,
        "AGIEval":45.74,
        "GPT4All":77.28,
        "TruthfulQA":72.29,
        "Bigbench":49.23,
        "URL":"https:\/\/huggingface.co\/shadowml\/WestBeagle-7B-gen3",
        "Likes":0
    },
    {
        "Model":"shadowml\/BeagleX-7B",
        "Average":61.11,
        "AGIEval":45.39,
        "GPT4All":77.52,
        "TruthfulQA":72.91,
        "Bigbench":48.63,
        "URL":"https:\/\/huggingface.co\/shadowml\/BeagleX-7B",
        "Likes":0
    },
    {
        "Model":"flemmingmiguel\/MBX-7B-v3",
        "Average":61.01,
        "AGIEval":45.08,
        "GPT4All":77.72,
        "TruthfulQA":72.61,
        "Bigbench":48.63,
        "URL":"https:\/\/huggingface.co\/flemmingmiguel\/MBX-7B-v3",
        "Likes":5
    },
    {
        "Model":"shadowml\/FoxBeagle-7B",
        "Average":60.97,
        "AGIEval":45.46,
        "GPT4All":77.42,
        "TruthfulQA":72.08,
        "Bigbench":48.91,
        "URL":"https:\/\/huggingface.co\/shadowml\/FoxBeagle-7B",
        "Likes":0
    },
    {
        "Model":"shadowml\/Beaglake-7B",
        "Average":60.97,
        "AGIEval":45.03,
        "GPT4All":77.8,
        "TruthfulQA":72.58,
        "Bigbench":48.48,
        "URL":"https:\/\/huggingface.co\/shadowml\/Beaglake-7B",
        "Likes":0
    },
    {
        "Model":"shadowml\/Beagwake-7B",
        "Average":60.88,
        "AGIEval":45.03,
        "GPT4All":77.54,
        "TruthfulQA":72.37,
        "Bigbench":48.56,
        "URL":"https:\/\/huggingface.co\/shadowml\/Beagwake-7B",
        "Likes":0
    },
    {
        "Model":"shadowml\/TurdusBeagle-7B-gen3",
        "Average":60.41,
        "AGIEval":45.08,
        "GPT4All":77.52,
        "TruthfulQA":70.36,
        "Bigbench":48.69,
        "URL":"https:\/\/huggingface.co\/shadowml\/TurdusBeagle-7B-gen3",
        "Likes":0
    },
    {
        "Model":"shadowml\/TurdusBeagle-7B",
        "Average":60.41,
        "AGIEval":45.08,
        "GPT4All":77.52,
        "TruthfulQA":70.36,
        "Bigbench":48.69,
        "URL":"https:\/\/huggingface.co\/shadowml\/TurdusBeagle-7B",
        "Likes":0
    },
    {
        "Model":"flemmingmiguel\/MBX-7B-v2",
        "Average":60.25,
        "AGIEval":44.23,
        "GPT4All":77.27,
        "TruthfulQA":71.04,
        "Bigbench":48.47,
        "URL":"https:\/\/huggingface.co\/flemmingmiguel\/MBX-7B-v2",
        "Likes":1
    },
    {
        "Model":"mlabonne\/NeuralBeagle14-7B",
        "Average":60.25,
        "AGIEval":46.06,
        "GPT4All":76.77,
        "TruthfulQA":70.32,
        "Bigbench":47.86,
        "URL":"https:\/\/huggingface.co\/mlabonne\/NeuralBeagle14-7B",
        "Likes":139
    },
    {
        "Model":"mlabonne\/FrakenBeagle14-11B",
        "Average":60.17,
        "AGIEval":45.08,
        "GPT4All":76.08,
        "TruthfulQA":70.93,
        "Bigbench":48.58,
        "URL":"https:\/\/huggingface.co\/mlabonne\/FrakenBeagle14-11B",
        "Likes":2
    },
    {
        "Model":"mlabonne\/DareBeagle-7B-v2",
        "Average":59.93,
        "AGIEval":45.6,
        "GPT4All":76.58,
        "TruthfulQA":69.48,
        "Bigbench":48.07,
        "URL":"https:\/\/huggingface.co\/mlabonne\/DareBeagle-7B-v2",
        "Likes":0
    },
    {
        "Model":"shadowml\/DareBeagle-7B",
        "Average":59.91,
        "AGIEval":45.47,
        "GPT4All":76.63,
        "TruthfulQA":69.48,
        "Bigbench":48.05,
        "URL":"https:\/\/huggingface.co\/shadowml\/DareBeagle-7B",
        "Likes":0
    },
    {
        "Model":"shadowml\/mibe-7B",
        "Average":59.91,
        "AGIEval":44.22,
        "GPT4All":76.9,
        "TruthfulQA":71.25,
        "Bigbench":47.27,
        "URL":"https:\/\/huggingface.co\/shadowml\/mibe-7B",
        "Likes":0
    },
    {
        "Model":"shadowml\/DareBeagel-2x7B",
        "Average":59.83,
        "AGIEval":45.51,
        "GPT4All":76.56,
        "TruthfulQA":69.45,
        "Bigbench":47.82,
        "URL":"https:\/\/huggingface.co\/shadowml\/DareBeagel-2x7B",
        "Likes":2
    },
    {
        "Model":"mlabonne\/Beagle14-7B",
        "Average":59.4,
        "AGIEval":44.38,
        "GPT4All":76.53,
        "TruthfulQA":69.44,
        "Bigbench":47.25,
        "URL":"https:\/\/huggingface.co\/mlabonne\/Beagle14-7B",
        "Likes":12
    },
    {
        "Model":"mlabonne\/NeuralDaredevil-7B",
        "Average":59.39,
        "AGIEval":45.23,
        "GPT4All":76.2,
        "TruthfulQA":67.61,
        "Bigbench":48.52,
        "URL":"https:\/\/huggingface.co\/mlabonne\/NeuralDaredevil-7B",
        "Likes":22
    },
    {
        "Model":"argilla\/distilabeled-Marcoro14-7B-slerp",
        "Average":58.93,
        "AGIEval":45.38,
        "GPT4All":76.48,
        "TruthfulQA":65.68,
        "Bigbench":48.18,
        "URL":"https:\/\/huggingface.co\/argilla\/distilabeled-Marcoro14-7B-slerp",
        "Likes":9
    },
    {
        "Model":"mlabonne\/NeuralMarcoro14-7B",
        "Average":58.4,
        "AGIEval":44.59,
        "GPT4All":76.17,
        "TruthfulQA":65.94,
        "Bigbench":46.9,
        "URL":"https:\/\/huggingface.co\/mlabonne\/NeuralMarcoro14-7B",
        "Likes":37
    },
    {
        "Model":"shadowml\/Daredevil-7B",
        "Average":58.22,
        "AGIEval":44.85,
        "GPT4All":76.07,
        "TruthfulQA":64.89,
        "Bigbench":47.07,
        "URL":"https:\/\/huggingface.co\/shadowml\/Daredevil-7B",
        "Likes":1
    },
    {
        "Model":"occultml\/CatMarcoro14-7B-slerp",
        "Average":58.06,
        "AGIEval":45.21,
        "GPT4All":75.91,
        "TruthfulQA":63.81,
        "Bigbench":47.31,
        "URL":"https:\/\/huggingface.co\/occultml\/CatMarcoro14-7B-slerp",
        "Likes":0
    },
    {
        "Model":"mlabonne\/NeuralDarewin-7B",
        "Average":57.85,
        "AGIEval":45.6,
        "GPT4All":74.29,
        "TruthfulQA":63.15,
        "Bigbench":48.35,
        "URL":"https:\/\/huggingface.co\/mlabonne\/NeuralDarewin-7B",
        "Likes":1
    },
    {
        "Model":"fblgit\/una-cybertron-7b-v2-bf16",
        "Average":57.76,
        "AGIEval":43.29,
        "GPT4All":74.98,
        "TruthfulQA":65.32,
        "Bigbench":47.45,
        "URL":"https:\/\/huggingface.co\/fblgit\/una-cybertron-7b-v2-bf16",
        "Likes":113
    },
    {
        "Model":"mlabonne\/Marcoro14-7B-slerp",
        "Average":57.67,
        "AGIEval":44.66,
        "GPT4All":76.24,
        "TruthfulQA":64.15,
        "Bigbench":45.64,
        "URL":"https:\/\/huggingface.co\/mlabonne\/Marcoro14-7B-slerp",
        "Likes":20
    },
    {
        "Model":"Weyaxi\/OpenHermes-2.5-neural-chat-v3-3-Slerp",
        "Average":57.27,
        "AGIEval":43.5,
        "GPT4All":74.88,
        "TruthfulQA":63.22,
        "Bigbench":47.5,
        "URL":"https:\/\/huggingface.co\/Weyaxi\/OpenHermes-2.5-neural-chat-v3-3-Slerp",
        "Likes":39
    },
    {
        "Model":"mlabonne\/Darewin-7B",
        "Average":57.2,
        "AGIEval":45.08,
        "GPT4All":75.36,
        "TruthfulQA":60.94,
        "Bigbench":47.44,
        "URL":"https:\/\/huggingface.co\/mlabonne\/Darewin-7B",
        "Likes":1
    },
    {
        "Model":"mlabonne\/Beyonder-4x7B-v2",
        "Average":57.13,
        "AGIEval":45.29,
        "GPT4All":75.95,
        "TruthfulQA":60.86,
        "Bigbench":46.4,
        "URL":"https:\/\/huggingface.co\/mlabonne\/Beyonder-4x7B-v2",
        "Likes":109
    },
    {
        "Model":"OpenPipe\/mistral-ft-optimized-1218",
        "Average":56.85,
        "AGIEval":44.74,
        "GPT4All":75.6,
        "TruthfulQA":59.89,
        "Bigbench":47.17,
        "URL":"https:\/\/huggingface.co\/OpenPipe\/mistral-ft-optimized-1218",
        "Likes":146
    },
    {
        "Model":"mistralai\/Mistral-7B-Instruct-v0.2",
        "Average":54.81,
        "AGIEval":38.5,
        "GPT4All":71.64,
        "TruthfulQA":66.82,
        "Bigbench":42.29,
        "URL":"https:\/\/huggingface.co\/mistralai\/Mistral-7B-Instruct-v0.2",
        "Likes":889
    },
    {
        "Model":"cognitivecomputations\/dolphin-2.6-mistral-7b-dpo-laser",
        "Average":53.92,
        "AGIEval":38.32,
        "GPT4All":73.77,
        "TruthfulQA":61.03,
        "Bigbench":42.58,
        "URL":"https:\/\/huggingface.co\/cognitivecomputations\/dolphin-2.6-mistral-7b-dpo-laser",
        "Likes":88
    },
    {
        "Model":"openchat\/openchat-3.5-0106",
        "Average":53.71,
        "AGIEval":44.17,
        "GPT4All":73.72,
        "TruthfulQA":52.53,
        "Bigbench":44.4,
        "URL":"https:\/\/huggingface.co\/openchat\/openchat-3.5-0106",
        "Likes":177
    },
    {
        "Model":"mlabonne\/NeuralHermes-2.5-Mistral-7B-laser",
        "Average":53.62,
        "AGIEval":43.54,
        "GPT4All":73.44,
        "TruthfulQA":55.26,
        "Bigbench":42.24,
        "URL":"https:\/\/huggingface.co\/mlabonne\/NeuralHermes-2.5-Mistral-7B-laser",
        "Likes":8
    },
    {
        "Model":"mlabonne\/NeuralHermes-2.5-Mistral-7B",
        "Average":53.51,
        "AGIEval":43.67,
        "GPT4All":73.24,
        "TruthfulQA":55.37,
        "Bigbench":41.76,
        "URL":"https:\/\/huggingface.co\/mlabonne\/NeuralHermes-2.5-Mistral-7B",
        "Likes":135
    },
    {
        "Model":"openchat\/openchat-3.5-1210",
        "Average":53.14,
        "AGIEval":42.62,
        "GPT4All":72.84,
        "TruthfulQA":53.21,
        "Bigbench":43.88,
        "URL":"https:\/\/huggingface.co\/openchat\/openchat-3.5-1210",
        "Likes":261
    },
    {
        "Model":"teknium\/OpenHermes-2.5-Mistral-7B",
        "Average":52.42,
        "AGIEval":42.75,
        "GPT4All":72.99,
        "TruthfulQA":52.99,
        "Bigbench":40.94,
        "URL":"https:\/\/huggingface.co\/teknium\/OpenHermes-2.5-Mistral-7B",
        "Likes":644
    },
    {
        "Model":"HuggingFaceH4\/zephyr-7b-alpha",
        "Average":51.72,
        "AGIEval":38.0,
        "GPT4All":72.24,
        "TruthfulQA":56.06,
        "Bigbench":40.57,
        "URL":"https:\/\/huggingface.co\/HuggingFaceH4\/zephyr-7b-alpha",
        "Likes":1043
    },
    {
        "Model":"mlabonne\/zephusion-2x7b",
        "Average":51.66,
        "AGIEval":37.82,
        "GPT4All":72.14,
        "TruthfulQA":55.96,
        "Bigbench":40.71,
        "URL":"https:\/\/huggingface.co\/mlabonne\/zephusion-2x7b",
        "Likes":0
    },
    {
        "Model":"Open-Orca\/Mistral-7B-OpenOrca",
        "Average":51.39,
        "AGIEval":39.24,
        "GPT4All":72.39,
        "TruthfulQA":52.27,
        "Bigbench":41.65,
        "URL":"https:\/\/huggingface.co\/Open-Orca\/Mistral-7B-OpenOrca",
        "Likes":598
    },
    {
        "Model":"openchat\/openchat_3.5",
        "Average":51.34,
        "AGIEval":42.67,
        "GPT4All":72.92,
        "TruthfulQA":47.27,
        "Bigbench":42.51,
        "URL":"https:\/\/huggingface.co\/openchat\/openchat_3.5",
        "Likes":1038
    },
    {
        "Model":"berkeley-nest\/Starling-LM-7B-alpha",
        "Average":51.16,
        "AGIEval":42.06,
        "GPT4All":72.72,
        "TruthfulQA":47.33,
        "Bigbench":42.53,
        "URL":"https:\/\/huggingface.co\/berkeley-nest\/Starling-LM-7B-alpha",
        "Likes":461
    },
    {
        "Model":"cognitivecomputations\/dolphin-2.2.1-mistral-7b",
        "Average":51.05,
        "AGIEval":38.64,
        "GPT4All":72.24,
        "TruthfulQA":54.09,
        "Bigbench":39.22,
        "URL":"https:\/\/huggingface.co\/cognitivecomputations\/dolphin-2.2.1-mistral-7b",
        "Likes":164
    },
    {
        "Model":"HuggingFaceH4\/zephyr-7b-beta",
        "Average":50.99,
        "AGIEval":37.33,
        "GPT4All":71.83,
        "TruthfulQA":55.1,
        "Bigbench":39.7,
        "URL":"https:\/\/huggingface.co\/HuggingFaceH4\/zephyr-7b-beta",
        "Likes":1288
    },
    {
        "Model":"mlabonne\/Darewin-7B-v2",
        "Average":50.33,
        "AGIEval":37.67,
        "GPT4All":73.16,
        "TruthfulQA":49.5,
        "Bigbench":41.01,
        "URL":"https:\/\/huggingface.co\/mlabonne\/Darewin-7B-v2",
        "Likes":0
    },
    {
        "Model":"mistralai\/Mistral-7B-Instruct-v0.1",
        "Average":49.15,
        "AGIEval":33.36,
        "GPT4All":67.87,
        "TruthfulQA":55.89,
        "Bigbench":39.48,
        "URL":"https:\/\/huggingface.co\/mistralai\/Mistral-7B-Instruct-v0.1",
        "Likes":1315
    },
    {
        "Model":"mlabonne\/Mistralpaca-7B",
        "Average":48.53,
        "AGIEval":33.48,
        "GPT4All":70.71,
        "TruthfulQA":52.89,
        "Bigbench":37.06,
        "URL":"https:\/\/huggingface.co\/mlabonne\/Mistralpaca-7B",
        "Likes":2
    },
    {
        "Model":"shadowml\/phixtral-4x2_8odd",
        "Average":48.42,
        "AGIEval":34.46,
        "GPT4All":72.34,
        "TruthfulQA":49.56,
        "Bigbench":37.3,
        "URL":"https:\/\/huggingface.co\/shadowml\/phixtral-4x2_8odd",
        "Likes":1
    },
    {
        "Model":"mlabonne\/phixtral-3x2_8",
        "Average":48.23,
        "AGIEval":33.58,
        "GPT4All":72.1,
        "TruthfulQA":49.59,
        "Bigbench":37.67,
        "URL":"https:\/\/huggingface.co\/mlabonne\/phixtral-3x2_8",
        "Likes":1
    },
    {
        "Model":"shadowml\/phixtral-4x2_8odo",
        "Average":48.08,
        "AGIEval":33.74,
        "GPT4All":71.93,
        "TruthfulQA":48.68,
        "Bigbench":37.95,
        "URL":"https:\/\/huggingface.co\/shadowml\/phixtral-4x2_8odo",
        "Likes":0
    },
    {
        "Model":"meetkai\/functionary-small-v2.2",
        "Average":47.99,
        "AGIEval":33.15,
        "GPT4All":70.35,
        "TruthfulQA":51.5,
        "Bigbench":36.97,
        "URL":"https:\/\/huggingface.co\/meetkai\/functionary-small-v2.2",
        "Likes":10
    },
    {
        "Model":"rhysjones\/phi-2-orange",
        "Average":47.97,
        "AGIEval":33.37,
        "GPT4All":71.33,
        "TruthfulQA":49.87,
        "Bigbench":37.3,
        "URL":"https:\/\/huggingface.co\/rhysjones\/phi-2-orange",
        "Likes":29
    },
    {
        "Model":"mlabonne\/phixtral-2x2_8",
        "Average":47.78,
        "AGIEval":34.1,
        "GPT4All":70.44,
        "TruthfulQA":48.78,
        "Bigbench":37.82,
        "URL":"https:\/\/huggingface.co\/mlabonne\/phixtral-2x2_8",
        "Likes":126
    },
    {
        "Model":"mlabonne\/phixtral-4x2_8",
        "Average":47.7,
        "AGIEval":33.91,
        "GPT4All":70.44,
        "TruthfulQA":48.78,
        "Bigbench":37.68,
        "URL":"https:\/\/huggingface.co\/mlabonne\/phixtral-4x2_8",
        "Likes":186
    },
    {
        "Model":"lxuechen\/phi-2-dpo",
        "Average":46.93,
        "AGIEval":30.39,
        "GPT4All":71.68,
        "TruthfulQA":50.75,
        "Bigbench":34.9,
        "URL":"https:\/\/huggingface.co\/lxuechen\/phi-2-dpo",
        "Likes":13
    },
    {
        "Model":"cognitivecomputations\/dolphin-2_6-phi-2",
        "Average":46.89,
        "AGIEval":33.12,
        "GPT4All":69.85,
        "TruthfulQA":47.39,
        "Bigbench":37.2,
        "URL":"https:\/\/huggingface.co\/cognitivecomputations\/dolphin-2_6-phi-2",
        "Likes":167
    },
    {
        "Model":"meta-math\/MetaMath-Mistral-7B",
        "Average":46.64,
        "AGIEval":33.91,
        "GPT4All":70.12,
        "TruthfulQA":44.83,
        "Bigbench":37.71,
        "URL":"https:\/\/huggingface.co\/meta-math\/MetaMath-Mistral-7B",
        "Likes":66
    },
    {
        "Model":"marcel\/phixtral-4x2_8-gates-poc",
        "Average":46.51,
        "AGIEval":31.78,
        "GPT4All":70.22,
        "TruthfulQA":47.01,
        "Bigbench":37.02,
        "URL":"https:\/\/huggingface.co\/marcel\/phixtral-4x2_8-gates-poc",
        "Likes":4
    },
    {
        "Model":"Yhyu13\/phi-2-sft-dpo-gpt4_en-ep1",
        "Average":46.43,
        "AGIEval":30.61,
        "GPT4All":71.13,
        "TruthfulQA":48.74,
        "Bigbench":35.23,
        "URL":"https:\/\/huggingface.co\/Yhyu13\/phi-2-sft-dpo-gpt4_en-ep1",
        "Likes":7
    },
    {
        "Model":"deepseek-ai\/deepseek-moe-16b-chat",
        "Average":45.72,
        "AGIEval":30.42,
        "GPT4All":68.72,
        "TruthfulQA":48.73,
        "Bigbench":35.02,
        "URL":"https:\/\/huggingface.co\/deepseek-ai\/deepseek-moe-16b-chat",
        "Likes":93
    },
    {
        "Model":"g-ronimo\/phi-2-OpenHermes-2.5",
        "Average":45.3,
        "AGIEval":30.27,
        "GPT4All":71.18,
        "TruthfulQA":43.87,
        "Bigbench":35.9,
        "URL":"https:\/\/huggingface.co\/g-ronimo\/phi-2-OpenHermes-2.5",
        "Likes":9
    },
    {
        "Model":"microsoft\/phi-2",
        "Average":44.61,
        "AGIEval":27.98,
        "GPT4All":70.8,
        "TruthfulQA":44.43,
        "Bigbench":35.21,
        "URL":"https:\/\/huggingface.co\/microsoft\/phi-2",
        "Likes":2749
    },
    {
        "Model":"stabilityai\/stablelm-zephyr-3b",
        "Average":44.42,
        "AGIEval":34.04,
        "GPT4All":62.07,
        "TruthfulQA":46.46,
        "Bigbench":35.11,
        "URL":"https:\/\/huggingface.co\/stabilityai\/stablelm-zephyr-3b",
        "Likes":202
    },
    {
        "Model":"venkycs\/phi-2-instruct",
        "Average":43.86,
        "AGIEval":25.8,
        "GPT4All":67.93,
        "TruthfulQA":44.82,
        "Bigbench":36.88,
        "URL":"https:\/\/huggingface.co\/venkycs\/phi-2-instruct",
        "Likes":29
    }
]