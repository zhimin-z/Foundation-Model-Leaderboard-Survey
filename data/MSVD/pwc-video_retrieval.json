[
    {
        "table_id":1137,
        "row_id":70641,
        "rank":1,
        "method":"HunYuan_tvr (huge)",
        "mlmodel":{

        },
        "method_short":"HunYuan_tvr ",
        "method_details":"huge",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "text-to-video R@1":"59.0",
            "text-to-video R@5":"84.0",
            "text-to-video R@10":"90.3",
            "text-to-video Median Rank":" 1.0",
            "text-to-video Mean Rank":"7.6 ",
            "text-to-video R@50":null,
            "video-to-text R@1":"73.0",
            "video-to-text R@5":"94.5",
            "video-to-text R@10":"96.6",
            "video-to-text Median Rank":"1.0",
            "video-to-text Mean Rank":"7.6"
        },
        "raw_metrics":{
            "text-to-video R@1":59.0,
            "text-to-video R@5":84.0,
            "text-to-video R@10":90.3,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":7.6,
            "text-to-video R@50":null,
            "video-to-text R@1":73.0,
            "video-to-text R@5":94.5,
            "video-to-text R@10":96.6,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":7.6
        },
        "uses_additional_data":true,
        "paper":{
            "id":990765,
            "title":"Tencent Text-Video Retrieval: Hierarchical Cross-Modal Interactions with Multi-Level Representations",
            "url":"\/paper\/hunyuan-tvr-for-text-video-retrivial",
            "published":"2022-04-07T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":86722,
        "rank":2,
        "method":"InternVideo",
        "mlmodel":{

        },
        "method_short":"InternVideo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "text-to-video R@1":"58.4",
            "text-to-video R@5":null,
            "text-to-video R@10":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "text-to-video R@50":null,
            "video-to-text R@1":"76.3",
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":58.4,
            "text-to-video R@5":null,
            "text-to-video R@10":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "text-to-video R@50":null,
            "video-to-text R@1":76.3,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124231,
            "title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
            "url":"\/paper\/internvideo-general-video-foundation-models",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internvideo-general-video-foundation-models\/review\/?hl=86722"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":51589,
        "rank":3,
        "method":"HunYuan_tvr",
        "mlmodel":{

        },
        "method_short":"HunYuan_tvr",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "text-to-video R@1":"58.2",
            "text-to-video R@5":"83.5",
            "text-to-video R@10":"90.1",
            "text-to-video Median Rank":"1",
            "text-to-video Mean Rank":"7.8",
            "text-to-video R@50":null,
            "video-to-text R@1":"69.1",
            "video-to-text R@5":"91.5",
            "video-to-text R@10":"95.0",
            "video-to-text Median Rank":"1.0",
            "video-to-text Mean Rank":"3.8"
        },
        "raw_metrics":{
            "text-to-video R@1":58.2,
            "text-to-video R@5":83.5,
            "text-to-video R@10":90.1,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":7.8,
            "text-to-video R@50":null,
            "video-to-text R@1":69.1,
            "video-to-text R@5":91.5,
            "video-to-text R@10":95.0,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":3.8
        },
        "uses_additional_data":true,
        "paper":{
            "id":990765,
            "title":"Tencent Text-Video Retrieval: Hierarchical Cross-Modal Interactions with Multi-Level Representations",
            "url":"\/paper\/hunyuan-tvr-for-text-video-retrivial",
            "published":"2022-04-07T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":103556,
        "rank":4,
        "method":"VLAB",
        "mlmodel":{

        },
        "method_short":"VLAB",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-22",
        "metrics":{
            "text-to-video R@1":"57.5",
            "text-to-video R@5":"83.6",
            "text-to-video R@10":"89.9",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "text-to-video R@50":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":57.5,
            "text-to-video R@5":83.6,
            "text-to-video R@10":89.9,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "text-to-video R@50":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1212983,
            "title":"VLAB: Enhancing Video Language Pre-training by Feature Adapting and Blending",
            "url":"\/paper\/vlab-enhancing-video-language-pre-training-by",
            "published":"2023-05-22T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/vlab-enhancing-video-language-pre-training-by\/review\/?hl=103556"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":49161,
        "rank":5,
        "method":"MDMMT-2",
        "mlmodel":{

        },
        "method_short":"MDMMT-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-14",
        "metrics":{
            "text-to-video R@1":"56.8",
            "text-to-video R@5":"83.1",
            "text-to-video R@10":"89.2",
            "text-to-video Median Rank":"1.0",
            "text-to-video Mean Rank":"8.8",
            "text-to-video R@50":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":56.8,
            "text-to-video R@5":83.1,
            "text-to-video R@10":89.2,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":8.8,
            "text-to-video R@50":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":976160,
            "title":"MDMMT-2: Multidomain Multimodal Transformer for Video Retrieval, One More Step Towards Generalization",
            "url":"\/paper\/mdmmt-2-multidomain-multimodal-transformer",
            "published":"2022-03-14T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/mdmmt-2-multidomain-multimodal-transformer\/review\/?hl=49161"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":112697,
        "rank":6,
        "method":"Side4Video",
        "mlmodel":{

        },
        "method_short":"Side4Video",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "text-to-video R@1":"56.1",
            "text-to-video R@5":"81.7",
            "text-to-video R@10":"88.8",
            "text-to-video Median Rank":"1.0",
            "text-to-video Mean Rank":"8.4",
            "text-to-video R@50":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":56.1,
            "text-to-video R@5":81.7,
            "text-to-video R@10":88.8,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":8.4,
            "text-to-video R@50":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327957,
            "title":"Side4Video: Spatial-Temporal Side Network for Memory-Efficient Image-to-Video Transfer Learning",
            "url":"\/paper\/side4video-spatial-temporal-side-network-for",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/side4video-spatial-temporal-side-network-for\/review\/?hl=112697"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":35396,
        "rank":7,
        "method":"CAMoE",
        "mlmodel":{

        },
        "method_short":"CAMoE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-09",
        "metrics":{
            "text-to-video R@1":"51.8",
            "text-to-video R@5":"87.6",
            "text-to-video R@10":"87.6",
            "text-to-video Median Rank":"1",
            "text-to-video Mean Rank":"8.9",
            "text-to-video R@50":null,
            "video-to-text R@1":"69.3",
            "video-to-text R@5":"90.6",
            "video-to-text R@10":"94.6",
            "video-to-text Median Rank":"1",
            "video-to-text Mean Rank":"3.1"
        },
        "raw_metrics":{
            "text-to-video R@1":51.8,
            "text-to-video R@5":87.6,
            "text-to-video R@10":87.6,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":8.9,
            "text-to-video R@50":null,
            "video-to-text R@1":69.3,
            "video-to-text R@5":90.6,
            "video-to-text R@10":94.6,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":3.1
        },
        "uses_additional_data":true,
        "paper":{
            "id":864259,
            "title":"Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss",
            "url":"\/paper\/improving-video-text-retrieval-by-multi",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improving-video-text-retrieval-by-multi\/review\/?hl=35396"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":88697,
        "rank":8,
        "method":"Cap4Video",
        "mlmodel":{

        },
        "method_short":"Cap4Video",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-31",
        "metrics":{
            "text-to-video R@1":"51.8",
            "text-to-video R@5":"80.8",
            "text-to-video R@10":"88.3",
            "text-to-video Median Rank":"1",
            "text-to-video Mean Rank":"8.3",
            "text-to-video R@50":null,
            "video-to-text R@1":"70.0",
            "video-to-text R@5":"93.2",
            "video-to-text R@10":"96.2",
            "video-to-text Median Rank":"1",
            "video-to-text Mean Rank":"2.4"
        },
        "raw_metrics":{
            "text-to-video R@1":51.8,
            "text-to-video R@5":80.8,
            "text-to-video R@10":88.3,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":8.3,
            "text-to-video R@50":null,
            "video-to-text R@1":70.0,
            "video-to-text R@5":93.2,
            "video-to-text R@10":96.2,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":2.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136847,
            "title":"Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?",
            "url":"\/paper\/cap4video-what-can-auxiliary-captions-do-for",
            "published":"2022-12-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cap4video-what-can-auxiliary-captions-do-for\/review\/?hl=88697"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":53736,
        "rank":9,
        "method":"CenterCLIP (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"CenterCLIP ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-02",
        "metrics":{
            "text-to-video R@1":"50.6",
            "text-to-video R@5":"80.3",
            "text-to-video R@10":"88.4",
            "text-to-video Median Rank":"1",
            "text-to-video Mean Rank":"8.4",
            "text-to-video R@50":null,
            "video-to-text R@1":"68.4",
            "video-to-text R@5":"90.1",
            "video-to-text R@10":"95.0",
            "video-to-text Median Rank":"1",
            "video-to-text Mean Rank":"3.0"
        },
        "raw_metrics":{
            "text-to-video R@1":50.6,
            "text-to-video R@5":80.3,
            "text-to-video R@10":88.4,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":8.4,
            "text-to-video R@50":null,
            "video-to-text R@1":68.4,
            "video-to-text R@5":90.1,
            "video-to-text R@10":95.0,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":3.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1002430,
            "title":"CenterCLIP: Token Clustering for Efficient Text-Video Retrieval",
            "url":"\/paper\/centerclip-token-clustering-for-efficient",
            "published":"2022-05-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/centerclip-token-clustering-for-efficient\/review\/?hl=53736"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":65459,
        "rank":10,
        "method":"X-CLIP",
        "mlmodel":{

        },
        "method_short":"X-CLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-15",
        "metrics":{
            "text-to-video R@1":"50.4",
            "text-to-video R@5":"80.6",
            "text-to-video R@10":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":"8.4",
            "text-to-video R@50":null,
            "video-to-text R@1":"66.8",
            "video-to-text R@5":null,
            "video-to-text R@10":"90.4",
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":"4.2"
        },
        "raw_metrics":{
            "text-to-video R@1":50.4,
            "text-to-video R@5":80.6,
            "text-to-video R@10":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":8.4,
            "text-to-video R@50":null,
            "video-to-text R@1":66.8,
            "video-to-text R@5":null,
            "video-to-text R@10":90.4,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":4.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1044539,
            "title":"X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval",
            "url":"\/paper\/x-clip-end-to-end-multi-grained-contrastive",
            "published":"2022-07-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-clip-end-to-end-multi-grained-contrastive\/review\/?hl=65459"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":109147,
        "rank":11,
        "method":"DMAE\n(ViT-B\/32)",
        "mlmodel":{

        },
        "method_short":"DMAE\n",
        "method_details":"ViT-B\/32",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-20",
        "metrics":{
            "text-to-video R@1":"48.7",
            "text-to-video R@5":"78.4",
            "text-to-video R@10":"86.3",
            "text-to-video Median Rank":"2.0",
            "text-to-video Mean Rank":"9.8",
            "text-to-video R@50":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":48.7,
            "text-to-video R@5":78.4,
            "text-to-video R@10":86.3,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":9.8,
            "text-to-video R@50":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1284092,
            "title":"Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning",
            "url":"\/paper\/dual-modal-attention-enhanced-text-video",
            "published":"2023-09-20T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/dual-modal-attention-enhanced-text-video\/review\/?hl=109147"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":45083,
        "rank":12,
        "method":"QB-Norm+CLIP2Video",
        "mlmodel":{

        },
        "method_short":"QB-Norm+CLIP2Video",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "text-to-video R@1":"48.0",
            "text-to-video R@5":"77.9",
            "text-to-video R@10":"86.2",
            "text-to-video Median Rank":"2.0",
            "text-to-video Mean Rank":null,
            "text-to-video R@50":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":48.0,
            "text-to-video R@5":77.9,
            "text-to-video R@10":86.2,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":null,
            "text-to-video R@50":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":933091,
            "title":"Cross Modal Retrieval with Querybank Normalisation",
            "url":"\/paper\/cross-modal-retrieval-with-querybank",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cross-modal-retrieval-with-querybank\/review\/?hl=45083"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":100111,
        "rank":13,
        "method":"DiffusionRet+QB-Norm",
        "mlmodel":{

        },
        "method_short":"DiffusionRet+QB-Norm",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-17",
        "metrics":{
            "text-to-video R@1":"47.9",
            "text-to-video R@5":"77.2",
            "text-to-video R@10":"84.8",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":"15.6",
            "text-to-video R@50":null,
            "video-to-text R@1":"60.3",
            "video-to-text R@5":"86.4",
            "video-to-text R@10":"92",
            "video-to-text Median Rank":"1.0",
            "video-to-text Mean Rank":"4.5"
        },
        "raw_metrics":{
            "text-to-video R@1":47.9,
            "text-to-video R@5":77.2,
            "text-to-video R@10":84.8,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":15.6,
            "text-to-video R@50":null,
            "video-to-text R@1":60.3,
            "video-to-text R@5":86.4,
            "video-to-text R@10":92.0,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":4.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1175924,
            "title":"DiffusionRet: Generative Text-Video Retrieval with Diffusion Model",
            "url":"\/paper\/diffusionret-generative-text-video-retrieval",
            "published":"2023-03-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diffusionret-generative-text-video-retrieval\/review\/?hl=100111"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":50778,
        "rank":14,
        "method":"X-Pool",
        "mlmodel":{

        },
        "method_short":"X-Pool",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-28",
        "metrics":{
            "text-to-video R@1":"47.2",
            "text-to-video R@5":"77.4",
            "text-to-video R@10":"86.0",
            "text-to-video Median Rank":"2.0",
            "text-to-video Mean Rank":"9.3",
            "text-to-video R@50":null,
            "video-to-text R@1":"66.4",
            "video-to-text R@5":"90.0",
            "video-to-text R@10":"94.2",
            "video-to-text Median Rank":"1.0",
            "video-to-text Mean Rank":"3.3"
        },
        "raw_metrics":{
            "text-to-video R@1":47.2,
            "text-to-video R@5":77.4,
            "text-to-video R@10":86.0,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":9.3,
            "text-to-video R@50":null,
            "video-to-text R@1":66.4,
            "video-to-text R@5":90.0,
            "video-to-text R@10":94.2,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":3.3
        },
        "uses_additional_data":true,
        "paper":{
            "id":985243,
            "title":"X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval",
            "url":"\/paper\/x-pool-cross-modal-language-video-attention",
            "published":"2022-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":100109,
        "rank":15,
        "method":"DiffusionRet",
        "mlmodel":{

        },
        "method_short":"DiffusionRet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-17",
        "metrics":{
            "text-to-video R@1":"46.6",
            "text-to-video R@5":"75.9",
            "text-to-video R@10":"84.1",
            "text-to-video Median Rank":"2.0",
            "text-to-video Mean Rank":"15.7",
            "text-to-video R@50":null,
            "video-to-text R@1":"61.9",
            "video-to-text R@5":"88.3",
            "video-to-text R@10":"92.9",
            "video-to-text Median Rank":"1.0",
            "video-to-text Mean Rank":"4.5"
        },
        "raw_metrics":{
            "text-to-video R@1":46.6,
            "text-to-video R@5":75.9,
            "text-to-video R@10":84.1,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":15.7,
            "text-to-video R@50":null,
            "video-to-text R@1":61.9,
            "video-to-text R@5":88.3,
            "video-to-text R@10":92.9,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":4.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1175924,
            "title":"DiffusionRet: Generative Text-Video Retrieval with Diffusion Model",
            "url":"\/paper\/diffusionret-generative-text-video-retrieval",
            "published":"2023-03-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diffusionret-generative-text-video-retrieval\/review\/?hl=100109"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":30332,
        "rank":16,
        "method":"CLIP4Clip",
        "mlmodel":{

        },
        "method_short":"CLIP4Clip",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-18",
        "metrics":{
            "text-to-video R@1":"46.2",
            "text-to-video R@5":"76.1",
            "text-to-video R@10":"84.6",
            "text-to-video Median Rank":"2",
            "text-to-video Mean Rank":"10.0",
            "text-to-video R@50":null,
            "video-to-text R@1":"62.0",
            "video-to-text R@5":"87.3",
            "video-to-text R@10":"92.6",
            "video-to-text Median Rank":"1",
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":46.2,
            "text-to-video R@5":76.1,
            "text-to-video R@10":84.6,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":10.0,
            "text-to-video R@50":null,
            "video-to-text R@1":62.0,
            "video-to-text R@5":87.3,
            "video-to-text R@10":92.6,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":784398,
            "title":"CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval",
            "url":"\/paper\/clip4clip-an-empirical-study-of-clip-for-end",
            "published":"2021-04-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clip4clip-an-empirical-study-of-clip-for-end\/review\/?hl=30332"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":43909,
        "rank":17,
        "method":"LAFF",
        "mlmodel":{

        },
        "method_short":"LAFF",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-03",
        "metrics":{
            "text-to-video R@1":"45.4",
            "text-to-video R@5":"76.0",
            "text-to-video R@10":"84.6",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "text-to-video R@50":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":45.4,
            "text-to-video R@5":76.0,
            "text-to-video R@10":84.6,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "text-to-video R@50":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":925470,
            "title":"Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval",
            "url":"\/paper\/lightweight-attentional-feature-fusion-for",
            "published":"2021-12-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lightweight-attentional-feature-fusion-for\/review\/?hl=43909"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":27319,
        "rank":18,
        "method":"CLIP",
        "mlmodel":{

        },
        "method_short":"CLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-24",
        "metrics":{
            "text-to-video R@1":"37",
            "text-to-video R@5":"64.1",
            "text-to-video R@10":"73.8",
            "text-to-video Median Rank":"3.0",
            "text-to-video Mean Rank":null,
            "text-to-video R@50":null,
            "video-to-text R@1":"59.9",
            "video-to-text R@5":"85.2",
            "video-to-text R@10":"90.7",
            "video-to-text Median Rank":"1",
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":37.0,
            "text-to-video R@5":64.1,
            "text-to-video R@10":73.8,
            "text-to-video Median Rank":3.0,
            "text-to-video Mean Rank":null,
            "text-to-video R@50":null,
            "video-to-text R@1":59.9,
            "video-to-text R@5":85.2,
            "video-to-text R@10":90.7,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":748268,
            "title":"A Straightforward Framework For Video Retrieval Using CLIP",
            "url":"\/paper\/a-straightforward-framework-for-video",
            "published":"2021-02-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-straightforward-framework-for-video\/review\/?hl=27319"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":29236,
        "rank":19,
        "method":"FROZEN",
        "mlmodel":{

        },
        "method_short":"FROZEN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-01",
        "metrics":{
            "text-to-video R@1":"33.7",
            "text-to-video R@5":"64.7",
            "text-to-video R@10":"76.3",
            "text-to-video Median Rank":"3",
            "text-to-video Mean Rank":null,
            "text-to-video R@50":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":33.7,
            "text-to-video R@5":64.7,
            "text-to-video R@10":76.3,
            "text-to-video Median Rank":3.0,
            "text-to-video Mean Rank":null,
            "text-to-video R@50":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":773452,
            "title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
            "url":"\/paper\/frozen-in-time-a-joint-video-and-image",
            "published":"2021-04-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/frozen-in-time-a-joint-video-and-image\/review\/?hl=29236"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":10262,
        "rank":20,
        "method":"SSML",
        "mlmodel":{

        },
        "method_short":"SSML",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-06",
        "metrics":{
            "text-to-video R@1":"20.3",
            "text-to-video R@5":"49.0",
            "text-to-video R@10":"63.3",
            "text-to-video Median Rank":"6.0",
            "text-to-video Mean Rank":"--",
            "text-to-video R@50":"--",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":20.3,
            "text-to-video R@5":49.0,
            "text-to-video R@10":63.3,
            "text-to-video Median Rank":6.0,
            "text-to-video Mean Rank":0,
            "text-to-video R@50":0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":186098,
            "title":"Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning",
            "url":"\/paper\/noise-estimation-using-density-estimation-for",
            "published":"2020-03-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/noise-estimation-using-density-estimation-for\/review\/?hl=10262"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1137,
        "row_id":12880,
        "rank":21,
        "method":"Collaborative Experts",
        "mlmodel":{

        },
        "method_short":"Collaborative Experts",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-31",
        "metrics":{
            "text-to-video R@1":"19.8",
            "text-to-video R@5":"49.0",
            "text-to-video R@10":"63.8",
            "text-to-video Median Rank":"6.0",
            "text-to-video Mean Rank":"23.1",
            "text-to-video R@50":"89.0",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":19.8,
            "text-to-video R@5":49.0,
            "text-to-video R@10":63.8,
            "text-to-video Median Rank":6.0,
            "text-to-video Mean Rank":23.1,
            "text-to-video R@50":89.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":148748,
            "title":"Use What You Have: Video Retrieval Using Representations From Collaborative Experts",
            "url":"\/paper\/use-what-you-have-video-retrieval-using",
            "published":"2019-07-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/use-what-you-have-video-retrieval-using\/review\/?hl=12880"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]