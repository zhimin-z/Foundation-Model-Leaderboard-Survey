[
    {
        "table_id":2828,
        "row_id":103551,
        "rank":1,
        "method":"VLAB",
        "mlmodel":{

        },
        "method_short":"VLAB",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-22",
        "metrics":{
            "Accuracy":"0.61"
        },
        "raw_metrics":{
            "Accuracy":0.61
        },
        "uses_additional_data":true,
        "paper":{
            "id":1212983,
            "title":"VLAB: Enhancing Video Language Pre-training by Feature Adapting and Blending",
            "url":"\/paper\/vlab-enhancing-video-language-pre-training-by",
            "published":"2023-05-22T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/vlab-enhancing-video-language-pre-training-by\/review\/?hl=103551"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":106252,
        "rank":2,
        "method":"MaMMUT (ours)",
        "mlmodel":{

        },
        "method_short":"MaMMUT ",
        "method_details":"ours",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-29",
        "metrics":{
            "Accuracy":".602"
        },
        "raw_metrics":{
            "Accuracy":0.602
        },
        "uses_additional_data":true,
        "paper":{
            "id":1182696,
            "title":"MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks",
            "url":"\/paper\/mammut-a-simple-architecture-for-joint",
            "published":"2023-03-29T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/mammut-a-simple-architecture-for-joint\/review\/?hl=106252"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":101417,
        "rank":3,
        "method":"VALOR",
        "mlmodel":{

        },
        "method_short":"VALOR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-17",
        "metrics":{
            "Accuracy":"0.60"
        },
        "raw_metrics":{
            "Accuracy":0.6
        },
        "uses_additional_data":true,
        "paper":{
            "id":1191887,
            "title":"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset",
            "url":"\/paper\/valor-vision-audio-language-omni-perception",
            "published":"2023-04-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/valor-vision-audio-language-omni-perception\/review\/?hl=101417"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":104637,
        "rank":4,
        "method":"VAST",
        "mlmodel":{

        },
        "method_short":"VAST",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Accuracy":"0.60"
        },
        "raw_metrics":{
            "Accuracy":0.6
        },
        "uses_additional_data":true,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":96428,
        "rank":5,
        "method":"mPLUG-2",
        "mlmodel":{

        },
        "method_short":"mPLUG-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-01",
        "metrics":{
            "Accuracy":"0.581"
        },
        "raw_metrics":{
            "Accuracy":0.581
        },
        "uses_additional_data":true,
        "paper":{
            "id":1151002,
            "title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",
            "url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation",
            "published":"2023-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation\/review\/?hl=96428"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":87223,
        "rank":6,
        "method":"VideoCoCa",
        "mlmodel":{

        },
        "method_short":"VideoCoCa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Accuracy":"0.569"
        },
        "raw_metrics":{
            "Accuracy":0.569
        },
        "uses_additional_data":true,
        "paper":{
            "id":1126258,
            "title":"VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners",
            "url":"\/paper\/video-text-modeling-with-zero-shot-transfer",
            "published":"2022-12-09T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/video-text-modeling-with-zero-shot-transfer\/review\/?hl=87223"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":89493,
        "rank":7,
        "method":"GIT",
        "mlmodel":{

        },
        "method_short":"GIT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-27",
        "metrics":{
            "Accuracy":"0.568"
        },
        "raw_metrics":{
            "Accuracy":0.568
        },
        "uses_additional_data":true,
        "paper":{
            "id":1017294,
            "title":"GIT: A Generative Image-to-text Transformer for Vision and Language",
            "url":"\/paper\/git-a-generative-image-to-text-transformer",
            "published":"2022-05-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/git-a-generative-image-to-text-transformer\/review\/?hl=89493"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":108831,
        "rank":8,
        "method":"FrozenBiLM+",
        "mlmodel":{

        },
        "method_short":"FrozenBiLM+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Accuracy":"0.558"
        },
        "raw_metrics":{
            "Accuracy":0.558
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265311,
            "title":"Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models",
            "url":"\/paper\/open-vocabulary-video-question-answering-a",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/open-vocabulary-video-question-answering-a\/review\/?hl=108831"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":88492,
        "rank":9,
        "method":"HiTeA",
        "mlmodel":{

        },
        "method_short":"HiTeA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-30",
        "metrics":{
            "Accuracy":"0.556"
        },
        "raw_metrics":{
            "Accuracy":0.556
        },
        "uses_additional_data":true,
        "paper":{
            "id":1136078,
            "title":"HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training",
            "url":"\/paper\/hitea-hierarchical-temporal-aware-video",
            "published":"2022-12-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/hitea-hierarchical-temporal-aware-video\/review\/?hl=88492"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":86728,
        "rank":10,
        "method":"InternVideo",
        "mlmodel":{

        },
        "method_short":"InternVideo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Accuracy":"0.555"
        },
        "raw_metrics":{
            "Accuracy":0.555
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124231,
            "title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
            "url":"\/paper\/internvideo-general-video-foundation-models",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internvideo-general-video-foundation-models\/review\/?hl=86728"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":100395,
        "rank":11,
        "method":"UMT-L (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"UMT-L ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "Accuracy":"0.552"
        },
        "raw_metrics":{
            "Accuracy":0.552
        },
        "uses_additional_data":true,
        "paper":{
            "id":1181934,
            "title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
            "url":"\/paper\/unmasked-teacher-towards-training-efficient",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":70082,
        "rank":12,
        "method":"FrozenBiLM",
        "mlmodel":{

        },
        "method_short":"FrozenBiLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-16",
        "metrics":{
            "Accuracy":"0.548"
        },
        "raw_metrics":{
            "Accuracy":0.548
        },
        "uses_additional_data":true,
        "paper":{
            "id":1028387,
            "title":"Zero-Shot Video Question Answering via Frozen Bidirectional Language Models",
            "url":"\/paper\/zero-shot-video-question-answering-via-frozen",
            "published":"2022-06-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/zero-shot-video-question-answering-via-frozen\/review\/?hl=70082"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":96094,
        "rank":13,
        "method":"VIOLETv2",
        "mlmodel":{

        },
        "method_short":"VIOLETv2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-04",
        "metrics":{
            "Accuracy":"0.547"
        },
        "raw_metrics":{
            "Accuracy":0.547
        },
        "uses_additional_data":true,
        "paper":{
            "id":1069338,
            "title":"An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling",
            "url":"\/paper\/an-empirical-study-of-end-to-end-video",
            "published":"2022-09-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-empirical-study-of-end-to-end-video\/review\/?hl=96094"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":99057,
        "rank":14,
        "method":"MuLTI",
        "mlmodel":{

        },
        "method_short":"MuLTI",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-10",
        "metrics":{
            "Accuracy":"0.547"
        },
        "raw_metrics":{
            "Accuracy":0.547
        },
        "uses_additional_data":true,
        "paper":{
            "id":1171789,
            "title":"MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler and Multiple Choice Modeling",
            "url":"\/paper\/multi-efficient-video-and-language",
            "published":"2023-03-10T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/multi-efficient-video-and-language\/review\/?hl=99057"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":80496,
        "rank":15,
        "method":"X2-VLM (large)",
        "mlmodel":{

        },
        "method_short":"X2-VLM ",
        "method_details":"large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-22",
        "metrics":{
            "Accuracy":"0.546"
        },
        "raw_metrics":{
            "Accuracy":0.546
        },
        "uses_additional_data":true,
        "paper":{
            "id":1116038,
            "title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
            "url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for",
            "published":"2022-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for\/review\/?hl=80496"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":80495,
        "rank":16,
        "method":"X2-VLM (base)",
        "mlmodel":{

        },
        "method_short":"X2-VLM ",
        "method_details":"base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-22",
        "metrics":{
            "Accuracy":"0.528"
        },
        "raw_metrics":{
            "Accuracy":0.528
        },
        "uses_additional_data":true,
        "paper":{
            "id":1116038,
            "title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
            "url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for",
            "published":"2022-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for\/review\/?hl=80495"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":88025,
        "rank":17,
        "method":"Clover",
        "mlmodel":{

        },
        "method_short":"Clover",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-16",
        "metrics":{
            "Accuracy":"0.524"
        },
        "raw_metrics":{
            "Accuracy":0.524
        },
        "uses_additional_data":true,
        "paper":{
            "id":1045027,
            "title":"Clover: Towards A Unified Video-Language Alignment and Fusion Model",
            "url":"\/paper\/clover-towards-a-unified-video-language",
            "published":"2022-07-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clover-towards-a-unified-video-language\/review\/?hl=88025"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":100002,
        "rank":18,
        "method":"VIOLET + MELTR",
        "mlmodel":{

        },
        "method_short":"VIOLET + MELTR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "Accuracy":"0.517"
        },
        "raw_metrics":{
            "Accuracy":0.517
        },
        "uses_additional_data":true,
        "paper":{
            "id":1179150,
            "title":"MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models",
            "url":"\/paper\/meltr-meta-loss-transformer-for-learning-to",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meltr-meta-loss-transformer-for-learning-to\/review\/?hl=100002"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":69067,
        "rank":19,
        "method":"OmniVL",
        "mlmodel":{

        },
        "method_short":"OmniVL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-15",
        "metrics":{
            "Accuracy":"0.510"
        },
        "raw_metrics":{
            "Accuracy":0.51
        },
        "uses_additional_data":true,
        "paper":{
            "id":1075042,
            "title":"OmniVL:One Foundation Model for Image-Language and Video-Language Tasks",
            "url":"\/paper\/omnivl-one-foundation-model-for-image",
            "published":"2022-09-15T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivl-one-foundation-model-for-image\/review\/?hl=69067"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":108833,
        "rank":20,
        "method":"VIOLET+",
        "mlmodel":{

        },
        "method_short":"VIOLET+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Accuracy":"0.495"
        },
        "raw_metrics":{
            "Accuracy":0.495
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265311,
            "title":"Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models",
            "url":"\/paper\/open-vocabulary-video-question-answering-a",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/open-vocabulary-video-question-answering-a\/review\/?hl=108833"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":75346,
        "rank":21,
        "method":"Co-Tokenization",
        "mlmodel":{

        },
        "method_short":"Co-Tokenization",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-01",
        "metrics":{
            "Accuracy":".486"
        },
        "raw_metrics":{
            "Accuracy":0.486
        },
        "uses_additional_data":true,
        "paper":{
            "id":1052402,
            "title":"Video Question Answering with Iterative Video-Text Co-Tokenization",
            "url":"\/paper\/video-question-answering-with-iterative-video",
            "published":"2022-08-01T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/video-question-answering-with-iterative-video\/review\/?hl=75346"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":52528,
        "rank":22,
        "method":"All-in-one-B",
        "mlmodel":{

        },
        "method_short":"All-in-one-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-14",
        "metrics":{
            "Accuracy":"0.483"
        },
        "raw_metrics":{
            "Accuracy":0.483
        },
        "uses_additional_data":true,
        "paper":{
            "id":976242,
            "title":"All in One: Exploring Unified Video-Language Pre-training",
            "url":"\/paper\/all-in-one-exploring-unified-video-language",
            "published":"2022-03-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/all-in-one-exploring-unified-video-language\/review\/?hl=52528"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":106338,
        "rank":23,
        "method":"LRCE",
        "mlmodel":{

        },
        "method_short":"LRCE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-30",
        "metrics":{
            "Accuracy":"0.478"
        },
        "raw_metrics":{
            "Accuracy":0.478
        },
        "uses_additional_data":false,
        "paper":{
            "id":1250520,
            "title":"Lightweight Recurrent Cross-modal Encoder for Video Question Answering",
            "url":"\/paper\/lightweight-recurrent-cross-modal-encoder-for",
            "published":"2023-06-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":108832,
        "rank":24,
        "method":"JustAsk+",
        "mlmodel":{

        },
        "method_short":"JustAsk+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Accuracy":"0.477"
        },
        "raw_metrics":{
            "Accuracy":0.477
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265311,
            "title":"Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models",
            "url":"\/paper\/open-vocabulary-video-question-answering-a",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/open-vocabulary-video-question-answering-a\/review\/?hl=108832"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":37326,
        "rank":25,
        "method":"Just Ask",
        "mlmodel":{

        },
        "method_short":"Just Ask",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Accuracy":"0.463"
        },
        "raw_metrics":{
            "Accuracy":0.463
        },
        "uses_additional_data":true,
        "paper":{
            "id":238417,
            "title":"Just Ask: Learning to Answer Questions from Millions of Narrated Videos",
            "url":"\/paper\/just-ask-learning-to-answer-questions-from",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/just-ask-learning-to-answer-questions-from\/review\/?hl=37326"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":54475,
        "rank":26,
        "method":"ALPRO",
        "mlmodel":{

        },
        "method_short":"ALPRO",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-17",
        "metrics":{
            "Accuracy":"0.459"
        },
        "raw_metrics":{
            "Accuracy":0.459
        },
        "uses_additional_data":true,
        "paper":{
            "id":932382,
            "title":"Align and Prompt: Video-and-Language Pre-training with Entity Prompts",
            "url":"\/paper\/align-and-prompt-video-and-language-pre",
            "published":"2021-12-17T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":108827,
        "rank":27,
        "method":"All-in-one+",
        "mlmodel":{

        },
        "method_short":"All-in-one+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Accuracy":"0.438"
        },
        "raw_metrics":{
            "Accuracy":0.438
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265311,
            "title":"Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models",
            "url":"\/paper\/open-vocabulary-video-question-answering-a",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/open-vocabulary-video-question-answering-a\/review\/?hl=108827"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":36597,
        "rank":28,
        "method":"DualVGR",
        "mlmodel":{

        },
        "method_short":"DualVGR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-10",
        "metrics":{
            "Accuracy":"0.390"
        },
        "raw_metrics":{
            "Accuracy":0.39
        },
        "uses_additional_data":false,
        "paper":{
            "id":833787,
            "title":"DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering",
            "url":"\/paper\/dualvgr-a-dual-visual-graph-reasoning-unit",
            "published":"2021-07-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dualvgr-a-dual-visual-graph-reasoning-unit\/review\/?hl=36597"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":15548,
        "rank":29,
        "method":"HCRN",
        "mlmodel":{

        },
        "method_short":"HCRN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-02-25",
        "metrics":{
            "Accuracy":"0.361"
        },
        "raw_metrics":{
            "Accuracy":0.361
        },
        "uses_additional_data":false,
        "paper":{
            "id":184719,
            "title":"Hierarchical Conditional Relation Networks for Video Question Answering",
            "url":"\/paper\/hierarchical-conditional-relation-networks",
            "published":"2020-02-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hierarchical-conditional-relation-networks\/review\/?hl=15548"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":15547,
        "rank":30,
        "method":"SSML",
        "mlmodel":{

        },
        "method_short":"SSML",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-06",
        "metrics":{
            "Accuracy":"0.351"
        },
        "raw_metrics":{
            "Accuracy":0.351
        },
        "uses_additional_data":false,
        "paper":{
            "id":186098,
            "title":"Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning",
            "url":"\/paper\/noise-estimation-using-density-estimation-for",
            "published":"2020-03-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/noise-estimation-using-density-estimation-for\/review\/?hl=15547"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":15545,
        "rank":31,
        "method":"HMEMA",
        "mlmodel":{

        },
        "method_short":"HMEMA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-08",
        "metrics":{
            "Accuracy":"0.337"
        },
        "raw_metrics":{
            "Accuracy":0.337
        },
        "uses_additional_data":false,
        "paper":{
            "id":111163,
            "title":"Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering",
            "url":"\/paper\/heterogeneous-memory-enhanced-multimodal",
            "published":"2019-04-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/heterogeneous-memory-enhanced-multimodal\/review\/?hl=15545"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":15544,
        "rank":32,
        "method":"Co-Mem",
        "mlmodel":{

        },
        "method_short":"Co-Mem",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-03-29",
        "metrics":{
            "Accuracy":"0.317"
        },
        "raw_metrics":{
            "Accuracy":0.317
        },
        "uses_additional_data":false,
        "paper":{
            "id":7192,
            "title":"Motion-Appearance Co-Memory Networks for Video Question Answering",
            "url":"\/paper\/motion-appearance-co-memory-networks-for",
            "published":"2018-03-29T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/motion-appearance-co-memory-networks-for\/review\/?hl=15544"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2828,
        "row_id":15546,
        "rank":33,
        "method":"ST-VQA",
        "mlmodel":{

        },
        "method_short":"ST-VQA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-04-14",
        "metrics":{
            "Accuracy":"0.313"
        },
        "raw_metrics":{
            "Accuracy":0.313
        },
        "uses_additional_data":false,
        "paper":{
            "id":13620,
            "title":"TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering",
            "url":"\/paper\/tgif-qa-toward-spatio-temporal-reasoning-in",
            "published":"2017-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tgif-qa-toward-spatio-temporal-reasoning-in\/review\/?hl=15546"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]