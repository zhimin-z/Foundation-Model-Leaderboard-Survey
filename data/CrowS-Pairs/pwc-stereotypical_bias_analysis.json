[
    {
        "table_id":17281,
        "row_id":85026,
        "rank":1,
        "method":"GAL 120B",
        "mlmodel":{

        },
        "method_short":"GAL 120B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-16",
        "metrics":{
            "Gender":"51.9",
            "Religion":"51.9",
            "Race\/Color":"59.9",
            "Sexual Orientation":"77.4",
            "Age":"69",
            "Nationality":"51.6",
            "Disability":"66.7",
            "Physical Appearance":"58.7",
            "Socioeconomic status":"65.7",
            "Overall":"60.5"
        },
        "raw_metrics":{
            "Gender":51.9,
            "Religion":51.9,
            "Race\/Color":59.9,
            "Sexual Orientation":77.4,
            "Age":69.0,
            "Nationality":51.6,
            "Disability":66.7,
            "Physical Appearance":58.7,
            "Socioeconomic status":65.7,
            "Overall":60.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1112728,
            "title":"Galactica: A Large Language Model for Science",
            "url":"\/paper\/galactica-a-large-language-model-for-science-1",
            "published":"2022-11-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/galactica-a-large-language-model-for-science-1\/review\/?hl=85026"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":17281,
        "row_id":53920,
        "rank":2,
        "method":"GPT-3",
        "mlmodel":{

        },
        "method_short":"GPT-3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-02",
        "metrics":{
            "Gender":"62.6",
            "Religion":"62.6",
            "Race\/Color":"64.7",
            "Sexual Orientation":"76.2",
            "Age":"64.4",
            "Nationality":"61.6",
            "Disability":"76.7",
            "Physical Appearance":"74.6",
            "Socioeconomic status":"73.8",
            "Overall":"67.2"
        },
        "raw_metrics":{
            "Gender":62.6,
            "Religion":62.6,
            "Race\/Color":64.7,
            "Sexual Orientation":76.2,
            "Age":64.4,
            "Nationality":61.6,
            "Disability":76.7,
            "Physical Appearance":74.6,
            "Socioeconomic status":73.8,
            "Overall":67.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1002572,
            "title":"OPT: Open Pre-trained Transformer Language Models",
            "url":"\/paper\/opt-open-pre-trained-transformer-language",
            "published":"2022-05-02T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":17281,
        "row_id":53919,
        "rank":3,
        "method":"OPT-175B",
        "mlmodel":{

        },
        "method_short":"OPT-175B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-02",
        "metrics":{
            "Gender":"65.7",
            "Religion":"65.7",
            "Race\/Color":"68.6",
            "Sexual Orientation":"78.6",
            "Age":"67.8",
            "Nationality":"62.9",
            "Disability":"76.7",
            "Physical Appearance":"76.2",
            "Socioeconomic status":"76.2",
            "Overall":"69.5"
        },
        "raw_metrics":{
            "Gender":65.7,
            "Religion":65.7,
            "Race\/Color":68.6,
            "Sexual Orientation":78.6,
            "Age":67.8,
            "Nationality":62.9,
            "Disability":76.7,
            "Physical Appearance":76.2,
            "Socioeconomic status":76.2,
            "Overall":69.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1002572,
            "title":"OPT: Open Pre-trained Transformer Language Models",
            "url":"\/paper\/opt-open-pre-trained-transformer-language",
            "published":"2022-05-02T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":17281,
        "row_id":97686,
        "rank":4,
        "method":"LLaMA 65B",
        "mlmodel":{

        },
        "method_short":"LLaMA 65B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Gender":"70.6",
            "Religion":"70.6",
            "Race\/Color":"57.0",
            "Sexual Orientation":"81.0",
            "Age":"70.1",
            "Nationality":"64.2",
            "Disability":"66.7",
            "Physical Appearance":"77.8",
            "Socioeconomic status":"71.5",
            "Overall":"66.6"
        },
        "raw_metrics":{
            "Gender":70.6,
            "Religion":70.6,
            "Race\/Color":57.0,
            "Sexual Orientation":81.0,
            "Age":70.1,
            "Nationality":64.2,
            "Disability":66.7,
            "Physical Appearance":77.8,
            "Socioeconomic status":71.5,
            "Overall":66.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97686"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]