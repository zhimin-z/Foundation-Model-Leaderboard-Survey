[
    {
        "table_id":188,
        "row_id":102986,
        "rank":1,
        "Model":"ONE-PEACE",
        "mlmodel":{

        },
        "method_short":"ONE-PEACE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-18",
        "metrics":{
            "Validation mIoU":"63.0",
            "Test Score":null,
            "Params (M)":"1500",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":63.0,
            "Test Score":null,
            "Params (M)":1500.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1211430,
            "title":"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
            "url":"\/paper\/one-peace-exploring-one-general",
            "published":"2023-05-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-peace-exploring-one-general\/review\/?hl=102986"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":102386,
        "rank":2,
        "Model":"InternImage-H",
        "mlmodel":{

        },
        "method_short":"InternImage-H",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-10",
        "metrics":{
            "Validation mIoU":"62.9",
            "Test Score":null,
            "Params (M)":"1310",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":"4635"
        },
        "raw_metrics":{
            "Validation mIoU":62.9,
            "Test Score":null,
            "Params (M)":1310.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":4635.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1109218,
            "title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
            "url":"\/paper\/internimage-exploring-large-scale-vision",
            "published":"2022-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internimage-exploring-large-scale-vision\/review\/?hl=102386"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":80337,
        "rank":3,
        "Model":"M3I Pre-training (InternImage-H)",
        "mlmodel":{

        },
        "method_short":"M3I Pre-training ",
        "method_details":"InternImage-H",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-17",
        "metrics":{
            "Validation mIoU":"62.9",
            "Test Score":null,
            "Params (M)":"1310",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":62.9,
            "Test Score":null,
            "Params (M)":1310.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1113300,
            "title":"Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information",
            "url":"\/paper\/towards-all-in-one-pre-training-via",
            "published":"2022-11-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/towards-all-in-one-pre-training-via\/review\/?hl=80337"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":64388,
        "rank":4,
        "Model":"BEiT-3",
        "mlmodel":{

        },
        "method_short":"BEiT-3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-22",
        "metrics":{
            "Validation mIoU":"62.8",
            "Test Score":null,
            "Params (M)":"1900",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":62.8,
            "Test Score":null,
            "Params (M)":1900.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1062207,
            "title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
            "url":"\/paper\/image-as-a-foreign-language-beit-pretraining",
            "published":"2022-08-22T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":393,
                "name":"ViT-Giant",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":78928,
        "rank":5,
        "Model":"EVA",
        "mlmodel":{

        },
        "method_short":"EVA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-14",
        "metrics":{
            "Validation mIoU":"62.3",
            "Test Score":null,
            "Params (M)":"1074",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":62.3,
            "Test Score":null,
            "Params (M)":1074.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1110592,
            "title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
            "url":"\/paper\/eva-exploring-the-limits-of-masked-visual",
            "published":"2022-11-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/eva-exploring-the-limits-of-masked-visual\/review\/?hl=78928"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":393,
                "name":"ViT-Giant",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":95763,
        "rank":6,
        "Model":"ViT-Adapter-L (Mask2Former, BEiTv2 pretrain)",
        "mlmodel":{

        },
        "method_short":"ViT-Adapter-L ",
        "method_details":"Mask2Former, BEiTv2 pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-17",
        "metrics":{
            "Validation mIoU":"61.5",
            "Test Score":null,
            "Params (M)":"571",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":61.5,
            "Test Score":null,
            "Params (M)":571.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1011398,
            "title":"Vision Transformer Adapter for Dense Predictions",
            "url":"\/paper\/vision-transformer-adapter-for-dense",
            "published":"2022-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-transformer-adapter-for-dense\/review\/?hl=95763"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":58898,
        "rank":7,
        "Model":"FD-SwinV2-G",
        "mlmodel":{

        },
        "method_short":"FD-SwinV2-G",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-27",
        "metrics":{
            "Validation mIoU":"61.4",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":61.4,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1017290,
            "title":"Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation",
            "url":"\/paper\/contrastive-learning-rivals-masked-image",
            "published":"2022-05-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/contrastive-learning-rivals-masked-image\/review\/?hl=58898"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":89211,
        "rank":8,
        "Model":"RevCol-H (Mask2Former)",
        "mlmodel":{

        },
        "method_short":"RevCol-H ",
        "method_details":"Mask2Former",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-22",
        "metrics":{
            "Validation mIoU":"61.0",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":61.0,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1132613,
            "title":"Reversible Column Networks",
            "url":"\/paper\/reversible-column-networks",
            "published":"2022-12-22T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":64477,
        "rank":9,
        "Model":"MasK DINO (SwinL, multi-scale)",
        "mlmodel":{

        },
        "method_short":"MasK DINO ",
        "method_details":"SwinL, multi-scale",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-06",
        "metrics":{
            "Validation mIoU":"60.8",
            "Test Score":null,
            "Params (M)":"223",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":60.8,
            "Test Score":null,
            "Params (M)":223.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1022258,
            "title":"Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation",
            "url":"\/paper\/mask-dino-towards-a-unified-transformer-based-1",
            "published":"2022-06-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mask-dino-towards-a-unified-transformer-based-1\/review\/?hl=64477"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":54700,
        "rank":10,
        "Model":"ViT-Adapter-L (Mask2Former, BEiT pretrain)",
        "mlmodel":{

        },
        "method_short":"ViT-Adapter-L ",
        "method_details":"Mask2Former, BEiT pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-17",
        "metrics":{
            "Validation mIoU":"60.5",
            "Test Score":null,
            "Params (M)":"571",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":60.5,
            "Test Score":null,
            "Params (M)":571.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1011398,
            "title":"Vision Transformer Adapter for Dense Predictions",
            "url":"\/paper\/vision-transformer-adapter-for-dense",
            "published":"2022-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-transformer-adapter-for-dense\/review\/?hl=54700"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":101355,
        "rank":11,
        "Model":"DINOv2 (ViT-g\/14 frozen model, w\/ ViT-Adapter + Mask2former)",
        "mlmodel":{

        },
        "method_short":"DINOv2 ",
        "method_details":"ViT-g\/14 frozen model, w\/ ViT-Adapter + Mask2former",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-14",
        "metrics":{
            "Validation mIoU":"60.2",
            "Test Score":null,
            "Params (M)":"1100",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":60.2,
            "Test Score":null,
            "Params (M)":1100.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1191620,
            "title":"DINOv2: Learning Robust Visual Features without Supervision",
            "url":"\/paper\/dinov2-learning-robust-visual-features",
            "published":"2023-04-14T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":43965,
        "rank":12,
        "Model":"SwinV2-G(UperNet)",
        "mlmodel":{

        },
        "method_short":"SwinV2-G",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-18",
        "metrics":{
            "Validation mIoU":"59.9",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":59.9,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912369,
            "title":"Swin Transformer V2: Scaling Up Capacity and Resolution",
            "url":"\/paper\/swin-transformer-v2-scaling-up-capacity-and",
            "published":"2021-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/swin-transformer-v2-scaling-up-capacity-and\/review\/?hl=43965"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":77316,
        "rank":13,
        "Model":"FocalNet-L (Mask2Former)",
        "mlmodel":{

        },
        "method_short":"FocalNet-L ",
        "method_details":"Mask2Former",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-22",
        "metrics":{
            "Validation mIoU":"58.5",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":58.5,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":980356,
            "title":"Focal Modulation Networks",
            "url":"\/paper\/focal-modulation-networks",
            "published":"2022-03-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/focal-modulation-networks\/review\/?hl=77316"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":56096,
        "rank":14,
        "Model":"ViT-Adapter-L (UperNet, BEiT pretrain)",
        "mlmodel":{

        },
        "method_short":"ViT-Adapter-L ",
        "method_details":"UperNet, BEiT pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-17",
        "metrics":{
            "Validation mIoU":"58.4",
            "Test Score":null,
            "Params (M)":"451",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":58.4,
            "Test Score":null,
            "Params (M)":451.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1011398,
            "title":"Vision Transformer Adapter for Dense Predictions",
            "url":"\/paper\/vision-transformer-adapter-for-dense",
            "published":"2022-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-transformer-adapter-for-dense\/review\/?hl=56096"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":88440,
        "rank":15,
        "Model":"RSSeg-ViT-L (BEiT pretrain)",
        "mlmodel":{

        },
        "method_short":"RSSeg-ViT-L ",
        "method_details":"BEiT pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-28",
        "metrics":{
            "Validation mIoU":"58.4",
            "Test Score":null,
            "Params (M)":"330",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":58.4,
            "Test Score":null,
            "Params (M)":330.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1135137,
            "title":"Representation Separation for Semantic Segmentation with Vision Transformers",
            "url":"\/paper\/representation-separation-for-semantic",
            "published":"2022-12-28T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/representation-separation-for-semantic\/review\/?hl=88440"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":105792,
        "rank":16,
        "Model":"SegViT-v2 (BEiT-v2-Large)",
        "mlmodel":{

        },
        "method_short":"SegViT-v2 ",
        "method_details":"BEiT-v2-Large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-09",
        "metrics":{
            "Validation mIoU":"58.2",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":"637.9",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":58.2,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":637.9,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1227533,
            "title":"SegViTv2: Exploring Efficient and Continual Semantic Segmentation with Plain Vision Transformers",
            "url":"\/paper\/segvitv2-exploring-efficient-and-continual",
            "published":"2023-06-09T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":44861,
        "rank":17,
        "Model":"SeMask (SeMask Swin-L FaPN-Mask2Former)",
        "mlmodel":{

        },
        "method_short":"SeMask ",
        "method_details":"SeMask Swin-L FaPN-Mask2Former",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "Validation mIoU":"58.2",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":58.2,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932883,
            "title":"SeMask: Semantically Masked Transformers for Semantic Segmentation",
            "url":"\/paper\/semask-semantically-masked-transformers-for-1",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/semask-semantically-masked-transformers-for-1\/review\/?hl=44861"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48170,
        "rank":18,
        "Model":"SeMask (SeMask Swin-L MSFaPN-Mask2Former)",
        "mlmodel":{

        },
        "method_short":"SeMask ",
        "method_details":"SeMask Swin-L MSFaPN-Mask2Former",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "Validation mIoU":"58.2",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":58.2,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932883,
            "title":"SeMask: Semantically Masked Transformers for Semantic Segmentation",
            "url":"\/paper\/semask-semantically-masked-transformers-for-1",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/semask-semantically-masked-transformers-for-1\/review\/?hl=48170"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":78423,
        "rank":19,
        "Model":"DiNAT-L (Mask2Former)",
        "mlmodel":{

        },
        "method_short":"DiNAT-L ",
        "method_details":"Mask2Former",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-29",
        "metrics":{
            "Validation mIoU":"58.1",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":58.1,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1082646,
            "title":"Dilated Neighborhood Attention Transformer",
            "url":"\/paper\/dilated-neighborhood-attention-transformer",
            "published":"2022-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dilated-neighborhood-attention-transformer\/review\/?hl=78423"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":265,
                "name":"Neighborhood Attention",
                "color":"#ba1330"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":60812,
        "rank":20,
        "Model":"HorNet-L (Mask2Former)",
        "mlmodel":{

        },
        "method_short":"HorNet-L ",
        "method_details":"Mask2Former",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-28",
        "metrics":{
            "Validation mIoU":"57.9",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":57.9,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1051716,
            "title":"HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions",
            "url":"\/paper\/hornet-efficient-high-order-spatial",
            "published":"2022-07-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hornet-efficient-high-order-spatial\/review\/?hl=60812"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48075,
        "rank":21,
        "Model":"Mask2Former (SwinL-FaPN)",
        "mlmodel":{

        },
        "method_short":"Mask2Former ",
        "method_details":"SwinL-FaPN",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Validation mIoU":"57.7",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":57.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924610,
            "title":"Masked-attention Mask Transformer for Universal Image Segmentation",
            "url":"\/paper\/masked-attention-mask-transformer-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-attention-mask-transformer-for\/review\/?hl=48075"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":51338,
        "rank":22,
        "Model":"FASeg (SwinL)",
        "mlmodel":{

        },
        "method_short":"FASeg ",
        "method_details":"SwinL",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Validation mIoU":"57.7",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":57.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988330,
            "title":"Dynamic Focus-aware Positional Queries for Semantic Segmentation",
            "url":"\/paper\/dynamic-focus-aware-positional-queries-for",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dynamic-focus-aware-positional-queries-for\/review\/?hl=51338"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":52833,
        "rank":23,
        "Model":"RR (BEiT-L)",
        "mlmodel":{

        },
        "method_short":"RR ",
        "method_details":"BEiT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-05",
        "metrics":{
            "Validation mIoU":"57.7",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":57.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":989471,
            "title":"Region Rebalance for Long-Tailed Semantic Segmentation",
            "url":"\/paper\/region-rebalance-for-long-tailed-semantic",
            "published":"2022-04-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/region-rebalance-for-long-tailed-semantic\/review\/?hl=52833"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":101762,
        "rank":24,
        "Model":"MOAT-4 (IN-22K pretraining, single-scale)",
        "mlmodel":{

        },
        "method_short":"MOAT-4 ",
        "method_details":"IN-22K pretraining, single-scale",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-04",
        "metrics":{
            "Validation mIoU":"57.6",
            "Test Score":null,
            "Params (M)":"496",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":57.6,
            "Test Score":null,
            "Params (M)":496.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087041,
            "title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models",
            "url":"\/paper\/moat-alternating-mobile-convolution-and",
            "published":"2022-10-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moat-alternating-mobile-convolution-and\/review\/?hl=101762"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":86895,
        "rank":25,
        "Model":"Frozen Backbone, SwinV2-G-ext22K (Mask2Former)",
        "mlmodel":{

        },
        "method_short":"Frozen Backbone, SwinV2-G-ext22K ",
        "method_details":"Mask2Former",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-03",
        "metrics":{
            "Validation mIoU":"57.6",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":57.6,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1104877,
            "title":"Could Giant Pretrained Image Models Extract Universal Representations?",
            "url":"\/paper\/could-giant-pretrained-image-models-extract",
            "published":"2022-11-03T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/could-giant-pretrained-image-models-extract\/review\/?hl=86895"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":44862,
        "rank":26,
        "Model":"SeMask (SeMask Swin-L Mask2Former)",
        "mlmodel":{

        },
        "method_short":"SeMask ",
        "method_details":"SeMask Swin-L Mask2Former",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "Validation mIoU":"57.5",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":57.5,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932883,
            "title":"SeMask: Semantically Masked Transformers for Semantic Segmentation",
            "url":"\/paper\/semask-semantically-masked-transformers-for-1",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/semask-semantically-masked-transformers-for-1\/review\/?hl=44862"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48076,
        "rank":27,
        "Model":"Mask2Former (SwinL)",
        "mlmodel":{

        },
        "method_short":"Mask2Former ",
        "method_details":"SwinL",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Validation mIoU":"57.3",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":57.3,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924610,
            "title":"Masked-attention Mask Transformer for Universal Image Segmentation",
            "url":"\/paper\/masked-attention-mask-transformer-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-attention-mask-transformer-for\/review\/?hl=48076"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57382,
        "rank":28,
        "Model":"SenFormer (BEiT-L)",
        "mlmodel":{

        },
        "method_short":"SenFormer ",
        "method_details":"BEiT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-26",
        "metrics":{
            "Validation mIoU":"57.1",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":57.1,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":920857,
            "title":"Efficient Self-Ensemble for Semantic Segmentation",
            "url":"\/paper\/efficient-self-ensemble-framework-for-1",
            "published":"2021-11-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficient-self-ensemble-framework-for-1\/review\/?hl=57382"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":36222,
        "rank":29,
        "Model":"BEiT-L (ViT+UperNet)",
        "mlmodel":{

        },
        "method_short":"BEiT-L ",
        "method_details":"ViT+UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-15",
        "metrics":{
            "Validation mIoU":"57.0",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":57.0,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":818838,
            "title":"BEiT: BERT Pre-Training of Image Transformers",
            "url":"\/paper\/beit-bert-pre-training-of-image-transformers",
            "published":"2021-06-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57383,
        "rank":30,
        "Model":"SeMask(SeMask Swin-L MSFaPN-Mask2Former, single-scale)",
        "mlmodel":{

        },
        "method_short":"SeMask",
        "method_details":"SeMask Swin-L MSFaPN-Mask2Former, single-scale",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "Validation mIoU":"57.0",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":57.0,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932883,
            "title":"SeMask: Semantically Masked Transformers for Semantic Segmentation",
            "url":"\/paper\/semask-semantically-masked-transformers-for-1",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/semask-semantically-masked-transformers-for-1\/review\/?hl=57383"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":38255,
        "rank":31,
        "Model":"FaPN (MaskFormer, Swin-L, ImageNet-22k pretrain)",
        "mlmodel":{

        },
        "method_short":"FaPN ",
        "method_details":"MaskFormer, Swin-L, ImageNet-22k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-08-17",
        "metrics":{
            "Validation mIoU":"56.7",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":56.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":851714,
            "title":"FaPN: Feature-aligned Pyramid Network for Dense Image Prediction",
            "url":"\/paper\/fapn-feature-aligned-pyramid-network-for",
            "published":"2021-08-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fapn-feature-aligned-pyramid-network-for\/review\/?hl=38255"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":101763,
        "rank":32,
        "Model":"MOAT-3 (IN-22K pretraining, single-scale)",
        "mlmodel":{

        },
        "method_short":"MOAT-3 ",
        "method_details":"IN-22K pretraining, single-scale",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-04",
        "metrics":{
            "Validation mIoU":"56.5",
            "Test Score":null,
            "Params (M)":"198",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":56.5,
            "Test Score":null,
            "Params (M)":198.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087041,
            "title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models",
            "url":"\/paper\/moat-alternating-mobile-convolution-and",
            "published":"2022-10-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moat-alternating-mobile-convolution-and\/review\/?hl=101763"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57384,
        "rank":33,
        "Model":"Mask2Former (Swin-L-FaPN)",
        "mlmodel":{

        },
        "method_short":"Mask2Former ",
        "method_details":"Swin-L-FaPN",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Validation mIoU":"56.4",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":56.4,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924610,
            "title":"Masked-attention Mask Transformer for Universal Image Segmentation",
            "url":"\/paper\/masked-attention-mask-transformer-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-attention-mask-transformer-for\/review\/?hl=57384"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":44863,
        "rank":34,
        "Model":"SeMask (SeMask Swin-L MaskFormer)",
        "mlmodel":{

        },
        "method_short":"SeMask ",
        "method_details":"SeMask Swin-L MaskFormer",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "Validation mIoU":"56.2",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":56.2,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932883,
            "title":"SeMask: Semantically Masked Transformers for Semantic Segmentation",
            "url":"\/paper\/semask-semantically-masked-transformers-for-1",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/semask-semantically-masked-transformers-for-1\/review\/?hl=44863"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":76349,
        "rank":35,
        "Model":"dBOT ViT-L (CLIP)",
        "mlmodel":{

        },
        "method_short":"dBOT ViT-L ",
        "method_details":"CLIP",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-08",
        "metrics":{
            "Validation mIoU":"56.2",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":56.2,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1071401,
            "title":"Exploring Target Representations for Masked Autoencoders",
            "url":"\/paper\/exploring-target-representations-for-masked",
            "published":"2022-09-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-target-representations-for-masked\/review\/?hl=76349"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":112328,
        "rank":36,
        "Model":"Mask2Former+CBL(Swin-B)",
        "mlmodel":{

        },
        "method_short":"Mask2Former+CBL",
        "method_details":"Swin-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-05",
        "metrics":{
            "Validation mIoU":"56.1",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":56.1,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1322542,
            "title":"Conditional Boundary Loss for Semantic Segmentation",
            "url":"\/paper\/conditional-boundary-loss-for-semantic",
            "published":"2023-07-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":109787,
        "rank":37,
        "Model":"TADP",
        "mlmodel":{

        },
        "method_short":"TADP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-29",
        "metrics":{
            "Validation mIoU":"55.9",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":55.9,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1292165,
            "title":"Text-image Alignment for Diffusion-based Perception",
            "url":"\/paper\/text-image-alignment-for-diffusion-based",
            "published":"2023-09-29T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/text-image-alignment-for-diffusion-based\/review\/?hl=109787"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":36895,
        "rank":38,
        "Model":"CSWin-L (UperNet, ImageNet-22k pretrain)",
        "mlmodel":{

        },
        "method_short":"CSWin-L ",
        "method_details":"UperNet, ImageNet-22k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-01",
        "metrics":{
            "Validation mIoU":"55.70",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":55.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":828741,
            "title":"CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows",
            "url":"\/paper\/cswin-transformer-a-general-vision",
            "published":"2021-07-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cswin-transformer-a-general-vision\/review\/?hl=36895"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":112845,
        "rank":39,
        "Model":"UniRepLKNet-XL",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-XL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Validation mIoU":"55.6",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":55.6,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112845"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":36239,
        "rank":40,
        "Model":"Focal-L (UperNet, ImageNet-22k pretrain)",
        "mlmodel":{

        },
        "method_short":"Focal-L ",
        "method_details":"UperNet, ImageNet-22k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-01",
        "metrics":{
            "Validation mIoU":"55.40",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":55.4,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":828702,
            "title":"Focal Self-attention for Local-Global Interactions in Vision Transformers",
            "url":"\/paper\/focal-self-attention-for-local-global",
            "published":"2021-07-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/focal-self-attention-for-local-global\/review\/?hl=36239"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":56,
                "name":"Focal-Transformer",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":102385,
        "rank":41,
        "Model":"InternImage-XL",
        "mlmodel":{

        },
        "method_short":"InternImage-XL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-10",
        "metrics":{
            "Validation mIoU":"55.3",
            "Test Score":null,
            "Params (M)":"368",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":"3142"
        },
        "raw_metrics":{
            "Validation mIoU":55.3,
            "Test Score":null,
            "Params (M)":368.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":3142.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1109218,
            "title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
            "url":"\/paper\/internimage-exploring-large-scale-vision",
            "published":"2022-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internimage-exploring-large-scale-vision\/review\/?hl=102385"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":65694,
        "rank":42,
        "Model":"dBOT ViT-L",
        "mlmodel":{

        },
        "method_short":"dBOT ViT-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-08",
        "metrics":{
            "Validation mIoU":"55.2",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":55.2,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1071401,
            "title":"Exploring Target Representations for Masked Autoencoders",
            "url":"\/paper\/exploring-target-representations-for-masked",
            "published":"2022-09-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-target-representations-for-masked\/review\/?hl=65694"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":112329,
        "rank":43,
        "Model":"Mask2Former(Swin-B)",
        "mlmodel":{

        },
        "method_short":"Mask2Former",
        "method_details":"Swin-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Validation mIoU":"55.1",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":55.1,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924610,
            "title":"Masked-attention Mask Transformer for Universal Image Segmentation",
            "url":"\/paper\/masked-attention-mask-transformer-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-attention-mask-transformer-for\/review\/?hl=112329"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":88679,
        "rank":44,
        "Model":"ConvNeXt V2-H (FCMAE)",
        "mlmodel":{

        },
        "method_short":"ConvNeXt V2-H ",
        "method_details":"FCMAE",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Validation mIoU":"55",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":55.0,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136884,
            "title":"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
            "url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets\/review\/?hl=88679"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":112844,
        "rank":45,
        "Model":"UniRepLKNet-L++",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-L++",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Validation mIoU":"55",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":55.0,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112844"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":70636,
        "rank":46,
        "Model":"DiNAT-Large (UperNet)",
        "mlmodel":{

        },
        "method_short":"DiNAT-Large ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-29",
        "metrics":{
            "Validation mIoU":"54.9",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.9,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1082646,
            "title":"Dilated Neighborhood Attention Transformer",
            "url":"\/paper\/dilated-neighborhood-attention-transformer",
            "published":"2022-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dilated-neighborhood-attention-transformer\/review\/?hl=70636"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":112330,
        "rank":47,
        "Model":"MaskFormer+CBL(Swin-B)",
        "mlmodel":{

        },
        "method_short":"MaskFormer+CBL",
        "method_details":"Swin-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-05",
        "metrics":{
            "Validation mIoU":"54.9",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.9,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1322542,
            "title":"Conditional Boundary Loss for Semantic Segmentation",
            "url":"\/paper\/conditional-boundary-loss-for-semantic",
            "published":"2023-07-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":113266,
        "rank":48,
        "Model":"TransNeXt-Base (IN-1K pretrain, Mask2Former, 512)",
        "mlmodel":{

        },
        "method_short":"TransNeXt-Base ",
        "method_details":"IN-1K pretrain, Mask2Former, 512",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "Validation mIoU":"54.7",
            "Test Score":null,
            "Params (M)":"109",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.7,
            "Test Score":null,
            "Params (M)":109.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1331523,
            "title":"TransNeXt: Robust Foveal Visual Perception for Vision Transformers",
            "url":"\/paper\/transnext-robust-foveal-visual-perception-for",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transnext-robust-foveal-visual-perception-for\/review\/?hl=113266"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":101764,
        "rank":49,
        "Model":"MOAT-2 (IN-22K pretraining, single-scale)",
        "mlmodel":{

        },
        "method_short":"MOAT-2 ",
        "method_details":"IN-22K pretraining, single-scale",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-04",
        "metrics":{
            "Validation mIoU":"54.7",
            "Test Score":null,
            "Params (M)":"81",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.7,
            "Test Score":null,
            "Params (M)":81.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087041,
            "title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models",
            "url":"\/paper\/moat-alternating-mobile-convolution-and",
            "published":"2022-10-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moat-alternating-mobile-convolution-and\/review\/?hl=101764"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57178,
        "rank":50,
        "Model":"CAE (ViT-L, UperNet)",
        "mlmodel":{

        },
        "method_short":"CAE ",
        "method_details":"ViT-L, UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-07",
        "metrics":{
            "Validation mIoU":"54.7",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":956722,
            "title":"Context Autoencoder for Self-Supervised Representation Learning",
            "url":"\/paper\/context-autoencoder-for-self-supervised",
            "published":"2022-02-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/context-autoencoder-for-self-supervised\/review\/?hl=57178"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":60648,
        "rank":51,
        "Model":"VAN-B6",
        "mlmodel":{

        },
        "method_short":"VAN-B6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-20",
        "metrics":{
            "Validation mIoU":"54.7",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":964888,
            "title":"Visual Attention Network",
            "url":"\/paper\/visual-attention-network",
            "published":"2022-02-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-attention-network\/review\/?hl=60648"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":70639,
        "rank":52,
        "Model":"DiNAT_s-Large (UperNet)",
        "mlmodel":{

        },
        "method_short":"DiNAT_s-Large ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-29",
        "metrics":{
            "Validation mIoU":"54.6",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.6,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1082646,
            "title":"Dilated Neighborhood Attention Transformer",
            "url":"\/paper\/dilated-neighborhood-attention-transformer",
            "published":"2022-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dilated-neighborhood-attention-transformer\/review\/?hl=70639"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":105946,
        "rank":53,
        "Model":"DDP (Swin-L, step-3)",
        "mlmodel":{

        },
        "method_short":"DDP ",
        "method_details":"Swin-L, step-3",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Validation mIoU":"54.4",
            "Test Score":null,
            "Params (M)":"207",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.4,
            "Test Score":null,
            "Params (M)":207.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183407,
            "title":"DDP: Diffusion Model for Dense Visual Prediction",
            "url":"\/paper\/ddp-diffusion-model-for-dense-visual",
            "published":"2023-03-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ddp-diffusion-model-for-dense-visual\/review\/?hl=105946"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57385,
        "rank":54,
        "Model":"PatchDiverse + Swin-L (multi-scale test, upernet, ImageNet22k pretrain)",
        "mlmodel":{

        },
        "method_short":"PatchDiverse + Swin-L ",
        "method_details":"multi-scale test, upernet, ImageNet22k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-26",
        "metrics":{
            "Validation mIoU":"54.4",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.4,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":788343,
            "title":"Vision Transformers with Patch Diversification",
            "url":"\/paper\/improve-vision-transformers-training-by",
            "published":"2021-04-26T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":36460,
        "rank":55,
        "Model":"VOLO-D5",
        "mlmodel":{

        },
        "method_short":"VOLO-D5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Validation mIoU":"54.3",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.3,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":824294,
            "title":"VOLO: Vision Outlooker for Visual Recognition",
            "url":"\/paper\/volo-vision-outlooker-for-visual-recognition",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/volo-vision-outlooker-for-visual-recognition\/review\/?hl=36460"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57386,
        "rank":56,
        "Model":"K-Net",
        "mlmodel":{

        },
        "method_short":"K-Net",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-28",
        "metrics":{
            "Validation mIoU":"54.3",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.3,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":825613,
            "title":"K-Net: Towards Unified Image Segmentation",
            "url":"\/paper\/k-net-towards-unified-image-segmentation",
            "published":"2021-06-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/k-net-towards-unified-image-segmentation\/review\/?hl=57386"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":70355,
        "rank":57,
        "Model":"GPaCo (Swin-L)",
        "mlmodel":{

        },
        "method_short":"GPaCo ",
        "method_details":"Swin-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-26",
        "metrics":{
            "Validation mIoU":"54.3",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.3,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1080045,
            "title":"Generalized Parametric Contrastive Learning",
            "url":"\/paper\/generalized-parametric-contrastive-learning",
            "published":"2022-09-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/generalized-parametric-contrastive-learning\/review\/?hl=70355"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57387,
        "rank":58,
        "Model":"SenFormer (Swin-L)",
        "mlmodel":{

        },
        "method_short":"SenFormer ",
        "method_details":"Swin-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-26",
        "metrics":{
            "Validation mIoU":"54.2",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.2,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":920857,
            "title":"Efficient Self-Ensemble for Semantic Segmentation",
            "url":"\/paper\/efficient-self-ensemble-framework-for-1",
            "published":"2021-11-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficient-self-ensemble-framework-for-1\/review\/?hl=57387"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":88658,
        "rank":59,
        "Model":"Swin V2-H",
        "mlmodel":{

        },
        "method_short":"Swin V2-H",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Validation mIoU":"54.2",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.2,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136884,
            "title":"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
            "url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets\/review\/?hl=88658"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":102384,
        "rank":60,
        "Model":"InternImage-L",
        "mlmodel":{

        },
        "method_short":"InternImage-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-10",
        "metrics":{
            "Validation mIoU":"54.1",
            "Test Score":null,
            "Params (M)":"256",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":"2526"
        },
        "raw_metrics":{
            "Validation mIoU":54.1,
            "Test Score":null,
            "Params (M)":256.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":2526.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1109218,
            "title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
            "url":"\/paper\/internimage-exploring-large-scale-vision",
            "published":"2022-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internimage-exploring-large-scale-vision\/review\/?hl=102384"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":113587,
        "rank":61,
        "Model":"TransNeXt-Small (IN-1K pretrain, Mask2Former, 512)",
        "mlmodel":{

        },
        "method_short":"TransNeXt-Small ",
        "method_details":"IN-1K pretrain, Mask2Former, 512",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "Validation mIoU":"54.1",
            "Test Score":null,
            "Params (M)":"69",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.1,
            "Test Score":null,
            "Params (M)":69.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1331523,
            "title":"TransNeXt: Robust Foveal Visual Perception for Vision Transformers",
            "url":"\/paper\/transnext-robust-foveal-visual-perception-for",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transnext-robust-foveal-visual-perception-for\/review\/?hl=113587"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48159,
        "rank":62,
        "Model":"ConvNeXt-XL++",
        "mlmodel":{

        },
        "method_short":"ConvNeXt-XL++",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-10",
        "metrics":{
            "Validation mIoU":"54",
            "Test Score":null,
            "Params (M)":"391",
            "GFLOPs (512 x 512)":"3335",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.0,
            "Test Score":null,
            "Params (M)":391.0,
            "GFLOPs (512 x 512)":3335.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":942596,
            "title":"A ConvNet for the 2020s",
            "url":"\/paper\/a-convnet-for-the-2020s",
            "published":"2022-01-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-convnet-for-the-2020s\/review\/?hl=48159"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":88640,
        "rank":63,
        "Model":"Sequential Ensemble (SegFormer)",
        "mlmodel":{

        },
        "method_short":"Sequential Ensemble ",
        "method_details":"SegFormer",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-08",
        "metrics":{
            "Validation mIoU":"54",
            "Test Score":null,
            "Params (M)":"216.3",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.0,
            "Test Score":null,
            "Params (M)":216.3,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1089604,
            "title":"Sequential Ensembling for Semantic Segmentation",
            "url":"\/paper\/sequential-ensembling-for-semantic",
            "published":"2022-10-08T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":237,
                "name":"HRNet",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":284,
                "name":"Ensemble",
                "color":"#77bb41"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":99774,
        "rank":64,
        "Model":"MogaNet-XL (UperNet)",
        "mlmodel":{

        },
        "method_short":"MogaNet-XL ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-07",
        "metrics":{
            "Validation mIoU":"54",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":54.0,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1106694,
            "title":"Efficient Multi-order Gated Aggregation Network",
            "url":"\/paper\/efficient-multi-order-gated-aggregation",
            "published":"2022-11-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficient-multi-order-gated-aggregation\/review\/?hl=99774"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":112843,
        "rank":65,
        "Model":"UniRepLKNet-B++",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-B++",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Validation mIoU":"53.9",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":53.9,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112843"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":112331,
        "rank":66,
        "Model":"MaskFormer(Swin-B)",
        "mlmodel":{

        },
        "method_short":"MaskFormer",
        "method_details":"Swin-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-13",
        "metrics":{
            "Validation mIoU":"53.8",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":53.8,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":834841,
            "title":"Per-Pixel Classification is Not All You Need for Semantic Segmentation",
            "url":"\/paper\/per-pixel-classification-is-not-all-you-need",
            "published":"2021-07-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/per-pixel-classification-is-not-all-you-need\/review\/?hl=112331"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48158,
        "rank":67,
        "Model":"ConvNeXt-L++",
        "mlmodel":{

        },
        "method_short":"ConvNeXt-L++",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-10",
        "metrics":{
            "Validation mIoU":"53.7",
            "Test Score":null,
            "Params (M)":"235",
            "GFLOPs (512 x 512)":"2458",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":53.7,
            "Test Score":null,
            "Params (M)":235.0,
            "GFLOPs (512 x 512)":2458.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":942596,
            "title":"A ConvNet for the 2020s",
            "url":"\/paper\/a-convnet-for-the-2020s",
            "published":"2022-01-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-convnet-for-the-2020s\/review\/?hl=48158"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":64472,
        "rank":68,
        "Model":"SwinV2-G-HTC++ Liu et al. ([2021a])",
        "mlmodel":{

        },
        "method_short":"SwinV2-G-HTC++ Liu et al. ",
        "method_details":"[2021a]",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-18",
        "metrics":{
            "Validation mIoU":"53.7",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":53.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912369,
            "title":"Swin Transformer V2: Scaling Up Capacity and Resolution",
            "url":"\/paper\/swin-transformer-v2-scaling-up-capacity-and",
            "published":"2021-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/swin-transformer-v2-scaling-up-capacity-and\/review\/?hl=64472"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":88656,
        "rank":69,
        "Model":"ConvNeXt V2-L",
        "mlmodel":{

        },
        "method_short":"ConvNeXt V2-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Validation mIoU":"53.7",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":53.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136884,
            "title":"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
            "url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets\/review\/?hl=88656"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":33905,
        "rank":70,
        "Model":"Seg-L-Mask\/16 (MS)",
        "mlmodel":{

        },
        "method_short":"Seg-L-Mask\/16 ",
        "method_details":"MS",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-12",
        "metrics":{
            "Validation mIoU":"53.63",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":53.63,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":798062,
            "title":"Segmenter: Transformer for Semantic Segmentation",
            "url":"\/paper\/segmenter-transformer-for-semantic",
            "published":"2021-05-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/segmenter-transformer-for-semantic\/review\/?hl=33905"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":44091,
        "rank":71,
        "Model":"MAE (ViT-L, UperNet)",
        "mlmodel":{

        },
        "method_short":"MAE ",
        "method_details":"ViT-L, UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-11",
        "metrics":{
            "Validation mIoU":"53.6",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":53.6,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":908690,
            "title":"Masked Autoencoders Are Scalable Vision Learners",
            "url":"\/paper\/masked-autoencoders-are-scalable-vision",
            "published":"2021-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-autoencoders-are-scalable-vision\/review\/?hl=44091"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48167,
        "rank":72,
        "Model":"SeMask (SeMask Swin-L FPN)",
        "mlmodel":{

        },
        "method_short":"SeMask ",
        "method_details":"SeMask Swin-L FPN",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "Validation mIoU":"53.52",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":53.52,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932883,
            "title":"SeMask: Semantically Masked Transformers for Semantic Segmentation",
            "url":"\/paper\/semask-semantically-masked-transformers-for-1",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/semask-semantically-masked-transformers-for-1\/review\/?hl=48167"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":28912,
        "rank":73,
        "Model":"Swin-L (UperNet, ImageNet-22k pretrain)",
        "mlmodel":{

        },
        "method_short":"Swin-L ",
        "method_details":"UperNet, ImageNet-22k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-25",
        "metrics":{
            "Validation mIoU":"53.50",
            "Test Score":"62.8",
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":53.5,
            "Test Score":62.8,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":757245,
            "title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
            "url":"\/paper\/swin-transformer-hierarchical-vision",
            "published":"2021-03-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/swin-transformer-hierarchical-vision\/review\/?hl=28912"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":88657,
        "rank":74,
        "Model":"Swin-L",
        "mlmodel":{

        },
        "method_short":"Swin-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Validation mIoU":"53.5",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":53.5,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136884,
            "title":"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
            "url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets\/review\/?hl=88657"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":113588,
        "rank":75,
        "Model":"TransNeXt-Tiny (IN-1K pretrain, Mask2Former, 512)",
        "mlmodel":{

        },
        "method_short":"TransNeXt-Tiny ",
        "method_details":"IN-1K pretrain, Mask2Former, 512",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "Validation mIoU":"53.4",
            "Test Score":null,
            "Params (M)":"47.5",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":53.4,
            "Test Score":null,
            "Params (M)":47.5,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1331523,
            "title":"TransNeXt: Robust Foveal Visual Perception for Vision Transformers",
            "url":"\/paper\/transnext-robust-foveal-visual-perception-for",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transnext-robust-foveal-visual-perception-for\/review\/?hl=113588"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48157,
        "rank":76,
        "Model":"ConvNeXt-B++",
        "mlmodel":{

        },
        "method_short":"ConvNeXt-B++",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-10",
        "metrics":{
            "Validation mIoU":"53.1",
            "Test Score":null,
            "Params (M)":"122",
            "GFLOPs (512 x 512)":"1828",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":53.1,
            "Test Score":null,
            "Params (M)":122.0,
            "GFLOPs (512 x 512)":1828.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":942596,
            "title":"A ConvNet for the 2020s",
            "url":"\/paper\/a-convnet-for-the-2020s",
            "published":"2022-01-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-convnet-for-the-2020s\/review\/?hl=48157"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57388,
        "rank":77,
        "Model":"PatchConvNet-L120 (UperNet)",
        "mlmodel":{

        },
        "method_short":"PatchConvNet-L120 ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-27",
        "metrics":{
            "Validation mIoU":"52.9",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":52.9,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":933338,
            "title":"Augmenting Convolutional networks with attention-based aggregation",
            "url":"\/paper\/augmenting-convolutional-networks-with",
            "published":"2021-12-27T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":76348,
        "rank":78,
        "Model":"dBOT ViT-B (CLIP)",
        "mlmodel":{

        },
        "method_short":"dBOT ViT-B ",
        "method_details":"CLIP",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-08",
        "metrics":{
            "Validation mIoU":"52.9",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":52.9,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1071401,
            "title":"Exploring Target Representations for Masked Autoencoders",
            "url":"\/paper\/exploring-target-representations-for-masked",
            "published":"2022-09-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-target-representations-for-masked\/review\/?hl=76348"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57389,
        "rank":79,
        "Model":"PatchConvNet-B120\n(UperNet)",
        "mlmodel":{

        },
        "method_short":"PatchConvNet-B120\n",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-27",
        "metrics":{
            "Validation mIoU":"52.8",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":52.8,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":933338,
            "title":"Augmenting Convolutional networks with attention-based aggregation",
            "url":"\/paper\/augmenting-convolutional-networks-with",
            "published":"2021-12-27T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":88654,
        "rank":80,
        "Model":"Swin-B",
        "mlmodel":{

        },
        "method_short":"Swin-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Validation mIoU":"52.8",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":52.8,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136884,
            "title":"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
            "url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets\/review\/?hl=88654"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":112842,
        "rank":81,
        "Model":"UniRepLKNet-S++",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-S++",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Validation mIoU":"52.7",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":52.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112842"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":88653,
        "rank":82,
        "Model":"ConvNeXt V2-B",
        "mlmodel":{

        },
        "method_short":"ConvNeXt V2-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Validation mIoU":"52.1",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":52.1,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136884,
            "title":"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
            "url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets\/review\/?hl=88653"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":34561,
        "rank":83,
        "Model":"LV-ViT-L (UperNet, MS)",
        "mlmodel":{

        },
        "method_short":"LV-ViT-L ",
        "method_details":"UperNet, MS",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Validation mIoU":"51.8",
            "Test Score":null,
            "Params (M)":"209",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":51.8,
            "Test Score":null,
            "Params (M)":209.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787097,
            "title":"All Tokens Matter: Token Labeling for Training Better Vision Transformers",
            "url":"\/paper\/token-labeling-training-a-85-5-top-1-accuracy",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/token-labeling-training-a-85-5-top-1-accuracy\/review\/?hl=34561"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48179,
        "rank":84,
        "Model":"SegFormer-B5",
        "mlmodel":{

        },
        "method_short":"SegFormer-B5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Validation mIoU":"51.8",
            "Test Score":null,
            "Params (M)":"84.7",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":51.8,
            "Test Score":null,
            "Params (M)":84.7,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":808117,
            "title":"SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
            "url":"\/paper\/segformer-simple-and-efficient-design-for",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/segformer-simple-and-efficient-design-for\/review\/?hl=48179"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":99528,
        "rank":85,
        "Model":"BiFormer-B (IN1k pretrain, Upernet 160k)",
        "mlmodel":{

        },
        "method_short":"BiFormer-B ",
        "method_details":"IN1k pretrain, Upernet 160k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-15",
        "metrics":{
            "Validation mIoU":"51.7",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":51.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1174425,
            "title":"BiFormer: Vision Transformer with Bi-Level Routing Attention",
            "url":"\/paper\/biformer-vision-transformer-with-bi-level",
            "published":"2023-03-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/biformer-vision-transformer-with-bi-level\/review\/?hl=99528"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":88675,
        "rank":86,
        "Model":"ConvNeXt V2-L (Supervised)",
        "mlmodel":{

        },
        "method_short":"ConvNeXt V2-L ",
        "method_details":"Supervised",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Validation mIoU":"51.6",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":51.6,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136884,
            "title":"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
            "url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets\/review\/?hl=88675"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":50753,
        "rank":87,
        "Model":"Light-Ham (VAN-Huge)",
        "mlmodel":{

        },
        "method_short":"Light-Ham ",
        "method_details":"VAN-Huge",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-26",
        "metrics":{
            "Validation mIoU":"51.5",
            "Test Score":null,
            "Params (M)":"61.1",
            "GFLOPs (512 x 512)":"71.8",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":51.5,
            "Test Score":null,
            "Params (M)":61.1,
            "GFLOPs (512 x 512)":71.8,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":864941,
            "title":"Is Attention Better Than Matrix Decomposition?",
            "url":"\/paper\/is-attention-better-than-matrix-decomposition-1",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/is-attention-better-than-matrix-decomposition-1\/review\/?hl=50753"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57390,
        "rank":88,
        "Model":"CrossFormer (ImageNet1k-pretrain, UPerNet, multi-scale test)",
        "mlmodel":{

        },
        "method_short":"CrossFormer ",
        "method_details":"ImageNet1k-pretrain, UPerNet, multi-scale test",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-31",
        "metrics":{
            "Validation mIoU":"51.4",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":51.4,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":844977,
            "title":"CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention",
            "url":"\/paper\/crossformer-a-versatile-vision-transformer",
            "published":"2021-07-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/crossformer-a-versatile-vision-transformer\/review\/?hl=57390"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":102383,
        "rank":89,
        "Model":"InternImage-B",
        "mlmodel":{

        },
        "method_short":"InternImage-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-10",
        "metrics":{
            "Validation mIoU":"51.3",
            "Test Score":null,
            "Params (M)":"128",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":"1185"
        },
        "raw_metrics":{
            "Validation mIoU":51.3,
            "Test Score":null,
            "Params (M)":128.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":1185.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1109218,
            "title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
            "url":"\/paper\/internimage-exploring-large-scale-vision",
            "published":"2022-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internimage-exploring-large-scale-vision\/review\/?hl=102383"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":49597,
        "rank":90,
        "Model":"ActiveMLP-L(UperNet)",
        "mlmodel":{

        },
        "method_short":"ActiveMLP-L",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-11",
        "metrics":{
            "Validation mIoU":"51.1",
            "Test Score":null,
            "Params (M)":"108",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":51.1,
            "Test Score":null,
            "Params (M)":108.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":975658,
            "title":"Active Token Mixer",
            "url":"\/paper\/activemlp-an-mlp-like-architecture-with",
            "published":"2022-03-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/activemlp-an-mlp-like-architecture-with\/review\/?hl=49597"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48178,
        "rank":91,
        "Model":"SegFormer-B4",
        "mlmodel":{

        },
        "method_short":"SegFormer-B4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Validation mIoU":"51.1",
            "Test Score":null,
            "Params (M)":"64.1",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":51.1,
            "Test Score":null,
            "Params (M)":64.1,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":808117,
            "title":"SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
            "url":"\/paper\/segformer-simple-and-efficient-design-for",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/segformer-simple-and-efficient-design-for\/review\/?hl=48178"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57392,
        "rank":92,
        "Model":"PatchConvNet-B60 (UperNet)",
        "mlmodel":{

        },
        "method_short":"PatchConvNet-B60 ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-27",
        "metrics":{
            "Validation mIoU":"51.1",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":51.1,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":933338,
            "title":"Augmenting Convolutional networks with attention-based aggregation",
            "url":"\/paper\/augmenting-convolutional-networks-with",
            "published":"2021-12-27T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":50321,
        "rank":93,
        "Model":"Light-Ham (VAN-Large)",
        "mlmodel":{

        },
        "method_short":"Light-Ham ",
        "method_details":"VAN-Large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-26",
        "metrics":{
            "Validation mIoU":"51.0",
            "Test Score":null,
            "Params (M)":"45.6",
            "GFLOPs (512 x 512)":"55.0",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":51.0,
            "Test Score":null,
            "Params (M)":45.6,
            "GFLOPs (512 x 512)":55.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":864941,
            "title":"Is Attention Better Than Matrix Decomposition?",
            "url":"\/paper\/is-attention-better-than-matrix-decomposition-1",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/is-attention-better-than-matrix-decomposition-1\/review\/?hl=50321"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":86809,
        "rank":94,
        "Model":"TEC (Vit-B, Upernet)",
        "mlmodel":{

        },
        "method_short":"TEC ",
        "method_details":"Vit-B, Upernet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Validation mIoU":"51.0",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":51.0,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097111,
            "title":"Towards Sustainable Self-supervised Learning",
            "url":"\/paper\/towards-sustainable-self-supervised-learning",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/towards-sustainable-self-supervised-learning\/review\/?hl=86809"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":112841,
        "rank":95,
        "Model":"UniRepLKNet-S",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Validation mIoU":"51",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":51.0,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112841"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48166,
        "rank":96,
        "Model":"SeMask (SeMask Swin-B FPN)",
        "mlmodel":{

        },
        "method_short":"SeMask ",
        "method_details":"SeMask Swin-B FPN",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "Validation mIoU":"50.98",
            "Test Score":null,
            "Params (M)":"96",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":50.98,
            "Test Score":null,
            "Params (M)":96.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932883,
            "title":"SeMask: Semantically Masked Transformers for Semantic Segmentation",
            "url":"\/paper\/semask-semantically-masked-transformers-for-1",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/semask-semantically-masked-transformers-for-1\/review\/?hl=48166"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":102382,
        "rank":97,
        "Model":"InternImage-S",
        "mlmodel":{

        },
        "method_short":"InternImage-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-10",
        "metrics":{
            "Validation mIoU":"50.9",
            "Test Score":null,
            "Params (M)":"80",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":"1017"
        },
        "raw_metrics":{
            "Validation mIoU":50.9,
            "Test Score":null,
            "Params (M)":80.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":1017.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1109218,
            "title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
            "url":"\/paper\/internimage-exploring-large-scale-vision",
            "published":"2022-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internimage-exploring-large-scale-vision\/review\/?hl=102382"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":99773,
        "rank":98,
        "Model":"MogaNet-L (UperNet)",
        "mlmodel":{

        },
        "method_short":"MogaNet-L ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-07",
        "metrics":{
            "Validation mIoU":"50.9",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":"1176",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":50.9,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":1176.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1106694,
            "title":"Efficient Multi-order Gated Aggregation Network",
            "url":"\/paper\/efficient-multi-order-gated-aggregation",
            "published":"2022-11-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficient-multi-order-gated-aggregation\/review\/?hl=99773"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":65693,
        "rank":99,
        "Model":"dBOT ViT-B",
        "mlmodel":{

        },
        "method_short":"dBOT ViT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-08",
        "metrics":{
            "Validation mIoU":"50.8",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":50.8,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1071401,
            "title":"Exploring Target Representations for Masked Autoencoders",
            "url":"\/paper\/exploring-target-representations-for-masked",
            "published":"2022-09-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-target-representations-for-masked\/review\/?hl=65693"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":99529,
        "rank":100,
        "Model":"Upernet-BiFormer-S (IN1k pretrain, Upernet 160k)",
        "mlmodel":{

        },
        "method_short":"Upernet-BiFormer-S ",
        "method_details":"IN1k pretrain, Upernet 160k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-15",
        "metrics":{
            "Validation mIoU":"50.8",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":50.8,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1174425,
            "title":"BiFormer: Vision Transformer with Bi-Level Routing Attention",
            "url":"\/paper\/biformer-vision-transformer-with-bi-level",
            "published":"2023-03-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/biformer-vision-transformer-with-bi-level\/review\/?hl=99529"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57407,
        "rank":101,
        "Model":"UperNet Shuffle-B",
        "mlmodel":{

        },
        "method_short":"UperNet Shuffle-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-07",
        "metrics":{
            "Validation mIoU":"50.5",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":50.5,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":812458,
            "title":"Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer",
            "url":"\/paper\/shuffle-transformer-rethinking-spatial",
            "published":"2021-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/shuffle-transformer-rethinking-spatial\/review\/?hl=57407"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":88655,
        "rank":102,
        "Model":"ConvNeXt V1-L",
        "mlmodel":{

        },
        "method_short":"ConvNeXt V1-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Validation mIoU":"50.5",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":50.5,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136884,
            "title":"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
            "url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets\/review\/?hl=88655"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":70635,
        "rank":103,
        "Model":"DiNAT-Base (UperNet)",
        "mlmodel":{

        },
        "method_short":"DiNAT-Base ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-29",
        "metrics":{
            "Validation mIoU":"50.4",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":50.4,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1082646,
            "title":"Dilated Neighborhood Attention Transformer",
            "url":"\/paper\/dilated-neighborhood-attention-transformer",
            "published":"2022-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dilated-neighborhood-attention-transformer\/review\/?hl=70635"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57408,
        "rank":104,
        "Model":"ELSA-Swin-S",
        "mlmodel":{

        },
        "method_short":"ELSA-Swin-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "Validation mIoU":"50.3",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":50.3,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932838,
            "title":"ELSA: Enhanced Local Self-Attention for Vision Transformer",
            "url":"\/paper\/elsa-enhanced-local-self-attention-for-vision",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/elsa-enhanced-local-self-attention-for-vision\/review\/?hl=57408"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":97285,
        "rank":105,
        "Model":"SETR-MLA (160k, MS)",
        "mlmodel":{

        },
        "method_short":"SETR-MLA ",
        "method_details":"160k, MS",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-31",
        "metrics":{
            "Validation mIoU":"50.28",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":50.28,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":732315,
            "title":"Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers",
            "url":"\/paper\/rethinking-semantic-segmentation-from-a",
            "published":"2020-12-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-semantic-segmentation-from-a\/review\/?hl=97285"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":49046,
        "rank":106,
        "Model":"VAN-Large (HamNet)",
        "mlmodel":{

        },
        "method_short":"VAN-Large ",
        "method_details":"HamNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-20",
        "metrics":{
            "Validation mIoU":"50.2",
            "Test Score":null,
            "Params (M)":"55",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":50.2,
            "Test Score":null,
            "Params (M)":55.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":964888,
            "title":"Visual Attention Network",
            "url":"\/paper\/visual-attention-network",
            "published":"2022-02-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-attention-network\/review\/?hl=49046"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":56447,
        "rank":107,
        "Model":"HRViT-b3 (SegFormer, SS)",
        "mlmodel":{

        },
        "method_short":"HRViT-b3 ",
        "method_details":"SegFormer, SS",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-01",
        "metrics":{
            "Validation mIoU":"50.2",
            "Test Score":null,
            "Params (M)":"28.7",
            "GFLOPs (512 x 512)":"67.9",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":50.2,
            "Test Score":null,
            "Params (M)":28.7,
            "GFLOPs (512 x 512)":67.9,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":899298,
            "title":"Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation",
            "url":"\/paper\/hrvit-multi-scale-high-resolution-vision",
            "published":"2021-11-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hrvit-multi-scale-high-resolution-vision\/review\/?hl=56447"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":10,
                "name":"multiscale",
                "color":"#ba1330"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57409,
        "rank":108,
        "Model":"Twins-SVT-L (UperNet, ImageNet-1k pretrain)",
        "mlmodel":{

        },
        "method_short":"Twins-SVT-L ",
        "method_details":"UperNet, ImageNet-1k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-28",
        "metrics":{
            "Validation mIoU":"50.2",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":50.2,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":788788,
            "title":"Twins: Revisiting the Design of Spatial Attention in Vision Transformers",
            "url":"\/paper\/twins-revisiting-spatial-attention-design-in",
            "published":"2021-04-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/twins-revisiting-spatial-attention-design-in\/review\/?hl=57409"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":99772,
        "rank":109,
        "Model":"MogaNet-B (UperNet)",
        "mlmodel":{

        },
        "method_short":"MogaNet-B ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-07",
        "metrics":{
            "Validation mIoU":"50.1",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":"1050",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":50.1,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":1050.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1106694,
            "title":"Efficient Multi-order Gated Aggregation Network",
            "url":"\/paper\/efficient-multi-order-gated-aggregation",
            "published":"2022-11-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficient-multi-order-gated-aggregation\/review\/?hl=99772"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57410,
        "rank":110,
        "Model":"Seg-B-Mask\/16(MS, ViT-B)",
        "mlmodel":{

        },
        "method_short":"Seg-B-Mask\/16",
        "method_details":"MS, ViT-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-12",
        "metrics":{
            "Validation mIoU":"50.0",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":50.0,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":798062,
            "title":"Segmenter: Transformer for Semantic Segmentation",
            "url":"\/paper\/segmenter-transformer-for-semantic",
            "published":"2021-05-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/segmenter-transformer-for-semantic\/review\/?hl=57410"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":88763,
        "rank":111,
        "Model":"iBOT (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"iBOT ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-15",
        "metrics":{
            "Validation mIoU":"50.0",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":50.0,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":909858,
            "title":"iBOT: Image BERT Pre-Training with Online Tokenizer",
            "url":"\/paper\/ibot-image-bert-pre-training-with-online",
            "published":"2021-11-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48156,
        "rank":112,
        "Model":"ConvNeXt-B",
        "mlmodel":{

        },
        "method_short":"ConvNeXt-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-10",
        "metrics":{
            "Validation mIoU":"49.9",
            "Test Score":null,
            "Params (M)":"122",
            "GFLOPs (512 x 512)":"1170",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.9,
            "Test Score":null,
            "Params (M)":122.0,
            "GFLOPs (512 x 512)":1170.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":942596,
            "title":"A ConvNet for the 2020s",
            "url":"\/paper\/a-convnet-for-the-2020s",
            "published":"2022-01-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-convnet-for-the-2020s\/review\/?hl=48156"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":70634,
        "rank":113,
        "Model":"DiNAT-Small (UperNet)",
        "mlmodel":{

        },
        "method_short":"DiNAT-Small ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-29",
        "metrics":{
            "Validation mIoU":"49.9",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.9,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1082646,
            "title":"Dilated Neighborhood Attention Transformer",
            "url":"\/paper\/dilated-neighborhood-attention-transformer",
            "published":"2022-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dilated-neighborhood-attention-transformer\/review\/?hl=70634"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":88652,
        "rank":114,
        "Model":"ConvNeXt V1-B",
        "mlmodel":{

        },
        "method_short":"ConvNeXt V1-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-02",
        "metrics":{
            "Validation mIoU":"49.9",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.9,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136884,
            "title":"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
            "url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets",
            "published":"2023-01-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convnext-v2-co-designing-and-scaling-convnets\/review\/?hl=88652"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":52500,
        "rank":115,
        "Model":"NAT-Base",
        "mlmodel":{

        },
        "method_short":"NAT-Base",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Validation mIoU":"49.7",
            "Test Score":null,
            "Params (M)":"123",
            "GFLOPs (512 x 512)":"1137",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.7,
            "Test Score":null,
            "Params (M)":123.0,
            "GFLOPs (512 x 512)":1137.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994419,
            "title":"Neighborhood Attention Transformer",
            "url":"\/paper\/neighborhood-attention-transformer",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/neighborhood-attention-transformer\/review\/?hl=52500"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":266,
                "name":"NAT Transformer",
                "color":"#574896"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":265,
                "name":"Neighborhood Attention",
                "color":"#ba1330"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57411,
        "rank":116,
        "Model":"Swin-B (UperNet, ImageNet-1k pretrain)",
        "mlmodel":{

        },
        "method_short":"Swin-B ",
        "method_details":"UperNet, ImageNet-1k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-25",
        "metrics":{
            "Validation mIoU":"49.7",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":757245,
            "title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
            "url":"\/paper\/swin-transformer-hierarchical-vision",
            "published":"2021-03-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/swin-transformer-hierarchical-vision\/review\/?hl=57411"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57412,
        "rank":117,
        "Model":"Seg-B\/8 (MS, ViT-B)",
        "mlmodel":{

        },
        "method_short":"Seg-B\/8 ",
        "method_details":"MS, ViT-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-12",
        "metrics":{
            "Validation mIoU":"49.61",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.61,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":798062,
            "title":"Segmenter: Transformer for Semantic Segmentation",
            "url":"\/paper\/segmenter-transformer-for-semantic",
            "published":"2021-05-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/segmenter-transformer-for-semantic\/review\/?hl=57412"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48155,
        "rank":118,
        "Model":"ConvNeXt-S",
        "mlmodel":{

        },
        "method_short":"ConvNeXt-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-10",
        "metrics":{
            "Validation mIoU":"49.6",
            "Test Score":null,
            "Params (M)":"82",
            "GFLOPs (512 x 512)":"1027",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.6,
            "Test Score":null,
            "Params (M)":82.0,
            "GFLOPs (512 x 512)":1027.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":942596,
            "title":"A ConvNet for the 2020s",
            "url":"\/paper\/a-convnet-for-the-2020s",
            "published":"2022-01-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-convnet-for-the-2020s\/review\/?hl=48155"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":50322,
        "rank":119,
        "Model":"Light-Ham (VAN-Base)",
        "mlmodel":{

        },
        "method_short":"Light-Ham ",
        "method_details":"VAN-Base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-26",
        "metrics":{
            "Validation mIoU":"49.6",
            "Test Score":null,
            "Params (M)":"27.4",
            "GFLOPs (512 x 512)":"34.4",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.6,
            "Test Score":null,
            "Params (M)":27.4,
            "GFLOPs (512 x 512)":34.4,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":864941,
            "title":"Is Attention Better Than Matrix Decomposition?",
            "url":"\/paper\/is-attention-better-than-matrix-decomposition-1",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/is-attention-better-than-matrix-decomposition-1\/review\/?hl=50322"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":52499,
        "rank":120,
        "Model":"NAT-Small",
        "mlmodel":{

        },
        "method_short":"NAT-Small",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Validation mIoU":"49.5",
            "Test Score":null,
            "Params (M)":"82",
            "GFLOPs (512 x 512)":"1010",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.5,
            "Test Score":null,
            "Params (M)":82.0,
            "GFLOPs (512 x 512)":1010.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994419,
            "title":"Neighborhood Attention Transformer",
            "url":"\/paper\/neighborhood-attention-transformer",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/neighborhood-attention-transformer\/review\/?hl=52499"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":266,
                "name":"NAT Transformer",
                "color":"#574896"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":265,
                "name":"Neighborhood Attention",
                "color":"#ba1330"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":51983,
        "rank":121,
        "Model":"DaViT-B",
        "mlmodel":{

        },
        "method_short":"DaViT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "Validation mIoU":"49.4",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.4,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":990738,
            "title":"DaViT: Dual Attention Vision Transformers",
            "url":"\/paper\/davit-dual-attention-vision-transformers",
            "published":"2022-04-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/davit-dual-attention-vision-transformers\/review\/?hl=51983"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48173,
        "rank":122,
        "Model":"DAT-B (UperNet)",
        "mlmodel":{

        },
        "method_short":"DAT-B ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-03",
        "metrics":{
            "Validation mIoU":"49.38",
            "Test Score":null,
            "Params (M)":"121",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.38,
            "Test Score":null,
            "Params (M)":121.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":934161,
            "title":"Vision Transformer with Deformable Attention",
            "url":"\/paper\/vision-transformer-with-deformable-attention",
            "published":"2022-01-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-transformer-with-deformable-attention\/review\/?hl=48173"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57413,
        "rank":123,
        "Model":"PatchConvNet-S60 (UperNet)",
        "mlmodel":{

        },
        "method_short":"PatchConvNet-S60 ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-27",
        "metrics":{
            "Validation mIoU":"49.3",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.3,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":933338,
            "title":"Augmenting Convolutional networks with attention-based aggregation",
            "url":"\/paper\/augmenting-convolutional-networks-with",
            "published":"2021-12-27T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":99771,
        "rank":124,
        "Model":"MogaNet-S (UperNet)",
        "mlmodel":{

        },
        "method_short":"MogaNet-S ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-07",
        "metrics":{
            "Validation mIoU":"49.2",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":"946",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.2,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":946.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1106694,
            "title":"Efficient Multi-order Gated Aggregation Network",
            "url":"\/paper\/efficient-multi-order-gated-aggregation",
            "published":"2022-11-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficient-multi-order-gated-aggregation\/review\/?hl=99771"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":46845,
        "rank":125,
        "Model":"Shift-B (UperNet)",
        "mlmodel":{

        },
        "method_short":"Shift-B ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-26",
        "metrics":{
            "Validation mIoU":"49.2",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.2,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":951213,
            "title":"When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
            "url":"\/paper\/when-shift-operation-meets-vision-transformer",
            "published":"2022-01-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-shift-operation-meets-vision-transformer\/review\/?hl=46845"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":112840,
        "rank":126,
        "Model":"UniRepLKNet-T",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Validation mIoU":"49.1",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.1,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112840"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":28832,
        "rank":127,
        "Model":"DPT-Hybrid",
        "mlmodel":{

        },
        "method_short":"DPT-Hybrid",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-24",
        "metrics":{
            "Validation mIoU":"49.02",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.02,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":757407,
            "title":"Vision Transformers for Dense Prediction",
            "url":"\/paper\/vision-transformers-for-dense-prediction",
            "published":"2021-03-24T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":59103,
        "rank":128,
        "Model":"GC ViT-B",
        "mlmodel":{

        },
        "method_short":"GC ViT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-20",
        "metrics":{
            "Validation mIoU":"49",
            "Test Score":null,
            "Params (M)":"125",
            "GFLOPs (512 x 512)":"1348",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.0,
            "Test Score":null,
            "Params (M)":125.0,
            "GFLOPs (512 x 512)":1348.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1029616,
            "title":"Global Context Vision Transformers",
            "url":"\/paper\/global-context-vision-transformers",
            "published":"2022-06-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/global-context-vision-transformers\/review\/?hl=59103"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":78997,
        "rank":129,
        "Model":"A2MIM (ViT-B)",
        "mlmodel":{

        },
        "method_short":"A2MIM ",
        "method_details":"ViT-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-27",
        "metrics":{
            "Validation mIoU":"49",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.0,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1017241,
            "title":"Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN",
            "url":"\/paper\/architecture-agnostic-masked-image-modeling",
            "published":"2022-05-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/architecture-agnostic-masked-image-modeling\/review\/?hl=78997"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":108800,
        "rank":130,
        "Model":"EfficientViT-B3 (r512)",
        "mlmodel":{

        },
        "method_short":"EfficientViT-B3 ",
        "method_details":"r512",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-29",
        "metrics":{
            "Validation mIoU":"49",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":49.0,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1017975,
            "title":"EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction",
            "url":"\/paper\/efficientvit-enhanced-linear-attention-for",
            "published":"2022-05-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientvit-enhanced-linear-attention-for\/review\/?hl=108800"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":70633,
        "rank":131,
        "Model":"DiNAT-Tiny (UperNet)",
        "mlmodel":{

        },
        "method_short":"DiNAT-Tiny ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-29",
        "metrics":{
            "Validation mIoU":"48.8",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":48.8,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1082646,
            "title":"Dilated Neighborhood Attention Transformer",
            "url":"\/paper\/dilated-neighborhood-attention-transformer",
            "published":"2022-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dilated-neighborhood-attention-transformer\/review\/?hl=70633"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":56448,
        "rank":132,
        "Model":"HRViT-b2 (SegFormer, SS)",
        "mlmodel":{

        },
        "method_short":"HRViT-b2 ",
        "method_details":"SegFormer, SS",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-01",
        "metrics":{
            "Validation mIoU":"48.76",
            "Test Score":null,
            "Params (M)":"20.8",
            "GFLOPs (512 x 512)":"28.0",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":48.76,
            "Test Score":null,
            "Params (M)":20.8,
            "GFLOPs (512 x 512)":28.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":899298,
            "title":"Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation",
            "url":"\/paper\/hrvit-multi-scale-high-resolution-vision",
            "published":"2021-11-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hrvit-multi-scale-high-resolution-vision\/review\/?hl=56448"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":10,
                "name":"multiscale",
                "color":"#ba1330"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":52498,
        "rank":133,
        "Model":"NAT-Tiny",
        "mlmodel":{

        },
        "method_short":"NAT-Tiny",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Validation mIoU":"48.4",
            "Test Score":null,
            "Params (M)":"58",
            "GFLOPs (512 x 512)":"934",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":48.4,
            "Test Score":null,
            "Params (M)":58.0,
            "GFLOPs (512 x 512)":934.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994419,
            "title":"Neighborhood Attention Transformer",
            "url":"\/paper\/neighborhood-attention-transformer",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/neighborhood-attention-transformer\/review\/?hl=52498"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":266,
                "name":"NAT Transformer",
                "color":"#574896"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":265,
                "name":"Neighborhood Attention",
                "color":"#ba1330"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":35225,
        "rank":134,
        "Model":"XCiT-M24\/8 (UperNet)",
        "mlmodel":{

        },
        "method_short":"XCiT-M24\/8 ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-17",
        "metrics":{
            "Validation mIoU":"48.4",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":48.4,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":820317,
            "title":"XCiT: Cross-Covariance Image Transformers",
            "url":"\/paper\/xcit-cross-covariance-image-transformers",
            "published":"2021-06-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/xcit-cross-covariance-image-transformers\/review\/?hl=35225"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":10936,
        "rank":135,
        "Model":"ResNeSt-200",
        "mlmodel":{

        },
        "method_short":"ResNeSt-200",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-19",
        "metrics":{
            "Validation mIoU":"48.36",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":48.36,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":191448,
            "title":"ResNeSt: Split-Attention Networks",
            "url":"\/paper\/resnest-split-attention-networks",
            "published":"2020-04-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resnest-split-attention-networks\/review\/?hl=10936"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48172,
        "rank":136,
        "Model":"DAT-S (UperNet)",
        "mlmodel":{

        },
        "method_short":"DAT-S ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-03",
        "metrics":{
            "Validation mIoU":"48.31",
            "Test Score":null,
            "Params (M)":"81",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":48.31,
            "Test Score":null,
            "Params (M)":81.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":934161,
            "title":"Vision Transformer with Deformable Attention",
            "url":"\/paper\/vision-transformer-with-deformable-attention",
            "published":"2022-01-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-transformer-with-deformable-attention\/review\/?hl=48172"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":59102,
        "rank":137,
        "Model":"GC ViT-S",
        "mlmodel":{

        },
        "method_short":"GC ViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-20",
        "metrics":{
            "Validation mIoU":"48.3",
            "Test Score":null,
            "Params (M)":"84",
            "GFLOPs (512 x 512)":"1163",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":48.3,
            "Test Score":null,
            "Params (M)":84.0,
            "GFLOPs (512 x 512)":1163.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1029616,
            "title":"Global Context Vision Transformers",
            "url":"\/paper\/global-context-vision-transformers",
            "published":"2022-06-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/global-context-vision-transformers\/review\/?hl=59102"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":102381,
        "rank":138,
        "Model":"InternImage-T",
        "mlmodel":{

        },
        "method_short":"InternImage-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-10",
        "metrics":{
            "Validation mIoU":"48.1",
            "Test Score":null,
            "Params (M)":"59",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":"944"
        },
        "raw_metrics":{
            "Validation mIoU":48.1,
            "Test Score":null,
            "Params (M)":59.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":944.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1109218,
            "title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
            "url":"\/paper\/internimage-exploring-large-scale-vision",
            "published":"2022-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internimage-exploring-large-scale-vision\/review\/?hl=102381"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48163,
        "rank":139,
        "Model":"VAN-Large",
        "mlmodel":{

        },
        "method_short":"VAN-Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-20",
        "metrics":{
            "Validation mIoU":"48.1",
            "Test Score":null,
            "Params (M)":"49",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":48.1,
            "Test Score":null,
            "Params (M)":49.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":964888,
            "title":"Visual Attention Network",
            "url":"\/paper\/visual-attention-network",
            "published":"2022-02-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-attention-network\/review\/?hl=48163"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":35237,
        "rank":140,
        "Model":"XCiT-S24\/8 (UperNet)",
        "mlmodel":{

        },
        "method_short":"XCiT-S24\/8 ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-17",
        "metrics":{
            "Validation mIoU":"48.1",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":48.1,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":820317,
            "title":"XCiT: Cross-Covariance Image Transformers",
            "url":"\/paper\/xcit-cross-covariance-image-transformers",
            "published":"2021-06-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/xcit-cross-covariance-image-transformers\/review\/?hl=35237"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":42906,
        "rank":141,
        "Model":"MaskFormer(ResNet-101)",
        "mlmodel":{

        },
        "method_short":"MaskFormer",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-13",
        "metrics":{
            "Validation mIoU":"48.1",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":48.1,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":834841,
            "title":"Per-Pixel Classification is Not All You Need for Semantic Segmentation",
            "url":"\/paper\/per-pixel-classification-is-not-all-you-need",
            "published":"2021-07-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/per-pixel-classification-is-not-all-you-need\/review\/?hl=42906"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":44090,
        "rank":142,
        "Model":"MAE (ViT-B, UperNet)",
        "mlmodel":{

        },
        "method_short":"MAE ",
        "method_details":"ViT-B, UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-11",
        "metrics":{
            "Validation mIoU":"48.1",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":48.1,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":908690,
            "title":"Masked Autoencoders Are Scalable Vision Learners",
            "url":"\/paper\/masked-autoencoders-are-scalable-vision",
            "published":"2021-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-autoencoders-are-scalable-vision\/review\/?hl=44090"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57414,
        "rank":143,
        "Model":"HRNetV2 + OCR + RMI (PaddleClas pretrained)",
        "mlmodel":{

        },
        "method_short":"HRNetV2 + OCR + RMI ",
        "method_details":"PaddleClas pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-24",
        "metrics":{
            "Validation mIoU":"47.98",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":47.98,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":154800,
            "title":"Segmentation Transformer: Object-Contextual Representations for Semantic Segmentation",
            "url":"\/paper\/object-contextual-representations-for",
            "published":"2019-09-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/object-contextual-representations-for\/review\/?hl=57414"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":46839,
        "rank":144,
        "Model":"Shift-B",
        "mlmodel":{

        },
        "method_short":"Shift-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-26",
        "metrics":{
            "Validation mIoU":"47.9",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":47.9,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":951213,
            "title":"When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
            "url":"\/paper\/when-shift-operation-meets-vision-transformer",
            "published":"2022-01-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-shift-operation-meets-vision-transformer\/review\/?hl=46839"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":46838,
        "rank":145,
        "Model":"Shift-S",
        "mlmodel":{

        },
        "method_short":"Shift-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-26",
        "metrics":{
            "Validation mIoU":"47.8",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":47.8,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":951213,
            "title":"When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
            "url":"\/paper\/when-shift-operation-meets-vision-transformer",
            "published":"2022-01-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-shift-operation-meets-vision-transformer\/review\/?hl=46838"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":99770,
        "rank":146,
        "Model":"MogaNet-S (Semantic FPN)",
        "mlmodel":{

        },
        "method_short":"MogaNet-S ",
        "method_details":"Semantic FPN",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-07",
        "metrics":{
            "Validation mIoU":"47.7",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":"189",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":47.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":189.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1106694,
            "title":"Efficient Multi-order Gated Aggregation Network",
            "url":"\/paper\/efficient-multi-order-gated-aggregation",
            "published":"2022-11-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficient-multi-order-gated-aggregation\/review\/?hl=99770"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48165,
        "rank":147,
        "Model":"SeMask (SeMask Swin-S FPN)",
        "mlmodel":{

        },
        "method_short":"SeMask ",
        "method_details":"SeMask Swin-S FPN",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "Validation mIoU":"47.63",
            "Test Score":null,
            "Params (M)":"56",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":47.63,
            "Test Score":null,
            "Params (M)":56.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932883,
            "title":"SeMask: Semantically Masked Transformers for Semantic Segmentation",
            "url":"\/paper\/semask-semantically-masked-transformers-for-1",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/semask-semantically-masked-transformers-for-1\/review\/?hl=48165"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":10929,
        "rank":148,
        "Model":"ResNeSt-269",
        "mlmodel":{

        },
        "method_short":"ResNeSt-269",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-19",
        "metrics":{
            "Validation mIoU":"47.60",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":47.6,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":191448,
            "title":"ResNeSt: Split-Attention Networks",
            "url":"\/paper\/resnest-split-attention-networks",
            "published":"2020-04-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resnest-split-attention-networks\/review\/?hl=10929"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57415,
        "rank":149,
        "Model":"UperNet Shuffle-T",
        "mlmodel":{

        },
        "method_short":"UperNet Shuffle-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-07",
        "metrics":{
            "Validation mIoU":"47.6",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":47.6,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":812458,
            "title":"Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer",
            "url":"\/paper\/shuffle-transformer-rethinking-spatial",
            "published":"2021-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/shuffle-transformer-rethinking-spatial\/review\/?hl=57415"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":39763,
        "rank":150,
        "Model":"CondNet(ResNest-101)",
        "mlmodel":{

        },
        "method_short":"CondNet",
        "method_details":"ResNest-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-21",
        "metrics":{
            "Validation mIoU":"47.54",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":47.54,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":871350,
            "title":"CondNet: Conditional Classifier for Scene Segmentation",
            "url":"\/paper\/condnet-conditional-classifier-for-scene",
            "published":"2021-09-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/condnet-conditional-classifier-for-scene\/review\/?hl=39763"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":101766,
        "rank":151,
        "Model":"tiny-MOAT-3 (IN-1K pretraining, single scale)",
        "mlmodel":{

        },
        "method_short":"tiny-MOAT-3 ",
        "method_details":"IN-1K pretraining, single scale",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-04",
        "metrics":{
            "Validation mIoU":"47.5",
            "Test Score":null,
            "Params (M)":"24",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":47.5,
            "Test Score":null,
            "Params (M)":24.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087041,
            "title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models",
            "url":"\/paper\/moat-alternating-mobile-convolution-and",
            "published":"2022-10-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moat-alternating-mobile-convolution-and\/review\/?hl=101766"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":39765,
        "rank":152,
        "Model":"CondNet(ResNet-101)",
        "mlmodel":{

        },
        "method_short":"CondNet",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-21",
        "metrics":{
            "Validation mIoU":"47.38",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":47.38,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":871350,
            "title":"CondNet: Conditional Classifier for Scene Segmentation",
            "url":"\/paper\/condnet-conditional-classifier-for-scene",
            "published":"2021-09-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/condnet-conditional-classifier-for-scene\/review\/?hl=39765"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":70632,
        "rank":153,
        "Model":"DiNAT-Mini (UperNet)",
        "mlmodel":{

        },
        "method_short":"DiNAT-Mini ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-29",
        "metrics":{
            "Validation mIoU":"47.2",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":47.2,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1082646,
            "title":"Dilated Neighborhood Attention Transformer",
            "url":"\/paper\/dilated-neighborhood-attention-transformer",
            "published":"2022-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dilated-neighborhood-attention-transformer\/review\/?hl=70632"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57416,
        "rank":154,
        "Model":"DCNAS",
        "mlmodel":{

        },
        "method_short":"DCNAS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-26",
        "metrics":{
            "Validation mIoU":"47.12",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":47.12,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188386,
            "title":"DCNAS: Densely Connected Neural Architecture Search for Semantic Image Segmentation",
            "url":"\/paper\/dcnas-densely-connected-neural-architecture",
            "published":"2020-03-26T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/dcnas-densely-connected-neural-architecture\/review\/?hl=57416"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":35236,
        "rank":155,
        "Model":"XCiT-S24\/8 (Semantic-FPN)",
        "mlmodel":{

        },
        "method_short":"XCiT-S24\/8 ",
        "method_details":"Semantic-FPN",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-17",
        "metrics":{
            "Validation mIoU":"47.1",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":47.1,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":820317,
            "title":"XCiT: Cross-Covariance Image Transformers",
            "url":"\/paper\/xcit-cross-covariance-image-transformers",
            "published":"2021-06-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/xcit-cross-covariance-image-transformers\/review\/?hl=35236"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":10930,
        "rank":156,
        "Model":"ResNeSt-101",
        "mlmodel":{

        },
        "method_short":"ResNeSt-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-19",
        "metrics":{
            "Validation mIoU":"46.91",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":46.91,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":191448,
            "title":"ResNeSt: Split-Attention Networks",
            "url":"\/paper\/resnest-split-attention-networks",
            "published":"2020-04-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resnest-split-attention-networks\/review\/?hl=10930"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":35224,
        "rank":157,
        "Model":"XCiT-M24\/8 (Semantic-FPN)",
        "mlmodel":{

        },
        "method_short":"XCiT-M24\/8 ",
        "method_details":"Semantic-FPN",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-17",
        "metrics":{
            "Validation mIoU":"46.9",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":46.9,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":820317,
            "title":"XCiT: Cross-Covariance Image Transformers",
            "url":"\/paper\/xcit-cross-covariance-image-transformers",
            "published":"2021-06-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/xcit-cross-covariance-image-transformers\/review\/?hl=35224"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48939,
        "rank":158,
        "Model":"HamNet (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"HamNet ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-09-28",
        "metrics":{
            "Validation mIoU":"46.8",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":46.8,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":864941,
            "title":"Is Attention Better Than Matrix Decomposition?",
            "url":"\/paper\/is-attention-better-than-matrix-decomposition-1",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/is-attention-better-than-matrix-decomposition-1\/review\/?hl=48939"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":88639,
        "rank":159,
        "Model":"Sequential Ensemble (DeepLabv3+)",
        "mlmodel":{

        },
        "method_short":"Sequential Ensemble ",
        "method_details":"DeepLabv3+",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-08",
        "metrics":{
            "Validation mIoU":"46.8",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":46.8,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1089604,
            "title":"Sequential Ensembling for Semantic Segmentation",
            "url":"\/paper\/sequential-ensembling-for-semantic",
            "published":"2022-10-08T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":54,
                "name":"ResNet-101",
                "color":"#cc1e1e"
            },
            {
                "id":51,
                "name":"DeepLab v3+",
                "color":"#8627d3"
            },
            {
                "id":284,
                "name":"Ensemble",
                "color":"#77bb41"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48154,
        "rank":160,
        "Model":"ConvNeXt-T",
        "mlmodel":{

        },
        "method_short":"ConvNeXt-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-10",
        "metrics":{
            "Validation mIoU":"46.7",
            "Test Score":null,
            "Params (M)":"60",
            "GFLOPs (512 x 512)":"939",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":46.7,
            "Test Score":null,
            "Params (M)":60.0,
            "GFLOPs (512 x 512)":939.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":942596,
            "title":"A ConvNet for the 2020s",
            "url":"\/paper\/a-convnet-for-the-2020s",
            "published":"2022-01-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-convnet-for-the-2020s\/review\/?hl=48154"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48176,
        "rank":161,
        "Model":"VAN-Base (Semantic-FPN)",
        "mlmodel":{

        },
        "method_short":"VAN-Base ",
        "method_details":"Semantic-FPN",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-20",
        "metrics":{
            "Validation mIoU":"46.7",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":46.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":964888,
            "title":"Visual Attention Network",
            "url":"\/paper\/visual-attention-network",
            "published":"2022-02-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-attention-network\/review\/?hl=48176"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":35235,
        "rank":162,
        "Model":"XCiT-S12\/8 (UperNet)",
        "mlmodel":{

        },
        "method_short":"XCiT-S12\/8 ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-17",
        "metrics":{
            "Validation mIoU":"46.6",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":46.6,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":820317,
            "title":"XCiT: Cross-Covariance Image Transformers",
            "url":"\/paper\/xcit-cross-covariance-image-transformers",
            "published":"2021-06-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/xcit-cross-covariance-image-transformers\/review\/?hl=35235"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":59101,
        "rank":163,
        "Model":"GC ViT-T",
        "mlmodel":{

        },
        "method_short":"GC ViT-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-20",
        "metrics":{
            "Validation mIoU":"46.5",
            "Test Score":null,
            "Params (M)":"58",
            "GFLOPs (512 x 512)":"947",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":46.5,
            "Test Score":null,
            "Params (M)":58.0,
            "GFLOPs (512 x 512)":947.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1029616,
            "title":"Global Context Vision Transformers",
            "url":"\/paper\/global-context-vision-transformers",
            "published":"2022-06-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/global-context-vision-transformers\/review\/?hl=59101"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":52497,
        "rank":164,
        "Model":"NAT-Mini",
        "mlmodel":{

        },
        "method_short":"NAT-Mini",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Validation mIoU":"46.4",
            "Test Score":null,
            "Params (M)":"50",
            "GFLOPs (512 x 512)":"900",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":46.4,
            "Test Score":null,
            "Params (M)":50.0,
            "GFLOPs (512 x 512)":900.0,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994419,
            "title":"Neighborhood Attention Transformer",
            "url":"\/paper\/neighborhood-attention-transformer",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/neighborhood-attention-transformer\/review\/?hl=52497"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":266,
                "name":"NAT Transformer",
                "color":"#574896"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":265,
                "name":"Neighborhood Attention",
                "color":"#ba1330"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":46837,
        "rank":165,
        "Model":"Shift-T",
        "mlmodel":{

        },
        "method_short":"Shift-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-26",
        "metrics":{
            "Validation mIoU":"46.3",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":46.3,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":951213,
            "title":"When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
            "url":"\/paper\/when-shift-operation-meets-vision-transformer",
            "published":"2022-01-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-shift-operation-meets-vision-transformer\/review\/?hl=46837"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":51985,
        "rank":166,
        "Model":"DaViT-T",
        "mlmodel":{

        },
        "method_short":"DaViT-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "Validation mIoU":"46.3",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":46.3,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":990738,
            "title":"DaViT: Dual Attention Vision Transformers",
            "url":"\/paper\/davit-dual-attention-vision-transformers",
            "published":"2022-04-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/davit-dual-attention-vision-transformers\/review\/?hl=51985"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":10618,
        "rank":167,
        "Model":"CPN(ResNet-101)",
        "mlmodel":{

        },
        "method_short":"CPN",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-03",
        "metrics":{
            "Validation mIoU":"46.27",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":46.27,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":189418,
            "title":"Context Prior for Scene Segmentation",
            "url":"\/paper\/context-prior-for-scene-segmentation",
            "published":"2020-04-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/context-prior-for-scene-segmentation\/review\/?hl=10618"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":113222,
        "rank":168,
        "Model":"MultiMAE (ViT-B)",
        "mlmodel":{

        },
        "method_short":"MultiMAE ",
        "method_details":"ViT-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Validation mIoU":"46.2",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":46.2,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988362,
            "title":"MultiMAE: Multi-modal Multi-task Masked Autoencoders",
            "url":"\/paper\/multimae-multi-modal-multi-task-masked",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multimae-multi-modal-multi-task-masked\/review\/?hl=113222"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":18913,
        "rank":169,
        "Model":"DRAN(ResNet-101)",
        "mlmodel":{

        },
        "method_short":"DRAN",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-05",
        "metrics":{
            "Validation mIoU":"46.18",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":46.18,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":211688,
            "title":"Scene Segmentation with Dual Relation-aware Attention Network",
            "url":"\/paper\/scene-segmentation-with-dual-relation-aware",
            "published":"2020-08-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":17488,
        "rank":170,
        "Model":"PyConvSegNet-152",
        "mlmodel":{

        },
        "method_short":"PyConvSegNet-152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-20",
        "metrics":{
            "Validation mIoU":"45.99",
            "Test Score":"56.52",
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":45.99,
            "Test Score":56.52,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":205105,
            "title":"Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition",
            "url":"\/paper\/pyramidal-convolution-rethinking",
            "published":"2020-06-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pyramidal-convolution-rethinking\/review\/?hl=17488"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57419,
        "rank":171,
        "Model":"DNL",
        "mlmodel":{

        },
        "method_short":"DNL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-11",
        "metrics":{
            "Validation mIoU":"45.97",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":45.97,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":201700,
            "title":"Disentangled Non-Local Neural Networks",
            "url":"\/paper\/disentangled-non-local-neural-networks",
            "published":"2020-06-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/disentangled-non-local-neural-networks\/review\/?hl=57419"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57420,
        "rank":172,
        "Model":"ACNet (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"ACNet ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-05",
        "metrics":{
            "Validation mIoU":"45.90",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":45.9,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":169189,
            "title":"Adaptive Context Network for Scene Parsing",
            "url":"\/paper\/adaptive-context-network-for-scene-parsing-1",
            "published":"2019-11-05T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/adaptive-context-network-for-scene-parsing-1\/review\/?hl=57420"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57421,
        "rank":173,
        "Model":"ACNet\n(ResNet-101)",
        "mlmodel":{

        },
        "method_short":"ACNet\n",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-05",
        "metrics":{
            "Validation mIoU":"45.90",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":45.9,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":169189,
            "title":"Adaptive Context Network for Scene Parsing",
            "url":"\/paper\/adaptive-context-network-for-scene-parsing-1",
            "published":"2019-11-05T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/adaptive-context-network-for-scene-parsing-1\/review\/?hl=57421"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":56449,
        "rank":174,
        "Model":"HRViT-b1 (SegFormer, SS)",
        "mlmodel":{

        },
        "method_short":"HRViT-b1 ",
        "method_details":"SegFormer, SS",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-01",
        "metrics":{
            "Validation mIoU":"45.88",
            "Test Score":null,
            "Params (M)":"8.2",
            "GFLOPs (512 x 512)":"14.6",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":45.88,
            "Test Score":null,
            "Params (M)":8.2,
            "GFLOPs (512 x 512)":14.6,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":899298,
            "title":"Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation",
            "url":"\/paper\/hrvit-multi-scale-high-resolution-vision",
            "published":"2021-11-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hrvit-multi-scale-high-resolution-vision\/review\/?hl=56449"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":10,
                "name":"multiscale",
                "color":"#ba1330"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57422,
        "rank":175,
        "Model":"OCR(HRNetV2-W48)",
        "mlmodel":{

        },
        "method_short":"OCR",
        "method_details":"HRNetV2-W48",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-24",
        "metrics":{
            "Validation mIoU":"45.66",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":45.66,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":154800,
            "title":"Segmentation Transformer: Object-Contextual Representations for Semantic Segmentation",
            "url":"\/paper\/object-contextual-representations-for",
            "published":"2019-09-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/object-contextual-representations-for\/review\/?hl=57422"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":29009,
        "rank":176,
        "Model":"SPNet (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"SPNet ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-30",
        "metrics":{
            "Validation mIoU":"45.6",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":45.6,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188621,
            "title":"Strip Pooling: Rethinking Spatial Pooling for Scene Parsing",
            "url":"\/paper\/2003-13328",
            "published":"2020-03-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2003-13328\/review\/?hl=29009"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":32071,
        "rank":177,
        "Model":"Swin-T (UPerNet) MoBY",
        "mlmodel":{

        },
        "method_short":"Swin-T ",
        "method_details":"UPerNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-10",
        "metrics":{
            "Validation mIoU":"45.58",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":45.58,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":796086,
            "title":"Self-Supervised Learning with Swin Transformers",
            "url":"\/paper\/self-supervised-learning-with-swin",
            "published":"2021-05-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-supervised-learning-with-swin\/review\/?hl=32071"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48171,
        "rank":178,
        "Model":"DAT-T (UperNet)",
        "mlmodel":{

        },
        "method_short":"DAT-T ",
        "method_details":"UperNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-03",
        "metrics":{
            "Validation mIoU":"45.54",
            "Test Score":null,
            "Params (M)":"60",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":45.54,
            "Test Score":null,
            "Params (M)":60.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":934161,
            "title":"Vision Transformer with Deformable Attention",
            "url":"\/paper\/vision-transformer-with-deformable-attention",
            "published":"2022-01-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-transformer-with-deformable-attention\/review\/?hl=48171"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":88798,
        "rank":179,
        "Model":"iBOT (ViT-S\/16)",
        "mlmodel":{

        },
        "method_short":"iBOT ",
        "method_details":"ViT-S\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-15",
        "metrics":{
            "Validation mIoU":"45.4",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":45.4,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":909858,
            "title":"iBOT: Image BERT Pre-Training with Online Tokenizer",
            "url":"\/paper\/ibot-image-bert-pre-training-with-online",
            "published":"2021-11-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57423,
        "rank":180,
        "Model":"EANet\n(ResNet-101)",
        "mlmodel":{

        },
        "method_short":"EANet\n",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-05",
        "metrics":{
            "Validation mIoU":"45.33",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":45.33,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":794722,
            "title":"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks",
            "url":"\/paper\/beyond-self-attention-external-attention",
            "published":"2021-05-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/beyond-self-attention-external-attention\/review\/?hl=57423"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":96696,
        "rank":181,
        "Model":"OCR (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"OCR ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-24",
        "metrics":{
            "Validation mIoU":"45.28",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":45.28,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":154800,
            "title":"Segmentation Transformer: Object-Contextual Representations for Semantic Segmentation",
            "url":"\/paper\/object-contextual-representations-for",
            "published":"2019-09-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/object-contextual-representations-for\/review\/?hl=96696"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57425,
        "rank":182,
        "Model":"Asymmetric ALNN",
        "mlmodel":{

        },
        "method_short":"Asymmetric ALNN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-21",
        "metrics":{
            "Validation mIoU":"45.24",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":45.24,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":150786,
            "title":"Asymmetric Non-local Neural Networks for Semantic Segmentation",
            "url":"\/paper\/asymmetric-non-local-neural-networks-for",
            "published":"2019-08-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/asymmetric-non-local-neural-networks-for\/review\/?hl=57425"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":52552,
        "rank":183,
        "Model":"Light-Ham (VAN-Small, D=256)",
        "mlmodel":{

        },
        "method_short":"Light-Ham ",
        "method_details":"VAN-Small, D=256",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-26",
        "metrics":{
            "Validation mIoU":"45.2",
            "Test Score":null,
            "Params (M)":"13.8",
            "GFLOPs (512 x 512)":"15.8",
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":45.2,
            "Test Score":null,
            "Params (M)":13.8,
            "GFLOPs (512 x 512)":15.8,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":864941,
            "title":"Is Attention Better Than Matrix Decomposition?",
            "url":"\/paper\/is-attention-better-than-matrix-decomposition-1",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/is-attention-better-than-matrix-decomposition-1\/review\/?hl=52552"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":11816,
        "rank":184,
        "Model":"LaU-regression-loss",
        "mlmodel":{

        },
        "method_short":"LaU-regression-loss",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-13",
        "metrics":{
            "Validation mIoU":"45.02",
            "Test Score":"56.32",
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":45.02,
            "Test Score":56.32,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":172214,
            "title":"Location-aware Upsampling for Semantic Segmentation",
            "url":"\/paper\/location-aware-upsampling-for-semantic",
            "published":"2019-11-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/location-aware-upsampling-for-semantic\/review\/?hl=11816"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":4440,
        "rank":185,
        "Model":"PSPNet",
        "mlmodel":{

        },
        "method_short":"PSPNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-12-04",
        "metrics":{
            "Validation mIoU":"44.94",
            "Test Score":"55.38",
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":44.94,
            "Test Score":55.38,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":23531,
            "title":"Pyramid Scene Parsing Network",
            "url":"\/paper\/pyramid-scene-parsing-network",
            "published":"2016-12-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pyramid-scene-parsing-network\/review\/?hl=4440"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":101767,
        "rank":186,
        "Model":"tiny-MOAT-2 (IN-1K pretraining, single scale)",
        "mlmodel":{

        },
        "method_short":"tiny-MOAT-2 ",
        "method_details":"IN-1K pretraining, single scale",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-04",
        "metrics":{
            "Validation mIoU":"44.9",
            "Test Score":null,
            "Params (M)":"13",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":44.9,
            "Test Score":null,
            "Params (M)":13.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087041,
            "title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models",
            "url":"\/paper\/moat-alternating-mobile-convolution-and",
            "published":"2022-10-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moat-alternating-mobile-convolution-and\/review\/?hl=101767"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":10932,
        "rank":187,
        "Model":"CFNet(ResNet-101)",
        "mlmodel":{

        },
        "method_short":"CFNet",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-01",
        "metrics":{
            "Validation mIoU":"44.89",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":44.89,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":115260,
            "title":"Co-Occurrent Features in Semantic Segmentation",
            "url":"\/paper\/co-occurrent-features-in-semantic",
            "published":"2019-06-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":1936,
        "rank":188,
        "Model":"EncNet",
        "mlmodel":{

        },
        "method_short":"EncNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-03-23",
        "metrics":{
            "Validation mIoU":"44.65",
            "Test Score":"55.67",
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":44.65,
            "Test Score":55.67,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":7613,
            "title":"Context Encoding for Semantic Segmentation",
            "url":"\/paper\/context-encoding-for-semantic-segmentation",
            "published":"2018-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/context-encoding-for-semantic-segmentation\/review\/?hl=1936"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":11817,
        "rank":189,
        "Model":"LaU-offset-loss",
        "mlmodel":{

        },
        "method_short":"LaU-offset-loss",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-13",
        "metrics":{
            "Validation mIoU":"44.55",
            "Test Score":"56.41",
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":44.55,
            "Test Score":56.41,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":172214,
            "title":"Location-aware Upsampling for Semantic Segmentation",
            "url":"\/paper\/location-aware-upsampling-for-semantic",
            "published":"2019-11-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/location-aware-upsampling-for-semantic\/review\/?hl=11817"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":4439,
        "rank":190,
        "Model":"EncNet + JPU",
        "mlmodel":{

        },
        "method_short":"EncNet + JPU",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-03-28",
        "metrics":{
            "Validation mIoU":"44.34",
            "Test Score":"55.84",
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":44.34,
            "Test Score":55.84,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":109768,
            "title":"FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation",
            "url":"\/paper\/fastfcn-rethinking-dilated-convolution-in-the",
            "published":"2019-03-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fastfcn-rethinking-dilated-convolution-in-the\/review\/?hl=4439"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57426,
        "rank":191,
        "Model":"SGR (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"SGR ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-01",
        "metrics":{
            "Validation mIoU":"44.32",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":44.32,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":76200,
            "title":"Symbolic Graph Reasoning Meets Convolutions",
            "url":"\/paper\/symbolic-graph-reasoning-meets-convolutions",
            "published":"2018-12-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":35234,
        "rank":192,
        "Model":"XCiT-S12\/8 (Semantic-FPN)",
        "mlmodel":{

        },
        "method_short":"XCiT-S12\/8 ",
        "method_details":"Semantic-FPN",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-17",
        "metrics":{
            "Validation mIoU":"44.2",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":44.2,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":820317,
            "title":"XCiT: Cross-Covariance Image Transformers",
            "url":"\/paper\/xcit-cross-covariance-image-transformers",
            "published":"2021-06-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/xcit-cross-covariance-image-transformers\/review\/?hl=35234"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":8421,
        "rank":193,
        "Model":"Auto-DeepLab-L",
        "mlmodel":{

        },
        "method_short":"Auto-DeepLab-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-01-10",
        "metrics":{
            "Validation mIoU":"43.98",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":43.98,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":87202,
            "title":"Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation",
            "url":"\/paper\/auto-deeplab-hierarchical-neural-architecture",
            "published":"2019-01-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/auto-deeplab-hierarchical-neural-architecture\/review\/?hl=8421"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57427,
        "rank":194,
        "Model":"PSANet (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"PSANet ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-09-01",
        "metrics":{
            "Validation mIoU":"43.77",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":43.77,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":82082,
            "title":"PSANet: Point-wise Spatial Attention Network for Scene Parsing",
            "url":"\/paper\/psanet-point-wise-spatial-attention-network",
            "published":"2018-09-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57428,
        "rank":195,
        "Model":"DSSPN (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"DSSPN ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-03-16",
        "metrics":{
            "Validation mIoU":"43.68",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":43.68,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":8170,
            "title":"Dynamic-structured Semantic Propagation Network",
            "url":"\/paper\/dynamic-structured-semantic-propagation",
            "published":"2018-03-16T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/dynamic-structured-semantic-propagation\/review\/?hl=57428"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":102140,
        "rank":196,
        "Model":"PSPNet (ResNet-152)",
        "mlmodel":{

        },
        "method_short":"PSPNet ",
        "method_details":"ResNet-152",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-12-04",
        "metrics":{
            "Validation mIoU":"43.51",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":43.51,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":23531,
            "title":"Pyramid Scene Parsing Network",
            "url":"\/paper\/pyramid-scene-parsing-network",
            "published":"2016-12-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pyramid-scene-parsing-network\/review\/?hl=102140"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57430,
        "rank":197,
        "Model":"PSPNet\n(ResNet-101)",
        "mlmodel":{

        },
        "method_short":"PSPNet\n",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-12-04",
        "metrics":{
            "Validation mIoU":"43.29",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":43.29,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":23531,
            "title":"Pyramid Scene Parsing Network",
            "url":"\/paper\/pyramid-scene-parsing-network",
            "published":"2016-12-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pyramid-scene-parsing-network\/review\/?hl=57430"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":6209,
        "rank":198,
        "Model":"HRNetV2",
        "mlmodel":{

        },
        "method_short":"HRNetV2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-09",
        "metrics":{
            "Validation mIoU":"43.2",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":43.2,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":111122,
            "title":"High-Resolution Representations for Labeling Pixels and Regions",
            "url":"\/paper\/high-resolution-representations-for-labeling",
            "published":"2019-04-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/high-resolution-representations-for-labeling\/review\/?hl=6209"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48164,
        "rank":199,
        "Model":"SeMask (SeMask Swin-T FPN)",
        "mlmodel":{

        },
        "method_short":"SeMask ",
        "method_details":"SeMask Swin-T FPN",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "Validation mIoU":"43.16",
            "Test Score":null,
            "Params (M)":"35",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":43.16,
            "Test Score":null,
            "Params (M)":35.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932883,
            "title":"SeMask: Semantically Masked Transformers for Semantic Segmentation",
            "url":"\/paper\/semask-semantically-masked-transformers-for-1",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/semask-semantically-masked-transformers-for-1\/review\/?hl=48164"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":101768,
        "rank":200,
        "Model":"tiny-MOAT-1 (IN-1K pretraining, single scale)",
        "mlmodel":{

        },
        "method_short":"tiny-MOAT-1 ",
        "method_details":"IN-1K pretraining, single scale",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-04",
        "metrics":{
            "Validation mIoU":"43.1",
            "Test Score":null,
            "Params (M)":"8",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":43.1,
            "Test Score":null,
            "Params (M)":8.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087041,
            "title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models",
            "url":"\/paper\/moat-alternating-mobile-convolution-and",
            "published":"2022-10-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moat-alternating-mobile-convolution-and\/review\/?hl=101768"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48161,
        "rank":201,
        "Model":"VAN-Small",
        "mlmodel":{

        },
        "method_short":"VAN-Small",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-20",
        "metrics":{
            "Validation mIoU":"42.9",
            "Test Score":null,
            "Params (M)":"18",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":42.9,
            "Test Score":null,
            "Params (M)":18.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":964888,
            "title":"Visual Attention Network",
            "url":"\/paper\/visual-attention-network",
            "published":"2022-02-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-attention-network\/review\/?hl=48161"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":43406,
        "rank":202,
        "Model":"PoolFormer-M48",
        "mlmodel":{

        },
        "method_short":"PoolFormer-M48",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-22",
        "metrics":{
            "Validation mIoU":"42.7",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":42.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":914420,
            "title":"MetaFormer Is Actually What You Need for Vision",
            "url":"\/paper\/metaformer-is-actually-what-you-need-for",
            "published":"2021-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-is-actually-what-you-need-for\/review\/?hl=43406"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":57431,
        "rank":203,
        "Model":"UperNet (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"UperNet ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-07-26",
        "metrics":{
            "Validation mIoU":"42.66",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":42.66,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":53544,
            "title":"Unified Perceptual Parsing for Scene Understanding",
            "url":"\/paper\/unified-perceptual-parsing-for-scene",
            "published":"2018-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unified-perceptual-parsing-for-scene\/review\/?hl=57431"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":101769,
        "rank":204,
        "Model":"tiny-MOAT-0 (IN-1K pretraining, single scale)",
        "mlmodel":{

        },
        "method_short":"tiny-MOAT-0 ",
        "method_details":"IN-1K pretraining, single scale",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-04",
        "metrics":{
            "Validation mIoU":"41.2",
            "Test Score":null,
            "Params (M)":"6",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":41.2,
            "Test Score":null,
            "Params (M)":6.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087041,
            "title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models",
            "url":"\/paper\/moat-alternating-mobile-convolution-and",
            "published":"2022-10-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moat-alternating-mobile-convolution-and\/review\/?hl=101769"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":4441,
        "rank":205,
        "Model":"RefineNet",
        "mlmodel":{

        },
        "method_short":"RefineNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-11-20",
        "metrics":{
            "Validation mIoU":"40.7",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":40.7,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":28390,
            "title":"RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation",
            "url":"\/paper\/refinenet-multi-path-refinement-networks-for",
            "published":"2016-11-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/refinenet-multi-path-refinement-networks-for\/review\/?hl=4441"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":43137,
        "rank":206,
        "Model":"FBNetV5",
        "mlmodel":{

        },
        "method_short":"FBNetV5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-19",
        "metrics":{
            "Validation mIoU":"40.4",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":40.4,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912986,
            "title":"FBNetV5: Neural Architecture Search for Multiple Tasks in One Run",
            "url":"\/paper\/fbnetv5-neural-architecture-search-for",
            "published":"2021-11-19T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/fbnetv5-neural-architecture-search-for\/review\/?hl=43137"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":39267,
        "rank":207,
        "Model":"ConvMLP-L",
        "mlmodel":{

        },
        "method_short":"ConvMLP-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-09",
        "metrics":{
            "Validation mIoU":"40",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":40.0,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":864188,
            "title":"ConvMLP: Hierarchical Convolutional MLPs for Vision",
            "url":"\/paper\/convmlp-hierarchical-convolutional-mlps-for",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convmlp-hierarchical-convolutional-mlps-for\/review\/?hl=39267"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":39266,
        "rank":208,
        "Model":"ConvMLP-M",
        "mlmodel":{

        },
        "method_short":"ConvMLP-M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-09",
        "metrics":{
            "Validation mIoU":"38.6",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":38.6,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":864188,
            "title":"ConvMLP: Hierarchical Convolutional MLPs for Vision",
            "url":"\/paper\/convmlp-hierarchical-convolutional-mlps-for",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convmlp-hierarchical-convolutional-mlps-for\/review\/?hl=39266"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48160,
        "rank":209,
        "Model":"VAN-Tiny",
        "mlmodel":{

        },
        "method_short":"VAN-Tiny",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-20",
        "metrics":{
            "Validation mIoU":"38.5",
            "Test Score":null,
            "Params (M)":"8",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":38.5,
            "Test Score":null,
            "Params (M)":8.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":964888,
            "title":"Visual Attention Network",
            "url":"\/paper\/visual-attention-network",
            "published":"2022-02-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-attention-network\/review\/?hl=48160"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":78995,
        "rank":210,
        "Model":"A2MIM (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"A2MIM ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-27",
        "metrics":{
            "Validation mIoU":"38.3",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":38.3,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1017241,
            "title":"Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN",
            "url":"\/paper\/architecture-agnostic-masked-image-modeling",
            "published":"2022-05-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/architecture-agnostic-masked-image-modeling\/review\/?hl=78995"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":88801,
        "rank":211,
        "Model":"iBOT (ViT-B\/16) (linear head)",
        "mlmodel":{

        },
        "method_short":"iBOT ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-15",
        "metrics":{
            "Validation mIoU":"38.3",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":38.3,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":909858,
            "title":"iBOT: Image BERT Pre-Training with Online Tokenizer",
            "url":"\/paper\/ibot-image-bert-pre-training-with-online",
            "published":"2021-11-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":48177,
        "rank":212,
        "Model":"SegFormer-B0",
        "mlmodel":{

        },
        "method_short":"SegFormer-B0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Validation mIoU":"37.4",
            "Test Score":null,
            "Params (M)":"3.8",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":37.4,
            "Test Score":null,
            "Params (M)":3.8,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":808117,
            "title":"SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
            "url":"\/paper\/segformer-simple-and-efficient-design-for",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/segformer-simple-and-efficient-design-for\/review\/?hl=48177"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":16110,
        "rank":213,
        "Model":"MUXNet-m + PPM",
        "mlmodel":{

        },
        "method_short":"MUXNet-m + PPM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-31",
        "metrics":{
            "Validation mIoU":"35.8",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":35.8,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188988,
            "title":"MUXConv: Information Multiplexing in Convolutional Neural Networks",
            "url":"\/paper\/muxconv-information-multiplexing-in",
            "published":"2020-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/muxconv-information-multiplexing-in\/review\/?hl=16110"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":39265,
        "rank":214,
        "Model":"ConvMLP-S",
        "mlmodel":{

        },
        "method_short":"ConvMLP-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-09",
        "metrics":{
            "Validation mIoU":"35.8",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":35.8,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":864188,
            "title":"ConvMLP: Hierarchical Convolutional MLPs for Vision",
            "url":"\/paper\/convmlp-hierarchical-convolutional-mlps-for",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convmlp-hierarchical-convolutional-mlps-for\/review\/?hl=39265"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":16109,
        "rank":215,
        "Model":"MUXNet-m + C1",
        "mlmodel":{

        },
        "method_short":"MUXNet-m + C1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-31",
        "metrics":{
            "Validation mIoU":"32.42",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":32.42,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188988,
            "title":"MUXConv: Information Multiplexing in Convolutional Neural Networks",
            "url":"\/paper\/muxconv-information-multiplexing-in",
            "published":"2020-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/muxconv-information-multiplexing-in\/review\/?hl=16109"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":4442,
        "rank":216,
        "Model":"DilatedNet",
        "mlmodel":{

        },
        "method_short":"DilatedNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2015-11-23",
        "metrics":{
            "Validation mIoU":"32.31",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":32.31,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":34275,
            "title":"Multi-Scale Context Aggregation by Dilated Convolutions",
            "url":"\/paper\/multi-scale-context-aggregation-by-dilated",
            "published":"2015-11-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multi-scale-context-aggregation-by-dilated\/review\/?hl=4442"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":4444,
        "rank":217,
        "Model":"FCN",
        "mlmodel":{

        },
        "method_short":"FCN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2014-11-14",
        "metrics":{
            "Validation mIoU":"29.39",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":29.39,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":41777,
            "title":"Fully Convolutional Networks for Semantic Segmentation",
            "url":"\/paper\/fully-convolutional-networks-for-semantic-1",
            "published":"2014-11-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fully-convolutional-networks-for-semantic-1\/review\/?hl=4444"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":4443,
        "rank":218,
        "Model":"SegNet",
        "mlmodel":{

        },
        "method_short":"SegNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2015-11-02",
        "metrics":{
            "Validation mIoU":"21.64",
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":21.64,
            "Test Score":null,
            "Params (M)":null,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":29800,
            "title":"SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation",
            "url":"\/paper\/segnet-a-deep-convolutional-encoder-decoder",
            "published":"2015-11-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/segnet-a-deep-convolutional-encoder-decoder\/review\/?hl=4443"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":188,
        "row_id":77881,
        "rank":219,
        "Model":"InternImage-H (M3I Pre-training)",
        "mlmodel":{

        },
        "method_short":"InternImage-H ",
        "method_details":"M3I Pre-training",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-10",
        "metrics":{
            "Validation mIoU":null,
            "Test Score":null,
            "Params (M)":"1310",
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Validation mIoU":null,
            "Test Score":null,
            "Params (M)":1310.0,
            "GFLOPs (512 x 512)":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1109218,
            "title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
            "url":"\/paper\/internimage-exploring-large-scale-vision",
            "published":"2022-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internimage-exploring-large-scale-vision\/review\/?hl=77881"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":14,
                "name":"DCN",
                "color":"#2771D3"
            },
            {
                "id":337,
                "name":"Deformable Convolution",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    }
]