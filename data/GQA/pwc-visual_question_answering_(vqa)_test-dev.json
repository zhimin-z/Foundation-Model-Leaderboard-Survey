[
    {
        "table_id":2563,
        "row_id":40529,
        "rank":1,
        "Model":"CFR",
        "mlmodel":{

        },
        "method_short":"CFR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-06",
        "metrics":{
            "Accuracy":"72.1"
        },
        "raw_metrics":{
            "Accuracy":72.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":880243,
            "title":"Coarse-to-Fine Reasoning for Visual Question Answering",
            "url":"\/paper\/coarse-to-fine-reasoning-for-visual-question",
            "published":"2021-10-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coarse-to-fine-reasoning-for-visual-question\/review\/?hl=40529"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2563,
        "row_id":113811,
        "rank":2,
        "Model":"PaLI-X-VPD",
        "mlmodel":{

        },
        "method_short":"PaLI-X-VPD",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-12-05",
        "metrics":{
            "Accuracy":"67.3"
        },
        "raw_metrics":{
            "Accuracy":67.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1337954,
            "title":"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models",
            "url":"\/paper\/visual-program-distillation-distilling-tools",
            "published":"2023-12-05T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/visual-program-distillation-distilling-tools\/review\/?hl=113811"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2563,
        "row_id":14825,
        "rank":3,
        "Model":"NSM",
        "mlmodel":{

        },
        "method_short":"NSM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-09",
        "metrics":{
            "Accuracy":"62.95"
        },
        "raw_metrics":{
            "Accuracy":62.95
        },
        "uses_additional_data":false,
        "paper":{
            "id":145207,
            "title":"Learning by Abstraction: The Neural State Machine",
            "url":"\/paper\/learning-by-abstraction-the-neural-state",
            "published":"2019-07-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-by-abstraction-the-neural-state\/review\/?hl=14825"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2563,
        "row_id":14826,
        "rank":4,
        "Model":"LXMERT (Pre-train + scratch)",
        "mlmodel":{

        },
        "method_short":"LXMERT ",
        "method_details":"Pre-train + scratch",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-20",
        "metrics":{
            "Accuracy":"60.0"
        },
        "raw_metrics":{
            "Accuracy":60.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":150646,
            "title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
            "url":"\/paper\/lxmert-learning-cross-modality-encoder",
            "published":"2019-08-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lxmert-learning-cross-modality-encoder\/review\/?hl=14826"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2563,
        "row_id":25579,
        "rank":5,
        "Model":"single-hop + LCGN (ours)",
        "mlmodel":{

        },
        "method_short":"single-hop + LCGN ",
        "method_details":"ours",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-10",
        "metrics":{
            "Accuracy":"55.8"
        },
        "raw_metrics":{
            "Accuracy":55.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":114420,
            "title":"Language-Conditioned Graph Networks for Relational Reasoning",
            "url":"\/paper\/language-conditioned-graph-networks-for",
            "published":"2019-05-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-conditioned-graph-networks-for\/review\/?hl=25579"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2563,
        "row_id":96158,
        "rank":6,
        "Model":"BLIP-2 ViT-G FlanT5 XXL (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G FlanT5 XXL ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"44.7"
        },
        "raw_metrics":{
            "Accuracy":44.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96158"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2563,
        "row_id":96156,
        "rank":7,
        "Model":"BLIP-2 ViT-L FlanT5 XL (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-L FlanT5 XL ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"44.4"
        },
        "raw_metrics":{
            "Accuracy":44.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96156"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2563,
        "row_id":96157,
        "rank":8,
        "Model":"BLIP-2 ViT-G FlanT5 XL (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G FlanT5 XL ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"44.2"
        },
        "raw_metrics":{
            "Accuracy":44.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96157"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2563,
        "row_id":73807,
        "rank":9,
        "Model":"PNP-VQA",
        "mlmodel":{

        },
        "method_short":"PNP-VQA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-17",
        "metrics":{
            "Accuracy":"41.9"
        },
        "raw_metrics":{
            "Accuracy":41.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":1094335,
            "title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training",
            "url":"\/paper\/plug-and-play-vqa-zero-shot-vqa-by-conjoining",
            "published":"2022-10-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/plug-and-play-vqa-zero-shot-vqa-by-conjoining\/review\/?hl=73807"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2563,
        "row_id":96155,
        "rank":10,
        "Model":"BLIP-2 ViT-G OPT 6.7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G OPT 6.7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"36.4"
        },
        "raw_metrics":{
            "Accuracy":36.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96155"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2563,
        "row_id":96154,
        "rank":11,
        "Model":"BLIP-2 ViT-G OPT 2.7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G OPT 2.7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"34.6"
        },
        "raw_metrics":{
            "Accuracy":34.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96154"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2563,
        "row_id":96153,
        "rank":12,
        "Model":"BLIP-2 ViT-L OPT 2.7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-L OPT 2.7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Accuracy":"33.9"
        },
        "raw_metrics":{
            "Accuracy":33.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96153"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2563,
        "row_id":96152,
        "rank":13,
        "Model":"FewVLM (zero-shot)",
        "mlmodel":{

        },
        "method_short":"FewVLM ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-16",
        "metrics":{
            "Accuracy":"29.3"
        },
        "raw_metrics":{
            "Accuracy":29.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":890167,
            "title":"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models",
            "url":"\/paper\/a-good-prompt-is-worth-millions-of-parameters",
            "published":"2021-10-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-good-prompt-is-worth-millions-of-parameters\/review\/?hl=96152"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]