[
    {
        "table_id":4401,
        "row_id":70640,
        "rank":1,
        "method":"HunYuan_tvr (huge)",
        "mlmodel":{

        },
        "method_short":"HunYuan_tvr ",
        "method_details":"huge",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "text-to-video R@1":"62.9",
            "text-to-video R@5":"84.5",
            "text-to-video R@10":"90.8",
            "text-to-video Median Rank":"1.0",
            "text-to-video Mean Rank":"9.3 ",
            "video-to-text R@1":"64.8",
            "video-to-text R@5":"84.9",
            "video-to-text R@10":"91.1",
            "video-to-text Median Rank":"1.0",
            "video-to-text Mean Rank":"5.5"
        },
        "raw_metrics":{
            "text-to-video R@1":62.9,
            "text-to-video R@5":84.5,
            "text-to-video R@10":90.8,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":9.3,
            "video-to-text R@1":64.8,
            "video-to-text R@5":84.9,
            "video-to-text R@10":91.1,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":5.5
        },
        "uses_additional_data":true,
        "paper":{
            "id":990765,
            "title":"Tencent Text-Video Retrieval: Hierarchical Cross-Modal Interactions with Multi-Level Representations",
            "url":"\/paper\/hunyuan-tvr-for-text-video-retrivial",
            "published":"2022-04-07T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":68567,
        "rank":2,
        "method":"CLIP-ViP",
        "mlmodel":{

        },
        "method_short":"CLIP-ViP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-14",
        "metrics":{
            "text-to-video R@1":"57.7",
            "text-to-video R@5":"80.5",
            "text-to-video R@10":"88.2",
            "text-to-video Median Rank":"1.0",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":57.7,
            "text-to-video R@5":80.5,
            "text-to-video R@10":88.2,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1074966,
            "title":"CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment",
            "url":"\/paper\/clip-vip-adapting-pre-trained-image-text",
            "published":"2022-09-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clip-vip-adapting-pre-trained-image-text\/review\/?hl=68567"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":109128,
        "rank":3,
        "method":"DMAE\n(ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"DMAE\n",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-20",
        "metrics":{
            "text-to-video R@1":"55.5",
            "text-to-video R@5":"79.4",
            "text-to-video R@10":"87.1",
            "text-to-video Median Rank":"1.0",
            "text-to-video Mean Rank":"10.0",
            "video-to-text R@1":"55.7",
            "video-to-text R@5":"79.2",
            "video-to-text R@10":"87.2",
            "video-to-text Median Rank":"1.0",
            "video-to-text Mean Rank":"7.3"
        },
        "raw_metrics":{
            "text-to-video R@1":55.5,
            "text-to-video R@5":79.4,
            "text-to-video R@10":87.1,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":10.0,
            "video-to-text R@1":55.7,
            "video-to-text R@5":79.2,
            "video-to-text R@10":87.2,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":7.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1284092,
            "title":"Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning",
            "url":"\/paper\/dual-modal-attention-enhanced-text-video",
            "published":"2023-09-20T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/dual-modal-attention-enhanced-text-video\/review\/?hl=109128"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":64220,
        "rank":4,
        "method":"HunYuan_tvr",
        "mlmodel":{

        },
        "method_short":"HunYuan_tvr",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "text-to-video R@1":"55.0",
            "text-to-video R@5":null,
            "text-to-video R@10":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":"55.5",
            "video-to-text R@5":"78.4",
            "video-to-text R@10":"85.8",
            "video-to-text Median Rank":"1.0",
            "video-to-text Mean Rank":"7.7"
        },
        "raw_metrics":{
            "text-to-video R@1":55.0,
            "text-to-video R@5":null,
            "text-to-video R@10":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":55.5,
            "video-to-text R@5":78.4,
            "video-to-text R@10":85.8,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":7.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":990765,
            "title":"Tencent Text-Video Retrieval: Hierarchical Cross-Modal Interactions with Multi-Level Representations",
            "url":"\/paper\/hunyuan-tvr-for-text-video-retrivial",
            "published":"2022-04-07T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":99062,
        "rank":5,
        "method":"MuLTI",
        "mlmodel":{

        },
        "method_short":"MuLTI",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-10",
        "metrics":{
            "text-to-video R@1":"54.7",
            "text-to-video R@5":"77.7",
            "text-to-video R@10":"86.0",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":54.7,
            "text-to-video R@5":77.7,
            "text-to-video R@10":86.0,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1171789,
            "title":"MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler and Multiple Choice Modeling",
            "url":"\/paper\/multi-efficient-video-and-language",
            "published":"2023-03-10T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/multi-efficient-video-and-language\/review\/?hl=99062"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":97495,
        "rank":6,
        "method":"STAN",
        "mlmodel":{

        },
        "method_short":"STAN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-26",
        "metrics":{
            "text-to-video R@1":"54.1",
            "text-to-video R@5":"79.5",
            "text-to-video R@10":"87.8",
            "text-to-video Median Rank":"1",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":54.1,
            "text-to-video R@5":79.5,
            "text-to-video R@10":87.8,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1147773,
            "title":"Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring",
            "url":"\/paper\/revisiting-temporal-modeling-for-clip-based",
            "published":"2023-01-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revisiting-temporal-modeling-for-clip-based\/review\/?hl=97495"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":60003,
        "rank":7,
        "method":"TS2-Net",
        "mlmodel":{

        },
        "method_short":"TS2-Net",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-16",
        "metrics":{
            "text-to-video R@1":"54.0",
            "text-to-video R@5":"79.3",
            "text-to-video R@10":"87.4",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":54.0,
            "text-to-video R@5":79.3,
            "text-to-video R@10":87.4,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1045033,
            "title":"TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval",
            "url":"\/paper\/ts2-net-token-shift-and-selection-transformer",
            "published":"2022-07-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ts2-net-token-shift-and-selection-transformer\/review\/?hl=60003"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":49616,
        "rank":8,
        "method":"DRL",
        "mlmodel":{

        },
        "method_short":"DRL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-14",
        "metrics":{
            "text-to-video R@1":"53.3",
            "text-to-video R@5":"80.3",
            "text-to-video R@10":"87.6",
            "text-to-video Median Rank":"1",
            "text-to-video Mean Rank":"11.4",
            "video-to-text R@1":"56.2",
            "video-to-text R@5":"79.9",
            "video-to-text R@10":"87.4",
            "video-to-text Median Rank":"1.0",
            "video-to-text Mean Rank":"7.6"
        },
        "raw_metrics":{
            "text-to-video R@1":53.3,
            "text-to-video R@5":80.3,
            "text-to-video R@10":87.6,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":11.4,
            "video-to-text R@1":56.2,
            "video-to-text R@5":79.9,
            "video-to-text R@10":87.4,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":7.6
        },
        "uses_additional_data":true,
        "paper":{
            "id":976257,
            "title":"Disentangled Representation Learning for Text-Video Retrieval",
            "url":"\/paper\/disentangled-representation-learning-for-text",
            "published":"2022-03-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/disentangled-representation-learning-for-text\/review\/?hl=49616"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":96437,
        "rank":9,
        "method":"mPLUG-2",
        "mlmodel":{

        },
        "method_short":"mPLUG-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-01",
        "metrics":{
            "text-to-video R@1":"53.1",
            "text-to-video R@5":"77.6",
            "text-to-video R@10":"84.7",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":53.1,
            "text-to-video R@5":77.6,
            "text-to-video R@10":84.7,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1151002,
            "title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",
            "url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation",
            "published":"2023-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation\/review\/?hl=96437"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":35394,
        "rank":10,
        "method":"CLIP2TV",
        "mlmodel":{

        },
        "method_short":"CLIP2TV",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-10",
        "metrics":{
            "text-to-video R@1":"52.9",
            "text-to-video R@5":"78.5",
            "text-to-video R@10":"86.5",
            "text-to-video Median Rank":"1",
            "text-to-video Mean Rank":"12.8",
            "video-to-text R@1":"54.1",
            "video-to-text R@5":"77.4",
            "video-to-text R@10":"85.7",
            "video-to-text Median Rank":"1",
            "video-to-text Mean Rank":"9.0"
        },
        "raw_metrics":{
            "text-to-video R@1":52.9,
            "text-to-video R@5":78.5,
            "text-to-video R@10":86.5,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":12.8,
            "video-to-text R@1":54.1,
            "video-to-text R@5":77.4,
            "video-to-text R@10":85.7,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":9.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":908143,
            "title":"CLIP2TV: Align, Match and Distill for Video-Text Retrieval",
            "url":"\/paper\/clip2tv-an-empirical-study-on-transformer",
            "published":"2021-11-10T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/clip2tv-an-empirical-study-on-transformer\/review\/?hl=35394"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":112696,
        "rank":11,
        "method":"Side4Video",
        "mlmodel":{

        },
        "method_short":"Side4Video",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "text-to-video R@1":"52.3",
            "text-to-video R@5":"75.5",
            "text-to-video R@10":"84.2",
            "text-to-video Median Rank":"1.0",
            "text-to-video Mean Rank":"12.8",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":52.3,
            "text-to-video R@5":75.5,
            "text-to-video R@10":84.2,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":12.8,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327957,
            "title":"Side4Video: Spatial-Temporal Side Network for Memory-Efficient Image-to-Video Transfer Learning",
            "url":"\/paper\/side4video-spatial-temporal-side-network-for",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/side4video-spatial-temporal-side-network-for\/review\/?hl=112696"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":96062,
        "rank":12,
        "method":"EMCL-Net++",
        "mlmodel":{

        },
        "method_short":"EMCL-Net++",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-21",
        "metrics":{
            "text-to-video R@1":"51.6",
            "text-to-video R@5":"78.1",
            "text-to-video R@10":"85.3",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":"1",
            "video-to-text R@1":"51.8",
            "video-to-text R@5":"80.2",
            "video-to-text R@10":"88",
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":"1"
        },
        "raw_metrics":{
            "text-to-video R@1":51.6,
            "text-to-video R@5":78.1,
            "text-to-video R@10":85.3,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":1.0,
            "video-to-text R@1":51.8,
            "video-to-text R@5":80.2,
            "video-to-text R@10":88.0,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":1.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1114877,
            "title":"Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations",
            "url":"\/paper\/expectation-maximization-contrastive-learning",
            "published":"2022-11-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/expectation-maximization-contrastive-learning\/review\/?hl=96062"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":88700,
        "rank":13,
        "method":"Cap4Video",
        "mlmodel":{

        },
        "method_short":"Cap4Video",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-31",
        "metrics":{
            "text-to-video R@1":"51.4",
            "text-to-video R@5":"75.7",
            "text-to-video R@10":"83.9",
            "text-to-video Median Rank":"1",
            "text-to-video Mean Rank":"12.4",
            "video-to-text R@1":"49.0",
            "video-to-text R@5":"75.2",
            "video-to-text R@10":"85.0",
            "video-to-text Median Rank":"2",
            "video-to-text Mean Rank":"8.0"
        },
        "raw_metrics":{
            "text-to-video R@1":51.4,
            "text-to-video R@5":75.7,
            "text-to-video R@10":83.9,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":12.4,
            "video-to-text R@1":49.0,
            "video-to-text R@5":75.2,
            "video-to-text R@10":85.0,
            "video-to-text Median Rank":2.0,
            "video-to-text Mean Rank":8.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136847,
            "title":"Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?",
            "url":"\/paper\/cap4video-what-can-auxiliary-captions-do-for",
            "published":"2022-12-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cap4video-what-can-auxiliary-captions-do-for\/review\/?hl=88700"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":106541,
        "rank":14,
        "method":"SuMA (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"SuMA ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-19",
        "metrics":{
            "text-to-video R@1":"49.8",
            "text-to-video R@5":"75.1",
            "text-to-video R@10":"83.9",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":"47.3",
            "video-to-text R@5":"76",
            "video-to-text R@10":"84.3",
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":49.8,
            "text-to-video R@5":75.1,
            "text-to-video R@10":83.9,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":47.3,
            "video-to-text R@5":76.0,
            "video-to-text R@10":84.3,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1160359,
            "title":"Video-Text Retrieval by Supervised Sparse Multi-Grained Learning",
            "url":"\/paper\/video-text-retrieval-by-supervised-multi",
            "published":"2023-02-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-text-retrieval-by-supervised-multi\/review\/?hl=106541"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":80492,
        "rank":15,
        "method":"X2-VLM (large)",
        "mlmodel":{

        },
        "method_short":"X2-VLM ",
        "method_details":"large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-22",
        "metrics":{
            "text-to-video R@1":"49.6",
            "text-to-video R@5":"76.7",
            "text-to-video R@10":"84.2",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":49.6,
            "text-to-video R@5":76.7,
            "text-to-video R@10":84.2,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1116038,
            "title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
            "url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for",
            "published":"2022-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for\/review\/?hl=80492"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":65458,
        "rank":16,
        "method":"X-CLIP",
        "mlmodel":{

        },
        "method_short":"X-CLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-15",
        "metrics":{
            "text-to-video R@1":"49.3",
            "text-to-video R@5":"75.8",
            "text-to-video R@10":"84.8",
            "text-to-video Median Rank":"2.0",
            "text-to-video Mean Rank":"12.2",
            "video-to-text R@1":"48.9",
            "video-to-text R@5":"76.8",
            "video-to-text R@10":"84.5",
            "video-to-text Median Rank":"2.0",
            "video-to-text Mean Rank":"8.1"
        },
        "raw_metrics":{
            "text-to-video R@1":49.3,
            "text-to-video R@5":75.8,
            "text-to-video R@10":84.8,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":12.2,
            "video-to-text R@1":48.9,
            "video-to-text R@5":76.8,
            "video-to-text R@10":84.5,
            "video-to-text Median Rank":2.0,
            "video-to-text Mean Rank":8.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1044539,
            "title":"X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval",
            "url":"\/paper\/x-clip-end-to-end-multi-grained-contrastive",
            "published":"2022-07-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-clip-end-to-end-multi-grained-contrastive\/review\/?hl=65458"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":100102,
        "rank":17,
        "method":"DiffusionRet",
        "mlmodel":{

        },
        "method_short":"DiffusionRet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-17",
        "metrics":{
            "text-to-video R@1":"49.0",
            "text-to-video R@5":"75.2",
            "text-to-video R@10":"82.7",
            "text-to-video Median Rank":"2.0",
            "text-to-video Mean Rank":"12.1",
            "video-to-text R@1":"47.7",
            "video-to-text R@5":"73.8",
            "video-to-text R@10":"84.5",
            "video-to-text Median Rank":"2.0",
            "video-to-text Mean Rank":"8.8"
        },
        "raw_metrics":{
            "text-to-video R@1":49.0,
            "text-to-video R@5":75.2,
            "text-to-video R@10":82.7,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":12.1,
            "video-to-text R@1":47.7,
            "video-to-text R@5":73.8,
            "video-to-text R@10":84.5,
            "video-to-text Median Rank":2.0,
            "video-to-text Mean Rank":8.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1175924,
            "title":"DiffusionRet: Generative Text-Video Retrieval with Diffusion Model",
            "url":"\/paper\/diffusionret-generative-text-video-retrieval",
            "published":"2023-03-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diffusionret-generative-text-video-retrieval\/review\/?hl=100102"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":100104,
        "rank":18,
        "method":"DiffusionRet+QB-Norm",
        "mlmodel":{

        },
        "method_short":"DiffusionRet+QB-Norm",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-17",
        "metrics":{
            "text-to-video R@1":"48.9",
            "text-to-video R@5":"75.2",
            "text-to-video R@10":"83.1",
            "text-to-video Median Rank":"2.0",
            "text-to-video Mean Rank":"12.1",
            "video-to-text R@1":"49.3",
            "video-to-text R@5":"74.3",
            "video-to-text R@10":"83.8",
            "video-to-text Median Rank":"2.0",
            "video-to-text Mean Rank":"8.5"
        },
        "raw_metrics":{
            "text-to-video R@1":48.9,
            "text-to-video R@5":75.2,
            "text-to-video R@10":83.1,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":12.1,
            "video-to-text R@1":49.3,
            "video-to-text R@5":74.3,
            "video-to-text R@10":83.8,
            "video-to-text Median Rank":2.0,
            "video-to-text Mean Rank":8.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1175924,
            "title":"DiffusionRet: Generative Text-Video Retrieval with Diffusion Model",
            "url":"\/paper\/diffusionret-generative-text-video-retrieval",
            "published":"2023-03-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diffusionret-generative-text-video-retrieval\/review\/?hl=100104"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":42262,
        "rank":19,
        "method":"CAMoE",
        "mlmodel":{

        },
        "method_short":"CAMoE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-09",
        "metrics":{
            "text-to-video R@1":"48.8",
            "text-to-video R@5":"75.6",
            "text-to-video R@10":"85.3",
            "text-to-video Median Rank":"2",
            "text-to-video Mean Rank":"12.4",
            "video-to-text R@1":"50.3",
            "video-to-text R@5":"74.6",
            "video-to-text R@10":"83.8",
            "video-to-text Median Rank":"2",
            "video-to-text Mean Rank":"9.9"
        },
        "raw_metrics":{
            "text-to-video R@1":48.8,
            "text-to-video R@5":75.6,
            "text-to-video R@10":85.3,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":12.4,
            "video-to-text R@1":50.3,
            "video-to-text R@5":74.6,
            "video-to-text R@10":83.8,
            "video-to-text Median Rank":2.0,
            "video-to-text Mean Rank":9.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":864259,
            "title":"Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss",
            "url":"\/paper\/improving-video-text-retrieval-by-multi",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improving-video-text-retrieval-by-multi\/review\/?hl=42262"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":100335,
        "rank":20,
        "method":"HBI",
        "mlmodel":{

        },
        "method_short":"HBI",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-25",
        "metrics":{
            "text-to-video R@1":"48.6",
            "text-to-video R@5":"74.6",
            "text-to-video R@10":"83.4",
            "text-to-video Median Rank":"2.0",
            "text-to-video Mean Rank":"12.0",
            "video-to-text R@1":"46.8",
            "video-to-text R@5":"74.3",
            "video-to-text R@10":"84.3",
            "video-to-text Median Rank":"2.0",
            "video-to-text Mean Rank":"8.9"
        },
        "raw_metrics":{
            "text-to-video R@1":48.6,
            "text-to-video R@5":74.6,
            "text-to-video R@10":83.4,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":12.0,
            "video-to-text R@1":46.8,
            "video-to-text R@5":74.3,
            "video-to-text R@10":84.3,
            "video-to-text Median Rank":2.0,
            "video-to-text Mean Rank":8.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":1180602,
            "title":"Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning",
            "url":"\/paper\/video-text-as-game-players-hierarchical",
            "published":"2023-03-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-text-as-game-players-hierarchical\/review\/?hl=100335"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":53738,
        "rank":21,
        "method":"CenterCLIP (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"CenterCLIP ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-02",
        "metrics":{
            "text-to-video R@1":"48.4",
            "text-to-video R@5":"73.8",
            "text-to-video R@10":"82.0",
            "text-to-video Median Rank":"2",
            "text-to-video Mean Rank":"13.8",
            "video-to-text R@1":"47.7",
            "video-to-text R@5":"75.0",
            "video-to-text R@10":"83.3",
            "video-to-text Median Rank":"2",
            "video-to-text Mean Rank":"10.2"
        },
        "raw_metrics":{
            "text-to-video R@1":48.4,
            "text-to-video R@5":73.8,
            "text-to-video R@10":82.0,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":13.8,
            "video-to-text R@1":47.7,
            "video-to-text R@5":75.0,
            "video-to-text R@10":83.3,
            "video-to-text Median Rank":2.0,
            "video-to-text Mean Rank":10.2
        },
        "uses_additional_data":true,
        "paper":{
            "id":1002430,
            "title":"CenterCLIP: Token Clustering for Efficient Text-Video Retrieval",
            "url":"\/paper\/centerclip-token-clustering-for-efficient",
            "published":"2022-05-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/centerclip-token-clustering-for-efficient\/review\/?hl=53738"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":80491,
        "rank":22,
        "method":"X2-VLM (base)",
        "mlmodel":{

        },
        "method_short":"X2-VLM ",
        "method_details":"base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-22",
        "metrics":{
            "text-to-video R@1":"47.6",
            "text-to-video R@5":"74.1",
            "text-to-video R@10":"84.2",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":47.6,
            "text-to-video R@5":74.1,
            "text-to-video R@10":84.2,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1116038,
            "title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
            "url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for",
            "published":"2022-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for\/review\/?hl=80491"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":45084,
        "rank":23,
        "method":"QB-Norm+CLIP2Video",
        "mlmodel":{

        },
        "method_short":"QB-Norm+CLIP2Video",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "text-to-video R@1":"47.2",
            "text-to-video R@5":"73.0",
            "text-to-video R@10":"83.0",
            "text-to-video Median Rank":"2",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":47.2,
            "text-to-video R@5":73.0,
            "text-to-video R@10":83.0,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":933091,
            "title":"Cross Modal Retrieval with Querybank Normalisation",
            "url":"\/paper\/cross-modal-retrieval-with-querybank",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cross-modal-retrieval-with-querybank\/review\/?hl=45084"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":50777,
        "rank":24,
        "method":"X-Pool",
        "mlmodel":{

        },
        "method_short":"X-Pool",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-28",
        "metrics":{
            "text-to-video R@1":"46.9",
            "text-to-video R@5":"72.8",
            "text-to-video R@10":"82.2",
            "text-to-video Median Rank":"2",
            "text-to-video Mean Rank":"14.3",
            "video-to-text R@1":"44.4",
            "video-to-text R@5":"73.3",
            "video-to-text R@10":"84.0",
            "video-to-text Median Rank":"2.0",
            "video-to-text Mean Rank":"9.0"
        },
        "raw_metrics":{
            "text-to-video R@1":46.9,
            "text-to-video R@5":72.8,
            "text-to-video R@10":82.2,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":14.3,
            "video-to-text R@1":44.4,
            "video-to-text R@5":73.3,
            "video-to-text R@10":84.0,
            "video-to-text Median Rank":2.0,
            "video-to-text Mean Rank":9.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":985243,
            "title":"X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval",
            "url":"\/paper\/x-pool-cross-modal-language-video-attention",
            "published":"2022-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":96049,
        "rank":25,
        "method":"EMCL-Net",
        "mlmodel":{

        },
        "method_short":"EMCL-Net",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-21",
        "metrics":{
            "text-to-video R@1":"46.8",
            "text-to-video R@5":"73.1",
            "text-to-video R@10":"83.1",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":"2",
            "video-to-text R@1":"46.5",
            "video-to-text R@5":"73.5",
            "video-to-text R@10":"83.5",
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":"2"
        },
        "raw_metrics":{
            "text-to-video R@1":46.8,
            "text-to-video R@5":73.1,
            "text-to-video R@10":83.1,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":2.0,
            "video-to-text R@1":46.5,
            "video-to-text R@5":73.5,
            "video-to-text R@10":83.5,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":2.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1114877,
            "title":"Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations",
            "url":"\/paper\/expectation-maximization-contrastive-learning",
            "published":"2022-11-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/expectation-maximization-contrastive-learning\/review\/?hl=96049"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":88516,
        "rank":26,
        "method":"HiTeA",
        "mlmodel":{

        },
        "method_short":"HiTeA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-30",
        "metrics":{
            "text-to-video R@1":"46.8",
            "text-to-video R@5":"71.2",
            "text-to-video R@10":"81.9",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":46.8,
            "text-to-video R@5":71.2,
            "text-to-video R@10":81.9,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136078,
            "title":"HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training",
            "url":"\/paper\/hitea-hierarchical-temporal-aware-video",
            "published":"2022-12-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/hitea-hierarchical-temporal-aware-video\/review\/?hl=88516"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":43911,
        "rank":27,
        "method":"LAFF",
        "mlmodel":{

        },
        "method_short":"LAFF",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-03",
        "metrics":{
            "text-to-video R@1":"45.8",
            "text-to-video R@5":"71.5",
            "text-to-video R@10":"82",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":45.8,
            "text-to-video R@5":71.5,
            "text-to-video R@10":82.0,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":925470,
            "title":"Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval",
            "url":"\/paper\/lightweight-attentional-feature-fusion-for",
            "published":"2021-12-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lightweight-attentional-feature-fusion-for\/review\/?hl=43911"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":39239,
        "rank":28,
        "method":"CLIP2Video",
        "mlmodel":{

        },
        "method_short":"CLIP2Video",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-21",
        "metrics":{
            "text-to-video R@1":"45.6",
            "text-to-video R@5":"72.6",
            "text-to-video R@10":"81.7",
            "text-to-video Median Rank":"2",
            "text-to-video Mean Rank":"14.6",
            "video-to-text R@1":"43.3",
            "video-to-text R@5":"72.3",
            "video-to-text R@10":"82.1",
            "video-to-text Median Rank":"2",
            "video-to-text Mean Rank":"10.2"
        },
        "raw_metrics":{
            "text-to-video R@1":45.6,
            "text-to-video R@5":72.6,
            "text-to-video R@10":81.7,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":14.6,
            "video-to-text R@1":43.3,
            "video-to-text R@5":72.3,
            "video-to-text R@10":82.1,
            "video-to-text Median Rank":2.0,
            "video-to-text Mean Rank":10.2
        },
        "uses_additional_data":true,
        "paper":{
            "id":821802,
            "title":"CLIP2Video: Mastering Video-Text Retrieval via Image CLIP",
            "url":"\/paper\/clip2video-mastering-video-text-retrieval-via",
            "published":"2021-06-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clip2video-mastering-video-text-retrieval-via\/review\/?hl=39239"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":56553,
        "rank":29,
        "method":"Singularity",
        "mlmodel":{

        },
        "method_short":"Singularity",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-07",
        "metrics":{
            "text-to-video R@1":"41.5",
            "text-to-video R@5":"68.7",
            "text-to-video R@10":"77",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":41.5,
            "text-to-video R@5":68.7,
            "text-to-video R@10":77.0,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1023192,
            "title":"Revealing Single Frame Bias for Video-and-Language Learning",
            "url":"\/paper\/revealing-single-frame-bias-for-video-and",
            "published":"2022-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revealing-single-frame-bias-for-video-and\/review\/?hl=56553"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":99997,
        "rank":30,
        "method":"All-in-one + MELTR",
        "mlmodel":{

        },
        "method_short":"All-in-one + MELTR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "text-to-video R@1":"41.3",
            "text-to-video R@5":"73.5",
            "text-to-video R@10":"82.5",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":41.3,
            "text-to-video R@5":73.5,
            "text-to-video R@10":82.5,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1179150,
            "title":"MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models",
            "url":"\/paper\/meltr-meta-loss-transformer-for-learning-to",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meltr-meta-loss-transformer-for-learning-to\/review\/?hl=99997"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":88011,
        "rank":31,
        "method":"Clover",
        "mlmodel":{

        },
        "method_short":"Clover",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-16",
        "metrics":{
            "text-to-video R@1":"40.5",
            "text-to-video R@5":"69.8",
            "text-to-video R@10":"79.4",
            "text-to-video Median Rank":"2",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":40.5,
            "text-to-video R@5":69.8,
            "text-to-video R@10":79.4,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1045027,
            "title":"Clover: Towards A Unified Video-Language Alignment and Fusion Model",
            "url":"\/paper\/clover-towards-a-unified-video-language",
            "published":"2022-07-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clover-towards-a-unified-video-language\/review\/?hl=88011"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":28485,
        "rank":32,
        "method":"MDMMT",
        "mlmodel":{

        },
        "method_short":"MDMMT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-19",
        "metrics":{
            "text-to-video R@1":"38.9",
            "text-to-video R@5":"69.0",
            "text-to-video R@10":"79.7",
            "text-to-video Median Rank":"2",
            "text-to-video Mean Rank":"16.5",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":38.9,
            "text-to-video R@5":69.0,
            "text-to-video R@10":79.7,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":16.5,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":755432,
            "title":"MDMMT: Multidomain Multimodal Transformer for Video Retrieval",
            "url":"\/paper\/mdmmt-multidomain-multimodal-transformer-for",
            "published":"2021-03-19T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":87807,
        "rank":33,
        "method":"MAC",
        "mlmodel":{

        },
        "method_short":"MAC",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-02",
        "metrics":{
            "text-to-video R@1":"38.9",
            "text-to-video R@5":"63.1",
            "text-to-video R@10":"73.9",
            "text-to-video Median Rank":"3",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":38.9,
            "text-to-video R@5":63.1,
            "text-to-video R@10":73.9,
            "text-to-video Median Rank":3.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1122433,
            "title":"Masked Contrastive Pre-Training for Efficient Video-Text Retrieval",
            "url":"\/paper\/masked-contrastive-pre-training-for-efficient",
            "published":"2022-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-contrastive-pre-training-for-efficient\/review\/?hl=87807"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":52837,
        "rank":34,
        "method":"All-in-one-B",
        "mlmodel":{

        },
        "method_short":"All-in-one-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-14",
        "metrics":{
            "text-to-video R@1":"37.9",
            "text-to-video R@5":"68.1",
            "text-to-video R@10":"77.1",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":37.9,
            "text-to-video R@5":68.1,
            "text-to-video R@10":77.1,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":976242,
            "title":"All in One: Exploring Unified Video-Language Pre-training",
            "url":"\/paper\/all-in-one-exploring-unified-video-language",
            "published":"2022-03-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/all-in-one-exploring-unified-video-language\/review\/?hl=52837"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":45568,
        "rank":35,
        "method":"BridgeFormer",
        "mlmodel":{

        },
        "method_short":"BridgeFormer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-13",
        "metrics":{
            "text-to-video R@1":"37.6",
            "text-to-video R@5":"64.8",
            "text-to-video R@10":"75.1",
            "text-to-video Median Rank":"3",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":37.6,
            "text-to-video R@5":64.8,
            "text-to-video R@10":75.1,
            "text-to-video Median Rank":3.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":944982,
            "title":"Bridging Video-text Retrieval with Multiple Choice Questions",
            "url":"\/paper\/bridgeformer-bridging-video-text-retrieval",
            "published":"2022-01-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bridgeformer-bridging-video-text-retrieval\/review\/?hl=45568"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":43290,
        "rank":36,
        "method":"Florence",
        "mlmodel":{

        },
        "method_short":"Florence",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-22",
        "metrics":{
            "text-to-video R@1":"37.6",
            "text-to-video R@5":"63.8",
            "text-to-video R@10":"72.6",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":37.6,
            "text-to-video R@5":63.8,
            "text-to-video R@10":72.6,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":914419,
            "title":"Florence: A New Foundation Model for Computer Vision",
            "url":"\/paper\/florence-a-new-foundation-model-for-computer",
            "published":"2021-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/florence-a-new-foundation-model-for-computer\/review\/?hl=43290"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":60402,
        "rank":37,
        "method":"COTS",
        "mlmodel":{

        },
        "method_short":"COTS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-15",
        "metrics":{
            "text-to-video R@1":"36.8",
            "text-to-video R@5":"63.8",
            "text-to-video R@10":"73.2",
            "text-to-video Median Rank":"2",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":36.8,
            "text-to-video R@5":63.8,
            "text-to-video R@10":73.2,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":995103,
            "title":"COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval",
            "url":"\/paper\/cots-collaborative-two-stream-vision-language",
            "published":"2022-04-15T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/cots-collaborative-two-stream-vision-language\/review\/?hl=60402"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":99990,
        "rank":38,
        "method":"VIOLET + MELTR",
        "mlmodel":{

        },
        "method_short":"VIOLET + MELTR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "text-to-video R@1":"35.5",
            "text-to-video R@5":"67.2",
            "text-to-video R@10":"78.4",
            "text-to-video Median Rank":"3",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":35.5,
            "text-to-video R@5":67.2,
            "text-to-video R@10":78.4,
            "text-to-video Median Rank":3.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1179150,
            "title":"MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models",
            "url":"\/paper\/meltr-meta-loss-transformer-for-learning-to",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meltr-meta-loss-transformer-for-learning-to\/review\/?hl=99990"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":27317,
        "rank":39,
        "method":"CLIP",
        "mlmodel":{

        },
        "method_short":"CLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-24",
        "metrics":{
            "text-to-video R@1":"31.2",
            "text-to-video R@5":"53.7",
            "text-to-video R@10":"64.2",
            "text-to-video Median Rank":"4",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":"27.2",
            "video-to-text R@5":"51.7",
            "video-to-text R@10":"62.6",
            "video-to-text Median Rank":"5",
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":31.2,
            "text-to-video R@5":53.7,
            "text-to-video R@10":64.2,
            "text-to-video Median Rank":4.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":27.2,
            "video-to-text R@5":51.7,
            "video-to-text R@10":62.6,
            "video-to-text Median Rank":5.0,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":748268,
            "title":"A Straightforward Framework For Video Retrieval Using CLIP",
            "url":"\/paper\/a-straightforward-framework-for-video",
            "published":"2021-02-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-straightforward-framework-for-video\/review\/?hl=27317"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":99987,
        "rank":40,
        "method":"UniVL + MELTR",
        "mlmodel":{

        },
        "method_short":"UniVL + MELTR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "text-to-video R@1":"31.1",
            "text-to-video R@5":"55.7",
            "text-to-video R@10":"68.3",
            "text-to-video Median Rank":"4",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":31.1,
            "text-to-video R@5":55.7,
            "text-to-video R@10":68.3,
            "text-to-video Median Rank":4.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1179150,
            "title":"MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models",
            "url":"\/paper\/meltr-meta-loss-transformer-for-learning-to",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meltr-meta-loss-transformer-for-learning-to\/review\/?hl=99987"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":29235,
        "rank":41,
        "method":"FROZEN",
        "mlmodel":{

        },
        "method_short":"FROZEN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-01",
        "metrics":{
            "text-to-video R@1":"31.0",
            "text-to-video R@5":"59.5",
            "text-to-video R@10":"70.5",
            "text-to-video Median Rank":"3",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":31.0,
            "text-to-video R@5":59.5,
            "text-to-video R@10":70.5,
            "text-to-video Median Rank":3.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":773452,
            "title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
            "url":"\/paper\/frozen-in-time-a-joint-video-and-image",
            "published":"2021-04-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/frozen-in-time-a-joint-video-and-image\/review\/?hl=29235"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":56691,
        "rank":42,
        "method":"VideoCLIP",
        "mlmodel":{

        },
        "method_short":"VideoCLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-28",
        "metrics":{
            "text-to-video R@1":"30.9",
            "text-to-video R@5":"55.4",
            "text-to-video R@10":"66.8",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":30.9,
            "text-to-video R@5":55.4,
            "text-to-video R@10":66.8,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":875851,
            "title":"VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding",
            "url":"\/paper\/videoclip-contrastive-pre-training-for-zero",
            "published":"2021-09-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":38603,
        "rank":43,
        "method":"TACo",
        "mlmodel":{

        },
        "method_short":"TACo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-08-23",
        "metrics":{
            "text-to-video R@1":"28.4",
            "text-to-video R@5":"57.8",
            "text-to-video R@10":"71.2",
            "text-to-video Median Rank":"4",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":28.4,
            "text-to-video R@5":57.8,
            "text-to-video R@10":71.2,
            "text-to-video Median Rank":4.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":854963,
            "title":"TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment",
            "url":"\/paper\/taco-token-aware-cascade-contrastive-learning",
            "published":"2021-08-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/taco-token-aware-cascade-contrastive-learning\/review\/?hl=38603"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":56696,
        "rank":44,
        "method":"VLM",
        "mlmodel":{

        },
        "method_short":"VLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-20",
        "metrics":{
            "text-to-video R@1":"28.10",
            "text-to-video R@5":"55.50",
            "text-to-video R@10":"67.40",
            "text-to-video Median Rank":"4",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":28.1,
            "text-to-video R@5":55.5,
            "text-to-video R@10":67.4,
            "text-to-video Median Rank":4.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":803509,
            "title":"VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding",
            "url":"\/paper\/vlm-task-agnostic-video-language-model-pre",
            "published":"2021-05-20T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":22465,
        "rank":45,
        "method":"MMT-Pretrained",
        "mlmodel":{

        },
        "method_short":"MMT-Pretrained",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-21",
        "metrics":{
            "text-to-video R@1":"26.6",
            "text-to-video R@5":"57.1",
            "text-to-video R@10":"69.6",
            "text-to-video Median Rank":"4",
            "text-to-video Mean Rank":"24.0",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":26.6,
            "text-to-video R@5":57.1,
            "text-to-video R@10":69.6,
            "text-to-video Median Rank":4.0,
            "text-to-video Mean Rank":24.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":209798,
            "title":"Multi-modal Transformer for Video Retrieval",
            "url":"\/paper\/multi-modal-transformer-for-video-retrieval",
            "published":"2020-07-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multi-modal-transformer-for-video-retrieval\/review\/?hl=22465"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":45569,
        "rank":46,
        "method":"BridgeFormer (Zero-shot)",
        "mlmodel":{

        },
        "method_short":"BridgeFormer ",
        "method_details":"Zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-13",
        "metrics":{
            "text-to-video R@1":"26",
            "text-to-video R@5":"46.4",
            "text-to-video R@10":"56.4",
            "text-to-video Median Rank":"7",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":26.0,
            "text-to-video R@5":46.4,
            "text-to-video R@10":56.4,
            "text-to-video Median Rank":7.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":944982,
            "title":"Bridging Video-text Retrieval with Multiple Choice Questions",
            "url":"\/paper\/bridgeformer-bridging-video-text-retrieval",
            "published":"2022-01-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bridgeformer-bridging-video-text-retrieval\/review\/?hl=45569"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":22464,
        "rank":47,
        "method":"MMT",
        "mlmodel":{

        },
        "method_short":"MMT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-21",
        "metrics":{
            "text-to-video R@1":"24.6",
            "text-to-video R@5":"54.0",
            "text-to-video R@10":"67.1",
            "text-to-video Median Rank":"4",
            "text-to-video Mean Rank":"26.7",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":24.6,
            "text-to-video R@5":54.0,
            "text-to-video R@10":67.1,
            "text-to-video Median Rank":4.0,
            "text-to-video Mean Rank":26.7,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":209798,
            "title":"Multi-modal Transformer for Video Retrieval",
            "url":"\/paper\/multi-modal-transformer-for-video-retrieval",
            "published":"2020-07-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multi-modal-transformer-for-video-retrieval\/review\/?hl=22464"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":22466,
        "rank":48,
        "method":"Collaborative Experts",
        "mlmodel":{

        },
        "method_short":"Collaborative Experts",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-31",
        "metrics":{
            "text-to-video R@1":"20.9",
            "text-to-video R@5":"48.8",
            "text-to-video R@10":"62.4",
            "text-to-video Median Rank":"6",
            "text-to-video Mean Rank":"28.2",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":20.9,
            "text-to-video R@5":48.8,
            "text-to-video R@10":62.4,
            "text-to-video Median Rank":6.0,
            "text-to-video Mean Rank":28.2,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":148748,
            "title":"Use What You Have: Video Retrieval Using Representations From Collaborative Experts",
            "url":"\/paper\/use-what-you-have-video-retrieval-using",
            "published":"2019-07-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/use-what-you-have-video-retrieval-using\/review\/?hl=22466"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":22468,
        "rank":49,
        "method":"HT-Pretrained",
        "mlmodel":{

        },
        "method_short":"HT-Pretrained",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-07",
        "metrics":{
            "text-to-video R@1":"14.9",
            "text-to-video R@5":"40.2",
            "text-to-video R@10":"52.8",
            "text-to-video Median Rank":"9",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":14.9,
            "text-to-video R@5":40.2,
            "text-to-video R@10":52.8,
            "text-to-video Median Rank":9.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142178,
            "title":"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips",
            "url":"\/paper\/howto100m-learning-a-text-video-embedding-by",
            "published":"2019-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/howto100m-learning-a-text-video-embedding-by\/review\/?hl=22468"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":22467,
        "rank":50,
        "method":"HT",
        "mlmodel":{

        },
        "method_short":"HT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-07",
        "metrics":{
            "text-to-video R@1":"12.1",
            "text-to-video R@5":"35.0",
            "text-to-video R@10":"48.0",
            "text-to-video Median Rank":"12",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":12.1,
            "text-to-video R@5":35.0,
            "text-to-video R@10":48.0,
            "text-to-video Median Rank":12.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142178,
            "title":"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips",
            "url":"\/paper\/howto100m-learning-a-text-video-embedding-by",
            "published":"2019-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/howto100m-learning-a-text-video-embedding-by\/review\/?hl=22467"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":22469,
        "rank":51,
        "method":"JSFusion",
        "mlmodel":{

        },
        "method_short":"JSFusion",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-08-07",
        "metrics":{
            "text-to-video R@1":"10.2",
            "text-to-video R@5":"31.2",
            "text-to-video R@10":"43.2",
            "text-to-video Median Rank":"13",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":10.2,
            "text-to-video R@5":31.2,
            "text-to-video R@10":43.2,
            "text-to-video Median Rank":13.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":54495,
            "title":"A Joint Sequence Fusion Model for Video Question Answering and Retrieval",
            "url":"\/paper\/a-joint-sequence-fusion-model-for-video",
            "published":"2018-08-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-joint-sequence-fusion-model-for-video\/review\/?hl=22469"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":113030,
        "rank":52,
        "method":"OmniVec",
        "mlmodel":{

        },
        "method_short":"OmniVec",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-07",
        "metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":null,
            "text-to-video R@10":"89.4",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":null,
            "text-to-video R@10":89.4,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1319233,
            "title":"OmniVec: Learning robust representations with cross modal sharing",
            "url":"\/paper\/omnivec-learning-robust-representations-with",
            "published":"2023-11-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivec-learning-robust-representations-with\/review\/?hl=113030"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":30331,
        "rank":53,
        "method":"CLIP4Clip",
        "mlmodel":{

        },
        "method_short":"CLIP4Clip",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-18",
        "metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":null,
            "text-to-video R@10":"81.6",
            "text-to-video Median Rank":"2",
            "text-to-video Mean Rank":"15.3",
            "video-to-text R@1":"42.7",
            "video-to-text R@5":"70.9",
            "video-to-text R@10":"80.6",
            "video-to-text Median Rank":"2",
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":null,
            "text-to-video R@10":81.6,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":15.3,
            "video-to-text R@1":42.7,
            "video-to-text R@5":70.9,
            "video-to-text R@10":80.6,
            "video-to-text Median Rank":2.0,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":784398,
            "title":"CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval",
            "url":"\/paper\/clip4clip-an-empirical-study-of-clip-for-end",
            "published":"2021-04-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clip4clip-an-empirical-study-of-clip-for-end\/review\/?hl=30331"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":113031,
        "rank":54,
        "method":"OmniVec (pretrained)",
        "mlmodel":{

        },
        "method_short":"OmniVec ",
        "method_details":"pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-07",
        "metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":null,
            "text-to-video R@10":"78.6",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":null,
            "text-to-video R@10":78.6,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1319233,
            "title":"OmniVec: Learning robust representations with cross modal sharing",
            "url":"\/paper\/omnivec-learning-robust-representations-with",
            "published":"2023-11-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivec-learning-robust-representations-with\/review\/?hl=113031"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":4401,
        "row_id":53385,
        "rank":55,
        "method":"Socratic Models",
        "mlmodel":{

        },
        "method_short":"Socratic Models",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-01",
        "metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":null,
            "text-to-video R@10":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":"42.8",
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":null,
            "text-to-video R@10":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":42.8,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":987639,
            "title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
            "url":"\/paper\/socratic-models-composing-zero-shot",
            "published":"2022-04-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/socratic-models-composing-zero-shot\/review\/?hl=53385"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    }
]