[
    {
        "table_id":2829,
        "row_id":103550,
        "rank":1,
        "method":"VLAB",
        "mlmodel":{

        },
        "method_short":"VLAB",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-22",
        "metrics":{
            "Accuracy":"0.496"
        },
        "raw_metrics":{
            "Accuracy":0.496
        },
        "uses_additional_data":true,
        "paper":{
            "id":1212983,
            "title":"VLAB: Enhancing Video Language Pre-training by Feature Adapting and Blending",
            "url":"\/paper\/vlab-enhancing-video-language-pre-training-by",
            "published":"2023-05-22T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/vlab-enhancing-video-language-pre-training-by\/review\/?hl=103550"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":106251,
        "rank":2,
        "method":"MaMMUT",
        "mlmodel":{

        },
        "method_short":"MaMMUT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-29",
        "metrics":{
            "Accuracy":"0.495"
        },
        "raw_metrics":{
            "Accuracy":0.495
        },
        "uses_additional_data":true,
        "paper":{
            "id":1182696,
            "title":"MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks",
            "url":"\/paper\/mammut-a-simple-architecture-for-joint",
            "published":"2023-03-29T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/mammut-a-simple-architecture-for-joint\/review\/?hl=106251"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":96427,
        "rank":3,
        "method":"mPLUG-2",
        "mlmodel":{

        },
        "method_short":"mPLUG-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-01",
        "metrics":{
            "Accuracy":"0.480"
        },
        "raw_metrics":{
            "Accuracy":0.48
        },
        "uses_additional_data":true,
        "paper":{
            "id":1151002,
            "title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",
            "url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation",
            "published":"2023-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation\/review\/?hl=96427"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":99056,
        "rank":4,
        "method":"MuLTI",
        "mlmodel":{

        },
        "method_short":"MuLTI",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-10",
        "metrics":{
            "Accuracy":"0.478"
        },
        "raw_metrics":{
            "Accuracy":0.478
        },
        "uses_additional_data":true,
        "paper":{
            "id":1171789,
            "title":"MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler and Multiple Choice Modeling",
            "url":"\/paper\/multi-efficient-video-and-language",
            "published":"2023-03-10T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/multi-efficient-video-and-language\/review\/?hl=99056"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":79314,
        "rank":5,
        "method":"Flamingo",
        "mlmodel":{

        },
        "method_short":"Flamingo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-29",
        "metrics":{
            "Accuracy":"0.474"
        },
        "raw_metrics":{
            "Accuracy":0.474
        },
        "uses_additional_data":true,
        "paper":{
            "id":1001838,
            "title":"Flamingo: a Visual Language Model for Few-Shot Learning",
            "url":"\/paper\/flamingo-a-visual-language-model-for-few-shot-1",
            "published":"2022-04-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":86727,
        "rank":6,
        "method":"InternVideo",
        "mlmodel":{

        },
        "method_short":"InternVideo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Accuracy":"0.471"
        },
        "raw_metrics":{
            "Accuracy":0.471
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124231,
            "title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
            "url":"\/paper\/internvideo-general-video-foundation-models",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internvideo-general-video-foundation-models\/review\/?hl=86727"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":100396,
        "rank":7,
        "method":"UMT-L (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"UMT-L ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "Accuracy":"0.471"
        },
        "raw_metrics":{
            "Accuracy":0.471
        },
        "uses_additional_data":true,
        "paper":{
            "id":1181934,
            "title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
            "url":"\/paper\/unmasked-teacher-towards-training-efficient",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":70081,
        "rank":8,
        "method":"FrozenBiLM",
        "mlmodel":{

        },
        "method_short":"FrozenBiLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-16",
        "metrics":{
            "Accuracy":"0.470"
        },
        "raw_metrics":{
            "Accuracy":0.47
        },
        "uses_additional_data":true,
        "paper":{
            "id":1028387,
            "title":"Zero-Shot Video Question Answering via Frozen Bidirectional Language Models",
            "url":"\/paper\/zero-shot-video-question-answering-via-frozen",
            "published":"2022-06-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/zero-shot-video-question-answering-via-frozen\/review\/?hl=70081"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":108845,
        "rank":9,
        "method":"FrozenBiLM+",
        "mlmodel":{

        },
        "method_short":"FrozenBiLM+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Accuracy":"0.470"
        },
        "raw_metrics":{
            "Accuracy":0.47
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265311,
            "title":"Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models",
            "url":"\/paper\/open-vocabulary-video-question-answering-a",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/open-vocabulary-video-question-answering-a\/review\/?hl=108845"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":87221,
        "rank":10,
        "method":"VideoCoCa",
        "mlmodel":{

        },
        "method_short":"VideoCoCa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Accuracy":"0.463"
        },
        "raw_metrics":{
            "Accuracy":0.463
        },
        "uses_additional_data":true,
        "paper":{
            "id":1126258,
            "title":"VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners",
            "url":"\/paper\/video-text-modeling-with-zero-shot-transfer",
            "published":"2022-12-09T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/video-text-modeling-with-zero-shot-transfer\/review\/?hl=87221"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":100338,
        "rank":11,
        "method":"HBI",
        "mlmodel":{

        },
        "method_short":"HBI",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-25",
        "metrics":{
            "Accuracy":"0.462"
        },
        "raw_metrics":{
            "Accuracy":0.462
        },
        "uses_additional_data":false,
        "paper":{
            "id":1180602,
            "title":"Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning",
            "url":"\/paper\/video-text-as-game-players-hierarchical",
            "published":"2023-03-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-text-as-game-players-hierarchical\/review\/?hl=100338"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":88491,
        "rank":12,
        "method":"HiTeA",
        "mlmodel":{

        },
        "method_short":"HiTeA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-30",
        "metrics":{
            "Accuracy":"0.459"
        },
        "raw_metrics":{
            "Accuracy":0.459
        },
        "uses_additional_data":true,
        "paper":{
            "id":1136078,
            "title":"HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training",
            "url":"\/paper\/hitea-hierarchical-temporal-aware-video",
            "published":"2022-12-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/hitea-hierarchical-temporal-aware-video\/review\/?hl=88491"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":96052,
        "rank":13,
        "method":"EMCL-Net",
        "mlmodel":{

        },
        "method_short":"EMCL-Net",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-21",
        "metrics":{
            "Accuracy":"0.458"
        },
        "raw_metrics":{
            "Accuracy":0.458
        },
        "uses_additional_data":false,
        "paper":{
            "id":1114877,
            "title":"Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations",
            "url":"\/paper\/expectation-maximization-contrastive-learning",
            "published":"2022-11-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/expectation-maximization-contrastive-learning\/review\/?hl=96052"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":75345,
        "rank":14,
        "method":"Co-Tokenization",
        "mlmodel":{

        },
        "method_short":"Co-Tokenization",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-01",
        "metrics":{
            "Accuracy":".457"
        },
        "raw_metrics":{
            "Accuracy":0.457
        },
        "uses_additional_data":true,
        "paper":{
            "id":1052402,
            "title":"Video Question Answering with Iterative Video-Text Co-Tokenization",
            "url":"\/paper\/video-question-answering-with-iterative-video",
            "published":"2022-08-01T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/video-question-answering-with-iterative-video\/review\/?hl=75345"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":80494,
        "rank":15,
        "method":"X2-VLM (large)",
        "mlmodel":{

        },
        "method_short":"X2-VLM ",
        "method_details":"large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-22",
        "metrics":{
            "Accuracy":"0.455"
        },
        "raw_metrics":{
            "Accuracy":0.455
        },
        "uses_additional_data":true,
        "paper":{
            "id":1116038,
            "title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
            "url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for",
            "published":"2022-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for\/review\/?hl=80494"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":80493,
        "rank":16,
        "method":"X2-VLM (base)",
        "mlmodel":{

        },
        "method_short":"X2-VLM ",
        "method_details":"base",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-22",
        "metrics":{
            "Accuracy":"0.45"
        },
        "raw_metrics":{
            "Accuracy":0.45
        },
        "uses_additional_data":true,
        "paper":{
            "id":1116038,
            "title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
            "url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for",
            "published":"2022-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-2-vlm-all-in-one-pre-trained-model-for\/review\/?hl=80493"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":52529,
        "rank":17,
        "method":"All-in-one-B",
        "mlmodel":{

        },
        "method_short":"All-in-one-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-14",
        "metrics":{
            "Accuracy":"0.443"
        },
        "raw_metrics":{
            "Accuracy":0.443
        },
        "uses_additional_data":true,
        "paper":{
            "id":976242,
            "title":"All in One: Exploring Unified Video-Language Pre-training",
            "url":"\/paper\/all-in-one-exploring-unified-video-language",
            "published":"2022-03-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/all-in-one-exploring-unified-video-language\/review\/?hl=52529"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":69058,
        "rank":18,
        "method":"OmniVL",
        "mlmodel":{

        },
        "method_short":"OmniVL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-15",
        "metrics":{
            "Accuracy":"0.441"
        },
        "raw_metrics":{
            "Accuracy":0.441
        },
        "uses_additional_data":true,
        "paper":{
            "id":1075042,
            "title":"OmniVL:One Foundation Model for Image-Language and Video-Language Tasks",
            "url":"\/paper\/omnivl-one-foundation-model-for-image",
            "published":"2022-09-15T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivl-one-foundation-model-for-image\/review\/?hl=69058"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":88022,
        "rank":19,
        "method":"Clover",
        "mlmodel":{

        },
        "method_short":"Clover",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-16",
        "metrics":{
            "Accuracy":"0.441"
        },
        "raw_metrics":{
            "Accuracy":0.441
        },
        "uses_additional_data":true,
        "paper":{
            "id":1045027,
            "title":"Clover: Towards A Unified Video-Language Alignment and Fusion Model",
            "url":"\/paper\/clover-towards-a-unified-video-language",
            "published":"2022-07-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clover-towards-a-unified-video-language\/review\/?hl=88022"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":54476,
        "rank":20,
        "method":"ALPRO",
        "mlmodel":{

        },
        "method_short":"ALPRO",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-17",
        "metrics":{
            "Accuracy":"0.421"
        },
        "raw_metrics":{
            "Accuracy":0.421
        },
        "uses_additional_data":true,
        "paper":{
            "id":932382,
            "title":"Align and Prompt: Video-and-Language Pre-training with Entity Prompts",
            "url":"\/paper\/align-and-prompt-video-and-language-pre",
            "published":"2021-12-17T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":106339,
        "rank":21,
        "method":"LRCE",
        "mlmodel":{

        },
        "method_short":"LRCE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-30",
        "metrics":{
            "Accuracy":"0.42"
        },
        "raw_metrics":{
            "Accuracy":0.42
        },
        "uses_additional_data":false,
        "paper":{
            "id":1250520,
            "title":"Lightweight Recurrent Cross-modal Encoder for Video Question Answering",
            "url":"\/paper\/lightweight-recurrent-cross-modal-encoder-for",
            "published":"2023-06-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":108843,
        "rank":22,
        "method":"JustAsk+",
        "mlmodel":{

        },
        "method_short":"JustAsk+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Accuracy":"0.418"
        },
        "raw_metrics":{
            "Accuracy":0.418
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265311,
            "title":"Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models",
            "url":"\/paper\/open-vocabulary-video-question-answering-a",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/open-vocabulary-video-question-answering-a\/review\/?hl=108843"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":37327,
        "rank":23,
        "method":"Just Ask",
        "mlmodel":{

        },
        "method_short":"Just Ask",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Accuracy":"0.415"
        },
        "raw_metrics":{
            "Accuracy":0.415
        },
        "uses_additional_data":true,
        "paper":{
            "id":238417,
            "title":"Just Ask: Learning to Answer Questions from Millions of Narrated Videos",
            "url":"\/paper\/just-ask-learning-to-answer-questions-from",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/just-ask-learning-to-answer-questions-from\/review\/?hl=37327"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":108842,
        "rank":24,
        "method":"All-in-one+",
        "mlmodel":{

        },
        "method_short":"All-in-one+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Accuracy":"0.395"
        },
        "raw_metrics":{
            "Accuracy":0.395
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265311,
            "title":"Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models",
            "url":"\/paper\/open-vocabulary-video-question-answering-a",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/open-vocabulary-video-question-answering-a\/review\/?hl=108842"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":37624,
        "rank":25,
        "method":"CLIPBERT",
        "mlmodel":{

        },
        "method_short":"CLIPBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Accuracy":"0.374"
        },
        "raw_metrics":{
            "Accuracy":0.374
        },
        "uses_additional_data":true,
        "paper":{
            "id":744371,
            "title":"Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling",
            "url":"\/paper\/less-is-more-clipbert-for-video-and-language",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/less-is-more-clipbert-for-video-and-language\/review\/?hl=37624"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":15553,
        "rank":26,
        "method":"HCRN",
        "mlmodel":{

        },
        "method_short":"HCRN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-02-25",
        "metrics":{
            "Accuracy":"0.356"
        },
        "raw_metrics":{
            "Accuracy":0.356
        },
        "uses_additional_data":false,
        "paper":{
            "id":184719,
            "title":"Hierarchical Conditional Relation Networks for Video Question Answering",
            "url":"\/paper\/hierarchical-conditional-relation-networks",
            "published":"2020-02-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hierarchical-conditional-relation-networks\/review\/?hl=15553"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":36598,
        "rank":27,
        "method":"DualVGR",
        "mlmodel":{

        },
        "method_short":"DualVGR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-10",
        "metrics":{
            "Accuracy":"0.355"
        },
        "raw_metrics":{
            "Accuracy":0.355
        },
        "uses_additional_data":false,
        "paper":{
            "id":833787,
            "title":"DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering",
            "url":"\/paper\/dualvgr-a-dual-visual-graph-reasoning-unit",
            "published":"2021-07-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dualvgr-a-dual-visual-graph-reasoning-unit\/review\/?hl=36598"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":15551,
        "rank":28,
        "method":"SSML",
        "mlmodel":{

        },
        "method_short":"SSML",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-06",
        "metrics":{
            "Accuracy":"0.35"
        },
        "raw_metrics":{
            "Accuracy":0.35
        },
        "uses_additional_data":false,
        "paper":{
            "id":186098,
            "title":"Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning",
            "url":"\/paper\/noise-estimation-using-density-estimation-for",
            "published":"2020-03-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/noise-estimation-using-density-estimation-for\/review\/?hl=15551"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":15552,
        "rank":29,
        "method":"HMEMA",
        "mlmodel":{

        },
        "method_short":"HMEMA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-08",
        "metrics":{
            "Accuracy":"0.33"
        },
        "raw_metrics":{
            "Accuracy":0.33
        },
        "uses_additional_data":false,
        "paper":{
            "id":111163,
            "title":"Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering",
            "url":"\/paper\/heterogeneous-memory-enhanced-multimodal",
            "published":"2019-04-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/heterogeneous-memory-enhanced-multimodal\/review\/?hl=15552"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":15549,
        "rank":30,
        "method":"Co-Mem",
        "mlmodel":{

        },
        "method_short":"Co-Mem",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-03-29",
        "metrics":{
            "Accuracy":"0.32"
        },
        "raw_metrics":{
            "Accuracy":0.32
        },
        "uses_additional_data":false,
        "paper":{
            "id":7192,
            "title":"Motion-Appearance Co-Memory Networks for Video Question Answering",
            "url":"\/paper\/motion-appearance-co-memory-networks-for",
            "published":"2018-03-29T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/motion-appearance-co-memory-networks-for\/review\/?hl=15549"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":53746,
        "rank":31,
        "method":"Flamingo (32-shot)",
        "mlmodel":{

        },
        "method_short":"Flamingo ",
        "method_details":"32-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-29",
        "metrics":{
            "Accuracy":"0.310"
        },
        "raw_metrics":{
            "Accuracy":0.31
        },
        "uses_additional_data":false,
        "paper":{
            "id":1001838,
            "title":"Flamingo: a Visual Language Model for Few-Shot Learning",
            "url":"\/paper\/flamingo-a-visual-language-model-for-few-shot-1",
            "published":"2022-04-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":15550,
        "rank":32,
        "method":"ST-VQA",
        "mlmodel":{

        },
        "method_short":"ST-VQA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-04-14",
        "metrics":{
            "Accuracy":"0.309"
        },
        "raw_metrics":{
            "Accuracy":0.309
        },
        "uses_additional_data":false,
        "paper":{
            "id":13620,
            "title":"TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering",
            "url":"\/paper\/tgif-qa-toward-spatio-temporal-reasoning-in",
            "published":"2017-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tgif-qa-toward-spatio-temporal-reasoning-in\/review\/?hl=15550"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2829,
        "row_id":53747,
        "rank":33,
        "method":"Flamingo (0-shot)",
        "mlmodel":{

        },
        "method_short":"Flamingo ",
        "method_details":"0-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-29",
        "metrics":{
            "Accuracy":"0.174"
        },
        "raw_metrics":{
            "Accuracy":0.174
        },
        "uses_additional_data":false,
        "paper":{
            "id":1001838,
            "title":"Flamingo: a Visual Language Model for Few-Shot Learning",
            "url":"\/paper\/flamingo-a-visual-language-model-for-few-shot-1",
            "published":"2022-04-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]