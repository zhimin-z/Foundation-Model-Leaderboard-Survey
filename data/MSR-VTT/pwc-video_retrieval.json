[
    {
        "table_id":1136,
        "row_id":104627,
        "rank":1,
        "method":"VAST",
        "mlmodel":{

        },
        "Model":"VAST",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "text-to-video R@1":"63.9",
            "text-to-video R@5":"84.3",
            "text-to-video R@10":"89.6",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":63.9,
            "text-to-video R@5":84.3,
            "text-to-video R@10":89.6,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":101407,
        "rank":2,
        "method":"VALOR",
        "mlmodel":{

        },
        "Model":"VALOR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-17",
        "metrics":{
            "text-to-video R@1":"59.9",
            "text-to-video R@5":"83.5",
            "text-to-video R@10":"89.6",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":59.9,
            "text-to-video R@5":83.5,
            "text-to-video R@10":89.6,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1191887,
            "title":"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset",
            "url":"\/paper\/valor-vision-audio-language-omni-perception",
            "published":"2023-04-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/valor-vision-audio-language-omni-perception\/review\/?hl=101407"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":100392,
        "rank":3,
        "method":"UMT-L (ViT-L\/16)",
        "mlmodel":{

        },
        "Model":"UMT-L ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "text-to-video R@1":"58.8",
            "text-to-video R@5":"81.0",
            "text-to-video R@10":"87.1",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":"58.6",
            "video-to-text R@5":"81.6",
            "video-to-text R@10":"86.5",
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":58.8,
            "text-to-video R@5":81.0,
            "text-to-video R@10":87.1,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":58.6,
            "video-to-text R@5":81.6,
            "video-to-text R@10":86.5,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1181934,
            "title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
            "url":"\/paper\/unmasked-teacher-towards-training-efficient",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":86721,
        "rank":4,
        "method":"InternVideo",
        "mlmodel":{

        },
        "Model":"InternVideo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "text-to-video R@1":"55.2",
            "text-to-video R@5":null,
            "text-to-video R@10":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":"57.9",
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":55.2,
            "text-to-video R@5":null,
            "text-to-video R@10":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":57.9,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124231,
            "title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
            "url":"\/paper\/internvideo-general-video-foundation-models",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internvideo-general-video-foundation-models\/review\/?hl=86721"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":103555,
        "rank":5,
        "method":"VLAB",
        "mlmodel":{

        },
        "Model":"VLAB",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-22",
        "metrics":{
            "text-to-video R@1":"55.1",
            "text-to-video R@5":"78.8",
            "text-to-video R@10":"87.6",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":55.1,
            "text-to-video R@5":78.8,
            "text-to-video R@10":87.6,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1212983,
            "title":"VLAB: Enhancing Video Language Pre-training by Feature Adapting and Blending",
            "url":"\/paper\/vlab-enhancing-video-language-pre-training-by",
            "published":"2023-05-22T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/vlab-enhancing-video-language-pre-training-by\/review\/?hl=103555"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":111272,
        "rank":6,
        "method":"Aurora (ours, r=64)",
        "mlmodel":{

        },
        "Model":"Aurora ",
        "method_details":"ours, r=64",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "text-to-video R@1":"52.4",
            "text-to-video R@5":"73.9",
            "text-to-video R@10":"82",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":"1"
        },
        "raw_metrics":{
            "text-to-video R@1":52.4,
            "text-to-video R@5":73.9,
            "text-to-video R@10":82.0,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":1.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":112302,
        "rank":7,
        "method":"TEFAL",
        "mlmodel":{

        },
        "Model":"TEFAL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-24",
        "metrics":{
            "text-to-video R@1":"52",
            "text-to-video R@5":"76.6",
            "text-to-video R@10":"86.1",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":52.0,
            "text-to-video R@5":76.6,
            "text-to-video R@10":86.1,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1251303,
            "title":"Audio-Enhanced Text-to-Video Retrieval using Text-Conditioned Feature Alignment",
            "url":"\/paper\/audio-enhanced-text-to-video-retrieval-using",
            "published":"2023-07-24T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/audio-enhanced-text-to-video-retrieval-using\/review\/?hl=112302"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":109067,
        "rank":8,
        "method":"UCoFiA",
        "mlmodel":{

        },
        "Model":"UCoFiA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-18",
        "metrics":{
            "text-to-video R@1":"49.4",
            "text-to-video R@5":"72.1",
            "text-to-video R@10":"83.5",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":49.4,
            "text-to-video R@5":72.1,
            "text-to-video R@10":83.5,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1283783,
            "title":"Unified Coarse-to-Fine Alignment for Video-Text Retrieval",
            "url":"\/paper\/unified-coarse-to-fine-alignment-for-video",
            "published":"2023-09-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unified-coarse-to-fine-alignment-for-video\/review\/?hl=109067"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":69049,
        "rank":9,
        "method":"OmniVL",
        "mlmodel":{

        },
        "Model":"OmniVL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-15",
        "metrics":{
            "text-to-video R@1":"47.8",
            "text-to-video R@5":"74.2",
            "text-to-video R@10":"83.8",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":47.8,
            "text-to-video R@5":74.2,
            "text-to-video R@10":83.8,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1075042,
            "title":"OmniVL:One Foundation Model for Image-Language and Video-Language Tasks",
            "url":"\/paper\/omnivl-one-foundation-model-for-image",
            "published":"2022-09-15T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivl-one-foundation-model-for-image\/review\/?hl=69049"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":80520,
        "rank":10,
        "method":"CLIP4Clip-seqTransf",
        "mlmodel":{

        },
        "Model":"CLIP4Clip-seqTransf",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-18",
        "metrics":{
            "text-to-video R@1":"44.5",
            "text-to-video R@5":"71.4",
            "text-to-video R@10":"81.6",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":44.5,
            "text-to-video R@5":71.4,
            "text-to-video R@10":81.6,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":784398,
            "title":"CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval",
            "url":"\/paper\/clip4clip-an-empirical-study-of-clip-for-end",
            "published":"2021-04-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clip4clip-an-empirical-study-of-clip-for-end\/review\/?hl=80520"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":100013,
        "rank":11,
        "method":"All-in-one + MELTR",
        "mlmodel":{

        },
        "Model":"All-in-one + MELTR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "text-to-video R@1":"38.6",
            "text-to-video R@5":"74.4",
            "text-to-video R@10":"84.7",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":38.6,
            "text-to-video R@5":74.4,
            "text-to-video R@10":84.7,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1179150,
            "title":"MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models",
            "url":"\/paper\/meltr-meta-loss-transformer-for-learning-to",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meltr-meta-loss-transformer-for-learning-to\/review\/?hl=100013"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":96080,
        "rank":12,
        "method":"VIOLETv2",
        "mlmodel":{

        },
        "Model":"VIOLETv2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-04",
        "metrics":{
            "text-to-video R@1":"37.2",
            "text-to-video R@5":"64.8",
            "text-to-video R@10":"75.8",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":37.2,
            "text-to-video R@5":64.8,
            "text-to-video R@10":75.8,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1069338,
            "title":"An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling",
            "url":"\/paper\/an-empirical-study-of-end-to-end-video",
            "published":"2022-09-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-empirical-study-of-end-to-end-video\/review\/?hl=96080"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":110581,
        "rank":13,
        "method":"HD-VILA",
        "mlmodel":{

        },
        "Model":"HD-VILA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-19",
        "metrics":{
            "text-to-video R@1":"35.6",
            "text-to-video R@5":"65.3",
            "text-to-video R@10":"78",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":"3",
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":35.6,
            "text-to-video R@5":65.3,
            "text-to-video R@10":78.0,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":3.0,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912961,
            "title":"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions",
            "url":"\/paper\/advancing-high-resolution-video-language",
            "published":"2021-11-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/advancing-high-resolution-video-language\/review\/?hl=110581"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":87208,
        "rank":14,
        "method":"VideoCoCa (zero-shot)",
        "mlmodel":{

        },
        "Model":"VideoCoCa ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "text-to-video R@1":"34.3",
            "text-to-video R@5":"57.8",
            "text-to-video R@10":"67.0",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":"64.7",
            "video-to-text R@5":"85.2",
            "video-to-text R@10":"91.4",
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":34.3,
            "text-to-video R@5":57.8,
            "text-to-video R@10":67.0,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":64.7,
            "video-to-text R@5":85.2,
            "video-to-text R@10":91.4,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1126258,
            "title":"VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners",
            "url":"\/paper\/video-text-modeling-with-zero-shot-transfer",
            "published":"2022-12-09T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/video-text-modeling-with-zero-shot-transfer\/review\/?hl=87208"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":49158,
        "rank":15,
        "method":"MDMMT-2",
        "mlmodel":{

        },
        "Model":"MDMMT-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-14",
        "metrics":{
            "text-to-video R@1":"33.7",
            "text-to-video R@5":"60.5",
            "text-to-video R@10":"70.8",
            "text-to-video Mean Rank":"37.8",
            "text-to-video Median Rank":"3.0",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":33.7,
            "text-to-video R@5":60.5,
            "text-to-video R@10":70.8,
            "text-to-video Mean Rank":37.8,
            "text-to-video Median Rank":3.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":976160,
            "title":"MDMMT-2: Multidomain Multimodal Transformer for Video Retrieval, One More Step Towards Generalization",
            "url":"\/paper\/mdmmt-2-multidomain-multimodal-transformer",
            "published":"2022-03-14T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/mdmmt-2-multidomain-multimodal-transformer\/review\/?hl=49158"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":100011,
        "rank":16,
        "method":"VIOLET + MELTR",
        "mlmodel":{

        },
        "Model":"VIOLET + MELTR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "text-to-video R@1":"33.6",
            "text-to-video R@5":"63.7",
            "text-to-video R@10":"77.8",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":"3",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":33.6,
            "text-to-video R@5":63.7,
            "text-to-video R@10":77.8,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":3.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1179150,
            "title":"MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models",
            "url":"\/paper\/meltr-meta-loss-transformer-for-learning-to",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meltr-meta-loss-transformer-for-learning-to\/review\/?hl=100011"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":35395,
        "rank":17,
        "method":"CLIP2TV",
        "mlmodel":{

        },
        "Model":"CLIP2TV",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-10",
        "metrics":{
            "text-to-video R@1":"33.1",
            "text-to-video R@5":"58.9",
            "text-to-video R@10":"68.9",
            "text-to-video Mean Rank":"44.7",
            "text-to-video Median Rank":"3",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":33.1,
            "text-to-video R@5":58.9,
            "text-to-video R@10":68.9,
            "text-to-video Mean Rank":44.7,
            "text-to-video Median Rank":3.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":908143,
            "title":"CLIP2TV: Align, Match and Distill for Video-Text Retrieval",
            "url":"\/paper\/clip2tv-an-empirical-study-on-transformer",
            "published":"2021-11-10T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/clip2tv-an-empirical-study-on-transformer\/review\/?hl=35395"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":42263,
        "rank":18,
        "method":"CAMoE",
        "mlmodel":{

        },
        "Model":"CAMoE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-09",
        "metrics":{
            "text-to-video R@1":"32.9",
            "text-to-video R@5":"58.3",
            "text-to-video R@10":"68.4",
            "text-to-video Mean Rank":"42.6",
            "text-to-video Median Rank":"3",
            "video-to-text R@1":"59.8",
            "video-to-text R@5":"86.2",
            "video-to-text R@10":"92.8",
            "video-to-text Median Rank":"1",
            "video-to-text Mean Rank":"3.8",
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":32.9,
            "text-to-video R@5":58.3,
            "text-to-video R@10":68.4,
            "text-to-video Mean Rank":42.6,
            "text-to-video Median Rank":3.0,
            "video-to-text R@1":59.8,
            "video-to-text R@5":86.2,
            "video-to-text R@10":92.8,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":3.8,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":864259,
            "title":"Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss",
            "url":"\/paper\/improving-video-text-retrieval-by-multi",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improving-video-text-retrieval-by-multi\/review\/?hl=42263"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":69061,
        "rank":19,
        "method":"FROZEN",
        "mlmodel":{

        },
        "Model":"FROZEN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-01",
        "metrics":{
            "text-to-video R@1":"32.5",
            "text-to-video R@5":"61.5",
            "text-to-video R@10":"71.2",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":32.5,
            "text-to-video R@5":61.5,
            "text-to-video R@10":71.2,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":773452,
            "title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
            "url":"\/paper\/frozen-in-time-a-joint-video-and-image",
            "published":"2021-04-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/frozen-in-time-a-joint-video-and-image\/review\/?hl=69061"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":60403,
        "rank":20,
        "method":"COTS",
        "mlmodel":{

        },
        "Model":"COTS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-15",
        "metrics":{
            "text-to-video R@1":"32.1",
            "text-to-video R@5":"60.8",
            "text-to-video R@10":"70.2",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":"3",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":32.1,
            "text-to-video R@5":60.8,
            "text-to-video R@10":70.2,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":3.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":995103,
            "title":"COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval",
            "url":"\/paper\/cots-collaborative-two-stream-vision-language",
            "published":"2022-04-15T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/cots-collaborative-two-stream-vision-language\/review\/?hl=60403"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":54163,
        "rank":21,
        "method":"CoCa (zero-shot)",
        "mlmodel":{

        },
        "Model":"CoCa ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "text-to-video R@1":"30.0",
            "text-to-video R@5":"52.4",
            "text-to-video R@10":"61.6",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":"49.9",
            "video-to-text R@5":"73.4",
            "video-to-text R@10":"81.4",
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":30.0,
            "text-to-video R@5":52.4,
            "text-to-video R@10":61.6,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":49.9,
            "video-to-text R@5":73.4,
            "video-to-text R@10":81.4,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=54163"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":39884,
        "rank":22,
        "method":"CLIP2Video",
        "mlmodel":{

        },
        "Model":"CLIP2Video",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-21",
        "metrics":{
            "text-to-video R@1":"29.8",
            "text-to-video R@5":"55.5",
            "text-to-video R@10":"66.2",
            "text-to-video Mean Rank":"45.4",
            "text-to-video Median Rank":"4",
            "video-to-text R@1":"54.6",
            "video-to-text R@5":"82.1",
            "video-to-text R@10":"90.8",
            "video-to-text Median Rank":"1",
            "video-to-text Mean Rank":"5.3",
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":29.8,
            "text-to-video R@5":55.5,
            "text-to-video R@10":66.2,
            "text-to-video Mean Rank":45.4,
            "text-to-video Median Rank":4.0,
            "video-to-text R@1":54.6,
            "video-to-text R@5":82.1,
            "video-to-text R@10":90.8,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":5.3,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":821802,
            "title":"CLIP2Video: Mastering Video-Text Retrieval via Image CLIP",
            "url":"\/paper\/clip2video-mastering-video-text-retrieval-via",
            "published":"2021-06-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clip2video-mastering-video-text-retrieval-via\/review\/?hl=39884"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":43908,
        "rank":23,
        "method":"LAFF",
        "mlmodel":{

        },
        "Model":"LAFF",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-03",
        "metrics":{
            "text-to-video R@1":"29.1",
            "text-to-video R@5":"54.9",
            "text-to-video R@10":"65.8",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":29.1,
            "text-to-video R@5":54.9,
            "text-to-video R@10":65.8,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":925470,
            "title":"Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval",
            "url":"\/paper\/lightweight-attentional-feature-fusion-for",
            "published":"2021-12-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lightweight-attentional-feature-fusion-for\/review\/?hl=43908"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":100009,
        "rank":24,
        "method":"UniVL + MELTR",
        "mlmodel":{

        },
        "Model":"UniVL + MELTR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "text-to-video R@1":"28.5",
            "text-to-video R@5":"55.5",
            "text-to-video R@10":"67.6",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":"4",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":28.5,
            "text-to-video R@5":55.5,
            "text-to-video R@10":67.6,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":4.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1179150,
            "title":"MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models",
            "url":"\/paper\/meltr-meta-loss-transformer-for-learning-to",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meltr-meta-loss-transformer-for-learning-to\/review\/?hl=100009"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":48514,
        "rank":25,
        "method":"Ours",
        "mlmodel":{

        },
        "Model":"Ours",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-21",
        "metrics":{
            "text-to-video R@1":"26",
            "text-to-video R@5":"56.7",
            "text-to-video R@10":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":"3",
            "video-to-text R@1":"26.7",
            "video-to-text R@5":"56.5",
            "video-to-text R@10":null,
            "video-to-text Median Rank":"3",
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":26.0,
            "text-to-video R@5":56.7,
            "text-to-video R@10":null,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":3.0,
            "video-to-text R@1":26.7,
            "video-to-text R@5":56.5,
            "video-to-text R@10":null,
            "video-to-text Median Rank":3.0,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":892158,
            "title":"Video and Text Matching with Conditioned Embeddings",
            "url":"\/paper\/video-and-text-matching-with-conditioned",
            "published":"2021-10-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-and-text-matching-with-conditioned\/review\/?hl=48514"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":38602,
        "rank":26,
        "method":"TACo",
        "mlmodel":{

        },
        "Model":"TACo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-08-23",
        "metrics":{
            "text-to-video R@1":"24.8",
            "text-to-video R@5":"52.1",
            "text-to-video R@10":"64.0",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":"5",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":24.8,
            "text-to-video R@5":52.1,
            "text-to-video R@10":64.0,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":5.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":854963,
            "title":"TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment",
            "url":"\/paper\/taco-token-aware-cascade-contrastive-learning",
            "published":"2021-08-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/taco-token-aware-cascade-contrastive-learning\/review\/?hl=38602"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":28484,
        "rank":27,
        "method":"MDMMT",
        "mlmodel":{

        },
        "Model":"MDMMT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-19",
        "metrics":{
            "text-to-video R@1":"23.1",
            "text-to-video R@5":"49.8",
            "text-to-video R@10":"61.8",
            "text-to-video Mean Rank":"52.8",
            "text-to-video Median Rank":"6",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":23.1,
            "text-to-video R@5":49.8,
            "text-to-video R@10":61.8,
            "text-to-video Mean Rank":52.8,
            "text-to-video Median Rank":6.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":755432,
            "title":"MDMMT: Multidomain Multimodal Transformer for Video Retrieval",
            "url":"\/paper\/mdmmt-multidomain-multimodal-transformer-for",
            "published":"2021-03-19T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":27318,
        "rank":28,
        "method":"CLIP",
        "mlmodel":{

        },
        "Model":"CLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-24",
        "metrics":{
            "text-to-video R@1":"21.4",
            "text-to-video R@5":"41.1",
            "text-to-video R@10":"50.4",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":"10",
            "video-to-text R@1":"40.3",
            "video-to-text R@5":"69.7",
            "video-to-text R@10":"79.2",
            "video-to-text Median Rank":"2",
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":21.4,
            "text-to-video R@5":41.1,
            "text-to-video R@10":50.4,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":10.0,
            "video-to-text R@1":40.3,
            "video-to-text R@5":69.7,
            "video-to-text R@10":79.2,
            "video-to-text Median Rank":2.0,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":748268,
            "title":"A Straightforward Framework For Video Retrieval Using CLIP",
            "url":"\/paper\/a-straightforward-framework-for-video",
            "published":"2021-02-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-straightforward-framework-for-video\/review\/?hl=27318"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":28777,
        "rank":29,
        "method":"UniVL",
        "mlmodel":{

        },
        "Model":"UniVL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-02-15",
        "metrics":{
            "text-to-video R@1":"21.2",
            "text-to-video R@5":"49.6",
            "text-to-video R@10":"63.1",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":"6",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":21.2,
            "text-to-video R@5":49.6,
            "text-to-video R@10":63.1,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":6.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":183645,
            "title":"UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation",
            "url":"\/paper\/univilm-a-unified-video-and-language-pre",
            "published":"2020-02-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/univilm-a-unified-video-and-language-pre\/review\/?hl=28777"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":12876,
        "rank":30,
        "method":"Text-Video Embedding",
        "mlmodel":{

        },
        "Model":"Text-Video Embedding",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-07",
        "metrics":{
            "text-to-video R@1":"14.9",
            "text-to-video R@5":null,
            "text-to-video R@10":"52.8",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":"9",
            "video-to-text R@1":null,
            "video-to-text R@5":"40.2",
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":14.9,
            "text-to-video R@5":null,
            "text-to-video R@10":52.8,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":9.0,
            "video-to-text R@1":null,
            "video-to-text R@5":40.2,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142178,
            "title":"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips",
            "url":"\/paper\/howto100m-learning-a-text-video-embedding-by",
            "published":"2019-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/howto100m-learning-a-text-video-embedding-by\/review\/?hl=12876"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":60072,
        "rank":31,
        "method":"RoME",
        "mlmodel":{

        },
        "Model":"RoME",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-26",
        "metrics":{
            "text-to-video R@1":"10.7",
            "text-to-video R@5":"29.6",
            "text-to-video R@10":"41.2",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":"17",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":10.7,
            "text-to-video R@5":29.6,
            "text-to-video R@10":41.2,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":17.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1033365,
            "title":"RoME: Role-aware Mixture-of-Expert Transformer for Text-to-Video Retrieval",
            "url":"\/paper\/rome-role-aware-mixture-of-expert-transformer",
            "published":"2022-06-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rome-role-aware-mixture-of-expert-transformer\/review\/?hl=60072"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":12877,
        "rank":32,
        "method":"JSFusion",
        "mlmodel":{

        },
        "Model":"JSFusion",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-08-07",
        "metrics":{
            "text-to-video R@1":"10.2",
            "text-to-video R@5":null,
            "text-to-video R@10":"43.2",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":"13",
            "video-to-text R@1":null,
            "video-to-text R@5":"31.2",
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":10.2,
            "text-to-video R@5":null,
            "text-to-video R@10":43.2,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":13.0,
            "video-to-text R@1":null,
            "video-to-text R@5":31.2,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":54495,
            "title":"A Joint Sequence Fusion Model for Video Question Answering and Retrieval",
            "url":"\/paper\/a-joint-sequence-fusion-model-for-video",
            "published":"2018-08-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-joint-sequence-fusion-model-for-video\/review\/?hl=12877"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":6120,
        "rank":33,
        "method":"Collaborative Experts",
        "mlmodel":{

        },
        "Model":"Collaborative Experts",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-31",
        "metrics":{
            "text-to-video R@1":"10.0",
            "text-to-video R@5":"29.0",
            "text-to-video R@10":"41.2",
            "text-to-video Mean Rank":"86.8",
            "text-to-video Median Rank":"16",
            "video-to-text R@1":"15.6",
            "video-to-text R@5":"40.9",
            "video-to-text R@10":"55.2",
            "video-to-text Median Rank":"8.3",
            "video-to-text Mean Rank":"38.1",
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":10.0,
            "text-to-video R@5":29.0,
            "text-to-video R@10":41.2,
            "text-to-video Mean Rank":86.8,
            "text-to-video Median Rank":16.0,
            "video-to-text R@1":15.6,
            "video-to-text R@5":40.9,
            "video-to-text R@10":55.2,
            "video-to-text Median Rank":8.3,
            "video-to-text Mean Rank":38.1,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":148748,
            "title":"Use What You Have: Video Retrieval Using Representations From Collaborative Experts",
            "url":"\/paper\/use-what-you-have-video-retrieval-using",
            "published":"2019-07-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/use-what-you-have-video-retrieval-using\/review\/?hl=6120"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":6119,
        "rank":34,
        "method":"JEMC",
        "mlmodel":{

        },
        "Model":"JEMC",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-06-11",
        "metrics":{
            "text-to-video R@1":"7.0",
            "text-to-video R@5":"20.9",
            "text-to-video R@10":"29.7",
            "text-to-video Mean Rank":"213.8",
            "text-to-video Median Rank":"29.7",
            "video-to-text R@1":"12.5",
            "video-to-text R@5":"32.1",
            "video-to-text R@10":"42.2",
            "video-to-text Median Rank":"16",
            "video-to-text Mean Rank":"134",
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":7.0,
            "text-to-video R@5":20.9,
            "text-to-video R@10":29.7,
            "text-to-video Mean Rank":213.8,
            "text-to-video Median Rank":29.7,
            "video-to-text R@1":12.5,
            "video-to-text R@5":32.1,
            "video-to-text R@10":42.2,
            "video-to-text Median Rank":16.0,
            "video-to-text Mean Rank":134.0,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":148915,
            "title":"Learning Joint Embedding with Multimodal Cues for Cross-Modal Video-Text Retrieval",
            "url":"\/paper\/learning-joint-embedding-with-multimodal-cues",
            "published":"2018-06-11T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":12878,
        "rank":35,
        "method":"Kaufman",
        "mlmodel":{

        },
        "Model":"Kaufman",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-12-21",
        "metrics":{
            "text-to-video R@1":"4.7",
            "text-to-video R@5":null,
            "text-to-video R@10":"24.1",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":"41",
            "video-to-text R@1":null,
            "video-to-text R@5":"16.6",
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":4.7,
            "text-to-video R@5":null,
            "text-to-video R@10":24.1,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":41.0,
            "video-to-text R@1":null,
            "video-to-text R@5":16.6,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":24050,
            "title":"Temporal Tessellation: A Unified Approach for Video Analysis",
            "url":"\/paper\/temporal-tessellation-a-unified-approach-for",
            "published":"2016-12-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporal-tessellation-a-unified-approach-for\/review\/?hl=12878"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1136,
        "row_id":12879,
        "rank":36,
        "method":"C+LSTM+SA+FC7",
        "mlmodel":{

        },
        "Model":"C+LSTM+SA+FC7",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-09-26",
        "metrics":{
            "text-to-video R@1":"4.2",
            "text-to-video R@5":null,
            "text-to-video R@10":"19.9",
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":"55",
            "video-to-text R@1":null,
            "video-to-text R@5":"12.9",
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "raw_metrics":{
            "text-to-video R@1":4.2,
            "text-to-video R@5":null,
            "text-to-video R@10":19.9,
            "text-to-video Mean Rank":null,
            "text-to-video Median Rank":55.0,
            "video-to-text R@1":null,
            "video-to-text R@5":12.9,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-video MedianR":null,
            "text-to-videoMedian Rank":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":30250,
            "title":"Learning Language-Visual Embedding for Movie Understanding with Natural-Language",
            "url":"\/paper\/learning-language-visual-embedding-for-movie",
            "published":"2016-09-26T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/learning-language-visual-embedding-for-movie\/review\/?hl=12879"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":8,
                "name":"LSTM",
                "color":"#e60000"
            }
        ],
        "reports":[

        ]
    }
]