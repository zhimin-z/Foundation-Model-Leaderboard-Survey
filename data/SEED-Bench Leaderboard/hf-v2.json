[
    {
        "Model":"BLIP-2",
        "Language Model":"Flan-T5-XL",
        "Model Size":"3B",
        "Evaluation Method":"PPL",
        "Avg. Single":46.8,
        "Avg. Multi":22.8,
        "Avg. Video":36.0,
        "Avg. P1":44.2,
        "Avg. P2":37.3,
        "Avg. P3":0.0,
        "Scene Understanding":58.5,
        "Instance Identity":48.6,
        "Instance Attribute":49.0,
        "Instance Location":39.1,
        "Instance Counting":43.4,
        "Spatial Relation":36.2,
        "Instance Interaction":48.5,
        "Visual Reasoning":52.9,
        "Text Recognition":60.7,
        "Celebrity Recognition":51.8,
        "Landmark Recognition":51.4,
        "Chart Understanding":19.2,
        "Visual Referring Expression":43.2,
        "Science Knowledge":52.4,
        "Emotion Recognition":29.3,
        "Visual Mathematics":22.0,
        "Difference Spotting":17.8,
        "Meme Comprehension":38.6,
        "Global Video Understanding":42.5,
        "Action Recognition":37.7,
        "Action Predicion":36.2,
        "Procedure Understanding":22.9,
        "In-Context Captioning":40.0,
        "Interleaved Image-Text Analysis":30.6,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/salesforce\/LAVIS"
    },
    {
        "Model":"InstructBLIP",
        "Language Model":"Flan-T5-XL",
        "Model Size":"3B",
        "Evaluation Method":"PPL",
        "Avg. Single":52.4,
        "Avg. Multi":25.8,
        "Avg. Video":36.5,
        "Avg. P1":48.6,
        "Avg. P2":36.1,
        "Avg. P3":0.0,
        "Scene Understanding":58.9,
        "Instance Identity":49.7,
        "Instance Attribute":61.7,
        "Instance Location":35.1,
        "Instance Counting":58.1,
        "Spatial Relation":34.9,
        "Instance Interaction":47.4,
        "Visual Reasoning":55.9,
        "Text Recognition":61.4,
        "Celebrity Recognition":48.5,
        "Landmark Recognition":45.4,
        "Chart Understanding":26.4,
        "Visual Referring Expression":41.7,
        "Science Knowledge":47.7,
        "Emotion Recognition":34.5,
        "Visual Mathematics":21.2,
        "Difference Spotting":22.8,
        "Meme Comprehension":35.2,
        "Global Video Understanding":41.5,
        "Action Recognition":36.1,
        "Action Predicion":40.5,
        "Procedure Understanding":24.5,
        "In-Context Captioning":36.7,
        "Interleaved Image-Text Analysis":34.7,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/salesforce\/LAVIS"
    },
    {
        "Model":"InstructBLIP-Vicuna",
        "Language Model":"Vicuna-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":47.5,
        "Avg. Multi":41.1,
        "Avg. Video":33.0,
        "Avg. P1":44.2,
        "Avg. P2":28.4,
        "Avg. P3":0.0,
        "Scene Understanding":53.6,
        "Instance Identity":43.9,
        "Instance Attribute":49.0,
        "Instance Location":37.8,
        "Instance Counting":56.5,
        "Spatial Relation":35.8,
        "Instance Interaction":43.3,
        "Visual Reasoning":56.2,
        "Text Recognition":57.2,
        "Celebrity Recognition":60.3,
        "Landmark Recognition":44.4,
        "Chart Understanding":27.9,
        "Visual Referring Expression":39.2,
        "Science Knowledge":39.4,
        "Emotion Recognition":23.0,
        "Visual Mathematics":26.5,
        "Difference Spotting":36.5,
        "Meme Comprehension":55.4,
        "Global Video Understanding":40.4,
        "Action Recognition":38.6,
        "Action Predicion":31.2,
        "Procedure Understanding":15.6,
        "In-Context Captioning":26.7,
        "Interleaved Image-Text Analysis":32.7,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/salesforce\/LAVIS"
    },
    {
        "Model":"LLaVA",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":42.4,
        "Avg. Multi":32.5,
        "Avg. Video":32.6,
        "Avg. P1":40.2,
        "Avg. P2":34.3,
        "Avg. P3":0.0,
        "Scene Understanding":53.8,
        "Instance Identity":47.5,
        "Instance Attribute":38.3,
        "Instance Location":34.2,
        "Instance Counting":42.0,
        "Spatial Relation":34.7,
        "Instance Interaction":40.2,
        "Visual Reasoning":52.9,
        "Text Recognition":46.4,
        "Celebrity Recognition":51.8,
        "Landmark Recognition":45.6,
        "Chart Understanding":30.3,
        "Visual Referring Expression":40.2,
        "Science Knowledge":37.6,
        "Emotion Recognition":34.3,
        "Visual Mathematics":20.5,
        "Difference Spotting":27.0,
        "Meme Comprehension":50.0,
        "Global Video Understanding":44.1,
        "Action Recognition":36.2,
        "Action Predicion":25.1,
        "Procedure Understanding":18.6,
        "In-Context Captioning":40.0,
        "Interleaved Image-Text Analysis":20.4,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/haotian-liu\/LLaVA"
    },
    {
        "Model":"MiniGPT-4",
        "Language Model":"Vicuna-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":45.0,
        "Avg. Multi":25.7,
        "Avg. Video":34.3,
        "Avg. P1":42.5,
        "Avg. P2":39.0,
        "Avg. P3":0.0,
        "Scene Understanding":56.3,
        "Instance Identity":49.2,
        "Instance Attribute":45.8,
        "Instance Location":37.9,
        "Instance Counting":45.3,
        "Spatial Relation":32.6,
        "Instance Interaction":47.4,
        "Visual Reasoning":57.1,
        "Text Recognition":41.8,
        "Celebrity Recognition":55.2,
        "Landmark Recognition":45.2,
        "Chart Understanding":20.2,
        "Visual Referring Expression":41.2,
        "Science Knowledge":43.3,
        "Emotion Recognition":24.2,
        "Visual Mathematics":25.0,
        "Difference Spotting":19.0,
        "Meme Comprehension":46.7,
        "Global Video Understanding":39.0,
        "Action Recognition":38.7,
        "Action Predicion":27.4,
        "Procedure Understanding":28.6,
        "In-Context Captioning":45.8,
        "Interleaved Image-Text Analysis":22.5,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/Vision-CAIR\/MiniGPT-4"
    },
    {
        "Model":"VPGTrans",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":36.6,
        "Avg. Multi":29.3,
        "Avg. Video":33.2,
        "Avg. P1":35.8,
        "Avg. P2":21.9,
        "Avg. P3":0.0,
        "Scene Understanding":46.9,
        "Instance Identity":38.6,
        "Instance Attribute":33.6,
        "Instance Location":35.6,
        "Instance Counting":27.5,
        "Spatial Relation":34.4,
        "Instance Interaction":33.0,
        "Visual Reasoning":50.8,
        "Text Recognition":47.6,
        "Celebrity Recognition":52.4,
        "Landmark Recognition":38.2,
        "Chart Understanding":30.1,
        "Visual Referring Expression":34.7,
        "Science Knowledge":36.1,
        "Emotion Recognition":31.5,
        "Visual Mathematics":27.3,
        "Difference Spotting":24.6,
        "Meme Comprehension":44.0,
        "Global Video Understanding":37.8,
        "Action Recognition":38.2,
        "Action Predicion":20.9,
        "Procedure Understanding":33.5,
        "In-Context Captioning":19.2,
        "Interleaved Image-Text Analysis":28.6,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/VPGTrans\/VPGTrans"
    },
    {
        "Model":"MultiModal-GPT",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":36.7,
        "Avg. Multi":44.1,
        "Avg. Video":32.6,
        "Avg. P1":35.9,
        "Avg. P2":36.7,
        "Avg. P3":0.0,
        "Scene Understanding":46.9,
        "Instance Identity":42.5,
        "Instance Attribute":32.0,
        "Instance Location":32.3,
        "Instance Counting":27.7,
        "Spatial Relation":29.7,
        "Instance Interaction":29.9,
        "Visual Reasoning":48.3,
        "Text Recognition":35.2,
        "Celebrity Recognition":60.9,
        "Landmark Recognition":50.4,
        "Chart Understanding":24.2,
        "Visual Referring Expression":42.2,
        "Science Knowledge":37.6,
        "Emotion Recognition":32.1,
        "Visual Mathematics":27.3,
        "Difference Spotting":40.1,
        "Meme Comprehension":56.5,
        "Global Video Understanding":37.6,
        "Action Recognition":38.7,
        "Action Predicion":25.3,
        "Procedure Understanding":24.4,
        "In-Context Captioning":39.2,
        "Interleaved Image-Text Analysis":30.6,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/open-mmlab\/Multimodal-GPT"
    },
    {
        "Model":"Otter",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":36.0,
        "Avg. Multi":32.0,
        "Avg. Video":32.3,
        "Avg. P1":35.2,
        "Avg. P2":39.0,
        "Avg. P3":0.0,
        "Scene Understanding":45.9,
        "Instance Identity":39.7,
        "Instance Attribute":31.9,
        "Instance Location":31.6,
        "Instance Counting":26.4,
        "Spatial Relation":32.0,
        "Instance Interaction":33.0,
        "Visual Reasoning":49.2,
        "Text Recognition":39.3,
        "Celebrity Recognition":59.7,
        "Landmark Recognition":53.0,
        "Chart Understanding":23.6,
        "Visual Referring Expression":41.2,
        "Science Knowledge":36.1,
        "Emotion Recognition":37.3,
        "Visual Mathematics":22.0,
        "Difference Spotting":27.4,
        "Meme Comprehension":46.7,
        "Global Video Understanding":36.6,
        "Action Recognition":37.9,
        "Action Predicion":26.0,
        "Procedure Understanding":24.8,
        "In-Context Captioning":42.5,
        "Interleaved Image-Text Analysis":30.6,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/Luodian\/Otter"
    },
    {
        "Model":"OpenFlamingo",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":36.6,
        "Avg. Multi":43.5,
        "Avg. Video":32.4,
        "Avg. P1":35.8,
        "Avg. P2":36.7,
        "Avg. P3":0.0,
        "Scene Understanding":46.7,
        "Instance Identity":42.3,
        "Instance Attribute":31.7,
        "Instance Location":33.4,
        "Instance Counting":27.4,
        "Spatial Relation":29.8,
        "Instance Interaction":29.9,
        "Visual Reasoning":47.7,
        "Text Recognition":35.6,
        "Celebrity Recognition":60.3,
        "Landmark Recognition":49.8,
        "Chart Understanding":24.2,
        "Visual Referring Expression":42.2,
        "Science Knowledge":39.0,
        "Emotion Recognition":32.1,
        "Visual Mathematics":27.3,
        "Difference Spotting":39.9,
        "Meme Comprehension":54.9,
        "Global Video Understanding":37.6,
        "Action Recognition":38.4,
        "Action Predicion":25.2,
        "Procedure Understanding":24.1,
        "In-Context Captioning":38.3,
        "Interleaved Image-Text Analysis":32.7,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/mlfoundations\/open_flamingo"
    },
    {
        "Model":"LLaMA-AdapterV2",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":36.0,
        "Avg. Multi":34.7,
        "Avg. Video":31.4,
        "Avg. P1":35.1,
        "Avg. P2":0.0,
        "Avg. P3":0.0,
        "Scene Understanding":45.2,
        "Instance Identity":38.5,
        "Instance Attribute":29.3,
        "Instance Location":33.0,
        "Instance Counting":29.7,
        "Spatial Relation":35.5,
        "Instance Interaction":39.2,
        "Visual Reasoning":52.0,
        "Text Recognition":48.7,
        "Celebrity Recognition":58.5,
        "Landmark Recognition":46.4,
        "Chart Understanding":24.2,
        "Visual Referring Expression":41.2,
        "Science Knowledge":40.1,
        "Emotion Recognition":39.7,
        "Visual Mathematics":23.5,
        "Difference Spotting":29.1,
        "Meme Comprehension":52.2,
        "Global Video Understanding":41.9,
        "Action Recognition":38.2,
        "Action Predicion":18.8,
        "Procedure Understanding":20.3,
        "In-Context Captioning":0.0,
        "Interleaved Image-Text Analysis":0.0,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/OpenGVLab\/LLaMA-Adapter"
    },
    {
        "Model":"GVT",
        "Language Model":"Vicuna-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":34.9,
        "Avg. Multi":45.8,
        "Avg. Video":31.0,
        "Avg. P1":34.2,
        "Avg. P2":40.2,
        "Avg. P3":0.0,
        "Scene Understanding":41.7,
        "Instance Identity":35.5,
        "Instance Attribute":31.8,
        "Instance Location":29.5,
        "Instance Counting":36.2,
        "Spatial Relation":32.0,
        "Instance Interaction":32.0,
        "Visual Reasoning":51.1,
        "Text Recognition":35.2,
        "Celebrity Recognition":39.4,
        "Landmark Recognition":36.4,
        "Chart Understanding":25.0,
        "Visual Referring Expression":36.2,
        "Science Knowledge":31.1,
        "Emotion Recognition":20.6,
        "Visual Mathematics":22.7,
        "Difference Spotting":41.5,
        "Meme Comprehension":59.2,
        "Global Video Understanding":40.4,
        "Action Recognition":29.7,
        "Action Predicion":26.3,
        "Procedure Understanding":24.1,
        "In-Context Captioning":42.5,
        "Interleaved Image-Text Analysis":34.7,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/TencentARC\/GVT"
    },
    {
        "Model":"mPLUG-Owl",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":38.6,
        "Avg. Multi":38.7,
        "Avg. Video":31.1,
        "Avg. P1":36.9,
        "Avg. P2":29.0,
        "Avg. P3":0.0,
        "Scene Understanding":49.7,
        "Instance Identity":45.3,
        "Instance Attribute":32.5,
        "Instance Location":36.7,
        "Instance Counting":27.3,
        "Spatial Relation":32.7,
        "Instance Interaction":44.3,
        "Visual Reasoning":54.7,
        "Text Recognition":49.2,
        "Celebrity Recognition":70.9,
        "Landmark Recognition":49.6,
        "Chart Understanding":23.2,
        "Visual Referring Expression":44.2,
        "Science Knowledge":44.0,
        "Emotion Recognition":32.5,
        "Visual Mathematics":23.5,
        "Difference Spotting":33.5,
        "Meme Comprehension":54.9,
        "Global Video Understanding":42.0,
        "Action Recognition":37.8,
        "Action Predicion":18.3,
        "Procedure Understanding":19.3,
        "In-Context Captioning":29.2,
        "Interleaved Image-Text Analysis":28.6,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/X-PLUG\/mPLUG-Owl"
    },
    {
        "Model":"Kosmos-2",
        "Language Model":"Decoder only 1.3B",
        "Model Size":"1.3B",
        "Evaluation Method":"PPL",
        "Avg. Single":52.4,
        "Avg. Multi":29.4,
        "Avg. Video":40.7,
        "Avg. P1":49.6,
        "Avg. P2":23.7,
        "Avg. P3":0.0,
        "Scene Understanding":63.4,
        "Instance Identity":57.1,
        "Instance Attribute":58.5,
        "Instance Location":44.0,
        "Instance Counting":41.4,
        "Spatial Relation":37.9,
        "Instance Interaction":55.7,
        "Visual Reasoning":60.7,
        "Text Recognition":68.1,
        "Celebrity Recognition":82.1,
        "Landmark Recognition":51.4,
        "Chart Understanding":21.2,
        "Visual Referring Expression":48.2,
        "Science Knowledge":43.7,
        "Emotion Recognition":30.7,
        "Visual Mathematics":28.0,
        "Difference Spotting":25.2,
        "Meme Comprehension":42.8,
        "Global Video Understanding":48.5,
        "Action Recognition":40.8,
        "Action Predicion":39.5,
        "Procedure Understanding":30.0,
        "In-Context Captioning":24.2,
        "Interleaved Image-Text Analysis":22.5,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/microsoft\/unilm\/tree\/master\/kosmos-2"
    },
    {
        "Model":"Qwen-VL-Chat",
        "Language Model":"Qwen-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":50.3,
        "Avg. Multi":37.4,
        "Avg. Video":34.3,
        "Avg. P1":46.6,
        "Avg. P2":38.5,
        "Avg. P3":0.0,
        "Scene Understanding":56.5,
        "Instance Identity":47.6,
        "Instance Attribute":54.8,
        "Instance Location":46.9,
        "Instance Counting":54.2,
        "Spatial Relation":40.3,
        "Instance Interaction":55.7,
        "Visual Reasoning":55.0,
        "Text Recognition":47.4,
        "Celebrity Recognition":62.4,
        "Landmark Recognition":55.6,
        "Chart Understanding":25.2,
        "Visual Referring Expression":43.7,
        "Science Knowledge":41.2,
        "Emotion Recognition":20.6,
        "Visual Mathematics":28.8,
        "Difference Spotting":34.3,
        "Meme Comprehension":47.2,
        "Global Video Understanding":39.7,
        "Action Recognition":42.8,
        "Action Predicion":29.6,
        "Procedure Understanding":19.1,
        "In-Context Captioning":42.5,
        "Interleaved Image-Text Analysis":28.6,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/huggingface.co\/Qwen\/Qwen-VL-Chat"
    },
    {
        "Model":"Qwen-VL-plus",
        "Language Model":"Qwen-LM",
        "Model Size":"-",
        "Evaluation Method":"PPL for A\/B\/C\/D",
        "Avg. Single":71.9,
        "Avg. Multi":70.3,
        "Avg. Video":46.7,
        "Avg. P1":66.1,
        "Avg. P2":29.6,
        "Avg. P3":0.0,
        "Scene Understanding":76.6,
        "Instance Identity":77.7,
        "Instance Attribute":76.3,
        "Instance Location":65.1,
        "Instance Counting":65.8,
        "Spatial Relation":55.9,
        "Instance Interaction":73.2,
        "Visual Reasoning":77.9,
        "Text Recognition":61.8,
        "Celebrity Recognition":97.0,
        "Landmark Recognition":97.2,
        "Chart Understanding":39.5,
        "Visual Referring Expression":73.4,
        "Science Knowledge":75.8,
        "Emotion Recognition":51.7,
        "Visual Mathematics":38.6,
        "Difference Spotting":66.7,
        "Meme Comprehension":81.8,
        "Global Video Understanding":51.8,
        "Action Recognition":54.5,
        "Action Predicion":29.3,
        "Procedure Understanding":48.0,
        "In-Context Captioning":28.3,
        "Interleaved Image-Text Analysis":32.7,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/QwenLM\/Qwen-VL\/tree\/master?tab=readme-ov-file#qwen-vl-plus"
    },
    {
        "Model":"LLaVA-1.5",
        "Language Model":"vicuna-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":58.3,
        "Avg. Multi":39.2,
        "Avg. Video":36.9,
        "Avg. P1":53.3,
        "Avg. P2":34.4,
        "Avg. P3":0.0,
        "Scene Understanding":63.7,
        "Instance Identity":62.4,
        "Instance Attribute":66.7,
        "Instance Location":51.3,
        "Instance Counting":60.2,
        "Spatial Relation":38.5,
        "Instance Interaction":47.4,
        "Visual Reasoning":59.8,
        "Text Recognition":69.0,
        "Celebrity Recognition":60.6,
        "Landmark Recognition":49.8,
        "Chart Understanding":25.0,
        "Visual Referring Expression":45.7,
        "Science Knowledge":56.7,
        "Emotion Recognition":31.1,
        "Visual Mathematics":24.2,
        "Difference Spotting":35.7,
        "Meme Comprehension":50.3,
        "Global Video Understanding":46.1,
        "Action Recognition":39.4,
        "Action Predicion":29.4,
        "Procedure Understanding":28.1,
        "In-Context Captioning":39.2,
        "Interleaved Image-Text Analysis":22.5,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/haotian-liu\/LLaVA"
    },
    {
        "Model":"IDEFICS-9b-instruct",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":38.8,
        "Avg. Multi":54.5,
        "Avg. Video":32.9,
        "Avg. P1":37.5,
        "Avg. P2":42.6,
        "Avg. P3":0.0,
        "Scene Understanding":48.2,
        "Instance Identity":38.2,
        "Instance Attribute":37.8,
        "Instance Location":32.9,
        "Instance Counting":29.0,
        "Spatial Relation":32.4,
        "Instance Interaction":37.1,
        "Visual Reasoning":54.1,
        "Text Recognition":45.5,
        "Celebrity Recognition":52.4,
        "Landmark Recognition":52.8,
        "Chart Understanding":22.6,
        "Visual Referring Expression":42.7,
        "Science Knowledge":33.2,
        "Emotion Recognition":26.6,
        "Visual Mathematics":21.2,
        "Difference Spotting":56.5,
        "Meme Comprehension":48.4,
        "Global Video Understanding":42.7,
        "Action Recognition":38.6,
        "Action Predicion":23.6,
        "Procedure Understanding":20.5,
        "In-Context Captioning":45.8,
        "Interleaved Image-Text Analysis":34.7,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/huggingface.co\/HuggingFaceM4\/idefics-9b-instruct"
    },
    {
        "Model":"InternLM-XComposer-VL",
        "Language Model":"InternLM-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":65.4,
        "Avg. Multi":49.8,
        "Avg. Video":44.9,
        "Avg. P1":60.6,
        "Avg. P2":30.2,
        "Avg. P3":0.0,
        "Scene Understanding":74.8,
        "Instance Identity":70.5,
        "Instance Attribute":67.6,
        "Instance Location":60.5,
        "Instance Counting":55.3,
        "Spatial Relation":53.4,
        "Instance Interaction":76.3,
        "Visual Reasoning":76.1,
        "Text Recognition":61.4,
        "Celebrity Recognition":86.1,
        "Landmark Recognition":78.0,
        "Chart Understanding":27.2,
        "Visual Referring Expression":60.3,
        "Science Knowledge":84.8,
        "Emotion Recognition":68.9,
        "Visual Mathematics":25.8,
        "Difference Spotting":47.7,
        "Meme Comprehension":56.6,
        "Global Video Understanding":58.6,
        "Action Recognition":49.9,
        "Action Predicion":37.6,
        "Procedure Understanding":24.9,
        "In-Context Captioning":27.5,
        "Interleaved Image-Text Analysis":36.7,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/InternLM\/InternLM-XComposer"
    },
    {
        "Model":"SPHINXv1-1k",
        "Language Model":"LLaMA-2-13B",
        "Model Size":"13B",
        "Evaluation Method":"Generate",
        "Avg. Single":68.5,
        "Avg. Multi":37.7,
        "Avg. Video":40.0,
        "Avg. P1":61.7,
        "Avg. P2":32.5,
        "Avg. P3":0.0,
        "Scene Understanding":75.5,
        "Instance Identity":72.4,
        "Instance Attribute":75.1,
        "Instance Location":63.1,
        "Instance Counting":67.6,
        "Spatial Relation":50.4,
        "Instance Interaction":64.9,
        "Visual Reasoning":76.7,
        "Text Recognition":60.0,
        "Celebrity Recognition":81.5,
        "Landmark Recognition":82.4,
        "Chart Understanding":21.8,
        "Visual Referring Expression":60.3,
        "Science Knowledge":58.5,
        "Emotion Recognition":65.5,
        "Visual Mathematics":32.6,
        "Difference Spotting":35.9,
        "Meme Comprehension":43.4,
        "Global Video Understanding":52.4,
        "Action Recognition":41.2,
        "Action Predicion":33.9,
        "Procedure Understanding":26.1,
        "In-Context Captioning":33.3,
        "Interleaved Image-Text Analysis":30.6,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/Alpha-VLLM\/LLaMA2-Accessory\/tree\/main\/SPHINX"
    },
    {
        "Model":"SPHINXv2-1k",
        "Language Model":"LLaMA-2-13B",
        "Model Size":"13B",
        "Evaluation Method":"Generate",
        "Avg. Single":72.1,
        "Avg. Multi":44.8,
        "Avg. Video":44.2,
        "Avg. P1":65.4,
        "Avg. P2":35.5,
        "Avg. P3":0.0,
        "Scene Understanding":77.5,
        "Instance Identity":78.5,
        "Instance Attribute":76.6,
        "Instance Location":69.0,
        "Instance Counting":71.0,
        "Spatial Relation":57.5,
        "Instance Interaction":73.2,
        "Visual Reasoning":77.6,
        "Text Recognition":62.1,
        "Celebrity Recognition":82.7,
        "Landmark Recognition":85.2,
        "Chart Understanding":44.5,
        "Visual Referring Expression":62.3,
        "Science Knowledge":60.3,
        "Emotion Recognition":65.3,
        "Visual Mathematics":23.5,
        "Difference Spotting":45.7,
        "Meme Comprehension":42.1,
        "Global Video Understanding":54.6,
        "Action Recognition":48.1,
        "Action Predicion":37.9,
        "Procedure Understanding":29.9,
        "In-Context Captioning":33.3,
        "Interleaved Image-Text Analysis":40.8,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/Alpha-VLLM\/LLaMA2-Accessory\/tree\/main\/SPHINX"
    },
    {
        "Model":"Emu",
        "Language Model":"LLaMA-13B",
        "Model Size":"13B",
        "Evaluation Method":"PPL",
        "Avg. Single":46.4,
        "Avg. Multi":31.2,
        "Avg. Video":37.4,
        "Avg. P1":44.2,
        "Avg. P2":45.6,
        "Avg. P3":45.7,
        "Scene Understanding":59.0,
        "Instance Identity":50.0,
        "Instance Attribute":43.7,
        "Instance Location":37.1,
        "Instance Counting":44.3,
        "Spatial Relation":33.6,
        "Instance Interaction":49.5,
        "Visual Reasoning":58.3,
        "Text Recognition":61.4,
        "Celebrity Recognition":68.8,
        "Landmark Recognition":61.6,
        "Chart Understanding":19.0,
        "Visual Referring Expression":45.7,
        "Science Knowledge":41.5,
        "Emotion Recognition":24.2,
        "Visual Mathematics":26.4,
        "Difference Spotting":29.3,
        "Meme Comprehension":37.1,
        "Global Video Understanding":41.9,
        "Action Recognition":42.7,
        "Action Predicion":37.9,
        "Procedure Understanding":21.8,
        "In-Context Captioning":51.7,
        "Interleaved Image-Text Analysis":30.6,
        "Text-to-Image Generation":46.8,
        "Next Image Prediction":43.2,
        "Text-Image Creation":34.2,
        "URL":"https:\/\/github.com\/baaivision\/Emu"
    },
    {
        "Model":"Next-GPT",
        "Language Model":"vicuna-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":31.0,
        "Avg. Multi":27.8,
        "Avg. Video":30.7,
        "Avg. P1":31.0,
        "Avg. P2":40.3,
        "Avg. P3":42.8,
        "Scene Understanding":36.4,
        "Instance Identity":35.1,
        "Instance Attribute":25.6,
        "Instance Location":29.9,
        "Instance Counting":36.1,
        "Spatial Relation":30.9,
        "Instance Interaction":39.2,
        "Visual Reasoning":41.7,
        "Text Recognition":31.0,
        "Celebrity Recognition":30.9,
        "Landmark Recognition":27.4,
        "Chart Understanding":21.2,
        "Visual Referring Expression":34.2,
        "Science Knowledge":31.8,
        "Emotion Recognition":24.4,
        "Visual Mathematics":17.4,
        "Difference Spotting":24.2,
        "Meme Comprehension":39.0,
        "Global Video Understanding":35.5,
        "Action Recognition":33.8,
        "Action Predicion":25.6,
        "Procedure Understanding":24.5,
        "In-Context Captioning":46.7,
        "Interleaved Image-Text Analysis":24.5,
        "Text-to-Image Generation":45.1,
        "Next Image Prediction":19.8,
        "Text-Image Creation":36.7,
        "URL":"https:\/\/github.com\/NExT-GPT\/NExT-GPT"
    },
    {
        "Model":"SEED-LLaMA",
        "Language Model":"LLaMA2-Chat-13B",
        "Model Size":"13B",
        "Evaluation Method":"PPL",
        "Avg. Single":49.9,
        "Avg. Multi":32.4,
        "Avg. Video":39.0,
        "Avg. P1":47.3,
        "Avg. P2":48.0,
        "Avg. P3":50.6,
        "Scene Understanding":64.0,
        "Instance Identity":55.0,
        "Instance Attribute":51.3,
        "Instance Location":45.4,
        "Instance Counting":43.3,
        "Spatial Relation":37.9,
        "Instance Interaction":56.7,
        "Visual Reasoning":59.2,
        "Text Recognition":57.0,
        "Celebrity Recognition":55.5,
        "Landmark Recognition":52.8,
        "Chart Understanding":18.8,
        "Visual Referring Expression":49.3,
        "Science Knowledge":44.8,
        "Emotion Recognition":28.8,
        "Visual Mathematics":24.4,
        "Difference Spotting":29.5,
        "Meme Comprehension":41.5,
        "Global Video Understanding":46.7,
        "Action Recognition":39.4,
        "Action Predicion":43.9,
        "Procedure Understanding":20.3,
        "In-Context Captioning":54.2,
        "Interleaved Image-Text Analysis":32.7,
        "Text-to-Image Generation":50.2,
        "Next Image Prediction":40.7,
        "Text-Image Creation":65.8,
        "URL":"https:\/\/github.com\/AILab-CVC\/SEED"
    },
    {
        "Model":"GPT-4V",
        "Language Model":"\\-",
        "Model Size":"-",
        "Evaluation Method":"Generate",
        "Avg. Single":69.8,
        "Avg. Multi":73.1,
        "Avg. Video":61.7,
        "Avg. P1":68.1,
        "Avg. P2":37.9,
        "Avg. P3":0.0,
        "Scene Understanding":77.5,
        "Instance Identity":73.9,
        "Instance Attribute":70.6,
        "Instance Location":61.8,
        "Instance Counting":56.8,
        "Spatial Relation":56.9,
        "Instance Interaction":74.2,
        "Visual Reasoning":78.5,
        "Text Recognition":82.3,
        "Celebrity Recognition":91.8,
        "Landmark Recognition":97.4,
        "Chart Understanding":45.1,
        "Visual Referring Expression":71.9,
        "Science Knowledge":66.1,
        "Emotion Recognition":71.1,
        "Visual Mathematics":43.9,
        "Difference Spotting":67.9,
        "Meme Comprehension":89.3,
        "Global Video Understanding":64.5,
        "Action Recognition":65.7,
        "Action Predicion":51.7,
        "Procedure Understanding":63.4,
        "In-Context Captioning":29.2,
        "Interleaved Image-Text Analysis":59.2,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/openai.com\/research\/gpt-4v-system-card"
    },
    {
        "Model":"VideoChat",
        "Language Model":"Vicuna-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":36.7,
        "Avg. Multi":35.4,
        "Avg. Video":34.2,
        "Avg. P1":36.2,
        "Avg. P2":37.3,
        "Avg. P3":0.0,
        "Scene Understanding":44.3,
        "Instance Identity":40.7,
        "Instance Attribute":32.2,
        "Instance Location":36.9,
        "Instance Counting":32.9,
        "Spatial Relation":32.6,
        "Instance Interaction":42.3,
        "Visual Reasoning":51.1,
        "Text Recognition":45.7,
        "Celebrity Recognition":35.2,
        "Landmark Recognition":46.8,
        "Chart Understanding":20.6,
        "Visual Referring Expression":43.2,
        "Science Knowledge":39.4,
        "Emotion Recognition":34.3,
        "Visual Mathematics":19.7,
        "Difference Spotting":30.3,
        "Meme Comprehension":51.6,
        "Global Video Understanding":41.5,
        "Action Recognition":34.0,
        "Action Predicion":30.6,
        "Procedure Understanding":27.4,
        "In-Context Captioning":40.0,
        "Interleaved Image-Text Analysis":30.6,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/OpenGVLab\/Ask-Anything"
    },
    {
        "Model":"Video-ChatGPT",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. Single":38.3,
        "Avg. Multi":49.8,
        "Avg. Video":31.6,
        "Avg. P1":36.9,
        "Avg. P2":33.7,
        "Avg. P3":0.0,
        "Scene Understanding":44.1,
        "Instance Identity":37.0,
        "Instance Attribute":35.8,
        "Instance Location":30.7,
        "Instance Counting":44.2,
        "Spatial Relation":31.1,
        "Instance Interaction":29.9,
        "Visual Reasoning":49.9,
        "Text Recognition":39.8,
        "Celebrity Recognition":49.7,
        "Landmark Recognition":40.6,
        "Chart Understanding":22.0,
        "Visual Referring Expression":33.2,
        "Science Knowledge":37.2,
        "Emotion Recognition":22.4,
        "Visual Mathematics":25.0,
        "Difference Spotting":46.1,
        "Meme Comprehension":61.4,
        "Global Video Understanding":42.6,
        "Action Recognition":32.2,
        "Action Predicion":27.0,
        "Procedure Understanding":19.0,
        "In-Context Captioning":37.5,
        "Interleaved Image-Text Analysis":24.5,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/mbzuai-oryx\/Video-ChatGPT"
    },
    {
        "Model":"Valley",
        "Language Model":"LLaMA-13B",
        "Model Size":"13B",
        "Evaluation Method":"PPL",
        "Avg. Single":35.3,
        "Avg. Multi":40.7,
        "Avg. Video":28.5,
        "Avg. P1":33.9,
        "Avg. P2":33.7,
        "Avg. P3":0.0,
        "Scene Understanding":45.3,
        "Instance Identity":36.4,
        "Instance Attribute":33.7,
        "Instance Location":30.6,
        "Instance Counting":27.1,
        "Spatial Relation":31.5,
        "Instance Interaction":35.1,
        "Visual Reasoning":52.0,
        "Text Recognition":35.2,
        "Celebrity Recognition":44.9,
        "Landmark Recognition":43.4,
        "Chart Understanding":23.8,
        "Visual Referring Expression":33.2,
        "Science Knowledge":37.2,
        "Emotion Recognition":26.0,
        "Visual Mathematics":22.7,
        "Difference Spotting":37.1,
        "Meme Comprehension":52.2,
        "Global Video Understanding":31.5,
        "Action Recognition":32.1,
        "Action Predicion":21.9,
        "Procedure Understanding":26.5,
        "In-Context Captioning":35.8,
        "Interleaved Image-Text Analysis":28.6,
        "Text-to-Image Generation":0.0,
        "Next Image Prediction":0.0,
        "Text-Image Creation":0.0,
        "URL":"https:\/\/github.com\/RupertLuo\/Valley"
    }
]