[
    {
        "Model Type":"LLM",
        "Model":"[Flan-T5](https:\/\/huggingface.co\/google\/flan-t5-xl)",
        "Language Model":"Flan-T5-XL",
        "Model Size":"3B",
        "Evaluation Method":"PPL",
        "Avg. All":27.7,
        "Avg. Img":27.3,
        "Avg. Video":28.6,
        "Scene Understanding":23.0,
        "Instance Identity":29.0,
        "Instance Attribute":32.8,
        "Instance Location":31.8,
        "Instance Counting":20.5,
        "Spatial Relation":31.8,
        "Instance Interaction":33.0,
        "Visual Reasoning":18.2,
        "Text Recognition":19.4,
        "Action Recognition":23.2,
        "Action Prediction":34.9,
        "Procedure Understanding":25.4
    },
    {
        "Model Type":"LLM",
        "Model":"[Vicuna](https:\/\/huggingface.co\/lmsys\/vicuna-7b-v1.3)",
        "Language Model":"Vicuna-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":28.5,
        "Avg. Img":28.2,
        "Avg. Video":29.5,
        "Scene Understanding":23.4,
        "Instance Identity":30.7,
        "Instance Attribute":29.7,
        "Instance Location":30.9,
        "Instance Counting":30.8,
        "Spatial Relation":28.6,
        "Instance Interaction":29.8,
        "Visual Reasoning":18.5,
        "Text Recognition":13.4,
        "Action Recognition":27.3,
        "Action Prediction":34.5,
        "Procedure Understanding":23.8
    },
    {
        "Model Type":"LLM",
        "Model":"[LLaMA](https:\/\/research.facebook.com\/publications\/llama-open-and-efficient-foundation-language-models\/)",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":26.8,
        "Avg. Img":26.6,
        "Avg. Video":27.3,
        "Scene Understanding":26.3,
        "Instance Identity":27.4,
        "Instance Attribute":26.2,
        "Instance Location":28.3,
        "Instance Counting":25.1,
        "Spatial Relation":28.8,
        "Instance Interaction":19.2,
        "Visual Reasoning":37.0,
        "Text Recognition":9.0,
        "Action Recognition":33.0,
        "Action Prediction":23.1,
        "Procedure Understanding":26.2
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[BLIP-2](https:\/\/github.com\/salesforce\/LAVIS)",
        "Language Model":"Flan-T5-XL",
        "Model Size":"3B",
        "Evaluation Method":"PPL",
        "Avg. All":46.4,
        "Avg. Img":49.7,
        "Avg. Video":36.7,
        "Scene Understanding":59.1,
        "Instance Identity":53.9,
        "Instance Attribute":49.2,
        "Instance Location":42.3,
        "Instance Counting":43.2,
        "Spatial Relation":36.7,
        "Instance Interaction":55.7,
        "Visual Reasoning":45.6,
        "Text Recognition":25.9,
        "Action Recognition":32.6,
        "Action Prediction":47.5,
        "Procedure Understanding":24.0
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[InstructBLIP](https:\/\/github.com\/salesforce\/LAVIS)",
        "Language Model":"Flan-T5-XL",
        "Model Size":"3B",
        "Evaluation Method":"PPL",
        "Avg. All":52.7,
        "Avg. Img":57.8,
        "Avg. Video":38.3,
        "Scene Understanding":60.3,
        "Instance Identity":58.5,
        "Instance Attribute":63.4,
        "Instance Location":40.6,
        "Instance Counting":58.4,
        "Spatial Relation":38.7,
        "Instance Interaction":51.6,
        "Visual Reasoning":45.9,
        "Text Recognition":25.9,
        "Action Recognition":33.1,
        "Action Prediction":49.1,
        "Procedure Understanding":27.1
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[InstructBLIP-Vicuna](https:\/\/github.com\/salesforce\/LAVIS)",
        "Language Model":"Vicuna-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":53.4,
        "Avg. Img":58.8,
        "Avg. Video":38.1,
        "Scene Understanding":60.2,
        "Instance Identity":58.9,
        "Instance Attribute":65.6,
        "Instance Location":43.6,
        "Instance Counting":57.2,
        "Spatial Relation":40.3,
        "Instance Interaction":52.6,
        "Visual Reasoning":47.7,
        "Text Recognition":43.5,
        "Action Recognition":34.5,
        "Action Prediction":49.6,
        "Procedure Understanding":23.1
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[LLaVA-1.5](https:\/\/github.com\/haotian-liu\/LLaVA)",
        "Language Model":"Vicuna-13B",
        "Model Size":"13B",
        "Evaluation Method":"Generate",
        "Avg. All":61.6,
        "Avg. Img":68.2,
        "Avg. Video":42.7,
        "Scene Understanding":74.9,
        "Instance Identity":71.3,
        "Instance Attribute":68.9,
        "Instance Location":63.5,
        "Instance Counting":61.3,
        "Spatial Relation":51.4,
        "Instance Interaction":73.2,
        "Visual Reasoning":77.0,
        "Text Recognition":60.5,
        "Action Recognition":48.9,
        "Action Prediction":41.1,
        "Procedure Understanding":36.6
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[LLaVA-v1.5-13B-LoRA](https:\/\/llava-vl.github.io)",
        "Language Model":"Vicuna-13B-v1.5",
        "Model Size":"13B",
        "Evaluation Method":"PPL",
        "Avg. All":62.4,
        "Avg. Img":68.2,
        "Avg. Video":40.5,
        "Scene Understanding":74.9,
        "Instance Identity":70.9,
        "Instance Attribute":70.1,
        "Instance Location":62.5,
        "Instance Counting":60.6,
        "Spatial Relation":52.4,
        "Instance Interaction":74.2,
        "Visual Reasoning":77.3,
        "Text Recognition":26.7,
        "Action Recognition":47.5,
        "Action Prediction":36.0,
        "Procedure Understanding":35.7
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[LLaVA-v1.5-LoRA](https:\/\/llava-vl.github.io)",
        "Language Model":"Vicuna-13B-v1.5",
        "Model Size":"13B",
        "Evaluation Method":"PPL for A\/B\/C\/D",
        "Avg. All":62.8,
        "Avg. Img":68.9,
        "Avg. Video":39.5,
        "Scene Understanding":75.2,
        "Instance Identity":71.4,
        "Instance Attribute":72.0,
        "Instance Location":62.7,
        "Instance Counting":59.8,
        "Spatial Relation":51.1,
        "Instance Interaction":71.1,
        "Visual Reasoning":80.7,
        "Text Recognition":39.5,
        "Action Recognition":46.1,
        "Action Prediction":37.1,
        "Procedure Understanding":32.6
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[MiniGPT-4](https:\/\/github.com\/Vision-CAIR\/MiniGPT-4)",
        "Language Model":"Vicuna-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":42.8,
        "Avg. Img":47.4,
        "Avg. Video":29.9,
        "Scene Understanding":56.3,
        "Instance Identity":49.2,
        "Instance Attribute":45.8,
        "Instance Location":37.9,
        "Instance Counting":45.3,
        "Spatial Relation":32.6,
        "Instance Interaction":47.4,
        "Visual Reasoning":57.1,
        "Text Recognition":11.8,
        "Action Recognition":38.2,
        "Action Prediction":24.5,
        "Procedure Understanding":27.1
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[VPGTrans](https:\/\/github.com\/VPGTrans\/VPGTrans)",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":39.1,
        "Avg. Img":41.8,
        "Avg. Video":31.4,
        "Scene Understanding":51.9,
        "Instance Identity":44.1,
        "Instance Attribute":39.9,
        "Instance Location":36.1,
        "Instance Counting":33.7,
        "Spatial Relation":36.4,
        "Instance Interaction":32.0,
        "Visual Reasoning":53.2,
        "Text Recognition":30.6,
        "Action Recognition":39.5,
        "Action Prediction":24.3,
        "Procedure Understanding":31.9
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[MultiModal-GPT](https:\/\/github.com\/open-mmlab\/Multimodal-GPT)",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":33.2,
        "Avg. Img":34.5,
        "Avg. Video":29.2,
        "Scene Understanding":43.6,
        "Instance Identity":37.9,
        "Instance Attribute":31.5,
        "Instance Location":30.8,
        "Instance Counting":27.3,
        "Spatial Relation":30.1,
        "Instance Interaction":29.9,
        "Visual Reasoning":51.4,
        "Text Recognition":18.8,
        "Action Recognition":36.9,
        "Action Prediction":25.8,
        "Procedure Understanding":24.0
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[Otter](https:\/\/github.com\/Luodian\/Otter)",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":33.9,
        "Avg. Img":35.2,
        "Avg. Video":30.4,
        "Scene Understanding":44.9,
        "Instance Identity":38.6,
        "Instance Attribute":32.2,
        "Instance Location":30.9,
        "Instance Counting":26.3,
        "Spatial Relation":31.8,
        "Instance Interaction":32.0,
        "Visual Reasoning":51.4,
        "Text Recognition":31.8,
        "Action Recognition":37.9,
        "Action Prediction":27.2,
        "Procedure Understanding":24.8
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[Otter](https:\/\/github.com\/Luodian\/Otter)",
        "Language Model":"MPT-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":39.7,
        "Avg. Img":42.9,
        "Avg. Video":30.6,
        "Scene Understanding":51.3,
        "Instance Identity":43.5,
        "Instance Attribute":42.3,
        "Instance Location":34.2,
        "Instance Counting":38.4,
        "Spatial Relation":30.9,
        "Instance Interaction":40.2,
        "Visual Reasoning":55.3,
        "Text Recognition":24.7,
        "Action Recognition":36.8,
        "Action Prediction":29.2,
        "Procedure Understanding":23.8
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[OpenFlamingo](https:\/\/github.com\/mlfoundations\/open_flamingo)",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":33.1,
        "Avg. Img":34.5,
        "Avg. Video":29.3,
        "Scene Understanding":43.9,
        "Instance Identity":38.1,
        "Instance Attribute":31.3,
        "Instance Location":30.1,
        "Instance Counting":27.3,
        "Spatial Relation":30.6,
        "Instance Interaction":29.9,
        "Visual Reasoning":50.2,
        "Text Recognition":20.0,
        "Action Recognition":37.2,
        "Action Prediction":25.4,
        "Procedure Understanding":24.2
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[OpenFlamingo](https:\/\/github.com\/mlfoundations\/open_flamingo)",
        "Language Model":"MPT-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":40.9,
        "Avg. Img":42.7,
        "Avg. Video":35.7,
        "Scene Understanding":53.2,
        "Instance Identity":45.3,
        "Instance Attribute":40.0,
        "Instance Location":31.2,
        "Instance Counting":39.3,
        "Spatial Relation":32.6,
        "Instance Interaction":36.1,
        "Visual Reasoning":51.4,
        "Text Recognition":25.9,
        "Action Recognition":42.9,
        "Action Prediction":34.7,
        "Procedure Understanding":26.9
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[LLaMA-AdapterV2](https:\/\/github.com\/OpenGVLab\/LLaMA-Adapter)",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":32.7,
        "Avg. Img":35.2,
        "Avg. Video":25.8,
        "Scene Understanding":45.2,
        "Instance Identity":38.5,
        "Instance Attribute":29.3,
        "Instance Location":33.0,
        "Instance Counting":29.7,
        "Spatial Relation":35.5,
        "Instance Interaction":39.2,
        "Visual Reasoning":52.0,
        "Text Recognition":24.7,
        "Action Recognition":38.6,
        "Action Prediction":18.5,
        "Procedure Understanding":19.6
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[GVT](https:\/\/github.com\/TencentARC\/GVT)",
        "Language Model":"Vicuna-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":33.5,
        "Avg. Img":35.5,
        "Avg. Video":27.8,
        "Scene Understanding":41.7,
        "Instance Identity":35.5,
        "Instance Attribute":31.8,
        "Instance Location":29.5,
        "Instance Counting":36.2,
        "Spatial Relation":32.0,
        "Instance Interaction":32.0,
        "Visual Reasoning":51.1,
        "Text Recognition":27.1,
        "Action Recognition":33.9,
        "Action Prediction":25.4,
        "Procedure Understanding":23.0
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[mPLUG-Owl](https:\/\/github.com\/X-PLUG\/mPLUG-Owl)",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":34.0,
        "Avg. Img":37.9,
        "Avg. Video":23.0,
        "Scene Understanding":49.7,
        "Instance Identity":45.3,
        "Instance Attribute":32.5,
        "Instance Location":36.7,
        "Instance Counting":27.3,
        "Spatial Relation":32.7,
        "Instance Interaction":44.3,
        "Visual Reasoning":54.7,
        "Text Recognition":28.8,
        "Action Recognition":26.7,
        "Action Prediction":17.9,
        "Procedure Understanding":26.5
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[Kosmos-2](https:\/\/github.com\/microsoft\/unilm\/tree\/master\/kosmos-2)",
        "Language Model":"Decoder Only 1.3B",
        "Model Size":"1.3B",
        "Evaluation Method":"PPL",
        "Avg. All":50.0,
        "Avg. Img":54.4,
        "Avg. Video":37.5,
        "Scene Understanding":63.4,
        "Instance Identity":57.1,
        "Instance Attribute":58.5,
        "Instance Location":44.0,
        "Instance Counting":41.4,
        "Spatial Relation":37.9,
        "Instance Interaction":55.7,
        "Visual Reasoning":60.7,
        "Text Recognition":25.9,
        "Action Recognition":41.3,
        "Action Prediction":40.4,
        "Procedure Understanding":27.0
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[Qwen-VL-Chat](https:\/\/huggingface.co\/Qwen\/Qwen-VL-Chat)",
        "Language Model":"Qwen-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL for A\/B\/C\/D",
        "Avg. All":58.2,
        "Avg. Img":65.4,
        "Avg. Video":37.8,
        "Scene Understanding":73.3,
        "Instance Identity":67.3,
        "Instance Attribute":69.6,
        "Instance Location":57.7,
        "Instance Counting":52.9,
        "Spatial Relation":48.2,
        "Instance Interaction":59.8,
        "Visual Reasoning":74.6,
        "Text Recognition":53.5,
        "Action Recognition":43.9,
        "Action Prediction":39.2,
        "Procedure Understanding":26.7
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[Qwen-VL](https:\/\/huggingface.co\/Qwen\/Qwen-VL)",
        "Language Model":"Qwen-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL for A\/B\/C\/D",
        "Avg. All":56.3,
        "Avg. Img":62.3,
        "Avg. Video":39.1,
        "Scene Understanding":71.2,
        "Instance Identity":66.4,
        "Instance Attribute":67.7,
        "Instance Location":53.5,
        "Instance Counting":44.8,
        "Spatial Relation":43.8,
        "Instance Interaction":62.9,
        "Visual Reasoning":74.9,
        "Text Recognition":51.2,
        "Action Recognition":44.7,
        "Action Prediction":38.5,
        "Procedure Understanding":32.0
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[IDEFICS-9b-instruct](https:\/\/huggingface.co\/HuggingFaceM4\/idefics-9b-instruct)",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"NG",
        "Avg. All":0.0,
        "Avg. Img":44.5,
        "Avg. Video":0.0,
        "Scene Understanding":55.8,
        "Instance Identity":45.3,
        "Instance Attribute":42.3,
        "Instance Location":40.2,
        "Instance Counting":36.8,
        "Spatial Relation":34.9,
        "Instance Interaction":37.1,
        "Visual Reasoning":55.9,
        "Text Recognition":38.8,
        "Action Recognition":0.0,
        "Action Prediction":0.0,
        "Procedure Understanding":0.0
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[IDEFICS-80b-instruct](https:\/\/huggingface.co\/HuggingFaceM4\/idefics-9b-instruct)",
        "Language Model":"LLaMA-65B",
        "Model Size":"65B",
        "Evaluation Method":"NG",
        "Avg. All":0.0,
        "Avg. Img":53.2,
        "Avg. Video":0.0,
        "Scene Understanding":64.0,
        "Instance Identity":52.6,
        "Instance Attribute":50.8,
        "Instance Location":48.3,
        "Instance Counting":46.1,
        "Spatial Relation":45.5,
        "Instance Interaction":62.9,
        "Visual Reasoning":68.0,
        "Text Recognition":51.8,
        "Action Recognition":0.0,
        "Action Prediction":0.0,
        "Procedure Understanding":0.0
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[InternLM-XComposer-VL](https:\/\/github.com\/InternLM\/InternLM-XComposer)",
        "Language Model":"InternLM-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":0.0,
        "Avg. Img":66.9,
        "Avg. Video":0.0,
        "Scene Understanding":75.0,
        "Instance Identity":71.7,
        "Instance Attribute":67.6,
        "Instance Location":60.8,
        "Instance Counting":56.2,
        "Spatial Relation":55.3,
        "Instance Interaction":74.4,
        "Visual Reasoning":77.0,
        "Text Recognition":48.5,
        "Action Recognition":0.0,
        "Action Prediction":0.0,
        "Procedure Understanding":0.0
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[SEED-LLaMA](https:\/\/github.com\/AILab-CVC\/SEED)",
        "Language Model":"LLaMA2-Chat-13B",
        "Model Size":"13B",
        "Evaluation Method":"PPL",
        "Avg. All":48.9,
        "Avg. Img":53.7,
        "Avg. Video":35.4,
        "Scene Understanding":64.1,
        "Instance Identity":54.2,
        "Instance Attribute":54.1,
        "Instance Location":46.5,
        "Instance Counting":45.3,
        "Spatial Relation":38.2,
        "Instance Interaction":51.6,
        "Visual Reasoning":60.7,
        "Text Recognition":44.7,
        "Action Recognition":37.8,
        "Action Prediction":45.3,
        "Procedure Understanding":20.0
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[mPLUG-Owl2](https:\/\/github.com\/X-PLUG\/mPLUG-Owl)",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"NG",
        "Avg. All":57.8,
        "Avg. Img":64.1,
        "Avg. Video":39.8,
        "Scene Understanding":72.7,
        "Instance Identity":67.6,
        "Instance Attribute":63.6,
        "Instance Location":53.6,
        "Instance Counting":58.5,
        "Spatial Relation":50.8,
        "Instance Interaction":70.1,
        "Visual Reasoning":76.4,
        "Text Recognition":30.2,
        "Action Recognition":46.0,
        "Action Prediction":38.7,
        "Procedure Understanding":32.9
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[LLaMA-VID-7B](https:\/\/github.com\/dvlab-research\/LLaMA-VID)",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"Generate",
        "Avg. All":59.9,
        "Avg. Img":67.6,
        "Avg. Video":37.9,
        "Scene Understanding":75.4,
        "Instance Identity":71.2,
        "Instance Attribute":68.9,
        "Instance Location":62.9,
        "Instance Counting":58.4,
        "Spatial Relation":50.7,
        "Instance Interaction":70.1,
        "Visual Reasoning":76.1,
        "Text Recognition":54.7,
        "Action Recognition":42.8,
        "Action Prediction":35.2,
        "Procedure Understanding":35.6
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[Pink-LLaMA2](https:\/\/github.com\/SY-Xuan\/Pink\/stargazers)",
        "Language Model":"LLaMA2-7B",
        "Model Size":"7B",
        "Evaluation Method":"NG",
        "Avg. All":0.0,
        "Avg. Img":67.0,
        "Avg. Video":0.0,
        "Scene Understanding":75.2,
        "Instance Identity":70.1,
        "Instance Attribute":70.1,
        "Instance Location":63.3,
        "Instance Counting":53.8,
        "Spatial Relation":50.2,
        "Instance Interaction":69.1,
        "Visual Reasoning":74.3,
        "Text Recognition":50.0,
        "Action Recognition":0.0,
        "Action Prediction":0.0,
        "Procedure Understanding":0.0
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[InfMLLM-13B](https:\/\/github.com\/mightyzau\/InfMLLM)",
        "Language Model":"Vicuna-13B",
        "Model Size":"13B",
        "Evaluation Method":"NG",
        "Avg. All":62.3,
        "Avg. Img":69.6,
        "Avg. Video":41.5,
        "Scene Understanding":75.5,
        "Instance Identity":73.0,
        "Instance Attribute":70.4,
        "Instance Location":66.2,
        "Instance Counting":63.3,
        "Spatial Relation":54.2,
        "Instance Interaction":72.2,
        "Visual Reasoning":77.9,
        "Text Recognition":37.2,
        "Action Recognition":49.5,
        "Action Prediction":39.0,
        "Procedure Understanding":33.9
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[ShareGPT4V-7B](https:\/\/github.com\/InternLM\/InternLM-XComposer\/tree\/main\/projects\/ShareGPT4V)",
        "Language Model":"Vicuna-7B",
        "Model Size":"7B",
        "Evaluation Method":"Generate",
        "Avg. All":0.0,
        "Avg. Img":69.7,
        "Avg. Video":0.0,
        "Scene Understanding":75.3,
        "Instance Identity":71.4,
        "Instance Attribute":72.3,
        "Instance Location":63.1,
        "Instance Counting":62.0,
        "Spatial Relation":53.9,
        "Instance Interaction":70.1,
        "Visual Reasoning":79.8,
        "Text Recognition":54.7,
        "Action Recognition":0.0,
        "Action Prediction":0.0,
        "Procedure Understanding":0.0
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[ShareGPT4V-13B](https:\/\/github.com\/InternLM\/InternLM-XComposer\/tree\/main\/projects\/ShareGPT4V)",
        "Language Model":"Vicuna-13B",
        "Model Size":"13B",
        "Evaluation Method":"Generate",
        "Avg. All":0.0,
        "Avg. Img":70.8,
        "Avg. Video":0.0,
        "Scene Understanding":75.9,
        "Instance Identity":74.1,
        "Instance Attribute":73.5,
        "Instance Location":66.8,
        "Instance Counting":62.4,
        "Spatial Relation":54.8,
        "Instance Interaction":75.3,
        "Visual Reasoning":77.3,
        "Text Recognition":46.5,
        "Action Recognition":0.0,
        "Action Prediction":0.0,
        "Procedure Understanding":0.0
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[Honeybee-13B](https:\/\/github.com\/kakaobrain\/honeybee)",
        "Language Model":"Vicuna-13B",
        "Model Size":"13B",
        "Evaluation Method":"Generate",
        "Avg. All":0.0,
        "Avg. Img":68.6,
        "Avg. Video":0.0,
        "Scene Understanding":75.4,
        "Instance Identity":72.8,
        "Instance Attribute":69.0,
        "Instance Location":64.5,
        "Instance Counting":60.6,
        "Spatial Relation":55.1,
        "Instance Interaction":72.2,
        "Visual Reasoning":77.9,
        "Text Recognition":41.9,
        "Action Recognition":0.0,
        "Action Prediction":0.0,
        "Procedure Understanding":0.0
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[SPHINXv1-1k](https:\/\/github.com\/Alpha-VLLM\/LLaMA2-Accessory\/tree\/main\/SPHINX)",
        "Language Model":"LLaMA-2-13B",
        "Model Size":"13B",
        "Evaluation Method":"Generate",
        "Avg. All":0.0,
        "Avg. Img":71.6,
        "Avg. Video":0.0,
        "Scene Understanding":75.4,
        "Instance Identity":72.2,
        "Instance Attribute":75.1,
        "Instance Location":64.2,
        "Instance Counting":68.2,
        "Spatial Relation":49.3,
        "Instance Interaction":66.0,
        "Visual Reasoning":78.6,
        "Text Recognition":62.4,
        "Action Recognition":0.0,
        "Action Prediction":0.0,
        "Procedure Understanding":0.0
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[SPHINXv2-1k](https:\/\/github.com\/Alpha-VLLM\/LLaMA2-Accessory\/tree\/main\/SPHINX)",
        "Language Model":"LLaMA-2-13B",
        "Model Size":"13B",
        "Evaluation Method":"Generate",
        "Avg. All":0.0,
        "Avg. Img":74.8,
        "Avg. Video":0.0,
        "Scene Understanding":77.7,
        "Instance Identity":77.4,
        "Instance Attribute":76.8,
        "Instance Location":69.4,
        "Instance Counting":71.2,
        "Spatial Relation":59.4,
        "Instance Interaction":70.1,
        "Visual Reasoning":78.3,
        "Text Recognition":74.1,
        "Action Recognition":0.0,
        "Action Prediction":0.0,
        "Procedure Understanding":0.0
    },
    {
        "Model Type":"ImageLLM",
        "Model":"[GPT-4V](https:\/\/openai.com\/research\/gpt-4v-system-card)",
        "Language Model":"\\-",
        "Model Size":"-",
        "Evaluation Method":"Generate",
        "Avg. All":67.3,
        "Avg. Img":69.1,
        "Avg. Video":60.5,
        "Scene Understanding":77.5,
        "Instance Identity":73.9,
        "Instance Attribute":70.6,
        "Instance Location":61.8,
        "Instance Counting":56.8,
        "Spatial Relation":56.9,
        "Instance Interaction":74.2,
        "Visual Reasoning":78.5,
        "Text Recognition":57.6,
        "Action Recognition":65.7,
        "Action Prediction":51.7,
        "Procedure Understanding":63.4
    },
    {
        "Model Type":"VideoLLM",
        "Model":"[VideoChat](https:\/\/github.com\/OpenGVLab\/Ask-Anything)",
        "Language Model":"Vicuna-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":37.6,
        "Avg. Img":39.0,
        "Avg. Video":33.7,
        "Scene Understanding":47.1,
        "Instance Identity":43.8,
        "Instance Attribute":34.9,
        "Instance Location":40.0,
        "Instance Counting":32.8,
        "Spatial Relation":34.6,
        "Instance Interaction":42.3,
        "Visual Reasoning":50.5,
        "Text Recognition":17.7,
        "Action Recognition":34.9,
        "Action Prediction":36.4,
        "Procedure Understanding":27.3
    },
    {
        "Model Type":"VideoLLM",
        "Model":"[Video-ChatGPT](https:\/\/github.com\/mbzuai-oryx\/Video-ChatGPT)",
        "Language Model":"LLaMA-7B",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":31.2,
        "Avg. Img":33.9,
        "Avg. Video":23.5,
        "Scene Understanding":37.2,
        "Instance Identity":31.4,
        "Instance Attribute":33.2,
        "Instance Location":28.4,
        "Instance Counting":35.5,
        "Spatial Relation":29.5,
        "Instance Interaction":23.7,
        "Visual Reasoning":42.3,
        "Text Recognition":25.9,
        "Action Recognition":27.6,
        "Action Prediction":21.3,
        "Procedure Understanding":21.1
    },
    {
        "Model Type":"VideoLLM",
        "Model":"[Valley](https:\/\/github.com\/RupertLuo\/Valley)",
        "Language Model":"LLaMA-13B",
        "Model Size":"13B",
        "Evaluation Method":"PPL",
        "Avg. All":30.3,
        "Avg. Img":32.0,
        "Avg. Video":25.4,
        "Scene Understanding":39.3,
        "Instance Identity":32.9,
        "Instance Attribute":31.6,
        "Instance Location":27.9,
        "Instance Counting":24.2,
        "Spatial Relation":30.1,
        "Instance Interaction":27.8,
        "Visual Reasoning":43.8,
        "Text Recognition":11.8,
        "Action Recognition":31.3,
        "Action Prediction":23.2,
        "Procedure Understanding":20.7
    },
    {
        "Model Type":"Other",
        "Model":"[Unified-IO-2 7B (2.5M)]()",
        "Language Model":"from scratch",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":60.5,
        "Avg. Img":65.6,
        "Avg. Video":46.0,
        "Scene Understanding":70.7,
        "Instance Identity":69.0,
        "Instance Attribute":67.4,
        "Instance Location":55.4,
        "Instance Counting":62.6,
        "Spatial Relation":45.5,
        "Instance Interaction":60.8,
        "Visual Reasoning":67.1,
        "Text Recognition":58.1,
        "Action Recognition":57.5,
        "Action Prediction":43.2,
        "Procedure Understanding":34.0
    },
    {
        "Model Type":"Other",
        "Model":"[Unified-IO-2 7B]()",
        "Language Model":"from scratch",
        "Model Size":"7B",
        "Evaluation Method":"PPL",
        "Avg. All":60.4,
        "Avg. Img":65.5,
        "Avg. Video":46.0,
        "Scene Understanding":71.3,
        "Instance Identity":68.8,
        "Instance Attribute":67.5,
        "Instance Location":55.5,
        "Instance Counting":61.2,
        "Spatial Relation":45.4,
        "Instance Interaction":62.9,
        "Visual Reasoning":66.5,
        "Text Recognition":59.3,
        "Action Recognition":58.0,
        "Action Prediction":42.7,
        "Procedure Understanding":34.0
    },
    {
        "Model Type":"Other",
        "Model":"[Unified-IO-2 3B (3M)]()",
        "Language Model":"from scratch",
        "Model Size":"3B",
        "Evaluation Method":"PPL",
        "Avg. All":60.2,
        "Avg. Img":64.1,
        "Avg. Video":45.6,
        "Scene Understanding":69.0,
        "Instance Identity":66.6,
        "Instance Attribute":66.5,
        "Instance Location":54.3,
        "Instance Counting":62.0,
        "Spatial Relation":42.3,
        "Instance Interaction":50.5,
        "Visual Reasoning":65.3,
        "Text Recognition":44.2,
        "Action Recognition":57.5,
        "Action Prediction":36.2,
        "Procedure Understanding":39.4
    },
    {
        "Model Type":"Other",
        "Model":"[Unified-IO-2 3B]()",
        "Language Model":"from scratch",
        "Model Size":"3B",
        "Evaluation Method":"PPL",
        "Avg. All":58.7,
        "Avg. Img":63.8,
        "Avg. Video":44.2,
        "Scene Understanding":68.8,
        "Instance Identity":65.8,
        "Instance Attribute":67.2,
        "Instance Location":52.9,
        "Instance Counting":60.4,
        "Spatial Relation":43.1,
        "Instance Interaction":55.7,
        "Visual Reasoning":64.0,
        "Text Recognition":41.9,
        "Action Recognition":57.5,
        "Action Prediction":36.0,
        "Procedure Understanding":39.0
    },
    {
        "Model Type":"Other",
        "Model":"[Unified-IO-2 1B]()",
        "Language Model":"from scratch",
        "Model Size":"1B",
        "Evaluation Method":"PPL",
        "Avg. All":49.6,
        "Avg. Img":55.1,
        "Avg. Video":34.0,
        "Scene Understanding":63.8,
        "Instance Identity":57.7,
        "Instance Attribute":54.6,
        "Instance Location":41.9,
        "Instance Counting":53.7,
        "Spatial Relation":33.3,
        "Instance Interaction":51.5,
        "Visual Reasoning":58.3,
        "Text Recognition":47.7,
        "Action Recognition":39.8,
        "Action Prediction":34.5,
        "Procedure Understanding":24.6
    }
]