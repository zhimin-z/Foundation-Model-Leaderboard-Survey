[
    {
        "table_id":947,
        "row_id":113024,
        "rank":1,
        "method":"OmniVec",
        "mlmodel":{

        },
        "method_short":"OmniVec",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-07",
        "metrics":{
            "Top 1 Accuracy":"49.8",
            "Top 5 Accuracy":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":49.8,
            "Top 5 Accuracy":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1319233,
            "title":"OmniVec: Learning robust representations with cross modal sharing",
            "url":"\/paper\/omnivec-learning-robust-representations-with",
            "published":"2023-11-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivec-learning-robust-representations-with\/review\/?hl=113024"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":54004,
        "rank":2,
        "method":"CoCa (finetuned)",
        "mlmodel":{

        },
        "method_short":"CoCa ",
        "method_details":"finetuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Top 1 Accuracy":"49.0",
            "Top 5 Accuracy":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":49.0,
            "Top 5 Accuracy":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=54004"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":100401,
        "rank":3,
        "method":"UMT-L (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"UMT-L ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "Top 1 Accuracy":"48.7",
            "Top 5 Accuracy":"78.2"
        },
        "raw_metrics":{
            "Top 1 Accuracy":48.7,
            "Top 5 Accuracy":78.2
        },
        "uses_additional_data":true,
        "paper":{
            "id":1181934,
            "title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
            "url":"\/paper\/unmasked-teacher-towards-training-efficient",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":77322,
        "rank":4,
        "method":"UniFormerV2-L",
        "mlmodel":{

        },
        "method_short":"UniFormerV2-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-22",
        "metrics":{
            "Top 1 Accuracy":"47.8",
            "Top 5 Accuracy":"76.9"
        },
        "raw_metrics":{
            "Top 1 Accuracy":47.8,
            "Top 5 Accuracy":76.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":1087805,
            "title":"UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer",
            "url":"\/paper\/uniformerv2-spatiotemporal-learning-by-arming",
            "published":"2022-09-22T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":54000,
        "rank":5,
        "method":"CoCa (frozen)",
        "mlmodel":{

        },
        "method_short":"CoCa ",
        "method_details":"frozen",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Top 1 Accuracy":"47.4",
            "Top 5 Accuracy":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":47.4,
            "Top 5 Accuracy":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=54000"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":45379,
        "rank":6,
        "method":"MTV-H (WTS 60M)",
        "mlmodel":{

        },
        "method_short":"MTV-H ",
        "method_details":"WTS 60M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-12",
        "metrics":{
            "Top 1 Accuracy":"47.2",
            "Top 5 Accuracy":"75.7"
        },
        "raw_metrics":{
            "Top 1 Accuracy":47.2,
            "Top 5 Accuracy":75.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":944453,
            "title":"Multiview Transformers for Video Recognition",
            "url":"\/paper\/multiview-transformers-for-video-recognition",
            "published":"2022-01-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiview-transformers-for-video-recognition\/review\/?hl=45379"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":172,
                "name":"MTV",
                "color":"#27b6d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":44418,
        "rank":7,
        "method":"CoVeR(JFT-3B)",
        "mlmodel":{

        },
        "method_short":"CoVeR",
        "method_details":"JFT-3B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-14",
        "metrics":{
            "Top 1 Accuracy":"46.1",
            "Top 5 Accuracy":"75.4"
        },
        "raw_metrics":{
            "Top 1 Accuracy":46.1,
            "Top 5 Accuracy":75.4
        },
        "uses_additional_data":true,
        "paper":{
            "id":931113,
            "title":"Co-training Transformer with Videos and Images Improves Action Recognition",
            "url":"\/paper\/co-training-transformer-with-videos-and",
            "published":"2021-12-14T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":44419,
        "rank":8,
        "method":"CoVeR(JFT-300M)",
        "mlmodel":{

        },
        "method_short":"CoVeR",
        "method_details":"JFT-300M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-14",
        "metrics":{
            "Top 1 Accuracy":"45.0",
            "Top 5 Accuracy":"73.9"
        },
        "raw_metrics":{
            "Top 1 Accuracy":45.0,
            "Top 5 Accuracy":73.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":931113,
            "title":"Co-training Transformer with Videos and Images Improves Action Recognition",
            "url":"\/paper\/co-training-transformer-with-videos-and",
            "published":"2021-12-14T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":30519,
        "rank":9,
        "method":"VATT-Large",
        "mlmodel":{

        },
        "method_short":"VATT-Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Top 1 Accuracy":"41.1",
            "Top 5 Accuracy":"67.7"
        },
        "raw_metrics":{
            "Top 1 Accuracy":41.1,
            "Top 5 Accuracy":67.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":787028,
            "title":"VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",
            "url":"\/paper\/vatt-transformers-for-multimodal-self",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vatt-transformers-for-multimodal-self\/review\/?hl=30519"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":28450,
        "rank":10,
        "method":"MoViNet-A6",
        "mlmodel":{

        },
        "method_short":"MoViNet-A6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top 1 Accuracy":"40.2",
            "Top 5 Accuracy":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":40.2,
            "Top 5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28450"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":28451,
        "rank":11,
        "method":"MoViNet-A5",
        "mlmodel":{

        },
        "method_short":"MoViNet-A5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top 1 Accuracy":"39.1",
            "Top 5 Accuracy":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":39.1,
            "Top 5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28451"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":28475,
        "rank":12,
        "method":"MoViNet-A4",
        "mlmodel":{

        },
        "method_short":"MoViNet-A4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top 1 Accuracy":"37.9",
            "Top 5 Accuracy":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":37.9,
            "Top 5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28475"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":37570,
        "rank":13,
        "method":"VTN",
        "mlmodel":{

        },
        "method_short":"VTN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-01",
        "metrics":{
            "Top 1 Accuracy":"37.4",
            "Top 5 Accuracy":"65.4"
        },
        "raw_metrics":{
            "Top 1 Accuracy":37.4,
            "Top 5 Accuracy":65.4
        },
        "uses_additional_data":true,
        "paper":{
            "id":741425,
            "title":"Video Transformer Network",
            "url":"\/paper\/video-transformer-network",
            "published":"2021-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-transformer-network\/review\/?hl=37570"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":37548,
        "rank":14,
        "method":"MBT (AV)",
        "mlmodel":{

        },
        "method_short":"MBT ",
        "method_details":"AV",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-30",
        "metrics":{
            "Top 1 Accuracy":"37.3",
            "Top 5 Accuracy":"61.2"
        },
        "raw_metrics":{
            "Top 1 Accuracy":37.3,
            "Top 5 Accuracy":61.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":828708,
            "title":"Attention Bottlenecks for Multimodal Fusion",
            "url":"\/paper\/attention-bottlenecks-for-multimodal-fusion",
            "published":"2021-06-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/attention-bottlenecks-for-multimodal-fusion\/review\/?hl=37548"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":28476,
        "rank":15,
        "method":"MoViNet-A3",
        "mlmodel":{

        },
        "method_short":"MoViNet-A3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top 1 Accuracy":"35.6",
            "Top 5 Accuracy":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":35.6,
            "Top 5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28476"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":28477,
        "rank":16,
        "method":"MoViNet-A2",
        "mlmodel":{

        },
        "method_short":"MoViNet-A2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top 1 Accuracy":"34.3",
            "Top 5 Accuracy":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":34.3,
            "Top 5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28477"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":5493,
        "rank":17,
        "method":"AssembleNet",
        "mlmodel":{

        },
        "method_short":"AssembleNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-30",
        "metrics":{
            "Top 1 Accuracy":"34.27%",
            "Top 5 Accuracy":"62.71%"
        },
        "raw_metrics":{
            "Top 1 Accuracy":34.27,
            "Top 5 Accuracy":62.71
        },
        "uses_additional_data":false,
        "paper":{
            "id":117859,
            "title":"AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures",
            "url":"\/paper\/assemblenet-searching-for-multi-stream-neural",
            "published":"2019-05-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/assemblenet-searching-for-multi-stream-neural\/review\/?hl=5493"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":17735,
        "rank":18,
        "method":"SRTG r3d-101",
        "mlmodel":{

        },
        "method_short":"SRTG r3d-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-15",
        "metrics":{
            "Top 1 Accuracy":"33.56",
            "Top 5 Accuracy":"58.49"
        },
        "raw_metrics":{
            "Top 1 Accuracy":33.56,
            "Top 5 Accuracy":58.49
        },
        "uses_additional_data":false,
        "paper":{
            "id":202256,
            "title":"Learn to cycle: Time-consistent feature discovery for action recognition",
            "url":"\/paper\/learn-to-cycle-time-consistent-feature",
            "published":"2020-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learn-to-cycle-time-consistent-feature\/review\/?hl=17735"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":6667,
        "rank":19,
        "method":"CoST (ResNet-101, 32 frames)",
        "mlmodel":{

        },
        "method_short":"CoST ",
        "method_details":"ResNet-101, 32 frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-01",
        "metrics":{
            "Top 1 Accuracy":"32.4%",
            "Top 5 Accuracy":"60.0%"
        },
        "raw_metrics":{
            "Top 1 Accuracy":32.4,
            "Top 5 Accuracy":60.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":116001,
            "title":"Collaborative Spatiotemporal Feature Learning for Video Action Recognition",
            "url":"\/paper\/collaborative-spatiotemporal-feature-learning",
            "published":"2019-06-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":28478,
        "rank":20,
        "method":"MoViNet-A1",
        "mlmodel":{

        },
        "method_short":"MoViNet-A1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top 1 Accuracy":"32.0",
            "Top 5 Accuracy":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":32.0,
            "Top 5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28478"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":6356,
        "rank":21,
        "method":"EvaNet",
        "mlmodel":{

        },
        "method_short":"EvaNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-11-26",
        "metrics":{
            "Top 1 Accuracy":"31.8%",
            "Top 5 Accuracy":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":31.8,
            "Top 5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":63551,
            "title":"Evolving Space-Time Neural Architectures for Videos",
            "url":"\/paper\/evolving-space-time-neural-architectures-for",
            "published":"2018-11-26T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/evolving-space-time-neural-architectures-for\/review\/?hl=6356"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":17744,
        "rank":22,
        "method":"SRTG r(2+1)d-50",
        "mlmodel":{

        },
        "method_short":"SRTG r",
        "method_details":"2+1",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-15",
        "metrics":{
            "Top 1 Accuracy":"31.60",
            "Top 5 Accuracy":"56.80"
        },
        "raw_metrics":{
            "Top 1 Accuracy":31.6,
            "Top 5 Accuracy":56.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":202256,
            "title":"Learn to cycle: Time-consistent feature discovery for action recognition",
            "url":"\/paper\/learn-to-cycle-time-consistent-feature",
            "published":"2020-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learn-to-cycle-time-consistent-feature\/review\/?hl=17744"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":17730,
        "rank":23,
        "method":"SRTG r3d-50",
        "mlmodel":{

        },
        "method_short":"SRTG r3d-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-15",
        "metrics":{
            "Top 1 Accuracy":"30.72",
            "Top 5 Accuracy":"55.65"
        },
        "raw_metrics":{
            "Top 1 Accuracy":30.72,
            "Top 5 Accuracy":55.65
        },
        "uses_additional_data":false,
        "paper":{
            "id":202256,
            "title":"Learn to cycle: Time-consistent feature discovery for action recognition",
            "url":"\/paper\/learn-to-cycle-time-consistent-feature",
            "published":"2020-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learn-to-cycle-time-consistent-feature\/review\/?hl=17730"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":5502,
        "rank":24,
        "method":"I3D",
        "mlmodel":{

        },
        "method_short":"I3D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-05-22",
        "metrics":{
            "Top 1 Accuracy":"29.51%",
            "Top 5 Accuracy":"56.06%"
        },
        "raw_metrics":{
            "Top 1 Accuracy":29.51,
            "Top 5 Accuracy":56.06
        },
        "uses_additional_data":false,
        "paper":{
            "id":10430,
            "title":"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
            "url":"\/paper\/quo-vadis-action-recognition-a-new-model-and",
            "published":"2017-05-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/quo-vadis-action-recognition-a-new-model-and\/review\/?hl=5502"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":17739,
        "rank":25,
        "method":"SRTG r(2+1)d-34",
        "mlmodel":{

        },
        "method_short":"SRTG r",
        "method_details":"2+1",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-15",
        "metrics":{
            "Top 1 Accuracy":"28.97",
            "Top 5 Accuracy":"54.18"
        },
        "raw_metrics":{
            "Top 1 Accuracy":28.97,
            "Top 5 Accuracy":54.18
        },
        "uses_additional_data":false,
        "paper":{
            "id":202256,
            "title":"Learn to cycle: Time-consistent feature discovery for action recognition",
            "url":"\/paper\/learn-to-cycle-time-consistent-feature",
            "published":"2020-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learn-to-cycle-time-consistent-feature\/review\/?hl=17739"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":17724,
        "rank":26,
        "method":"SRTG r3d-34",
        "mlmodel":{

        },
        "method_short":"SRTG r3d-34",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-15",
        "metrics":{
            "Top 1 Accuracy":"28.55",
            "Top 5 Accuracy":"52.35"
        },
        "raw_metrics":{
            "Top 1 Accuracy":28.55,
            "Top 5 Accuracy":52.35
        },
        "uses_additional_data":false,
        "paper":{
            "id":202256,
            "title":"Learn to cycle: Time-consistent feature discovery for action recognition",
            "url":"\/paper\/learn-to-cycle-time-consistent-feature",
            "published":"2020-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learn-to-cycle-time-consistent-feature\/review\/?hl=17724"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":5503,
        "rank":27,
        "method":"TRN-Multiscale",
        "mlmodel":{

        },
        "method_short":"TRN-Multiscale",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-22",
        "metrics":{
            "Top 1 Accuracy":"28.27",
            "Top 5 Accuracy":"53.87"
        },
        "raw_metrics":{
            "Top 1 Accuracy":28.27,
            "Top 5 Accuracy":53.87
        },
        "uses_additional_data":false,
        "paper":{
            "id":14233,
            "title":"Temporal Relational Reasoning in Videos",
            "url":"\/paper\/temporal-relational-reasoning-in-videos",
            "published":"2017-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporal-relational-reasoning-in-videos\/review\/?hl=5503"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":28479,
        "rank":28,
        "method":"MoViNet-A0",
        "mlmodel":{

        },
        "method_short":"MoViNet-A0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-21",
        "metrics":{
            "Top 1 Accuracy":"27.5",
            "Top 5 Accuracy":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":27.5,
            "Top 5 Accuracy":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755811,
            "title":"MoViNets: Mobile Video Networks for Efficient Video Recognition",
            "url":"\/paper\/movinets-mobile-video-networks-for-efficient",
            "published":"2021-03-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/movinets-mobile-video-networks-for-efficient\/review\/?hl=28479"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":29019,
        "rank":29,
        "method":"ViViT-L\/16x2",
        "mlmodel":{

        },
        "method_short":"ViViT-L\/16x2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top 1 Accuracy":null,
            "Top 5 Accuracy":"64.9"
        },
        "raw_metrics":{
            "Top 1 Accuracy":null,
            "Top 5 Accuracy":64.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":758440,
            "title":"ViViT: A Video Vision Transformer",
            "url":"\/paper\/2103-15691",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-15691\/review\/?hl=29019"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":947,
        "row_id":5504,
        "rank":30,
        "method":"TSN-2Stream",
        "mlmodel":{

        },
        "method_short":"TSN-2Stream",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-05-08",
        "metrics":{
            "Top 1 Accuracy":null,
            "Top 5 Accuracy":"50.10%"
        },
        "raw_metrics":{
            "Top 1 Accuracy":null,
            "Top 5 Accuracy":50.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":23152,
            "title":"Temporal Segment Networks for Action Recognition in Videos",
            "url":"\/paper\/temporal-segment-networks-for-action",
            "published":"2017-05-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporal-segment-networks-for-action\/review\/?hl=5504"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]