[
    {
        "table_id":2952,
        "row_id":96431,
        "rank":1,
        "Model":"mPLUG-2",
        "mlmodel":{

        },
        "method_short":"mPLUG-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-01",
        "metrics":{
            "CIDEr":"80.3",
            "METEOR":"34.9",
            "ROUGE-L":"70.1",
            "BLEU-4":"57.8"
        },
        "raw_metrics":{
            "CIDEr":80.3,
            "METEOR":34.9,
            "ROUGE-L":70.1,
            "BLEU-4":57.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1151002,
            "title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",
            "url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation",
            "published":"2023-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation\/review\/?hl=96431"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":104641,
        "rank":2,
        "Model":"VAST",
        "mlmodel":{

        },
        "method_short":"VAST",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "CIDEr":"78.0",
            "METEOR":null,
            "ROUGE-L":null,
            "BLEU-4":"56.7"
        },
        "raw_metrics":{
            "CIDEr":78.0,
            "METEOR":null,
            "ROUGE-L":null,
            "BLEU-4":56.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":87218,
        "rank":3,
        "Model":"GIT2",
        "mlmodel":{

        },
        "method_short":"GIT2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-27",
        "metrics":{
            "CIDEr":"75.9",
            "METEOR":"33.1",
            "ROUGE-L":"68.2",
            "BLEU-4":"54.8"
        },
        "raw_metrics":{
            "CIDEr":75.9,
            "METEOR":33.1,
            "ROUGE-L":68.2,
            "BLEU-4":54.8
        },
        "uses_additional_data":true,
        "paper":{
            "id":1017294,
            "title":"GIT: A Generative Image-to-text Transformer for Vision and Language",
            "url":"\/paper\/git-a-generative-image-to-text-transformer",
            "published":"2022-05-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/git-a-generative-image-to-text-transformer\/review\/?hl=87218"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":103553,
        "rank":4,
        "Model":"VLAB",
        "mlmodel":{

        },
        "method_short":"VLAB",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-22",
        "metrics":{
            "CIDEr":"74.9",
            "METEOR":"33.4",
            "ROUGE-L":"68.3",
            "BLEU-4":"54.6"
        },
        "raw_metrics":{
            "CIDEr":74.9,
            "METEOR":33.4,
            "ROUGE-L":68.3,
            "BLEU-4":54.6
        },
        "uses_additional_data":true,
        "paper":{
            "id":1212983,
            "title":"VLAB: Enhancing Video Language Pre-training by Feature Adapting and Blending",
            "url":"\/paper\/vlab-enhancing-video-language-pre-training-by",
            "published":"2023-05-22T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/vlab-enhancing-video-language-pre-training-by\/review\/?hl=103553"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":101414,
        "rank":5,
        "Model":"VALOR",
        "mlmodel":{

        },
        "method_short":"VALOR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-17",
        "metrics":{
            "CIDEr":"74.0",
            "METEOR":"32.9",
            "ROUGE-L":"68.0",
            "BLEU-4":"54.4"
        },
        "raw_metrics":{
            "CIDEr":74.0,
            "METEOR":32.9,
            "ROUGE-L":68.0,
            "BLEU-4":54.4
        },
        "uses_additional_data":true,
        "paper":{
            "id":1191887,
            "title":"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset",
            "url":"\/paper\/valor-vision-audio-language-omni-perception",
            "published":"2023-04-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/valor-vision-audio-language-omni-perception\/review\/?hl=101414"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":106249,
        "rank":6,
        "Model":"MaMMUT (ours)",
        "mlmodel":{

        },
        "method_short":"MaMMUT ",
        "method_details":"ours",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-29",
        "metrics":{
            "CIDEr":"73.6",
            "METEOR":null,
            "ROUGE-L":null,
            "BLEU-4":null
        },
        "raw_metrics":{
            "CIDEr":73.6,
            "METEOR":null,
            "ROUGE-L":null,
            "BLEU-4":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1182696,
            "title":"MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks",
            "url":"\/paper\/mammut-a-simple-architecture-for-joint",
            "published":"2023-03-29T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/mammut-a-simple-architecture-for-joint\/review\/?hl=106249"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":87219,
        "rank":7,
        "Model":"VideoCoCa",
        "mlmodel":{

        },
        "method_short":"VideoCoCa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "CIDEr":"73.2",
            "METEOR":null,
            "ROUGE-L":"68.0",
            "BLEU-4":"53.8"
        },
        "raw_metrics":{
            "CIDEr":73.2,
            "METEOR":null,
            "ROUGE-L":68.0,
            "BLEU-4":53.8
        },
        "uses_additional_data":true,
        "paper":{
            "id":1126258,
            "title":"VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners",
            "url":"\/paper\/video-text-modeling-with-zero-shot-transfer",
            "published":"2022-12-09T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/video-text-modeling-with-zero-shot-transfer\/review\/?hl=87219"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":88519,
        "rank":8,
        "Model":"HiTeA",
        "mlmodel":{

        },
        "method_short":"HiTeA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-30",
        "metrics":{
            "CIDEr":"65.1",
            "METEOR":"30.7",
            "ROUGE-L":"65.0",
            "BLEU-4":"49.2"
        },
        "raw_metrics":{
            "CIDEr":65.1,
            "METEOR":30.7,
            "ROUGE-L":65.0,
            "BLEU-4":49.2
        },
        "uses_additional_data":true,
        "paper":{
            "id":1136078,
            "title":"HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training",
            "url":"\/paper\/hitea-hierarchical-temporal-aware-video",
            "published":"2022-12-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/hitea-hierarchical-temporal-aware-video\/review\/?hl=88519"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":98738,
        "rank":9,
        "Model":"Vid2Seq",
        "mlmodel":{

        },
        "method_short":"Vid2Seq",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "CIDEr":"64.6",
            "METEOR":"30.8",
            "ROUGE-L":null,
            "BLEU-4":null
        },
        "raw_metrics":{
            "CIDEr":64.6,
            "METEOR":30.8,
            "ROUGE-L":null,
            "BLEU-4":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1165314,
            "title":"Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning",
            "url":"\/paper\/vid2seq-large-scale-pretraining-of-a-visual",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vid2seq-large-scale-pretraining-of-a-visual\/review\/?hl=98738"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":109118,
        "rank":10,
        "Model":"TextKG",
        "mlmodel":{

        },
        "method_short":"TextKG",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-22",
        "metrics":{
            "CIDEr":"60.8",
            "METEOR":"30.5",
            "ROUGE-L":"64.8",
            "BLEU-4":"46.6"
        },
        "raw_metrics":{
            "CIDEr":60.8,
            "METEOR":30.5,
            "ROUGE-L":64.8,
            "BLEU-4":46.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1178511,
            "title":"Text with Knowledge Graph Augmented Transformer for Video Captioning",
            "url":"\/paper\/text-with-knowledge-graph-augmented",
            "published":"2023-03-22T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/text-with-knowledge-graph-augmented\/review\/?hl=109118"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":113891,
        "rank":11,
        "Model":"IcoCap (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"IcoCap ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-05",
        "metrics":{
            "CIDEr":"60.2",
            "METEOR":"31.1",
            "ROUGE-L":"64.9",
            "BLEU-4":"47.0"
        },
        "raw_metrics":{
            "CIDEr":60.2,
            "METEOR":31.1,
            "ROUGE-L":64.9,
            "BLEU-4":47.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1340144,
            "title":"IcoCap: Improving Video Captioning by Compounding Images",
            "url":"\/paper\/icocap-improving-video-captioning-by",
            "published":"2023-10-05T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":87217,
        "rank":12,
        "Model":"MV-GPT",
        "mlmodel":{

        },
        "method_short":"MV-GPT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-20",
        "metrics":{
            "CIDEr":"60.0",
            "METEOR":"38.7",
            "ROUGE-L":"64.0",
            "BLEU-4":"48.9"
        },
        "raw_metrics":{
            "CIDEr":60.0,
            "METEOR":38.7,
            "ROUGE-L":64.0,
            "BLEU-4":48.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":948288,
            "title":"End-to-end Generative Pretraining for Multimodal Video Captioning",
            "url":"\/paper\/end-to-end-generative-pretraining-for",
            "published":"2022-01-20T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/end-to-end-generative-pretraining-for\/review\/?hl=87217"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":113892,
        "rank":13,
        "Model":"IcoCap (ViT-B\/32)",
        "mlmodel":{

        },
        "method_short":"IcoCap ",
        "method_details":"ViT-B\/32",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-05",
        "metrics":{
            "CIDEr":"59.1",
            "METEOR":"30.3",
            "ROUGE-L":"64.3",
            "BLEU-4":"46.1"
        },
        "raw_metrics":{
            "CIDEr":59.1,
            "METEOR":30.3,
            "ROUGE-L":64.3,
            "BLEU-4":46.1
        },
        "uses_additional_data":true,
        "paper":{
            "id":1340144,
            "title":"IcoCap: Improving Video Captioning by Compounding Images",
            "url":"\/paper\/icocap-improving-video-captioning-by",
            "published":"2023-10-05T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":97374,
        "rank":14,
        "Model":"CLIP-DCD",
        "mlmodel":{

        },
        "method_short":"CLIP-DCD",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-30",
        "metrics":{
            "CIDEr":"58.7",
            "METEOR":"31.3",
            "ROUGE-L":"64.8",
            "BLEU-4":"48.2"
        },
        "raw_metrics":{
            "CIDEr":58.7,
            "METEOR":31.3,
            "ROUGE-L":64.8,
            "BLEU-4":48.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":923018,
            "title":"CLIP Meets Video Captioning: Concept-Aware Representation Learning Does Matter",
            "url":"\/paper\/clip-meets-video-captioners-attribute-aware",
            "published":"2021-11-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clip-meets-video-captioners-attribute-aware\/review\/?hl=97374"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":96099,
        "rank":15,
        "Model":"VIOLETv2",
        "mlmodel":{

        },
        "method_short":"VIOLETv2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-04",
        "metrics":{
            "CIDEr":"58",
            "METEOR":null,
            "ROUGE-L":null,
            "BLEU-4":null
        },
        "raw_metrics":{
            "CIDEr":58.0,
            "METEOR":null,
            "ROUGE-L":null,
            "BLEU-4":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1069338,
            "title":"An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling",
            "url":"\/paper\/an-empirical-study-of-end-to-end-video",
            "published":"2022-09-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-empirical-study-of-end-to-end-video\/review\/?hl=96099"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":109132,
        "rank":16,
        "Model":"CoCap (ViT\/L14)",
        "mlmodel":{

        },
        "method_short":"CoCap ",
        "method_details":"ViT\/L14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-22",
        "metrics":{
            "CIDEr":"57.2",
            "METEOR":"30.3",
            "ROUGE-L":"63.4",
            "BLEU-4":"44.4"
        },
        "raw_metrics":{
            "CIDEr":57.2,
            "METEOR":30.3,
            "ROUGE-L":63.4,
            "BLEU-4":44.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1286060,
            "title":"Accurate and Fast Compressed Video Captioning",
            "url":"\/paper\/accurate-and-fast-compressed-video-captioning",
            "published":"2023-09-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/accurate-and-fast-compressed-video-captioning\/review\/?hl=109132"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":110290,
        "rank":17,
        "Model":"VASTA (Vatex-backbone)",
        "mlmodel":{

        },
        "method_short":"VASTA ",
        "method_details":"Vatex-backbone",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-19",
        "metrics":{
            "CIDEr":"56.08",
            "METEOR":"30.24",
            "ROUGE-L":"62.9",
            "BLEU-4":"44.21"
        },
        "raw_metrics":{
            "CIDEr":56.08,
            "METEOR":30.24,
            "ROUGE-L":62.9,
            "BLEU-4":44.21
        },
        "uses_additional_data":false,
        "paper":{
            "id":1061664,
            "title":"Diverse Video Captioning by Adaptive Spatio-temporal Attention",
            "url":"\/paper\/diverse-video-captioning-by-adaptive-spatio",
            "published":"2022-08-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diverse-video-captioning-by-adaptive-spatio\/review\/?hl=110290"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":110289,
        "rank":18,
        "Model":"VASTA (Kinetics-backbone)",
        "mlmodel":{

        },
        "method_short":"VASTA ",
        "method_details":"Kinetics-backbone",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-19",
        "metrics":{
            "CIDEr":"55",
            "METEOR":"30.2",
            "ROUGE-L":"62.5",
            "BLEU-4":"43.4"
        },
        "raw_metrics":{
            "CIDEr":55.0,
            "METEOR":30.2,
            "ROUGE-L":62.5,
            "BLEU-4":43.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1061664,
            "title":"Diverse Video Captioning by Adaptive Spatio-temporal Attention",
            "url":"\/paper\/diverse-video-captioning-by-adaptive-spatio",
            "published":"2022-08-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diverse-video-captioning-by-adaptive-spatio\/review\/?hl=110289"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":96071,
        "rank":19,
        "Model":"EMCL-Net",
        "mlmodel":{

        },
        "method_short":"EMCL-Net",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-21",
        "metrics":{
            "CIDEr":"54.6",
            "METEOR":"30.2",
            "ROUGE-L":"63.2",
            "BLEU-4":"45.3"
        },
        "raw_metrics":{
            "CIDEr":54.6,
            "METEOR":30.2,
            "ROUGE-L":63.2,
            "BLEU-4":45.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1114877,
            "title":"Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations",
            "url":"\/paper\/expectation-maximization-contrastive-learning",
            "published":"2022-11-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/expectation-maximization-contrastive-learning\/review\/?hl=96071"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2952,
        "row_id":100003,
        "rank":20,
        "Model":"UniVL + MELTR",
        "mlmodel":{

        },
        "method_short":"UniVL + MELTR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "CIDEr":"52.77",
            "METEOR":"29.26",
            "ROUGE-L":"62.35",
            "BLEU-4":"44.17"
        },
        "raw_metrics":{
            "CIDEr":52.77,
            "METEOR":29.26,
            "ROUGE-L":62.35,
            "BLEU-4":44.17
        },
        "uses_additional_data":false,
        "paper":{
            "id":1179150,
            "title":"MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models",
            "url":"\/paper\/meltr-meta-loss-transformer-for-learning-to",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meltr-meta-loss-transformer-for-learning-to\/review\/?hl=100003"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]