[
    {
        "table_id":20340,
        "row_id":104634,
        "rank":1,
        "method":"VAST",
        "mlmodel":{

        },
        "Model":"VAST",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "text-to-video R@1":"49.3",
            "text-to-video R@5":"68.3",
            "text-to-video R@10":"73.9",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":49.3,
            "text-to-video R@5":68.3,
            "text-to-video R@10":73.9,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102762,
        "rank":2,
        "method":"mPLUG-2",
        "mlmodel":{

        },
        "Model":"mPLUG-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-01",
        "metrics":{
            "text-to-video R@1":"47.1",
            "text-to-video R@5":"69.7",
            "text-to-video R@10":"79.0",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":47.1,
            "text-to-video R@5":69.7,
            "text-to-video R@10":79.0,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1151002,
            "title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",
            "url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation",
            "published":"2023-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation\/review\/?hl=102762"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":109720,
        "rank":3,
        "method":"LanguageBind",
        "mlmodel":{

        },
        "Model":"LanguageBind",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-03",
        "metrics":{
            "text-to-video R@1":"44.8",
            "text-to-video R@5":"70.0",
            "text-to-video R@10":"78.7",
            "text-to-video Median Rank":"2",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":"40.9",
            "video-to-text R@5":"66.4",
            "video-to-text R@10":"75.7",
            "video-to-text Median Rank":"2.",
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":44.8,
            "text-to-video R@5":70.0,
            "text-to-video R@10":78.7,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":40.9,
            "video-to-text R@5":66.4,
            "video-to-text R@10":75.7,
            "video-to-text Median Rank":2.0,
            "text-to-video MedianR":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1292471,
            "title":"LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment",
            "url":"\/paper\/languagebind-extending-video-language",
            "published":"2023-10-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/languagebind-extending-video-language\/review\/?hl=109720"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":100391,
        "rank":4,
        "method":"UMT-L (ViT-L\/16)",
        "mlmodel":{

        },
        "Model":"UMT-L ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "text-to-video R@1":"42.6",
            "text-to-video R@5":"64.4",
            "text-to-video R@10":"73.1",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":"38.6",
            "video-to-text R@5":"59.8",
            "video-to-text R@10":"69.6",
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":42.6,
            "text-to-video R@5":64.4,
            "text-to-video R@10":73.1,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":38.6,
            "video-to-text R@5":59.8,
            "video-to-text R@10":69.6,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1181934,
            "title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
            "url":"\/paper\/unmasked-teacher-towards-training-efficient",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":109687,
        "rank":5,
        "method":"BT-Adapter",
        "mlmodel":{

        },
        "Model":"BT-Adapter",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-27",
        "metrics":{
            "text-to-video R@1":"40.9",
            "text-to-video R@5":"64.7",
            "text-to-video R@10":"73.5",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":40.9,
            "text-to-video R@5":64.7,
            "text-to-video R@10":73.5,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1289477,
            "title":"One For All: Video Conversation is Feasible Without Video Instruction Tuning",
            "url":"\/paper\/one-for-all-video-conversation-is-feasible",
            "published":"2023-09-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-for-all-video-conversation-is-feasible\/review\/?hl=109687"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":86733,
        "rank":6,
        "method":"InternVideo",
        "mlmodel":{

        },
        "Model":"InternVideo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "text-to-video R@1":"40.7",
            "text-to-video R@5":null,
            "text-to-video R@10":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":"39.6",
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":40.7,
            "text-to-video R@5":null,
            "text-to-video R@10":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":39.6,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124231,
            "title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
            "url":"\/paper\/internvideo-general-video-foundation-models",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internvideo-general-video-foundation-models\/review\/?hl=86733"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102875,
        "rank":7,
        "method":"Florence",
        "mlmodel":{

        },
        "Model":"Florence",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-22",
        "metrics":{
            "text-to-video R@1":"37.6",
            "text-to-video R@5":"63.8",
            "text-to-video R@10":"72.6",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":37.6,
            "text-to-video R@5":63.8,
            "text-to-video R@10":72.6,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":914419,
            "title":"Florence: A New Foundation Model for Computer Vision",
            "url":"\/paper\/florence-a-new-foundation-model-for-computer",
            "published":"2021-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/florence-a-new-foundation-model-for-computer\/review\/?hl=102875"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102883,
        "rank":8,
        "method":"ImageBind",
        "mlmodel":{

        },
        "Model":"ImageBind",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-09",
        "metrics":{
            "text-to-video R@1":"36.8",
            "text-to-video R@5":"61.8",
            "text-to-video R@10":"70.0",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":36.8,
            "text-to-video R@5":61.8,
            "text-to-video R@10":70.0,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1204945,
            "title":"ImageBind: One Embedding Space To Bind Them All",
            "url":"\/paper\/imagebind-one-embedding-space-to-bind-them",
            "published":"2023-05-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/imagebind-one-embedding-space-to-bind-them\/review\/?hl=102883"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":69050,
        "rank":9,
        "method":"OmniVL",
        "mlmodel":{

        },
        "Model":"OmniVL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-15",
        "metrics":{
            "text-to-video R@1":"34.6",
            "text-to-video R@5":"58.4",
            "text-to-video R@10":"66.6",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":34.6,
            "text-to-video R@5":58.4,
            "text-to-video R@10":66.6,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1075042,
            "title":"OmniVL:One Foundation Model for Image-Language and Video-Language Tasks",
            "url":"\/paper\/omnivl-one-foundation-model-for-image",
            "published":"2022-09-15T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivl-one-foundation-model-for-image\/review\/?hl=69050"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102853,
        "rank":10,
        "method":"HiTeA-17M",
        "mlmodel":{

        },
        "Model":"HiTeA-17M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-30",
        "metrics":{
            "text-to-video R@1":"34.4",
            "text-to-video R@5":"60.0",
            "text-to-video R@10":"69.9",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":34.4,
            "text-to-video R@5":60.0,
            "text-to-video R@10":69.9,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136078,
            "title":"HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training",
            "url":"\/paper\/hitea-hierarchical-temporal-aware-video",
            "published":"2022-12-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/hitea-hierarchical-temporal-aware-video\/review\/?hl=102853"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102881,
        "rank":11,
        "method":"VideoCoCa",
        "mlmodel":{

        },
        "Model":"VideoCoCa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "text-to-video R@1":"34.3",
            "text-to-video R@5":"57.8",
            "text-to-video R@10":"67.0",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":"64.7",
            "video-to-text R@5":"85.2",
            "video-to-text R@10":"91.4",
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":34.3,
            "text-to-video R@5":57.8,
            "text-to-video R@10":67.0,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":64.7,
            "video-to-text R@5":85.2,
            "video-to-text R@10":91.4,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1126258,
            "title":"VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners",
            "url":"\/paper\/video-text-modeling-with-zero-shot-transfer",
            "published":"2022-12-09T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/video-text-modeling-with-zero-shot-transfer\/review\/?hl=102881"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102847,
        "rank":12,
        "method":"Singularity-17M",
        "mlmodel":{

        },
        "Model":"Singularity-17M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-07",
        "metrics":{
            "text-to-video R@1":"34.0",
            "text-to-video R@5":"56.7",
            "text-to-video R@10":"66.7",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":34.0,
            "text-to-video R@5":56.7,
            "text-to-video R@10":66.7,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1023192,
            "title":"Revealing Single Frame Bias for Video-and-Language Learning",
            "url":"\/paper\/revealing-single-frame-bias-for-video-and",
            "published":"2022-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revealing-single-frame-bias-for-video-and\/review\/?hl=102847"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102859,
        "rank":13,
        "method":"CLIP4Clip",
        "mlmodel":{

        },
        "Model":"CLIP4Clip",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-18",
        "metrics":{
            "text-to-video R@1":"32.0",
            "text-to-video R@5":"57.0",
            "text-to-video R@10":"66.9",
            "text-to-video Median Rank":"4",
            "text-to-video Mean Rank":"34.0",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":32.0,
            "text-to-video R@5":57.0,
            "text-to-video R@10":66.9,
            "text-to-video Median Rank":4.0,
            "text-to-video Mean Rank":34.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":784398,
            "title":"CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval",
            "url":"\/paper\/clip4clip-an-empirical-study-of-clip-for-end",
            "published":"2021-04-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clip4clip-an-empirical-study-of-clip-for-end\/review\/?hl=102859"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102852,
        "rank":14,
        "method":"HiTeA-5M",
        "mlmodel":{

        },
        "Model":"HiTeA-5M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-30",
        "metrics":{
            "text-to-video R@1":"29.9",
            "text-to-video R@5":"54.2",
            "text-to-video R@10":"62.9",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":29.9,
            "text-to-video R@5":54.2,
            "text-to-video R@10":62.9,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136078,
            "title":"HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training",
            "url":"\/paper\/hitea-hierarchical-temporal-aware-video",
            "published":"2022-12-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/hitea-hierarchical-temporal-aware-video\/review\/?hl=102852"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102848,
        "rank":15,
        "method":"Singularity-5M",
        "mlmodel":{

        },
        "Model":"Singularity-5M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-07",
        "metrics":{
            "text-to-video R@1":"28.4",
            "text-to-video R@5":"50.2",
            "text-to-video R@10":"59.5",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":28.4,
            "text-to-video R@5":50.2,
            "text-to-video R@10":59.5,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1023192,
            "title":"Revealing Single Frame Bias for Video-and-Language Learning",
            "url":"\/paper\/revealing-single-frame-bias-for-video-and",
            "published":"2022-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revealing-single-frame-bias-for-video-and\/review\/?hl=102848"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102615,
        "rank":16,
        "method":"Clover",
        "mlmodel":{

        },
        "Model":"Clover",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-16",
        "metrics":{
            "text-to-video R@1":"26.4",
            "text-to-video R@5":"49.5",
            "text-to-video R@10":"60",
            "text-to-video Median Rank":"6",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":26.4,
            "text-to-video R@5":49.5,
            "text-to-video R@10":60.0,
            "text-to-video Median Rank":6.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1045027,
            "title":"Clover: Towards A Unified Video-Language Alignment and Fusion Model",
            "url":"\/paper\/clover-towards-a-unified-video-language",
            "published":"2022-07-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clover-towards-a-unified-video-language\/review\/?hl=102615"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102863,
        "rank":17,
        "method":"Y. Ge et. al.",
        "mlmodel":{

        },
        "Model":"Y. Ge et. al.",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-13",
        "metrics":{
            "text-to-video R@1":"26.0",
            "text-to-video R@5":"46.4",
            "text-to-video R@10":"56.4",
            "text-to-video Median Rank":"7.0",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":26.0,
            "text-to-video R@5":46.4,
            "text-to-video R@10":56.4,
            "text-to-video Median Rank":7.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":944982,
            "title":"Bridging Video-text Retrieval with Multiple Choice Questions",
            "url":"\/paper\/bridgeformer-bridging-video-text-retrieval",
            "published":"2022-01-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bridgeformer-bridging-video-text-retrieval\/review\/?hl=102863"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102872,
        "rank":18,
        "method":"FROZEN",
        "mlmodel":{

        },
        "Model":"FROZEN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-01",
        "metrics":{
            "text-to-video R@1":"24.7",
            "text-to-video R@5":"46.9",
            "text-to-video R@10":"57.2",
            "text-to-video Median Rank":"7.0",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":24.7,
            "text-to-video R@5":46.9,
            "text-to-video R@10":57.2,
            "text-to-video Median Rank":7.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":773452,
            "title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
            "url":"\/paper\/frozen-in-time-a-joint-video-and-image",
            "published":"2021-04-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/frozen-in-time-a-joint-video-and-image\/review\/?hl=102872"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102876,
        "rank":19,
        "method":"ALPRO",
        "mlmodel":{

        },
        "Model":"ALPRO",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-17",
        "metrics":{
            "text-to-video R@1":"24.1",
            "text-to-video R@5":"44.7",
            "text-to-video R@10":"55.4",
            "text-to-video Median Rank":"8",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":24.1,
            "text-to-video R@5":44.7,
            "text-to-video R@10":55.4,
            "text-to-video Median Rank":8.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932382,
            "title":"Align and Prompt: Video-and-Language Pre-training with Entity Prompts",
            "url":"\/paper\/align-and-prompt-video-and-language-pre",
            "published":"2021-12-17T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":103259,
        "rank":20,
        "method":"A. Nagrani et. al.",
        "mlmodel":{

        },
        "Model":"A. Nagrani et. al.",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-01",
        "metrics":{
            "text-to-video R@1":"19.4",
            "text-to-video R@5":"39.5",
            "text-to-video R@10":"50.3",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":19.4,
            "text-to-video R@5":39.5,
            "text-to-video R@10":50.3,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":988278,
            "title":"Learning Audio-Video Modalities from Image Captions",
            "url":"\/paper\/learning-audio-video-modalities-from-image",
            "published":"2022-04-01T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/learning-audio-video-modalities-from-image\/review\/?hl=103259"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":110582,
        "rank":21,
        "method":"HD-VILA",
        "mlmodel":{

        },
        "Model":"HD-VILA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-19",
        "metrics":{
            "text-to-video R@1":"14.6",
            "text-to-video R@5":"34.4",
            "text-to-video R@10":"44.1",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":"15"
        },
        "raw_metrics":{
            "text-to-video R@1":14.6,
            "text-to-video R@5":34.4,
            "text-to-video R@10":44.1,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":15.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":912961,
            "title":"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions",
            "url":"\/paper\/advancing-high-resolution-video-language",
            "published":"2021-11-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/advancing-high-resolution-video-language\/review\/?hl=110582"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102870,
        "rank":22,
        "method":"VideoCLIP",
        "mlmodel":{

        },
        "Model":"VideoCLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-28",
        "metrics":{
            "text-to-video R@1":"10.4",
            "text-to-video R@5":"22.2",
            "text-to-video R@10":"30.0",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":10.4,
            "text-to-video R@5":22.2,
            "text-to-video R@10":30.0,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":875851,
            "title":"VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding",
            "url":"\/paper\/videoclip-contrastive-pre-training-for-zero",
            "published":"2021-09-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102867,
        "rank":23,
        "method":"TACo",
        "mlmodel":{

        },
        "Model":"TACo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-08-23",
        "metrics":{
            "text-to-video R@1":"9.8",
            "text-to-video R@5":"25.0",
            "text-to-video R@10":"33.4",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":9.8,
            "text-to-video R@5":25.0,
            "text-to-video R@10":33.4,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":854963,
            "title":"TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment",
            "url":"\/paper\/taco-token-aware-cascade-contrastive-learning",
            "published":"2021-08-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/taco-token-aware-cascade-contrastive-learning\/review\/?hl=102867"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102878,
        "rank":24,
        "method":"El. Amrani et. al.",
        "mlmodel":{

        },
        "Model":"El. Amrani et. al.",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-06",
        "metrics":{
            "text-to-video R@1":"8.0",
            "text-to-video R@5":"21.3",
            "text-to-video R@10":"29.3",
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":8.0,
            "text-to-video R@5":21.3,
            "text-to-video R@10":29.3,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":186098,
            "title":"Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning",
            "url":"\/paper\/noise-estimation-using-density-estimation-for",
            "published":"2020-03-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/noise-estimation-using-density-estimation-for\/review\/?hl=102878"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20340,
        "row_id":102862,
        "rank":25,
        "method":"MMT",
        "mlmodel":{

        },
        "Model":"MMT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-21",
        "metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":"14.4",
            "text-to-video R@10":null,
            "text-to-video Median Rank":"66",
            "text-to-video Mean Rank":"148.1",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "raw_metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":14.4,
            "text-to-video R@10":null,
            "text-to-video Median Rank":66.0,
            "text-to-video Mean Rank":148.1,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "text-to-video MedianR":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":209798,
            "title":"Multi-modal Transformer for Video Retrieval",
            "url":"\/paper\/multi-modal-transformer-for-video-retrieval",
            "published":"2020-07-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multi-modal-transformer-for-video-retrieval\/review\/?hl=102862"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]