[
    {
        "Model":"alpaca-7b",
        "Publisher":"Stanford",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.739,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.661,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"alpaca-13b",
        "Publisher":"Stanford",
        "Open?":"no",
        "Chatbot Arena Elo":1008.0,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"bloom-176b",
        "Publisher":"BigScience",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":0.744,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.155,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":0.299,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"cerebras-gpt-7b",
        "Publisher":"Cerebras",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.636,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":0.636,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":0.259,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":0.141,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"cerebras-gpt-13b",
        "Publisher":"Cerebras",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.635,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":0.635,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":0.258,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":0.146,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"chatglm-6b",
        "Publisher":"ChatGLM",
        "Open?":"yes",
        "Chatbot Arena Elo":985.0,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"chinchilla-70b",
        "Publisher":"DeepMind",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.808,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":0.774,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":0.675,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.749,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"codex-12b \/ code-cushman-001",
        "Publisher":"OpenAI",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.317,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"codegen-16B-mono",
        "Publisher":"Salesforce",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.293,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"codegen-16B-multi",
        "Publisher":"Salesforce",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.183,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"codegx-13b",
        "Publisher":"Tsinghua University",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.229,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"dolly-v2-12b",
        "Publisher":"Databricks",
        "Open?":"yes",
        "Chatbot Arena Elo":944.0,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.71,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.622,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"eleuther-pythia-7b",
        "Publisher":"EleutherAI",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.667,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":0.667,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":0.265,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":0.198,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.661,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"eleuther-pythia-12b",
        "Publisher":"EleutherAI",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.704,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":0.704,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":0.253,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":0.233,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.638,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"falcon-7b",
        "Publisher":"TII",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":0.781,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":0.35,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"falcon-40b",
        "Publisher":"TII",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":0.853,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":0.527,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"fastchat-t5-3b",
        "Publisher":"Lmsys.org",
        "Open?":"yes",
        "Chatbot Arena Elo":951.0,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"gal-120b",
        "Publisher":"Meta AI",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":0.526,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"gpt-3-7b \/ curie",
        "Publisher":"OpenAI",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":0.682,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":0.243,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"gpt-3-175b \/ davinci",
        "Publisher":"OpenAI",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":0.793,
        "HellaSwag (zero-shot)":0.789,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":0.439,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.702,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"gpt-3.5-175b \/ text-davinci-003",
        "Publisher":"OpenAI",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":0.822,
        "HellaSwag (zero-shot)":0.834,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.481,
        "LAMBADA (zero-shot)":0.762,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":0.569,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.758,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":0.816
    },
    {
        "Model":"gpt-3.5-175b \/ code-davinci-002",
        "Publisher":"OpenAI",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.463,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"gpt-4",
        "Publisher":"OpenAI",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":0.953,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.67,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":0.864,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":0.875
    },
    {
        "Model":"gpt4all-13b-snoozy",
        "Publisher":"Nomic AI",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.75,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.713,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"gpt-neox-20b",
        "Publisher":"EleutherAI",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":0.718,
        "HellaSwag (zero-shot)":0.719,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":0.719,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":0.269,
        "MMLU (few-shot)":0.276,
        "TriviaQA (zero-shot)":0.347,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"gpt-j-6b",
        "Publisher":"EleutherAI",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":0.663,
        "HellaSwag (zero-shot)":0.683,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":0.683,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":0.261,
        "MMLU (few-shot)":0.249,
        "TriviaQA (zero-shot)":0.234,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"koala-13b",
        "Publisher":"Berkeley BAIR",
        "Open?":"no",
        "Chatbot Arena Elo":1082.0,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.726,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.688,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"llama-7b",
        "Publisher":"Meta AI",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.738,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.105,
        "LAMBADA (zero-shot)":0.738,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":0.302,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":0.443,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.701,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"llama-13b",
        "Publisher":"Meta AI",
        "Open?":"no",
        "Chatbot Arena Elo":932.0,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.792,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.158,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.73,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"llama-33b",
        "Publisher":"Meta AI",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.828,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.217,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.76,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"llama-65b",
        "Publisher":"Meta AI",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.842,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.237,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":0.634,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.77,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"llama-2-70b",
        "Publisher":"Meta AI",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":0.873,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":0.698,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"mpt-7b",
        "Publisher":"MosaicML",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.761,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":0.702,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":0.296,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":0.343,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"oasst-pythia-12b",
        "Publisher":"Open Assistant",
        "Open?":"yes",
        "Chatbot Arena Elo":1065.0,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.681,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.65,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"opt-7b",
        "Publisher":"Meta AI",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.677,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":0.677,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":0.251,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":0.227,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"opt-13b",
        "Publisher":"Meta AI",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.692,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":0.692,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":0.257,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":0.282,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"opt-66b",
        "Publisher":"Meta AI",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":0.745,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":0.276,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"opt-175b",
        "Publisher":"Meta AI",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":0.791,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":0.318,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"palm-62b",
        "Publisher":"Google Research",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.77,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"palm-540b",
        "Publisher":"Google Research",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":0.838,
        "HellaSwag (zero-shot)":0.834,
        "HellaSwag (one-shot)":0.836,
        "HumanEval-Python (pass@1)":0.262,
        "LAMBADA (zero-shot)":0.779,
        "LAMBADA (one-shot)":0.818,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":0.693,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":0.814,
        "WinoGrande (zero-shot)":0.811,
        "WinoGrande (one-shot)":0.837,
        "WinoGrande (few-shot)":0.851
    },
    {
        "Model":"palm-coder-540b",
        "Publisher":"Google Research",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.359,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"palm-2-s",
        "Publisher":"Google Research",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":0.82,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":0.807,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":0.752,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":0.779,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"palm-2-s",
        "Publisher":"Google Research",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.376,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"palm-2-m",
        "Publisher":"Google Research",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":0.84,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":0.837,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":0.817,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":0.792,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"palm-2-l",
        "Publisher":"Google Research",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":0.868,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":0.869,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":0.861,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":0.83,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"palm-2-l-instruct",
        "Publisher":"Google Research",
        "Open?":"no",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":0.909
    },
    {
        "Model":"replit-code-v1-3b",
        "Publisher":"Replit",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.219,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"stablelm-base-alpha-7b",
        "Publisher":"Stability AI",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.412,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":0.533,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":0.251,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":0.049,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.501,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"stablelm-tuned-alpha-7b",
        "Publisher":"Stability AI",
        "Open?":"no",
        "Chatbot Arena Elo":858.0,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":0.536,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":0.548,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"starcoder-base-16b",
        "Publisher":"BigCode",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.304,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"starcoder-16b",
        "Publisher":"BigCode",
        "Open?":"yes",
        "Chatbot Arena Elo":null,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":0.336,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    },
    {
        "Model":"vicuna-13b",
        "Publisher":"Lmsys.org",
        "Open?":"no",
        "Chatbot Arena Elo":1169.0,
        "HellaSwag (few-shot)":null,
        "HellaSwag (zero-shot)":null,
        "HellaSwag (one-shot)":null,
        "HumanEval-Python (pass@1)":null,
        "LAMBADA (zero-shot)":null,
        "LAMBADA (one-shot)":null,
        "MMLU (zero-shot)":null,
        "MMLU (few-shot)":null,
        "TriviaQA (zero-shot)":null,
        "TriviaQA (one-shot)":null,
        "WinoGrande (zero-shot)":null,
        "WinoGrande (one-shot)":null,
        "WinoGrande (few-shot)":null
    }
]