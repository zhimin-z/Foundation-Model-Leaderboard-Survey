[
    {
        "table_id":1236,
        "row_id":86740,
        "rank":1,
        "Model":"InternVideo",
        "mlmodel":{

        },
        "method_short":"InternVideo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Top 1 Accuracy":"70.0",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":70.0,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124231,
            "title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
            "url":"\/paper\/internvideo-general-video-foundation-models",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internvideo-general-video-foundation-models\/review\/?hl=86740"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":100529,
        "rank":2,
        "Model":"VideoMAE V2-g",
        "mlmodel":{

        },
        "method_short":"VideoMAE V2-g",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-29",
        "metrics":{
            "Top 1 Accuracy":"68.7",
            "Top 5 Accuracy":"91.9",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":68.7,
            "Top 5 Accuracy":91.9,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1182705,
            "title":"VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
            "url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders",
            "published":"2023-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videomae-v2-scaling-video-masked-autoencoders\/review\/?hl=100529"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":112694,
        "rank":3,
        "Model":"Side4Video (EVA ViT-E\/14",
        "mlmodel":{

        },
        "method_short":"Side4Video ",
        "method_details":"EVA ViT-E\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Top 1 Accuracy":"67.3",
            "Top 5 Accuracy":"88.8",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":67.3,
            "Top 5 Accuracy":88.8,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327957,
            "title":"Side4Video: Spatial-Temporal Side Network for Memory-Efficient Image-to-Video Transfer Learning",
            "url":"\/paper\/side4video-spatial-temporal-side-network-for",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/side4video-spatial-temporal-side-network-for\/review\/?hl=112694"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":106199,
        "rank":4,
        "Model":"ATM",
        "mlmodel":{

        },
        "method_short":"ATM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-18",
        "metrics":{
            "Top 1 Accuracy":"65.6",
            "Top 5 Accuracy":"88.6",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":65.6,
            "Top 5 Accuracy":88.6,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1248104,
            "title":"What Can Simple Arithmetic Operations Do for Temporal Modeling?",
            "url":"\/paper\/what-can-simple-arithmetic-operations-do-for",
            "published":"2023-07-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/what-can-simple-arithmetic-operations-do-for\/review\/?hl=106199"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":107497,
        "rank":5,
        "Model":"TAdaFormer-L\/14",
        "mlmodel":{

        },
        "method_short":"TAdaFormer-L\/14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-10",
        "metrics":{
            "Top 1 Accuracy":"63.7",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":63.7,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1262390,
            "title":"Temporally-Adaptive Models for Efficient Video Understanding",
            "url":"\/paper\/temporally-adaptive-models-for-efficient",
            "published":"2023-08-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporally-adaptive-models-for-efficient\/review\/?hl=107497"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":77324,
        "rank":6,
        "Model":"UniFormerV2-L",
        "mlmodel":{

        },
        "method_short":"UniFormerV2-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-22",
        "metrics":{
            "Top 1 Accuracy":"62.7",
            "Top 5 Accuracy":"88.0",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":62.7,
            "Top 5 Accuracy":88.0,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1087805,
            "title":"UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer",
            "url":"\/paper\/uniformerv2-spatiotemporal-learning-by-arming",
            "published":"2022-09-22T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":43441,
        "rank":7,
        "Model":"UniFormer-B (IN-1K + Kinetics400)",
        "mlmodel":{

        },
        "method_short":"UniFormer-B ",
        "method_details":"IN-1K + Kinetics400",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top 1 Accuracy":"60.9",
            "Top 5 Accuracy":"87.3",
            "Param.":"50.1",
            "GFLOPs":"259x3"
        },
        "raw_metrics":{
            "Top 1 Accuracy":60.9,
            "Top 5 Accuracy":87.3,
            "Param.":50.1,
            "GFLOPs":259.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":883979,
            "title":"UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning",
            "url":"\/paper\/uniformer-unified-transformer-for-efficient",
            "published":"2021-09-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":173,
                "name":"UniFormer",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":107498,
        "rank":8,
        "Model":"TAdaConvNeXtV2-B",
        "mlmodel":{

        },
        "method_short":"TAdaConvNeXtV2-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-10",
        "metrics":{
            "Top 1 Accuracy":"60.7",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":60.7,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1262390,
            "title":"Temporally-Adaptive Models for Efficient Video Understanding",
            "url":"\/paper\/temporally-adaptive-models-for-efficient",
            "published":"2023-08-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporally-adaptive-models-for-efficient\/review\/?hl=107498"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":79070,
        "rank":9,
        "Model":"TPS",
        "mlmodel":{

        },
        "method_short":"TPS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-27",
        "metrics":{
            "Top 1 Accuracy":"58.3",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":58.3,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1050620,
            "title":"Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition",
            "url":"\/paper\/spatiotemporal-self-attention-modeling-with",
            "published":"2022-07-27T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":97429,
        "rank":10,
        "Model":"MSMA (8+16frames)",
        "mlmodel":{

        },
        "method_short":"MSMA ",
        "method_details":"8+16frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-19",
        "metrics":{
            "Top 1 Accuracy":"57.9",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":57.9,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1162721,
            "title":"Multi-scale Motion-Aware Module for Video Action Recognition",
            "url":"\/paper\/multi-scale-motion-aware-module-for-video",
            "published":"2023-02-19T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":43444,
        "rank":11,
        "Model":"UniFormer-B (IN-1K + Kinetics600)",
        "mlmodel":{

        },
        "method_short":"UniFormer-B ",
        "method_details":"IN-1K + Kinetics600",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top 1 Accuracy":"57.6",
            "Top 5 Accuracy":"84.9",
            "Param.":"21.4",
            "GFLOPs":"41.8x3"
        },
        "raw_metrics":{
            "Top 1 Accuracy":57.6,
            "Top 5 Accuracy":84.9,
            "Param.":21.4,
            "GFLOPs":41.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":883979,
            "title":"UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning",
            "url":"\/paper\/uniformer-unified-transformer-for-efficient",
            "published":"2021-09-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":173,
                "name":"UniFormer",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":79069,
        "rank":12,
        "Model":"SIFA",
        "mlmodel":{

        },
        "method_short":"SIFA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-14",
        "metrics":{
            "Top 1 Accuracy":"57.3",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":57.3,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1026933,
            "title":"Stand-Alone Inter-Frame Attention in Video Models",
            "url":"\/paper\/stand-alone-inter-frame-attention-in-video-1",
            "published":"2022-06-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/stand-alone-inter-frame-attention-in-video-1\/review\/?hl=79069"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":47,
                "name":"swin-transformer",
                "color":"#f75c2f"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":37350,
        "rank":13,
        "Model":"EAN ResNet50 (single clip, center crop,8+16 ensemble, with sparse Transformer)",
        "mlmodel":{

        },
        "method_short":"EAN ResNet50 ",
        "method_details":"single clip, center crop,8+16 ensemble, with sparse Transformer",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-22",
        "metrics":{
            "Top 1 Accuracy":"57.2",
            "Top 5 Accuracy":"83.9",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":57.2,
            "Top 5 Accuracy":83.9,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":839989,
            "title":"EAN: Event Adaptive Network for Enhanced Action Recognition",
            "url":"\/paper\/ean-event-adaptive-network-for-enhanced",
            "published":"2021-07-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ean-event-adaptive-network-for-enhanced\/review\/?hl=37350"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":87527,
        "rank":14,
        "Model":"TCM (Ensemble)",
        "mlmodel":{

        },
        "method_short":"TCM ",
        "method_details":"Ensemble",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-24",
        "metrics":{
            "Top 1 Accuracy":"57.2",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":57.2,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":967245,
            "title":"Motion-driven Visual Tempo Learning for Video-based Action Recognition",
            "url":"\/paper\/slow-fast-visual-tempo-learning-for-video",
            "published":"2022-02-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/slow-fast-visual-tempo-learning-for-video\/review\/?hl=87527"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":33,
                "name":"ResNet-50",
                "color":"#c17b9b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":40701,
        "rank":15,
        "Model":"BQNEn (ImageNet + K400 pretrained)",
        "mlmodel":{

        },
        "method_short":"BQNEn ",
        "method_details":"ImageNet + K400 pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top 1 Accuracy":"57.1",
            "Top 5 Accuracy":"84.2",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":57.1,
            "Top 5 Accuracy":84.2,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758504,
            "title":"Busy-Quiet Video Disentangling for Video Classification",
            "url":"\/paper\/video-classification-with-finecoarse-networks",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-classification-with-finecoarse-networks\/review\/?hl=40701"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":54,
                "name":"ResNet-101",
                "color":"#cc1e1e"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":30757,
        "rank":16,
        "Model":"TDN ResNet101 (one clip, center crop, 8+16 ensemble, ImageNet pretrained, RGB only)",
        "mlmodel":{

        },
        "method_short":"TDN ResNet101 ",
        "method_details":"one clip, center crop, 8+16 ensemble, ImageNet pretrained, RGB only",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-18",
        "metrics":{
            "Top 1 Accuracy":"56.8",
            "Top 5 Accuracy":"84.1",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":56.8,
            "Top 5 Accuracy":84.1,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":729685,
            "title":"TDN: Temporal Difference Networks for Efficient Action Recognition",
            "url":"\/paper\/tdn-temporal-difference-networks-for",
            "published":"2020-12-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tdn-temporal-difference-networks-for\/review\/?hl=30757"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":38658,
        "rank":17,
        "Model":"SELFYNet-TSM-R50En (8+16 frames, ImageNet pretrained, 2 clips)",
        "mlmodel":{

        },
        "method_short":"SELFYNet-TSM-R50En ",
        "method_details":"8+16 frames, ImageNet pretrained, 2 clips",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-14",
        "metrics":{
            "Top 1 Accuracy":"56.6",
            "Top 5 Accuracy":"84.4",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":56.6,
            "Top 5 Accuracy":84.4,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":745413,
            "title":"Learning Self-Similarity in Space and Time as Generalized Motion for Video Action Recognition",
            "url":"\/paper\/learning-self-similarity-in-space-and-time-as-1",
            "published":"2021-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-self-similarity-in-space-and-time-as-1\/review\/?hl=38658"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":36594,
        "rank":18,
        "Model":"CT-Net Ensemble (R50, 8+12+16+24)",
        "mlmodel":{

        },
        "method_short":"CT-Net Ensemble ",
        "method_details":"R50, 8+12+16+24",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-03",
        "metrics":{
            "Top 1 Accuracy":"56.6",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":56.6,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":810992,
            "title":"CT-Net: Channel Tensorization Network for Video Classification",
            "url":"\/paper\/ct-net-channel-tensorization-network-for-1",
            "published":"2021-06-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ct-net-channel-tensorization-network-for-1\/review\/?hl=36594"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":87529,
        "rank":19,
        "Model":"MoDS (8+16frames)",
        "mlmodel":{

        },
        "method_short":"MoDS ",
        "method_details":"8+16frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-15",
        "metrics":{
            "Top 1 Accuracy":"56.6",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":56.6,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1128066,
            "title":"Action Recognition With Motion Diversification and Dynamic Selection",
            "url":"\/paper\/action-recognition-with-motion",
            "published":"2022-07-15T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":33,
                "name":"ResNet-50",
                "color":"#c17b9b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":79071,
        "rank":20,
        "Model":"MLP-3D",
        "mlmodel":{

        },
        "method_short":"MLP-3D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-13",
        "metrics":{
            "Top 1 Accuracy":"56.5",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":56.5,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1025799,
            "title":"MLP-3D: A MLP-like 3D Architecture with Grouped Time Mixing",
            "url":"\/paper\/mlp-3d-a-mlp-like-3d-architecture-with-1",
            "published":"2022-06-13T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/mlp-3d-a-mlp-like-3d-architecture-with-1\/review\/?hl=79071"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":44001,
        "rank":21,
        "Model":"RSANet-R50 (8+16 frames, ImageNet pretrained, 2 clips)",
        "mlmodel":{

        },
        "method_short":"RSANet-R50 ",
        "method_details":"8+16 frames, ImageNet pretrained, 2 clips",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-02",
        "metrics":{
            "Top 1 Accuracy":"56.1",
            "Top 5 Accuracy":"82.8",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":56.1,
            "Top 5 Accuracy":82.8,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":899314,
            "title":"Relational Self-Attention: What's Missing in Attention for Video Understanding",
            "url":"\/paper\/relational-self-attention-what-s-missing-in",
            "published":"2021-11-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/relational-self-attention-what-s-missing-in\/review\/?hl=44001"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":38656,
        "rank":22,
        "Model":"SELFYNet-TSM-R50En (8+16 frames, ImageNet pretrained, a single clip)",
        "mlmodel":{

        },
        "method_short":"SELFYNet-TSM-R50En ",
        "method_details":"8+16 frames, ImageNet pretrained, a single clip",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-14",
        "metrics":{
            "Top 1 Accuracy":"55.8",
            "Top 5 Accuracy":"83.9",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":55.8,
            "Top 5 Accuracy":83.9,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":745413,
            "title":"Learning Self-Similarity in Space and Time as Generalized Motion for Video Action Recognition",
            "url":"\/paper\/learning-self-similarity-in-space-and-time-as-1",
            "published":"2021-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-self-similarity-in-space-and-time-as-1\/review\/?hl=38656"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":44000,
        "rank":23,
        "Model":"RSANet-R50 (8+16 frames, ImageNet pretrained, a single clip)",
        "mlmodel":{

        },
        "method_short":"RSANet-R50 ",
        "method_details":"8+16 frames, ImageNet pretrained, a single clip",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-02",
        "metrics":{
            "Top 1 Accuracy":"55.5",
            "Top 5 Accuracy":"82.6",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":55.5,
            "Top 5 Accuracy":82.6,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":899314,
            "title":"Relational Self-Attention: What's Missing in Attention for Video Understanding",
            "url":"\/paper\/relational-self-attention-what-s-missing-in",
            "published":"2021-11-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/relational-self-attention-what-s-missing-in\/review\/?hl=44000"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":19044,
        "rank":24,
        "Model":"PAN ResNet101 (RGB only, no Flow)",
        "mlmodel":{

        },
        "method_short":"PAN ResNet101 ",
        "method_details":"RGB only, no Flow",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-08-08",
        "metrics":{
            "Top 1 Accuracy":"55.3",
            "Top 5 Accuracy":"82.8",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":55.3,
            "Top 5 Accuracy":82.8,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":212522,
            "title":"PAN: Towards Fast Action Recognition via Learning Persistence of Appearance",
            "url":"\/paper\/pan-towards-fast-action-recognition-via",
            "published":"2020-08-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pan-towards-fast-action-recognition-via\/review\/?hl=19044"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":9291,
        "rank":25,
        "Model":"GSM Ensemble InceptionV3 (ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"GSM Ensemble InceptionV3 ",
        "method_details":"ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-15",
        "metrics":{
            "Top 1 Accuracy":"55.16",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":55.16,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":175141,
            "title":"Gate-Shift Networks for Video Action Recognition",
            "url":"\/paper\/gate-shift-networks-for-video-action",
            "published":"2019-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/gate-shift-networks-for-video-action\/review\/?hl=9291"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":20735,
        "rank":26,
        "Model":"MSNet-R50En (ensemble)",
        "mlmodel":{

        },
        "method_short":"MSNet-R50En ",
        "method_details":"ensemble",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-20",
        "metrics":{
            "Top 1 Accuracy":"55.1",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":55.1,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":209431,
            "title":"MotionSqueeze: Neural Motion Feature Learning for Video Understanding",
            "url":"\/paper\/motionsqueeze-neural-motion-feature-learning",
            "published":"2020-07-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/motionsqueeze-neural-motion-feature-learning\/review\/?hl=20735"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":87533,
        "rank":27,
        "Model":"AE-Net (8+16frames)",
        "mlmodel":{

        },
        "method_short":"AE-Net ",
        "method_details":"8+16frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-21",
        "metrics":{
            "Top 1 Accuracy":"55.0",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":55.0,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1128068,
            "title":"AE-Net:Adjoint Enhancement Network for Efficient Action Recognition in Video Understanding",
            "url":"\/paper\/ae-net-adjoint-enhancement-network-for",
            "published":"2022-07-21T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":22145,
        "rank":28,
        "Model":"VoV3D-L (32frames, Kinetics pretrained, single)",
        "mlmodel":{

        },
        "method_short":"VoV3D-L ",
        "method_details":"32frames, Kinetics pretrained, single",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Top 1 Accuracy":"54.59",
            "Top 5 Accuracy":"82.30",
            "Param.":"5.8M",
            "GFLOPs":"20.9x6"
        },
        "raw_metrics":{
            "Top 1 Accuracy":54.59,
            "Top 5 Accuracy":82.3,
            "Param.":5800000.0,
            "GFLOPs":20.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":238450,
            "title":"Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification",
            "url":"\/paper\/diverse-temporal-aggregation-and-depthwise",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diverse-temporal-aggregation-and-depthwise\/review\/?hl=22145"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":20722,
        "rank":29,
        "Model":"MSNet-R50En (8+16 ensemble, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"MSNet-R50En ",
        "method_details":"8+16 ensemble, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-20",
        "metrics":{
            "Top 1 Accuracy":"54.4",
            "Top 5 Accuracy":"83.8",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":54.4,
            "Top 5 Accuracy":83.8,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":209431,
            "title":"MotionSqueeze: Neural Motion Feature Learning for Video Understanding",
            "url":"\/paper\/motionsqueeze-neural-motion-feature-learning",
            "published":"2020-07-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/motionsqueeze-neural-motion-feature-learning\/review\/?hl=20722"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":38654,
        "rank":30,
        "Model":"SELFYNet-TSM-R50 (16 frames, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"SELFYNet-TSM-R50 ",
        "method_details":"16 frames, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-14",
        "metrics":{
            "Top 1 Accuracy":"54.3",
            "Top 5 Accuracy":"82.9",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":54.3,
            "Top 5 Accuracy":82.9,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":745413,
            "title":"Learning Self-Similarity in Space and Time as Generalized Motion for Video Action Recognition",
            "url":"\/paper\/learning-self-similarity-in-space-and-time-as-1",
            "published":"2021-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-self-similarity-in-space-and-time-as-1\/review\/?hl=38654"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":21227,
        "rank":31,
        "Model":"RNL+TSM Ensemble(R50+R101, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"RNL+TSM Ensemble",
        "method_details":"R50+R101, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-17",
        "metrics":{
            "Top 1 Accuracy":"54.1",
            "Top 5 Accuracy":"82.2",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":54.1,
            "Top 5 Accuracy":82.2,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":209203,
            "title":"Region-based Non-local Operation for Video Classification",
            "url":"\/paper\/region-based-non-local-operation-for-video",
            "published":"2020-07-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/region-based-non-local-operation-for-video\/review\/?hl=21227"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":43999,
        "rank":32,
        "Model":"RSANet-R50 (16 frames, ImageNet pretrained, a single clip)",
        "mlmodel":{

        },
        "method_short":"RSANet-R50 ",
        "method_details":"16 frames, ImageNet pretrained, a single clip",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-02",
        "metrics":{
            "Top 1 Accuracy":"54.0",
            "Top 5 Accuracy":"81.1",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":54.0,
            "Top 5 Accuracy":81.1,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":899314,
            "title":"Relational Self-Attention: What's Missing in Attention for Video Understanding",
            "url":"\/paper\/relational-self-attention-what-s-missing-in",
            "published":"2021-11-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/relational-self-attention-what-s-missing-in\/review\/?hl=43999"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":34625,
        "rank":33,
        "Model":"MVFNet-R50EN",
        "mlmodel":{

        },
        "method_short":"MVFNet-R50EN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-13",
        "metrics":{
            "Top 1 Accuracy":"54.0",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":54.0,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":727533,
            "title":"MVFNet: Multi-View Fusion Network for Efficient Video Recognition",
            "url":"\/paper\/mvfnet-multi-view-fusion-network-for",
            "published":"2020-12-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mvfnet-multi-view-fusion-network-for\/review\/?hl=34625"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":87523,
        "rank":34,
        "Model":"STPG (8+16frames)",
        "mlmodel":{

        },
        "method_short":"STPG ",
        "method_details":"8+16frames",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-09",
        "metrics":{
            "Top 1 Accuracy":"53.5",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":53.5,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1128057,
            "title":"Spatial-Temporal Pyramid Graph Reasoning for Action Recognition",
            "url":"\/paper\/spatial-temporal-pyramid-graph-reasoning-for",
            "published":"2022-08-09T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":6679,
        "rank":35,
        "Model":"GB + DF + LB (ResNet152, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"GB + DF + LB ",
        "method_details":"ResNet152, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-20",
        "metrics":{
            "Top 1 Accuracy":"53.4",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":53.4,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":150843,
            "title":"Action recognition with spatial-temporal discriminative filter banks",
            "url":"\/paper\/190807625",
            "published":"2019-08-20T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/190807625\/review\/?hl=6679"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":27845,
        "rank":36,
        "Model":"ip-CSN-152 (IG-65M pretraining)",
        "mlmodel":{

        },
        "method_short":"ip-CSN-152 ",
        "method_details":"IG-65M pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-04",
        "metrics":{
            "Top 1 Accuracy":"53.3",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":53.3,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":110722,
            "title":"Video Classification with Channel-Separated Convolutional Networks",
            "url":"\/paper\/video-classification-with-channel-separated",
            "published":"2019-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-classification-with-channel-separated\/review\/?hl=27845"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":6671,
        "rank":37,
        "Model":"MARS+RGB+Flow (64 frames, Kinetics pretrained)",
        "mlmodel":{

        },
        "method_short":"MARS+RGB+Flow ",
        "method_details":"64 frames, Kinetics pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-01",
        "metrics":{
            "Top 1 Accuracy":"53.0",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":53.0,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":116002,
            "title":"MARS: Motion-Augmented RGB Stream for Action Recognition",
            "url":"\/paper\/mars-motion-augmented-rgb-stream-for-action",
            "published":"2019-06-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":21226,
        "rank":38,
        "Model":"RNL+TSM Ensemble(ResNet50, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"RNL+TSM Ensemble",
        "method_details":"ResNet50, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-17",
        "metrics":{
            "Top 1 Accuracy":"52.7",
            "Top 5 Accuracy":"81.5",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":52.7,
            "Top 5 Accuracy":81.5,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":209203,
            "title":"Region-based Non-local Operation for Video Classification",
            "url":"\/paper\/region-based-non-local-operation-for-video",
            "published":"2020-07-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/region-based-non-local-operation-for-video\/review\/?hl=21226"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":22146,
        "rank":39,
        "Model":"VoV3D-M (32frames, Kinetics pretrained, single)",
        "mlmodel":{

        },
        "method_short":"VoV3D-M ",
        "method_details":"32frames, Kinetics pretrained, single",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Top 1 Accuracy":"52.68",
            "Top 5 Accuracy":"80.43",
            "Param.":"3.3M",
            "GFLOPs":"11.5x6"
        },
        "raw_metrics":{
            "Top 1 Accuracy":52.68,
            "Top 5 Accuracy":80.43,
            "Param.":3300000.0,
            "GFLOPs":11.5
        },
        "uses_additional_data":true,
        "paper":{
            "id":238450,
            "title":"Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification",
            "url":"\/paper\/diverse-temporal-aggregation-and-depthwise",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diverse-temporal-aggregation-and-depthwise\/review\/?hl=22146"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":15908,
        "rank":40,
        "Model":"TSM+W3 (16 frames, ResNet50)",
        "mlmodel":{

        },
        "method_short":"TSM+W3 ",
        "method_details":"16 frames, ResNet50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-02",
        "metrics":{
            "Top 1 Accuracy":"52.6",
            "Top 5 Accuracy":"81.3",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":52.6,
            "Top 5 Accuracy":81.3,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":189470,
            "title":"Knowing What, Where and When to Look: Efficient Video Action Modeling with Attention",
            "url":"\/paper\/knowing-what-where-and-when-to-look-efficient",
            "published":"2020-04-02T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/knowing-what-where-and-when-to-look-efficient\/review\/?hl=15908"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":78913,
        "rank":41,
        "Model":"AK-Net",
        "mlmodel":{

        },
        "method_short":"AK-Net",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-17",
        "metrics":{
            "Top 1 Accuracy":"52.5",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":52.5,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":946180,
            "title":"Action Keypoint Network for Efficient Video Recognition",
            "url":"\/paper\/action-keypoint-network-for-efficient-video",
            "published":"2022-01-17T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/action-keypoint-network-for-efficient-video\/review\/?hl=78913"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":20725,
        "rank":42,
        "Model":"MSNet-R50 (16 frames, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"MSNet-R50 ",
        "method_details":"16 frames, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-20",
        "metrics":{
            "Top 1 Accuracy":"52.1",
            "Top 5 Accuracy":"82.3",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":52.1,
            "Top 5 Accuracy":82.3,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":209431,
            "title":"MotionSqueeze: Neural Motion Feature Learning for Video Understanding",
            "url":"\/paper\/motionsqueeze-neural-motion-feature-learning",
            "published":"2020-07-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/motionsqueeze-neural-motion-feature-learning\/review\/?hl=20725"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":27844,
        "rank":43,
        "Model":"ir-CSN-152 (IG-65M pretraining)",
        "mlmodel":{

        },
        "method_short":"ir-CSN-152 ",
        "method_details":"IG-65M pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-04",
        "metrics":{
            "Top 1 Accuracy":"52.1",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":52.1,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":110722,
            "title":"Video Classification with Channel-Separated Convolutional Networks",
            "url":"\/paper\/video-classification-with-channel-separated",
            "published":"2019-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-classification-with-channel-separated\/review\/?hl=27844"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":43998,
        "rank":44,
        "Model":"RSANet-R50 (8 frames, ImageNet pretrained, a single clip)",
        "mlmodel":{

        },
        "method_short":"RSANet-R50 ",
        "method_details":"8 frames, ImageNet pretrained, a single clip",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-02",
        "metrics":{
            "Top 1 Accuracy":"51.9",
            "Top 5 Accuracy":"79.6",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":51.9,
            "Top 5 Accuracy":79.6,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":899314,
            "title":"Relational Self-Attention: What's Missing in Attention for Video Understanding",
            "url":"\/paper\/relational-self-attention-what-s-missing-in",
            "published":"2021-11-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/relational-self-attention-what-s-missing-in\/review\/?hl=43998"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":9292,
        "rank":45,
        "Model":"GSM InceptionV3 (16 frames, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"GSM InceptionV3 ",
        "method_details":"16 frames, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-15",
        "metrics":{
            "Top 1 Accuracy":"51.68",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":51.68,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":175141,
            "title":"Gate-Shift Networks for Video Action Recognition",
            "url":"\/paper\/gate-shift-networks-for-video-action",
            "published":"2019-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/gate-shift-networks-for-video-action\/review\/?hl=9292"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":27843,
        "rank":46,
        "Model":"R(2+1)D-152 (IG-65M pretraining)",
        "mlmodel":{

        },
        "method_short":"R",
        "method_details":"2+1",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-04",
        "metrics":{
            "Top 1 Accuracy":"51.6",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":51.6,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":110722,
            "title":"Video Classification with Channel-Separated Convolutional Networks",
            "url":"\/paper\/video-classification-with-channel-separated",
            "published":"2019-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-classification-with-channel-separated\/review\/?hl=27843"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":20729,
        "rank":47,
        "Model":"MSNet-R50 (8 frames, ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"MSNet-R50 ",
        "method_details":"8 frames, ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-20",
        "metrics":{
            "Top 1 Accuracy":"50.9",
            "Top 5 Accuracy":"80.3",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":50.9,
            "Top 5 Accuracy":80.3,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":209431,
            "title":"MotionSqueeze: Neural Motion Feature Learning for Video Understanding",
            "url":"\/paper\/motionsqueeze-neural-motion-feature-learning",
            "published":"2020-07-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/motionsqueeze-neural-motion-feature-learning\/review\/?hl=20729"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":8488,
        "rank":48,
        "Model":"TSM (RGB + Flow)",
        "mlmodel":{

        },
        "method_short":"TSM ",
        "method_details":"RGB + Flow",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-11-20",
        "metrics":{
            "Top 1 Accuracy":"50.7",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":50.7,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":62848,
            "title":"TSM: Temporal Shift Module for Efficient Video Understanding",
            "url":"\/paper\/temporal-shift-module-for-efficient-video",
            "published":"2018-11-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporal-shift-module-for-efficient-video\/review\/?hl=8488"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":22147,
        "rank":49,
        "Model":"VoV3D-L (32frames, from scratch, single)",
        "mlmodel":{

        },
        "method_short":"VoV3D-L ",
        "method_details":"32frames, from scratch, single",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Top 1 Accuracy":"50.6",
            "Top 5 Accuracy":"78.7",
            "Param.":"5.8M",
            "GFLOPs":"20.9x6"
        },
        "raw_metrics":{
            "Top 1 Accuracy":50.6,
            "Top 5 Accuracy":78.7,
            "Param.":5800000.0,
            "GFLOPs":20.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":238450,
            "title":"Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification",
            "url":"\/paper\/diverse-temporal-aggregation-and-depthwise",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diverse-temporal-aggregation-and-depthwise\/review\/?hl=22147"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":6622,
        "rank":50,
        "Model":"ResNet50 I3D (Moments pretrained)",
        "mlmodel":{

        },
        "method_short":"ResNet50 I3D ",
        "method_details":"Moments pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-01-09",
        "metrics":{
            "Top 1 Accuracy":"50",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":50.0,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":11999,
            "title":"Moments in Time Dataset: one million videos for event understanding",
            "url":"\/paper\/moments-in-time-dataset-one-million-videos",
            "published":"2018-01-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moments-in-time-dataset-one-million-videos\/review\/?hl=6622"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":22148,
        "rank":51,
        "Model":"VoV3D-M (32frames, from scratch, single)",
        "mlmodel":{

        },
        "method_short":"VoV3D-M ",
        "method_details":"32frames, from scratch, single",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Top 1 Accuracy":"49.8",
            "Top 5 Accuracy":"78.0",
            "Param.":"3.3M",
            "GFLOPs":"11.5x6"
        },
        "raw_metrics":{
            "Top 1 Accuracy":49.8,
            "Top 5 Accuracy":78.0,
            "Param.":3300000.0,
            "GFLOPs":11.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":238450,
            "title":"Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification",
            "url":"\/paper\/diverse-temporal-aggregation-and-depthwise",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diverse-temporal-aggregation-and-depthwise\/review\/?hl=22148"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":18463,
        "rank":52,
        "Model":"TSMEn",
        "mlmodel":{

        },
        "method_short":"TSMEn",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-11-20",
        "metrics":{
            "Top 1 Accuracy":"49.7",
            "Top 5 Accuracy":"78.5",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":49.7,
            "Top 5 Accuracy":78.5,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":62848,
            "title":"TSM: Temporal Shift Module for Efficient Video Understanding",
            "url":"\/paper\/temporal-shift-module-for-efficient-video",
            "published":"2018-11-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporal-shift-module-for-efficient-video\/review\/?hl=18463"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":12959,
        "rank":53,
        "Model":"TRG (Inception-V3)",
        "mlmodel":{

        },
        "method_short":"TRG ",
        "method_details":"Inception-V3",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-27",
        "metrics":{
            "Top 1 Accuracy":"49.7",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":49.7,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":151332,
            "title":"Temporal Reasoning Graph for Activity Recognition",
            "url":"\/paper\/temporal-reasoning-graph-for-activity",
            "published":"2019-08-27T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/temporal-reasoning-graph-for-activity\/review\/?hl=12959"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":12960,
        "rank":54,
        "Model":"TRG (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"TRG ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-27",
        "metrics":{
            "Top 1 Accuracy":"49.5",
            "Top 5 Accuracy":"86.1",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":49.5,
            "Top 5 Accuracy":86.1,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":151332,
            "title":"Temporal Reasoning Graph for Activity Recognition",
            "url":"\/paper\/temporal-reasoning-graph-for-activity",
            "published":"2019-08-27T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/temporal-reasoning-graph-for-activity\/review\/?hl=12960"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":22149,
        "rank":55,
        "Model":"VoV3D-L (16frames, from scratch, single)",
        "mlmodel":{

        },
        "method_short":"VoV3D-L ",
        "method_details":"16frames, from scratch, single",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Top 1 Accuracy":"49.5",
            "Top 5 Accuracy":"78.0",
            "Param.":"5.8M",
            "GFLOPs":"9.3x6"
        },
        "raw_metrics":{
            "Top 1 Accuracy":49.5,
            "Top 5 Accuracy":78.0,
            "Param.":5800000.0,
            "GFLOPs":9.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":238450,
            "title":"Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification",
            "url":"\/paper\/diverse-temporal-aggregation-and-depthwise",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diverse-temporal-aggregation-and-depthwise\/review\/?hl=22149"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":6685,
        "rank":56,
        "Model":"ir-CSN-152",
        "mlmodel":{

        },
        "method_short":"ir-CSN-152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-04",
        "metrics":{
            "Top 1 Accuracy":"49.3",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":49.3,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":110722,
            "title":"Video Classification with Channel-Separated Convolutional Networks",
            "url":"\/paper\/video-classification-with-channel-separated",
            "published":"2019-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-classification-with-channel-separated\/review\/?hl=6685"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":8180,
        "rank":57,
        "Model":"RSTG (Kinetics pretrained)",
        "mlmodel":{

        },
        "method_short":"RSTG ",
        "method_details":"Kinetics pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-11",
        "metrics":{
            "Top 1 Accuracy":"49.2",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":49.2,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":111370,
            "title":"Recurrent Space-time Graph Neural Networks",
            "url":"\/paper\/recurrent-space-time-graphs-for-video",
            "published":"2019-04-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/recurrent-space-time-graphs-for-video\/review\/?hl=8180"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":6621,
        "rank":58,
        "Model":"ResNet50 I3D (Kinetics pretrained)",
        "mlmodel":{

        },
        "method_short":"ResNet50 I3D ",
        "method_details":"Kinetics pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-01-09",
        "metrics":{
            "Top 1 Accuracy":"48.6",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":48.6,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":11999,
            "title":"Moments in Time Dataset: one million videos for event understanding",
            "url":"\/paper\/moments-in-time-dataset-one-million-videos",
            "published":"2018-01-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moments-in-time-dataset-one-million-videos\/review\/?hl=6621"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":6684,
        "rank":59,
        "Model":"ir-CSN-101",
        "mlmodel":{

        },
        "method_short":"ir-CSN-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-04",
        "metrics":{
            "Top 1 Accuracy":"48.4",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":48.4,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":110722,
            "title":"Video Classification with Channel-Separated Convolutional Networks",
            "url":"\/paper\/video-classification-with-channel-separated",
            "published":"2019-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-classification-with-channel-separated\/review\/?hl=6684"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":6686,
        "rank":60,
        "Model":"S3D-G (ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"S3D-G ",
        "method_details":"ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-12-13",
        "metrics":{
            "Top 1 Accuracy":"48.2",
            "Top 5 Accuracy":"78.7",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":48.2,
            "Top 5 Accuracy":78.7,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":13041,
            "title":"Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
            "url":"\/paper\/rethinking-spatiotemporal-feature-learning",
            "published":"2017-12-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-spatiotemporal-feature-learning\/review\/?hl=6686"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":22150,
        "rank":61,
        "Model":"VoV3D-M (16frames, from scratch, single)",
        "mlmodel":{

        },
        "method_short":"VoV3D-M ",
        "method_details":"16frames, from scratch, single",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-01",
        "metrics":{
            "Top 1 Accuracy":"48.1",
            "Top 5 Accuracy":"76.9",
            "Param.":"3.3M",
            "GFLOPs":"5.7x6"
        },
        "raw_metrics":{
            "Top 1 Accuracy":48.1,
            "Top 5 Accuracy":76.9,
            "Param.":3300000.0,
            "GFLOPs":5.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":238450,
            "title":"Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification",
            "url":"\/paper\/diverse-temporal-aggregation-and-depthwise",
            "published":"2020-12-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diverse-temporal-aggregation-and-depthwise\/review\/?hl=22150"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":27768,
        "rank":62,
        "Model":"S3D",
        "mlmodel":{

        },
        "method_short":"S3D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-12-13",
        "metrics":{
            "Top 1 Accuracy":"47.3",
            "Top 5 Accuracy":"78.1",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":47.3,
            "Top 5 Accuracy":78.1,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":13041,
            "title":"Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
            "url":"\/paper\/rethinking-spatiotemporal-feature-learning",
            "published":"2017-12-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-spatiotemporal-feature-learning\/review\/?hl=27768"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":18462,
        "rank":63,
        "Model":"TSM",
        "mlmodel":{

        },
        "method_short":"TSM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-11-20",
        "metrics":{
            "Top 1 Accuracy":"47.2",
            "Top 5 Accuracy":"77.1",
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":47.2,
            "Top 5 Accuracy":77.1,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":62848,
            "title":"TSM: Temporal Shift Module for Efficient Video Understanding",
            "url":"\/paper\/temporal-shift-module-for-efficient-video",
            "published":"2018-11-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporal-shift-module-for-efficient-video\/review\/?hl=18462"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":6687,
        "rank":64,
        "Model":"ECO-Net (ImageNet pretrained)",
        "mlmodel":{

        },
        "method_short":"ECO-Net ",
        "method_details":"ImageNet pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-04-24",
        "metrics":{
            "Top 1 Accuracy":"46.4",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":46.4,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":4363,
            "title":"ECO: Efficient Convolutional Network for Online Video Understanding",
            "url":"\/paper\/eco-efficient-convolutional-network-for",
            "published":"2018-04-24T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":27849,
        "rank":65,
        "Model":"ECO-Net",
        "mlmodel":{

        },
        "method_short":"ECO-Net",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-04-24",
        "metrics":{
            "Top 1 Accuracy":"46.4",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":46.4,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":4363,
            "title":"ECO: Efficient Convolutional Network for Online Video Understanding",
            "url":"\/paper\/eco-efficient-convolutional-network-for",
            "published":"2018-04-24T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":27848,
        "rank":66,
        "Model":"NL I3D + GCN",
        "mlmodel":{

        },
        "method_short":"NL I3D + GCN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-06-05",
        "metrics":{
            "Top 1 Accuracy":"46.1",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":46.1,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1430,
            "title":"Videos as Space-Time Region Graphs",
            "url":"\/paper\/videos-as-space-time-region-graphs",
            "published":"2018-06-05T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/videos-as-space-time-region-graphs\/review\/?hl=27848"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":27,
                "name":"GCN",
                "color":"#aaa018"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":27847,
        "rank":67,
        "Model":"NL I3D",
        "mlmodel":{

        },
        "method_short":"NL I3D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-21",
        "metrics":{
            "Top 1 Accuracy":"44.4",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":44.4,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":6101,
            "title":"Non-local Neural Networks",
            "url":"\/paper\/non-local-neural-networks",
            "published":"2017-11-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/non-local-neural-networks\/review\/?hl=27847"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":6688,
        "rank":68,
        "Model":"Motion Feature Net",
        "mlmodel":{

        },
        "method_short":"Motion Feature Net",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-07-26",
        "metrics":{
            "Top 1 Accuracy":"43.9",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":43.9,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":53570,
            "title":"Motion Feature Network: Fixed Motion Filter for Action Recognition",
            "url":"\/paper\/motion-feature-network-fixed-motion-filter",
            "published":"2018-07-26T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/motion-feature-network-fixed-motion-filter\/review\/?hl=6688"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":6581,
        "rank":69,
        "Model":"2-Stream TRN",
        "mlmodel":{

        },
        "method_short":"2-Stream TRN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-22",
        "metrics":{
            "Top 1 Accuracy":"42.01",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":42.01,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":14233,
            "title":"Temporal Relational Reasoning in Videos",
            "url":"\/paper\/temporal-relational-reasoning-in-videos",
            "published":"2017-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporal-relational-reasoning-in-videos\/review\/?hl=6581"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":6636,
        "rank":70,
        "Model":"HF-TSN (ImageNet pretraining)",
        "mlmodel":{

        },
        "method_short":"HF-TSN ",
        "method_details":"ImageNet pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-29",
        "metrics":{
            "Top 1 Accuracy":"41.97",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":41.97,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":117681,
            "title":"Hierarchical Feature Aggregation Networks for Video Action Recognition",
            "url":"\/paper\/hierarchical-feature-aggregation-networks-for",
            "published":"2019-05-29T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/hierarchical-feature-aggregation-networks-for\/review\/?hl=6636"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":6672,
        "rank":71,
        "Model":"MARS+RGB+Flow (16 frames, Kinetics pretrained)",
        "mlmodel":{

        },
        "method_short":"MARS+RGB+Flow ",
        "method_details":"16 frames, Kinetics pretrained",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-01",
        "metrics":{
            "Top 1 Accuracy":"40.4",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":40.4,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":116002,
            "title":"MARS: Motion-Augmented RGB Stream for Action Recognition",
            "url":"\/paper\/mars-motion-augmented-rgb-stream-for-action",
            "published":"2019-06-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1236,
        "row_id":27846,
        "rank":72,
        "Model":"M-TRN",
        "mlmodel":{

        },
        "method_short":"M-TRN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-22",
        "metrics":{
            "Top 1 Accuracy":"34.4",
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":34.4,
            "Top 5 Accuracy":null,
            "Param.":null,
            "GFLOPs":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":14233,
            "title":"Temporal Relational Reasoning in Videos",
            "url":"\/paper\/temporal-relational-reasoning-in-videos",
            "published":"2017-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/temporal-relational-reasoning-in-videos\/review\/?hl=27846"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]